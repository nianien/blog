<!DOCTYPE html><html lang="zh-CN"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/129144073acbb2fa.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-42d55485b4428e47.js"/><script src="/_next/static/chunks/4bd1b696-8ec333fca6b38e39.js" async=""></script><script src="/_next/static/chunks/1684-a2aac8a674e5d38c.js" async=""></script><script src="/_next/static/chunks/main-app-2791dc86ed05573e.js" async=""></script><script src="/_next/static/chunks/6874-7791217feaf05c17.js" async=""></script><script src="/_next/static/chunks/app/layout-142e67ac4336647c.js" async=""></script><script src="/_next/static/chunks/968-d7155a2506e36f1d.js" async=""></script><script src="/_next/static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js" async=""></script><meta name="next-size-adjust" content=""/><title>Double Array Trie：高效字典树的压缩与检索实现 - Skyfalling Blog</title><meta name="description" content="深入解析Double Array Trie的DFA建模、BASE/CHECK双数组构建算法、动态更新策略及其在中文分词与信息检索中的工程应用"/><meta property="og:title" content="Double Array Trie：高效字典树的压缩与检索实现"/><meta property="og:description" content="深入解析Double Array Trie的DFA建模、BASE/CHECK双数组构建算法、动态更新策略及其在中文分词与信息检索中的工程应用"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2024-04-18"/><meta property="article:author" content="Skyfalling"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Double Array Trie：高效字典树的压缩与检索实现"/><meta name="twitter:description" content="深入解析Double Array Trie的DFA建模、BASE/CHECK双数组构建算法、动态更新策略及其在中文分词与信息检索中的工程应用"/><link rel="shortcut icon" href="/favicon.png"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><link rel="icon" href="/favicon.png"/><link rel="apple-touch-icon" href="/favicon.png"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_f367f3"><div hidden=""><!--$--><!--/$--></div><div class="min-h-screen flex flex-col"><header class="bg-[var(--background)]"><nav class="mx-auto flex max-w-7xl items-center justify-between p-6 lg:px-8" aria-label="Global"><div class="flex lg:flex-1"><a class="-m-1.5 p-1.5" href="/"><span class="sr-only">Skyfalling Blog</span><span class="text-2xl font-bold text-gray-900">Skyfalling</span></a></div><div class="flex lg:hidden"><button type="button" class="-m-2.5 inline-flex items-center justify-center rounded-md p-2.5 text-gray-700"><span class="sr-only">打开主菜单</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></button></div><div class="hidden lg:flex lg:gap-x-12"><a class="text-base font-semibold leading-6 transition-colors text-gray-900 hover:text-blue-600" href="/">首页</a><a class="text-base font-semibold leading-6 transition-colors text-blue-600 border-b-2 border-blue-600 pb-1" href="/blog/">博客</a><a class="text-base font-semibold leading-6 transition-colors text-gray-900 hover:text-blue-600" href="/about/">关于</a></div><div class="hidden lg:flex lg:flex-1 lg:justify-end"></div></nav></header><main class="flex-1"><article class="min-h-screen"><div class="mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8"><div class="rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12"><header class="mb-8"><nav class="flex items-center gap-1 text-sm mb-4"><a class="text-gray-500 hover:text-blue-600 transition-colors" href="/blog/page/1/">博客</a><span class="text-gray-300">/</span><a class="text-gray-500 hover:text-blue-600 transition-colors" href="/blog/category/engineering/page/1/">Engineering</a></nav><div class="flex items-center mb-6"><div class="inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal"><svg class="w-4 h-4 mr-2 text-gray-400" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z"></path></svg><time dateTime="2024-04-18">2024年04月18日</time></div></div><h1 class="text-4xl font-bold text-gray-900 mb-6 text-center">Double Array Trie：高效字典树的压缩与检索实现</h1><div class="flex flex-wrap gap-2 mb-6 justify-center"><a class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors" href="/blog/tag/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/page/1/">数据结构</a><a class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors" href="/blog/tag/Trie/page/1/">Trie</a><a class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors" href="/blog/tag/Double%20Array%20Trie/page/1/">Double Array Trie</a><a class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors" href="/blog/tag/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/page/1/">中文分词</a></div></header><div class="max-w-5xl mx-auto"><div class="prose prose-lg prose-gray mx-auto max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:text-blue-700 prose-strong:text-gray-900 prose-strong:font-semibold prose-li:text-gray-700 prose-hr:border-gray-300"><blockquote>
<p>Trie 树（前缀树、字典树）是字符串检索领域最经典的数据结构之一，广泛应用于中文分词、输入法联想、DNA 序列匹配等场景。然而，朴素 Trie 的空间开销极大——每个节点通常需要维护一个大小等于字符集的指针数组，在 Unicode 字符集下这一问题尤为突出。Double Array Trie（DAT）通过两个整型数组 BASE 和 CHECK 对 Trie 进行紧凑编码，在保留 O(m) 查询复杂度（m 为查询串长度）的前提下，将空间占用压缩到接近理论下限。本文从有限自动机的视角出发，系统剖析 DAT 的数据结构设计、构建算法、查询流程、动态更新策略及其工程应用。</p>
</blockquote>
<h2>朴素 Trie 的空间困境</h2>
<p>Trie 树的核心思想是利用字符串的公共前缀来减少存储冗余。给定一组字符串集合，Trie 将每个字符串拆解为字符序列，从根节点出发，沿着字符边依次向下延伸，共享相同前缀的路径。这种结构天然支持前缀匹配和最长前缀查找，查询时间复杂度仅与查询串长度 m 相关，与字典规模 n 无关。</p>
<p>然而朴素实现存在严重的空间浪费问题。最常见的做法是为每个节点分配一个大小为 <code>|Σ|</code> 的数组（<code>Σ</code> 为字符集），数组的第 i 个位置存储字符 i 对应的子节点指针。对于 ASCII 字符集，<code>|Σ| = 128</code>；对于中文场景，常用汉字约 6700 个，若考虑 Unicode 全集则更大。一棵包含数万个词条的中文词典 Trie，节点数可能达到数十万，每个节点都分配一个 6700 大小的指针数组，空间开销将高达 GB 级别，而这些数组中绝大多数位置是空的——一个典型的中文 Trie 节点平均只有 2~5 个子节点。</p>
<p>另一种做法是使用 HashMap 或链表来存储子节点映射，虽然能节省空间，但引入了哈希计算或链表遍历的额外开销，在高频查询场景下性能不够理想。</p>
<p>这就是 Double Array Trie 要解决的核心问题：如何在保持 Trie 的查询效率的同时，将空间占用降低到可接受的水平。</p>
<h2>从 Trie 到 DFA：有限自动机视角</h2>
<p>理解 Double Array Trie 的关键在于将 Trie 树重新建模为一个<strong>确定有限自动机</strong>（Deterministic Finite Automaton，DFA）。</p>
<h3>状态与变量的定义</h3>
<p>在 DFA 的视角下，Trie 的每一个节点对应一个<strong>状态（State）</strong>，每一条边对应一个<strong>输入变量（Input Symbol）</strong>。具体地：</p>
<ul>
<li><strong>状态集合 Q</strong>：Trie 中所有节点的集合。根节点为初始状态 <code>s₀</code>，所有标记为词尾的节点构成接受状态集 <code>F</code>。</li>
<li><strong>输入字母表 Σ</strong>：所有可能出现的字符构成的集合。每个字符被编码为一个正整数（变量编号），例如&quot;啊&quot;编为 1，&quot;阿&quot;编为 2，&quot;胶&quot;编为 5 等。</li>
<li><strong>状态转移函数 δ</strong>：<code>δ(s, c) = t</code> 表示在状态 s 下，接收输入字符 c 后转移到状态 t。</li>
</ul>
<h3>状态转移的核心语义</h3>
<p>以一个简单的中文词典为例，包含词条：<strong>啊、阿胶、阿根廷、阿拉伯、阿拉伯人、埃及</strong>。</p>
<p>从根节点出发，输入&quot;阿&quot;后进入&quot;阿&quot;节点，再输入&quot;胶&quot;进入&quot;阿胶&quot;节点。整个过程就是一系列状态转移的链式执行。DFA 的判定规则是：如果输入串消耗完毕后当前状态属于接受状态集 F，则该串被&quot;接受&quot;（即是词典中的合法词条）；否则拒绝。</p>
<p>这种建模方式的意义在于：DFA 是一个纯数学对象，其状态转移函数可以用任何满足语义约束的数据结构来实现。朴素 Trie 用指针数组实现 δ，而 Double Array Trie 用两个整型数组实现 δ——这正是 DAT 压缩空间的理论基础。</p>
<h2>Double Array Trie 的数据结构</h2>
<p>DAT 的核心数据结构极其简洁：仅由两个等长的整型数组 <code>base[]</code> 和 <code>check[]</code> 构成，整个 Trie 的拓扑结构和状态转移信息都被编码在这两个数组之中。</p>
<h3>BASE 数组与 CHECK 数组的语义</h3>
<p>设 Trie 中存在一条从状态 s 经字符 c 到达状态 t 的转移边（即 <code>δ(s, c) = t</code>），则 DAT 中的编码规则为：</p>
<pre><code>base[s] + c = t
check[t] = s
</code></pre>
<p>其中：</p>
<ul>
<li><code>s</code> 和 <code>t</code> 是状态在数组中的下标位置</li>
<li><code>c</code> 是字符的编号（正整数）</li>
<li><code>base[s]</code> 称为状态 s 的<strong>基地址（base value）</strong>，它决定了 s 的所有子状态在数组中的分布起点</li>
<li><code>check[t]</code> 记录状态 t 的<strong>父状态</strong>，用于验证状态转移的合法性</li>
</ul>
<p>这组公式的含义可以直观理解为：<strong>状态 s 的所有子节点在数组中以 <code>base[s]</code> 为偏移量排列，子节点 t 的位置由 <code>base[s] + c</code> 确定，同时 <code>check[t]</code> 反向指回父节点 s 以确保不冲突。</strong></p>
<h3>词结尾标记</h3>
<p>除了拓扑结构，DAT 还需要记录哪些状态是&quot;接受状态&quot;（即对应一个完整词条的结尾）。标记方式如下：</p>
<ul>
<li>如果状态 i 是某个词的结尾节点：<ul>
<li>若 <code>base[i] == 0</code>（该节点无子节点），则将 <code>base[i]</code> 设为 <code>-i</code></li>
<li>若 <code>base[i] != 0</code>（该节点有子节点，即某个词是另一个词的前缀），则将 <code>base[i]</code> 设为 <code>-base[i]</code>（取负值）</li>
</ul>
</li>
</ul>
<p>这样，通过检查 <code>base[i]</code> 是否为负数即可判定状态 i 是否为词的结尾。在查询时，使用 <code>|base[i]|</code> 取绝对值来恢复真正的基地址以继续转移。</p>
<h3>结构示意</h3>
<p>以词典 {啊, 阿胶, 阿根廷, 阿拉伯, 阿拉伯人, 埃及} 为例，假设变量编号如下：</p>
<table>
<thead>
<tr>
<th>字符</th>
<th>啊</th>
<th>阿</th>
<th>埃</th>
<th>根</th>
<th>胶</th>
<th>拉</th>
<th>及</th>
<th>廷</th>
<th>伯</th>
<th>人</th>
</tr>
</thead>
<tbody><tr>
<td>编号</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>6</td>
<td>7</td>
<td>8</td>
<td>9</td>
<td>10</td>
</tr>
</tbody></table>
<p>构建完成后的双数组（简化示意）大致如下：</p>
<table>
<thead>
<tr>
<th>下标 i</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
</tr>
</thead>
<tbody><tr>
<td>base[i]</td>
<td>0</td>
<td>-1</td>
<td>1</td>
<td>1</td>
<td>4</td>
<td>-5</td>
<td>-6</td>
<td>2</td>
<td>-8</td>
<td>-9</td>
<td>8</td>
<td>-11</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>check[i]</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>6</td>
<td>9</td>
<td>10</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody></table>
<p>在该表中，<code>base[1] = -1</code> 表示下标 1 对应的状态&quot;啊&quot;是一个完整词条且无子节点。<code>base[6] = -6</code> 表示&quot;阿胶&quot;既是完整词条，同时 <code>|base[6]| = 6</code> 还可以作为基地址继续向下转移（虽然在本例中&quot;阿胶&quot;无后续词）。</p>
<h2>构建算法</h2>
<p>DAT 的构建过程本质上是将 Trie 的树形结构&quot;铺平&quot;到一维数组中，核心挑战在于为每个父节点找到一个合适的 base 值，使得其所有子节点都能无冲突地映射到数组的空闲位置上。</p>
<h3>层次遍历构建过程</h3>
<p>构建过程采用层次遍历（BFS）策略，逐层处理 Trie 中的每一层节点。</p>
<p><strong>Step 1：处理第一层（根节点的子节点）。</strong></p>
<p>根节点的 base 值通常初始化为 0 或 1。将根的所有子节点按其变量编号直接放入数组。例如根有三个子节点&quot;啊&quot;(编号 1)、&quot;阿&quot;(编号 2)、&quot;埃&quot;(编号 3)，若 <code>base[root] = 0</code>，则它们分别放入位置 1、2、3，并设置对应的 check 值指向根节点。</p>
<p><strong>Step 2：为有子节点的状态分配 base 值。</strong></p>
<p>对于当前层中每一个拥有子节点的状态 s，需要找到一个正整数 k 作为 <code>base[s]</code> 的值，使得对于 s 的所有子节点变量编号 c₁, c₂, ..., cₙ，位置 <code>k + c₁, k + c₂, ..., k + cₙ</code> 在数组中全部为空（即 <code>check[k + cᵢ] == 0</code>）。</p>
<p>具体做法是从 k = 1 开始递增搜索，直到找到满足条件的 k 值。找到后：</p>
<ul>
<li>设置 <code>base[s] = k</code></li>
<li>对每个子节点变量 cᵢ，设置 <code>check[k + cᵢ] = s</code></li>
</ul>
<p><strong>Step 3：逐层重复。</strong></p>
<p>将当前层所有已分配位置的子节点加入下一层待处理队列，重复 Step 2 直至所有层处理完毕。</p>
<p><strong>Step 4：标记词结尾。</strong></p>
<p>遍历数组，对所有属于词尾的状态 i，按前述规则将 <code>base[i]</code> 取负。</p>
<h3>冲突解决：寻找可用偏移量</h3>
<p>Step 2 中寻找 k 的过程是构建算法的性能瓶颈。最朴素的做法是线性扫描，从 1 开始逐一尝试。这种方式的时间复杂度在最坏情况下可达 O(n * |Σ|)，其中 n 为数组长度。</p>
<p>实际工程实现中有若干优化手段：</p>
<ul>
<li><strong>空位链表</strong>：维护一个空闲位置的链表，跳过已占用的位置，减少无效扫描</li>
<li><strong>起始搜索位置优化</strong>：记录上一次成功分配的 k 值，下一次从该值附近开始搜索，利用局部性原理减少搜索范围</li>
<li><strong>字符编号排序</strong>：将子节点按编号从小到大排序，利用最小编号快速排除不可能的 k 值</li>
</ul>
<h3>完整构建示例</h3>
<p>以词典 {啊, 阿胶, 阿根廷, 阿拉伯, 阿拉伯人, 埃及} 为例，逐步演示构建过程。</p>
<p><strong>初始化</strong>：<code>base[root] = 0</code>，root 位于下标 0。</p>
<p><strong>第一层</strong>：根的子节点为&quot;啊&quot;(c=1)、&quot;阿&quot;(c=2)、&quot;埃&quot;(c=3)。</p>
<ul>
<li>k = 0（base[root] = 0）</li>
<li>&quot;啊&quot; → 位置 0+1 = 1，check[1] = 0</li>
<li>&quot;阿&quot; → 位置 0+2 = 2，check[2] = 0</li>
<li>&quot;埃&quot; → 位置 0+3 = 3，check[3] = 0</li>
</ul>
<p><strong>第二层</strong>：处理&quot;啊&quot;、&quot;阿&quot;、&quot;埃&quot;的子节点。</p>
<p>&quot;啊&quot;无子节点，标记为词尾：base[1] = -1。</p>
<p>&quot;阿&quot;有子节点&quot;根&quot;(c=4)、&quot;胶&quot;(c=5)、&quot;拉&quot;(c=6)。需要找 k 使得位置 k+4、k+5、k+6 均为空。k=1 时，位置 5、6、7 均空闲，满足条件。</p>
<ul>
<li>base[2] = 1</li>
<li>&quot;阿根&quot; → 位置 1+4 = 5，check[5] = 2（状态&quot;阿&quot;的下标）</li>
<li>&quot;阿胶&quot; → 位置 1+5 = 6，check[6] = 2</li>
<li>&quot;阿拉&quot; → 位置 1+6 = 7，check[7] = 2（注意此处是&quot;阿拉&quot;对应变量&quot;拉&quot;编号 6）</li>
</ul>
<p>&quot;埃&quot;有子节点&quot;及&quot;(c=7)。需要找 k 使得位置 k+7 为空。k=1 时，位置 8 空闲。</p>
<ul>
<li>base[3] = 1</li>
<li>&quot;埃及&quot; → 位置 1+7 = 8，check[8] = 3</li>
</ul>
<p><strong>第三层及后续</strong>：继续处理&quot;阿根&quot;、&quot;阿胶&quot;、&quot;阿拉&quot;、&quot;埃及&quot;等节点的子节点，按同样的规则分配 base 值和 check 值。</p>
<p>&quot;阿胶&quot;无子节点，标记词尾：base[6] = -6（若原本 base[6] 非零则取负）。</p>
<p>&quot;阿根&quot;有子节点&quot;廷&quot;(c=8)。找 k 使得 k+8 为空闲位置。</p>
<p>&quot;阿拉&quot;有子节点&quot;伯&quot;(c=9)。找 k 使得 k+9 为空闲位置。</p>
<p>以此类推，直到所有叶节点处理完毕，并对&quot;阿根廷&quot;&quot;阿拉伯&quot;&quot;阿拉伯人&quot;&quot;埃及&quot;等词尾状态做标记。</p>
<h2>查询算法</h2>
<p>DAT 的查询过程与 DFA 的运行语义完全一致：从初始状态出发，逐字符消耗输入串，通过状态转移函数判断路径是否合法。</p>
<h3>前缀匹配流程</h3>
<p>给定查询串 <code>w = c₁c₂...cₘ</code>，查询过程如下：</p>
<ol>
<li>初始化当前状态 <code>s = root</code>（下标 0）</li>
<li>对于每个字符 <code>cᵢ</code>（i 从 1 到 m）：<ul>
<li>计算目标位置 <code>t = |base[s]| + code(cᵢ)</code></li>
<li>检查 <code>check[t]</code> 是否等于 s</li>
<li>若相等，则转移成功，令 <code>s = t</code>，继续处理下一个字符</li>
<li>若不等，则转移失败，查询串不存在于词典中</li>
</ul>
</li>
<li>所有字符处理完毕后，检查 <code>base[s]</code> 是否为负数：<ul>
<li>若为负，则 s 是词尾状态，查询串是词典中的完整词条</li>
<li>若为正，则 s 不是词尾状态，查询串仅为某个词的前缀</li>
</ul>
</li>
</ol>
<h3>查询示例</h3>
<p>以查询&quot;阿胶及&quot;为例：</p>
<p><strong>第一步</strong>：当前状态 s = 0（根），输入字符&quot;阿&quot;，编号 2。</p>
<ul>
<li>计算 t = |base[0]| + 2 = 0 + 2 = 2</li>
<li>check[2] == 0（根的下标）？是 → 转移到状态 2</li>
</ul>
<p><strong>第二步</strong>：当前状态 s = 2，输入字符&quot;胶&quot;，编号 5。</p>
<ul>
<li>计算 t = |base[2]| + 5 = 1 + 5 = 6</li>
<li>check[6] == 2？是 → 转移到状态 6</li>
<li>此时 base[6] 为负数，说明&quot;阿胶&quot;是一个完整词条</li>
</ul>
<p><strong>第三步</strong>：当前状态 s = 6，输入字符&quot;及&quot;，编号 7。</p>
<ul>
<li>计算 t = |base[6]| + 7 = 6 + 7 = 13</li>
<li>check[13] == 6？否（check[13] 不等于 6）→ 转移失败</li>
<li>结论：&quot;阿胶及&quot;不是词典中的合法词条</li>
</ul>
<h3>查询失败的快速检测</h3>
<p>DAT 的查询具有&quot;fast-fail&quot;特性。在任意一步中，只要 <code>check[t] != s</code>，即可立即终止查询并返回&quot;不存在&quot;。这意味着：</p>
<ul>
<li>对于不存在的词条，查询通常在消耗少量字符后就能快速拒绝</li>
<li>查询时间复杂度严格为 O(m)，其中 m 为查询串长度</li>
<li>每一步仅涉及一次加法运算、一次数组访问和一次整数比较，Cache 友好且无分支预测负担</li>
</ul>
<p>这种效率是 HashMap 实现难以比拟的——HashMap 在最坏情况下可能退化为 O(m * n) 的逐一比对，而且哈希计算本身也有开销。</p>
<h2>动态更新</h2>
<p>DAT 的一个显著弱点在于动态更新的复杂性。与 HashMap 或链表 Trie 的 O(1) 插入不同，DAT 的插入操作可能触发级联的位置重分配。</p>
<h3>插入新词的冲突处理</h3>
<p>当向已构建好的 DAT 中插入一个新词时，可能出现以下情况：新词的某个前缀已存在于 DAT 中，但在需要分叉的节点处，新子节点的目标位置已被其他状态占用。</p>
<p>例如，假设词典中已有&quot;阿拉伯&quot;和&quot;阿拉伯人&quot;。现在要插入&quot;阿拉根&quot;。在处理到&quot;阿拉&quot;节点时，需要添加子节点&quot;根&quot;(c=4)。计算目标位置 <code>base[阿拉] + 4</code>，若该位置已被占用，则发生冲突。</p>
<h3>子树迁移策略</h3>
<p>冲突解决的基本思路是<strong>子树迁移</strong>：将冲突节点的整个子树迁移到一个新的基地址位置。具体步骤为：</p>
<ol>
<li><p><strong>确定需要迁移的节点</strong>：比较冲突双方（当前节点的子节点集合 vs. 占用位置的节点的子节点集合），选择子节点较少的一方进行迁移，以减少迁移开销。</p>
</li>
<li><p><strong>寻找新的 base 值</strong>：为待迁移的节点找到一个新的 k 值，使得其所有子节点（包括新增的）都能映射到空闲位置。</p>
</li>
<li><p><strong>执行迁移</strong>：</p>
<ul>
<li>将旧位置的子节点逐一复制到新位置</li>
<li>更新每个子节点的 check 值指向新的父节点位置</li>
<li>递归更新所有孙子节点的 check 值（因为它们的 check 指向的是父节点的旧位置）</li>
<li>清空旧位置</li>
</ul>
</li>
<li><p><strong>完成插入</strong>：冲突解除后，在正确位置插入新节点。</p>
</li>
</ol>
<h3>动态更新的性能开销</h3>
<p>子树迁移的时间复杂度取决于被迁移子树的规模。在最坏情况下，一次插入可能触发一个大型子树的完整迁移，导致 O(n) 的时间开销。但在实际应用中，以下策略可以降低平均开销：</p>
<ul>
<li><strong>静态构建优先</strong>：如果词典是已知的，优先采用离线批量构建，避免逐词插入带来的冲突</li>
<li><strong>预留空间</strong>：构建时适当增大数组长度，降低冲突概率</li>
<li><strong>增量更新缓冲</strong>：将增量更新暂存于辅助数据结构（如 HashMap），达到阈值后与主 DAT 合并重建</li>
</ul>
<p>在绝大多数工程场景中，DAT 被用作静态词典的存储结构，动态更新需求较少。因此，动态更新的高开销在实践中并不构成主要瓶颈。</p>
<h2>空间效率分析</h2>
<h3>与朴素 Trie 的空间对比</h3>
<p>空间效率是 DAT 最核心的优势。以下对比基于一个包含 30 万中文词条的词典：</p>
<table>
<thead>
<tr>
<th>结构</th>
<th>空间占用</th>
<th>查询复杂度</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>朴素 Trie（数组实现）</td>
<td>~10 GB</td>
<td>O(m)</td>
<td>每节点分配 6700 大小的指针数组</td>
</tr>
<tr>
<td>朴素 Trie（HashMap 实现）</td>
<td>~150 MB</td>
<td>O(m)（平均）</td>
<td>HashMap 有额外对象头、装载因子等开销</td>
</tr>
<tr>
<td>Double Array Trie</td>
<td>~8 MB</td>
<td>O(m)</td>
<td>仅两个 int 数组，无指针、无对象头</td>
</tr>
</tbody></table>
<p>DAT 的空间效率来源于两个方面：</p>
<ol>
<li><p><strong>共享寻址空间</strong>：不同父节点的子节点可以交错分布在同一段数组区域中，只要它们不发生冲突。这种&quot;紧凑排列&quot;使得数组的利用率远高于朴素 Trie 的稀疏数组。</p>
</li>
<li><p><strong>零指针开销</strong>：DAT 仅使用整数运算定位子节点，不需要存储任何指针或引用。在 64 位系统上，一个指针占 8 字节，而 DAT 中定位一个子节点仅需一个 int 加法。</p>
</li>
</ol>
<h3>与 HashMap 的权衡</h3>
<p>DAT 并非在所有场景下都优于 HashMap。两者的适用边界大致如下：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>Double Array Trie</th>
<th>HashMap</th>
</tr>
</thead>
<tbody><tr>
<td>空间</td>
<td>极为紧凑</td>
<td>有对象头、链表/红黑树额外开销</td>
</tr>
<tr>
<td>查询速度</td>
<td>O(m)，常数极小</td>
<td>O(m)（平均），哈希计算有固定开销</td>
</tr>
<tr>
<td>前缀查询</td>
<td>天然支持</td>
<td>不支持，需额外结构</td>
</tr>
<tr>
<td>动态更新</td>
<td>代价高</td>
<td>O(1) 均摊</td>
</tr>
<tr>
<td>构建成本</td>
<td>离线构建较慢</td>
<td>逐条插入即可</td>
</tr>
<tr>
<td>序列化/反序列化</td>
<td>极快（直接读写数组）</td>
<td>需要逐条重建</td>
</tr>
</tbody></table>
<p>结论是：<strong>当词典相对稳定、需要前缀匹配能力、对空间和查询延迟有严格要求时，DAT 是更优选择；当词典频繁变动或无前缀查询需求时，HashMap 更为实用。</strong></p>
<h2>工程应用</h2>
<h3>中文分词系统</h3>
<p>DAT 在中文分词领域有着广泛而深入的应用。中文分词的核心任务之一是快速判断一个字符序列是否为词典中的合法词条，以及找到所有可能的分词方案。DAT 的前缀匹配能力使其天然适合这一任务。</p>
<p><strong>HanLP</strong> 是目前主流的中文 NLP 工具包之一，其核心词典采用 DAT 作为底层存储结构。HanLP 在启动时将词典文件加载为 DAT，后续的分词、词性标注等操作均基于 DAT 进行高速检索。由于 DAT 可以直接序列化为字节数组写入文件，HanLP 的词典加载速度极快——30 万词条的词典加载通常在毫秒级完成。</p>
<p><strong>Jieba 分词</strong>（Java 版本）在其词典查询模块中同样使用了 DAT。Jieba 的前缀词典在 DAG（有向无环图）构建阶段需要高频执行前缀查询，DAT 的 O(m) 查询保证了这一阶段的性能。</p>
<h3>AC 自动机的底层存储</h3>
<p>AC 自动机（Aho-Corasick Automaton）是多模式字符串匹配的经典算法，被广泛应用于敏感词过滤、入侵检测等场景。AC 自动机的第一步就是构建一棵 Trie 树，然后在其上添加失败指针（failure link）。</p>
<p>在高性能实现中，AC 自动机底层的 Trie 通常替换为 DAT，以获得更优的空间效率和缓存命中率。例如，在一个包含数十万敏感词的过滤系统中，使用 DAT 替代朴素 Trie 可以将内存占用从数百 MB 降至数十 MB，同时查询性能因更好的 cache locality 而提升 20%~50%。</p>
<h3>输入法词库</h3>
<p>输入法引擎需要根据用户的按键序列实时检索候选词，这一过程对延迟和空间都有极严格的要求——用户每敲一个键，引擎需要在毫秒内返回候选列表，而词库规模通常在百万级。DAT 的常数级别查询延迟和极低的内存占用使其成为输入法词库的理想存储结构。多数主流中文输入法引擎（如 Google 日文输入法开源实现 Mozc）在其词典模块中采用了 DAT 或其变体。</p>
<h3>Darts-java 实现</h3>
<p><a href="https://github.com/komiya-atsushi/darts-java">Darts-java</a> 是 DAT 的一个经典 Java 实现，也是 HanLP 等工具的底层依赖之一。其核心代码结构清晰，值得参考：</p>
<pre><code class="language-java">public class DoubleArrayTrie {
    private int[] base;   // BASE 数组
    private int[] check;  // CHECK 数组

    /**
     * 精确匹配查询
     * @param key 查询字符串
     * @return 匹配结果（词条编号），-1 表示未找到
     */
    public int exactMatchSearch(String key) {
        int result = -1;
        int b = base[0];       // 从根节点出发
        int p;

        for (int i = 0; i &lt; key.length(); i++) {
            p = b + (int)(key.charAt(i)) + 1;
            if (b == check[p]) {
                b = base[p];
            } else {
                return result;  // 转移失败，快速返回
            }
        }

        p = b;                 // 检查是否为完整词条
        int n = base[p];
        if (b == check[p] &amp;&amp; n &lt; 0) {
            result = -n - 1;
        }
        return result;
    }
}
</code></pre>
<p>此外，Darts-java 还提供了 <code>commonPrefixSearch</code> 方法，用于查找查询串的所有前缀匹配结果，这是中文分词中构建词图（word lattice）的关键操作。</p>
<p>在实际工程中，DAT 通常与其他技术组合使用：与 AC 自动机结合实现多模式匹配，与维特比算法结合实现最优分词路径选择，与概率语言模型结合实现统计分词。DAT 扮演的角色始终是最底层的高效词典检索引擎——不起眼但不可或缺。</p>
</div></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><div class="mt-12 pt-8 border-t border-gray-200">加载导航中...</div><!--/$--><div class="mt-16 border-t border-gray-200 pt-8"><div class="mx-auto max-w-3xl"><h3 class="text-2xl font-bold text-gray-900 mb-8">评论</h3></div></div></div></div></article><!--$--><!--/$--></main><footer class="bg-[var(--background)]"><div class="mx-auto max-w-7xl px-6 py-12 lg:px-8"><p class="text-center text-xs leading-5 text-gray-400">© <!-- -->2026<!-- --> Skyfalling</p></div></footer></div><script src="/_next/static/chunks/webpack-42d55485b4428e47.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[10616,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"7177\",\"static/chunks/app/layout-142e67ac4336647c.js\"],\"default\"]\n3:I[87555,[],\"\"]\n4:I[31295,[],\"\"]\n6:I[59665,[],\"OutletBoundary\"]\n9:I[74911,[],\"AsyncMetadataOutlet\"]\nb:I[59665,[],\"ViewportBoundary\"]\nd:I[59665,[],\"MetadataBoundary\"]\nf:I[26614,[],\"\"]\n:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/129144073acbb2fa.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"6jwsMkq47CAcxpAFlh3iK\",\"p\":\"\",\"c\":[\"\",\"blog\",\"engineering\",\"algorithm\",\"Double%20Array%20Trie%EF%BC%9A%E9%AB%98%E6%95%88%E5%AD%97%E5%85%B8%E6%A0%91%E7%9A%84%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%A3%80%E7%B4%A2%E5%AE%9E%E7%8E%B0\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"engineering/algorithm/Double%20Array%20Trie%EF%BC%9A%E9%AB%98%E6%95%88%E5%AD%97%E5%85%B8%E6%A0%91%E7%9A%84%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%A3%80%E7%B4%A2%E5%AE%9E%E7%8E%B0\",\"c\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/129144073acbb2fa.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"zh-CN\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_f367f3\",\"children\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex flex-col\",\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"main\",null,{\"className\":\"flex-1\",\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"footer\",null,{\"className\":\"bg-[var(--background)]\",\"children\":[\"$\",\"div\",null,{\"className\":\"mx-auto max-w-7xl px-6 py-12 lg:px-8\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-center text-xs leading-5 text-gray-400\",\"children\":[\"© \",2026,\" Skyfalling\"]}]}]}]]}]}]}]]}],{\"children\":[\"blog\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"engineering/algorithm/Double%20Array%20Trie%EF%BC%9A%E9%AB%98%E6%95%88%E5%AD%97%E5%85%B8%E6%A0%91%E7%9A%84%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%A3%80%E7%B4%A2%E5%AE%9E%E7%8E%B0\",\"c\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",null,[\"$\",\"$L6\",null,{\"children\":[\"$L7\",\"$L8\",[\"$\",\"$L9\",null,{\"promise\":\"$@a\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"J4ED6MttoSBTq80gCxatNv\",{\"children\":[[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],[\"$\",\"$Ld\",null,{\"children\":\"$Le\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$f\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"10:\"$Sreact.suspense\"\n11:I[74911,[],\"AsyncMetadata\"]\n13:I[6874,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"\"]\n14:I[32923,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"default\"]\n16:I[40780,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"default\"]\n1a:I[85300,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"default\"]\ne:[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$10\",null,{\"fallback\":null,\"children\":[\"$\",\"$L11\",null,{\"promise\":\"$@12\"}]}]}]\n15:T5d30,"])</script><script>self.__next_f.push([1,"\u003cblockquote\u003e\n\u003cp\u003eTrie 树（前缀树、字典树）是字符串检索领域最经典的数据结构之一，广泛应用于中文分词、输入法联想、DNA 序列匹配等场景。然而，朴素 Trie 的空间开销极大——每个节点通常需要维护一个大小等于字符集的指针数组，在 Unicode 字符集下这一问题尤为突出。Double Array Trie（DAT）通过两个整型数组 BASE 和 CHECK 对 Trie 进行紧凑编码，在保留 O(m) 查询复杂度（m 为查询串长度）的前提下，将空间占用压缩到接近理论下限。本文从有限自动机的视角出发，系统剖析 DAT 的数据结构设计、构建算法、查询流程、动态更新策略及其工程应用。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e朴素 Trie 的空间困境\u003c/h2\u003e\n\u003cp\u003eTrie 树的核心思想是利用字符串的公共前缀来减少存储冗余。给定一组字符串集合，Trie 将每个字符串拆解为字符序列，从根节点出发，沿着字符边依次向下延伸，共享相同前缀的路径。这种结构天然支持前缀匹配和最长前缀查找，查询时间复杂度仅与查询串长度 m 相关，与字典规模 n 无关。\u003c/p\u003e\n\u003cp\u003e然而朴素实现存在严重的空间浪费问题。最常见的做法是为每个节点分配一个大小为 \u003ccode\u003e|Σ|\u003c/code\u003e 的数组（\u003ccode\u003eΣ\u003c/code\u003e 为字符集），数组的第 i 个位置存储字符 i 对应的子节点指针。对于 ASCII 字符集，\u003ccode\u003e|Σ| = 128\u003c/code\u003e；对于中文场景，常用汉字约 6700 个，若考虑 Unicode 全集则更大。一棵包含数万个词条的中文词典 Trie，节点数可能达到数十万，每个节点都分配一个 6700 大小的指针数组，空间开销将高达 GB 级别，而这些数组中绝大多数位置是空的——一个典型的中文 Trie 节点平均只有 2~5 个子节点。\u003c/p\u003e\n\u003cp\u003e另一种做法是使用 HashMap 或链表来存储子节点映射，虽然能节省空间，但引入了哈希计算或链表遍历的额外开销，在高频查询场景下性能不够理想。\u003c/p\u003e\n\u003cp\u003e这就是 Double Array Trie 要解决的核心问题：如何在保持 Trie 的查询效率的同时，将空间占用降低到可接受的水平。\u003c/p\u003e\n\u003ch2\u003e从 Trie 到 DFA：有限自动机视角\u003c/h2\u003e\n\u003cp\u003e理解 Double Array Trie 的关键在于将 Trie 树重新建模为一个\u003cstrong\u003e确定有限自动机\u003c/strong\u003e（Deterministic Finite Automaton，DFA）。\u003c/p\u003e\n\u003ch3\u003e状态与变量的定义\u003c/h3\u003e\n\u003cp\u003e在 DFA 的视角下，Trie 的每一个节点对应一个\u003cstrong\u003e状态（State）\u003c/strong\u003e，每一条边对应一个\u003cstrong\u003e输入变量（Input Symbol）\u003c/strong\u003e。具体地：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e状态集合 Q\u003c/strong\u003e：Trie 中所有节点的集合。根节点为初始状态 \u003ccode\u003es₀\u003c/code\u003e，所有标记为词尾的节点构成接受状态集 \u003ccode\u003eF\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e输入字母表 Σ\u003c/strong\u003e：所有可能出现的字符构成的集合。每个字符被编码为一个正整数（变量编号），例如\u0026quot;啊\u0026quot;编为 1，\u0026quot;阿\u0026quot;编为 2，\u0026quot;胶\u0026quot;编为 5 等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e状态转移函数 δ\u003c/strong\u003e：\u003ccode\u003eδ(s, c) = t\u003c/code\u003e 表示在状态 s 下，接收输入字符 c 后转移到状态 t。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e状态转移的核心语义\u003c/h3\u003e\n\u003cp\u003e以一个简单的中文词典为例，包含词条：\u003cstrong\u003e啊、阿胶、阿根廷、阿拉伯、阿拉伯人、埃及\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e从根节点出发，输入\u0026quot;阿\u0026quot;后进入\u0026quot;阿\u0026quot;节点，再输入\u0026quot;胶\u0026quot;进入\u0026quot;阿胶\u0026quot;节点。整个过程就是一系列状态转移的链式执行。DFA 的判定规则是：如果输入串消耗完毕后当前状态属于接受状态集 F，则该串被\u0026quot;接受\u0026quot;（即是词典中的合法词条）；否则拒绝。\u003c/p\u003e\n\u003cp\u003e这种建模方式的意义在于：DFA 是一个纯数学对象，其状态转移函数可以用任何满足语义约束的数据结构来实现。朴素 Trie 用指针数组实现 δ，而 Double Array Trie 用两个整型数组实现 δ——这正是 DAT 压缩空间的理论基础。\u003c/p\u003e\n\u003ch2\u003eDouble Array Trie 的数据结构\u003c/h2\u003e\n\u003cp\u003eDAT 的核心数据结构极其简洁：仅由两个等长的整型数组 \u003ccode\u003ebase[]\u003c/code\u003e 和 \u003ccode\u003echeck[]\u003c/code\u003e 构成，整个 Trie 的拓扑结构和状态转移信息都被编码在这两个数组之中。\u003c/p\u003e\n\u003ch3\u003eBASE 数组与 CHECK 数组的语义\u003c/h3\u003e\n\u003cp\u003e设 Trie 中存在一条从状态 s 经字符 c 到达状态 t 的转移边（即 \u003ccode\u003eδ(s, c) = t\u003c/code\u003e），则 DAT 中的编码规则为：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebase[s] + c = t\ncheck[t] = s\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e其中：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003es\u003c/code\u003e 和 \u003ccode\u003et\u003c/code\u003e 是状态在数组中的下标位置\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ec\u003c/code\u003e 是字符的编号（正整数）\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ebase[s]\u003c/code\u003e 称为状态 s 的\u003cstrong\u003e基地址（base value）\u003c/strong\u003e，它决定了 s 的所有子状态在数组中的分布起点\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003echeck[t]\u003c/code\u003e 记录状态 t 的\u003cstrong\u003e父状态\u003c/strong\u003e，用于验证状态转移的合法性\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这组公式的含义可以直观理解为：\u003cstrong\u003e状态 s 的所有子节点在数组中以 \u003ccode\u003ebase[s]\u003c/code\u003e 为偏移量排列，子节点 t 的位置由 \u003ccode\u003ebase[s] + c\u003c/code\u003e 确定，同时 \u003ccode\u003echeck[t]\u003c/code\u003e 反向指回父节点 s 以确保不冲突。\u003c/strong\u003e\u003c/p\u003e\n\u003ch3\u003e词结尾标记\u003c/h3\u003e\n\u003cp\u003e除了拓扑结构，DAT 还需要记录哪些状态是\u0026quot;接受状态\u0026quot;（即对应一个完整词条的结尾）。标记方式如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e如果状态 i 是某个词的结尾节点：\u003cul\u003e\n\u003cli\u003e若 \u003ccode\u003ebase[i] == 0\u003c/code\u003e（该节点无子节点），则将 \u003ccode\u003ebase[i]\u003c/code\u003e 设为 \u003ccode\u003e-i\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e若 \u003ccode\u003ebase[i] != 0\u003c/code\u003e（该节点有子节点，即某个词是另一个词的前缀），则将 \u003ccode\u003ebase[i]\u003c/code\u003e 设为 \u003ccode\u003e-base[i]\u003c/code\u003e（取负值）\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这样，通过检查 \u003ccode\u003ebase[i]\u003c/code\u003e 是否为负数即可判定状态 i 是否为词的结尾。在查询时，使用 \u003ccode\u003e|base[i]|\u003c/code\u003e 取绝对值来恢复真正的基地址以继续转移。\u003c/p\u003e\n\u003ch3\u003e结构示意\u003c/h3\u003e\n\u003cp\u003e以词典 {啊, 阿胶, 阿根廷, 阿拉伯, 阿拉伯人, 埃及} 为例，假设变量编号如下：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e字符\u003c/th\u003e\n\u003cth\u003e啊\u003c/th\u003e\n\u003cth\u003e阿\u003c/th\u003e\n\u003cth\u003e埃\u003c/th\u003e\n\u003cth\u003e根\u003c/th\u003e\n\u003cth\u003e胶\u003c/th\u003e\n\u003cth\u003e拉\u003c/th\u003e\n\u003cth\u003e及\u003c/th\u003e\n\u003cth\u003e廷\u003c/th\u003e\n\u003cth\u003e伯\u003c/th\u003e\n\u003cth\u003e人\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e编号\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e3\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e6\u003c/td\u003e\n\u003ctd\u003e7\u003c/td\u003e\n\u003ctd\u003e8\u003c/td\u003e\n\u003ctd\u003e9\u003c/td\u003e\n\u003ctd\u003e10\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e构建完成后的双数组（简化示意）大致如下：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e下标 i\u003c/th\u003e\n\u003cth\u003e0\u003c/th\u003e\n\u003cth\u003e1\u003c/th\u003e\n\u003cth\u003e2\u003c/th\u003e\n\u003cth\u003e3\u003c/th\u003e\n\u003cth\u003e4\u003c/th\u003e\n\u003cth\u003e5\u003c/th\u003e\n\u003cth\u003e6\u003c/th\u003e\n\u003cth\u003e7\u003c/th\u003e\n\u003cth\u003e8\u003c/th\u003e\n\u003cth\u003e9\u003c/th\u003e\n\u003cth\u003e10\u003c/th\u003e\n\u003cth\u003e11\u003c/th\u003e\n\u003cth\u003e12\u003c/th\u003e\n\u003cth\u003e13\u003c/th\u003e\n\u003cth\u003e14\u003c/th\u003e\n\u003cth\u003e15\u003c/th\u003e\n\u003cth\u003e16\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003ebase[i]\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e-1\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e-5\u003c/td\u003e\n\u003ctd\u003e-6\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e-8\u003c/td\u003e\n\u003ctd\u003e-9\u003c/td\u003e\n\u003ctd\u003e8\u003c/td\u003e\n\u003ctd\u003e-11\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003echeck[i]\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e3\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e6\u003c/td\u003e\n\u003ctd\u003e9\u003c/td\u003e\n\u003ctd\u003e10\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e在该表中，\u003ccode\u003ebase[1] = -1\u003c/code\u003e 表示下标 1 对应的状态\u0026quot;啊\u0026quot;是一个完整词条且无子节点。\u003ccode\u003ebase[6] = -6\u003c/code\u003e 表示\u0026quot;阿胶\u0026quot;既是完整词条，同时 \u003ccode\u003e|base[6]| = 6\u003c/code\u003e 还可以作为基地址继续向下转移（虽然在本例中\u0026quot;阿胶\u0026quot;无后续词）。\u003c/p\u003e\n\u003ch2\u003e构建算法\u003c/h2\u003e\n\u003cp\u003eDAT 的构建过程本质上是将 Trie 的树形结构\u0026quot;铺平\u0026quot;到一维数组中，核心挑战在于为每个父节点找到一个合适的 base 值，使得其所有子节点都能无冲突地映射到数组的空闲位置上。\u003c/p\u003e\n\u003ch3\u003e层次遍历构建过程\u003c/h3\u003e\n\u003cp\u003e构建过程采用层次遍历（BFS）策略，逐层处理 Trie 中的每一层节点。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 1：处理第一层（根节点的子节点）。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e根节点的 base 值通常初始化为 0 或 1。将根的所有子节点按其变量编号直接放入数组。例如根有三个子节点\u0026quot;啊\u0026quot;(编号 1)、\u0026quot;阿\u0026quot;(编号 2)、\u0026quot;埃\u0026quot;(编号 3)，若 \u003ccode\u003ebase[root] = 0\u003c/code\u003e，则它们分别放入位置 1、2、3，并设置对应的 check 值指向根节点。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 2：为有子节点的状态分配 base 值。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e对于当前层中每一个拥有子节点的状态 s，需要找到一个正整数 k 作为 \u003ccode\u003ebase[s]\u003c/code\u003e 的值，使得对于 s 的所有子节点变量编号 c₁, c₂, ..., cₙ，位置 \u003ccode\u003ek + c₁, k + c₂, ..., k + cₙ\u003c/code\u003e 在数组中全部为空（即 \u003ccode\u003echeck[k + cᵢ] == 0\u003c/code\u003e）。\u003c/p\u003e\n\u003cp\u003e具体做法是从 k = 1 开始递增搜索，直到找到满足条件的 k 值。找到后：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e设置 \u003ccode\u003ebase[s] = k\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e对每个子节点变量 cᵢ，设置 \u003ccode\u003echeck[k + cᵢ] = s\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eStep 3：逐层重复。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e将当前层所有已分配位置的子节点加入下一层待处理队列，重复 Step 2 直至所有层处理完毕。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 4：标记词结尾。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e遍历数组，对所有属于词尾的状态 i，按前述规则将 \u003ccode\u003ebase[i]\u003c/code\u003e 取负。\u003c/p\u003e\n\u003ch3\u003e冲突解决：寻找可用偏移量\u003c/h3\u003e\n\u003cp\u003eStep 2 中寻找 k 的过程是构建算法的性能瓶颈。最朴素的做法是线性扫描，从 1 开始逐一尝试。这种方式的时间复杂度在最坏情况下可达 O(n * |Σ|)，其中 n 为数组长度。\u003c/p\u003e\n\u003cp\u003e实际工程实现中有若干优化手段：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e空位链表\u003c/strong\u003e：维护一个空闲位置的链表，跳过已占用的位置，减少无效扫描\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e起始搜索位置优化\u003c/strong\u003e：记录上一次成功分配的 k 值，下一次从该值附近开始搜索，利用局部性原理减少搜索范围\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e字符编号排序\u003c/strong\u003e：将子节点按编号从小到大排序，利用最小编号快速排除不可能的 k 值\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e完整构建示例\u003c/h3\u003e\n\u003cp\u003e以词典 {啊, 阿胶, 阿根廷, 阿拉伯, 阿拉伯人, 埃及} 为例，逐步演示构建过程。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e初始化\u003c/strong\u003e：\u003ccode\u003ebase[root] = 0\u003c/code\u003e，root 位于下标 0。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e第一层\u003c/strong\u003e：根的子节点为\u0026quot;啊\u0026quot;(c=1)、\u0026quot;阿\u0026quot;(c=2)、\u0026quot;埃\u0026quot;(c=3)。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ek = 0（base[root] = 0）\u003c/li\u003e\n\u003cli\u003e\u0026quot;啊\u0026quot; → 位置 0+1 = 1，check[1] = 0\u003c/li\u003e\n\u003cli\u003e\u0026quot;阿\u0026quot; → 位置 0+2 = 2，check[2] = 0\u003c/li\u003e\n\u003cli\u003e\u0026quot;埃\u0026quot; → 位置 0+3 = 3，check[3] = 0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e第二层\u003c/strong\u003e：处理\u0026quot;啊\u0026quot;、\u0026quot;阿\u0026quot;、\u0026quot;埃\u0026quot;的子节点。\u003c/p\u003e\n\u003cp\u003e\u0026quot;啊\u0026quot;无子节点，标记为词尾：base[1] = -1。\u003c/p\u003e\n\u003cp\u003e\u0026quot;阿\u0026quot;有子节点\u0026quot;根\u0026quot;(c=4)、\u0026quot;胶\u0026quot;(c=5)、\u0026quot;拉\u0026quot;(c=6)。需要找 k 使得位置 k+4、k+5、k+6 均为空。k=1 时，位置 5、6、7 均空闲，满足条件。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebase[2] = 1\u003c/li\u003e\n\u003cli\u003e\u0026quot;阿根\u0026quot; → 位置 1+4 = 5，check[5] = 2（状态\u0026quot;阿\u0026quot;的下标）\u003c/li\u003e\n\u003cli\u003e\u0026quot;阿胶\u0026quot; → 位置 1+5 = 6，check[6] = 2\u003c/li\u003e\n\u003cli\u003e\u0026quot;阿拉\u0026quot; → 位置 1+6 = 7，check[7] = 2（注意此处是\u0026quot;阿拉\u0026quot;对应变量\u0026quot;拉\u0026quot;编号 6）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u0026quot;埃\u0026quot;有子节点\u0026quot;及\u0026quot;(c=7)。需要找 k 使得位置 k+7 为空。k=1 时，位置 8 空闲。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebase[3] = 1\u003c/li\u003e\n\u003cli\u003e\u0026quot;埃及\u0026quot; → 位置 1+7 = 8，check[8] = 3\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e第三层及后续\u003c/strong\u003e：继续处理\u0026quot;阿根\u0026quot;、\u0026quot;阿胶\u0026quot;、\u0026quot;阿拉\u0026quot;、\u0026quot;埃及\u0026quot;等节点的子节点，按同样的规则分配 base 值和 check 值。\u003c/p\u003e\n\u003cp\u003e\u0026quot;阿胶\u0026quot;无子节点，标记词尾：base[6] = -6（若原本 base[6] 非零则取负）。\u003c/p\u003e\n\u003cp\u003e\u0026quot;阿根\u0026quot;有子节点\u0026quot;廷\u0026quot;(c=8)。找 k 使得 k+8 为空闲位置。\u003c/p\u003e\n\u003cp\u003e\u0026quot;阿拉\u0026quot;有子节点\u0026quot;伯\u0026quot;(c=9)。找 k 使得 k+9 为空闲位置。\u003c/p\u003e\n\u003cp\u003e以此类推，直到所有叶节点处理完毕，并对\u0026quot;阿根廷\u0026quot;\u0026quot;阿拉伯\u0026quot;\u0026quot;阿拉伯人\u0026quot;\u0026quot;埃及\u0026quot;等词尾状态做标记。\u003c/p\u003e\n\u003ch2\u003e查询算法\u003c/h2\u003e\n\u003cp\u003eDAT 的查询过程与 DFA 的运行语义完全一致：从初始状态出发，逐字符消耗输入串，通过状态转移函数判断路径是否合法。\u003c/p\u003e\n\u003ch3\u003e前缀匹配流程\u003c/h3\u003e\n\u003cp\u003e给定查询串 \u003ccode\u003ew = c₁c₂...cₘ\u003c/code\u003e，查询过程如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e初始化当前状态 \u003ccode\u003es = root\u003c/code\u003e（下标 0）\u003c/li\u003e\n\u003cli\u003e对于每个字符 \u003ccode\u003ecᵢ\u003c/code\u003e（i 从 1 到 m）：\u003cul\u003e\n\u003cli\u003e计算目标位置 \u003ccode\u003et = |base[s]| + code(cᵢ)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e检查 \u003ccode\u003echeck[t]\u003c/code\u003e 是否等于 s\u003c/li\u003e\n\u003cli\u003e若相等，则转移成功，令 \u003ccode\u003es = t\u003c/code\u003e，继续处理下一个字符\u003c/li\u003e\n\u003cli\u003e若不等，则转移失败，查询串不存在于词典中\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e所有字符处理完毕后，检查 \u003ccode\u003ebase[s]\u003c/code\u003e 是否为负数：\u003cul\u003e\n\u003cli\u003e若为负，则 s 是词尾状态，查询串是词典中的完整词条\u003c/li\u003e\n\u003cli\u003e若为正，则 s 不是词尾状态，查询串仅为某个词的前缀\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e查询示例\u003c/h3\u003e\n\u003cp\u003e以查询\u0026quot;阿胶及\u0026quot;为例：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e第一步\u003c/strong\u003e：当前状态 s = 0（根），输入字符\u0026quot;阿\u0026quot;，编号 2。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e计算 t = |base[0]| + 2 = 0 + 2 = 2\u003c/li\u003e\n\u003cli\u003echeck[2] == 0（根的下标）？是 → 转移到状态 2\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e第二步\u003c/strong\u003e：当前状态 s = 2，输入字符\u0026quot;胶\u0026quot;，编号 5。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e计算 t = |base[2]| + 5 = 1 + 5 = 6\u003c/li\u003e\n\u003cli\u003echeck[6] == 2？是 → 转移到状态 6\u003c/li\u003e\n\u003cli\u003e此时 base[6] 为负数，说明\u0026quot;阿胶\u0026quot;是一个完整词条\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e第三步\u003c/strong\u003e：当前状态 s = 6，输入字符\u0026quot;及\u0026quot;，编号 7。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e计算 t = |base[6]| + 7 = 6 + 7 = 13\u003c/li\u003e\n\u003cli\u003echeck[13] == 6？否（check[13] 不等于 6）→ 转移失败\u003c/li\u003e\n\u003cli\u003e结论：\u0026quot;阿胶及\u0026quot;不是词典中的合法词条\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e查询失败的快速检测\u003c/h3\u003e\n\u003cp\u003eDAT 的查询具有\u0026quot;fast-fail\u0026quot;特性。在任意一步中，只要 \u003ccode\u003echeck[t] != s\u003c/code\u003e，即可立即终止查询并返回\u0026quot;不存在\u0026quot;。这意味着：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e对于不存在的词条，查询通常在消耗少量字符后就能快速拒绝\u003c/li\u003e\n\u003cli\u003e查询时间复杂度严格为 O(m)，其中 m 为查询串长度\u003c/li\u003e\n\u003cli\u003e每一步仅涉及一次加法运算、一次数组访问和一次整数比较，Cache 友好且无分支预测负担\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这种效率是 HashMap 实现难以比拟的——HashMap 在最坏情况下可能退化为 O(m * n) 的逐一比对，而且哈希计算本身也有开销。\u003c/p\u003e\n\u003ch2\u003e动态更新\u003c/h2\u003e\n\u003cp\u003eDAT 的一个显著弱点在于动态更新的复杂性。与 HashMap 或链表 Trie 的 O(1) 插入不同，DAT 的插入操作可能触发级联的位置重分配。\u003c/p\u003e\n\u003ch3\u003e插入新词的冲突处理\u003c/h3\u003e\n\u003cp\u003e当向已构建好的 DAT 中插入一个新词时，可能出现以下情况：新词的某个前缀已存在于 DAT 中，但在需要分叉的节点处，新子节点的目标位置已被其他状态占用。\u003c/p\u003e\n\u003cp\u003e例如，假设词典中已有\u0026quot;阿拉伯\u0026quot;和\u0026quot;阿拉伯人\u0026quot;。现在要插入\u0026quot;阿拉根\u0026quot;。在处理到\u0026quot;阿拉\u0026quot;节点时，需要添加子节点\u0026quot;根\u0026quot;(c=4)。计算目标位置 \u003ccode\u003ebase[阿拉] + 4\u003c/code\u003e，若该位置已被占用，则发生冲突。\u003c/p\u003e\n\u003ch3\u003e子树迁移策略\u003c/h3\u003e\n\u003cp\u003e冲突解决的基本思路是\u003cstrong\u003e子树迁移\u003c/strong\u003e：将冲突节点的整个子树迁移到一个新的基地址位置。具体步骤为：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e确定需要迁移的节点\u003c/strong\u003e：比较冲突双方（当前节点的子节点集合 vs. 占用位置的节点的子节点集合），选择子节点较少的一方进行迁移，以减少迁移开销。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e寻找新的 base 值\u003c/strong\u003e：为待迁移的节点找到一个新的 k 值，使得其所有子节点（包括新增的）都能映射到空闲位置。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e执行迁移\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将旧位置的子节点逐一复制到新位置\u003c/li\u003e\n\u003cli\u003e更新每个子节点的 check 值指向新的父节点位置\u003c/li\u003e\n\u003cli\u003e递归更新所有孙子节点的 check 值（因为它们的 check 指向的是父节点的旧位置）\u003c/li\u003e\n\u003cli\u003e清空旧位置\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e完成插入\u003c/strong\u003e：冲突解除后，在正确位置插入新节点。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e动态更新的性能开销\u003c/h3\u003e\n\u003cp\u003e子树迁移的时间复杂度取决于被迁移子树的规模。在最坏情况下，一次插入可能触发一个大型子树的完整迁移，导致 O(n) 的时间开销。但在实际应用中，以下策略可以降低平均开销：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e静态构建优先\u003c/strong\u003e：如果词典是已知的，优先采用离线批量构建，避免逐词插入带来的冲突\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e预留空间\u003c/strong\u003e：构建时适当增大数组长度，降低冲突概率\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e增量更新缓冲\u003c/strong\u003e：将增量更新暂存于辅助数据结构（如 HashMap），达到阈值后与主 DAT 合并重建\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在绝大多数工程场景中，DAT 被用作静态词典的存储结构，动态更新需求较少。因此，动态更新的高开销在实践中并不构成主要瓶颈。\u003c/p\u003e\n\u003ch2\u003e空间效率分析\u003c/h2\u003e\n\u003ch3\u003e与朴素 Trie 的空间对比\u003c/h3\u003e\n\u003cp\u003e空间效率是 DAT 最核心的优势。以下对比基于一个包含 30 万中文词条的词典：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e结构\u003c/th\u003e\n\u003cth\u003e空间占用\u003c/th\u003e\n\u003cth\u003e查询复杂度\u003c/th\u003e\n\u003cth\u003e说明\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e朴素 Trie（数组实现）\u003c/td\u003e\n\u003ctd\u003e~10 GB\u003c/td\u003e\n\u003ctd\u003eO(m)\u003c/td\u003e\n\u003ctd\u003e每节点分配 6700 大小的指针数组\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e朴素 Trie（HashMap 实现）\u003c/td\u003e\n\u003ctd\u003e~150 MB\u003c/td\u003e\n\u003ctd\u003eO(m)（平均）\u003c/td\u003e\n\u003ctd\u003eHashMap 有额外对象头、装载因子等开销\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eDouble Array Trie\u003c/td\u003e\n\u003ctd\u003e~8 MB\u003c/td\u003e\n\u003ctd\u003eO(m)\u003c/td\u003e\n\u003ctd\u003e仅两个 int 数组，无指针、无对象头\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eDAT 的空间效率来源于两个方面：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e共享寻址空间\u003c/strong\u003e：不同父节点的子节点可以交错分布在同一段数组区域中，只要它们不发生冲突。这种\u0026quot;紧凑排列\u0026quot;使得数组的利用率远高于朴素 Trie 的稀疏数组。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e零指针开销\u003c/strong\u003e：DAT 仅使用整数运算定位子节点，不需要存储任何指针或引用。在 64 位系统上，一个指针占 8 字节，而 DAT 中定位一个子节点仅需一个 int 加法。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e与 HashMap 的权衡\u003c/h3\u003e\n\u003cp\u003eDAT 并非在所有场景下都优于 HashMap。两者的适用边界大致如下：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e维度\u003c/th\u003e\n\u003cth\u003eDouble Array Trie\u003c/th\u003e\n\u003cth\u003eHashMap\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e空间\u003c/td\u003e\n\u003ctd\u003e极为紧凑\u003c/td\u003e\n\u003ctd\u003e有对象头、链表/红黑树额外开销\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e查询速度\u003c/td\u003e\n\u003ctd\u003eO(m)，常数极小\u003c/td\u003e\n\u003ctd\u003eO(m)（平均），哈希计算有固定开销\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e前缀查询\u003c/td\u003e\n\u003ctd\u003e天然支持\u003c/td\u003e\n\u003ctd\u003e不支持，需额外结构\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e动态更新\u003c/td\u003e\n\u003ctd\u003e代价高\u003c/td\u003e\n\u003ctd\u003eO(1) 均摊\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e构建成本\u003c/td\u003e\n\u003ctd\u003e离线构建较慢\u003c/td\u003e\n\u003ctd\u003e逐条插入即可\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e序列化/反序列化\u003c/td\u003e\n\u003ctd\u003e极快（直接读写数组）\u003c/td\u003e\n\u003ctd\u003e需要逐条重建\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e结论是：\u003cstrong\u003e当词典相对稳定、需要前缀匹配能力、对空间和查询延迟有严格要求时，DAT 是更优选择；当词典频繁变动或无前缀查询需求时，HashMap 更为实用。\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003e工程应用\u003c/h2\u003e\n\u003ch3\u003e中文分词系统\u003c/h3\u003e\n\u003cp\u003eDAT 在中文分词领域有着广泛而深入的应用。中文分词的核心任务之一是快速判断一个字符序列是否为词典中的合法词条，以及找到所有可能的分词方案。DAT 的前缀匹配能力使其天然适合这一任务。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHanLP\u003c/strong\u003e 是目前主流的中文 NLP 工具包之一，其核心词典采用 DAT 作为底层存储结构。HanLP 在启动时将词典文件加载为 DAT，后续的分词、词性标注等操作均基于 DAT 进行高速检索。由于 DAT 可以直接序列化为字节数组写入文件，HanLP 的词典加载速度极快——30 万词条的词典加载通常在毫秒级完成。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eJieba 分词\u003c/strong\u003e（Java 版本）在其词典查询模块中同样使用了 DAT。Jieba 的前缀词典在 DAG（有向无环图）构建阶段需要高频执行前缀查询，DAT 的 O(m) 查询保证了这一阶段的性能。\u003c/p\u003e\n\u003ch3\u003eAC 自动机的底层存储\u003c/h3\u003e\n\u003cp\u003eAC 自动机（Aho-Corasick Automaton）是多模式字符串匹配的经典算法，被广泛应用于敏感词过滤、入侵检测等场景。AC 自动机的第一步就是构建一棵 Trie 树，然后在其上添加失败指针（failure link）。\u003c/p\u003e\n\u003cp\u003e在高性能实现中，AC 自动机底层的 Trie 通常替换为 DAT，以获得更优的空间效率和缓存命中率。例如，在一个包含数十万敏感词的过滤系统中，使用 DAT 替代朴素 Trie 可以将内存占用从数百 MB 降至数十 MB，同时查询性能因更好的 cache locality 而提升 20%~50%。\u003c/p\u003e\n\u003ch3\u003e输入法词库\u003c/h3\u003e\n\u003cp\u003e输入法引擎需要根据用户的按键序列实时检索候选词，这一过程对延迟和空间都有极严格的要求——用户每敲一个键，引擎需要在毫秒内返回候选列表，而词库规模通常在百万级。DAT 的常数级别查询延迟和极低的内存占用使其成为输入法词库的理想存储结构。多数主流中文输入法引擎（如 Google 日文输入法开源实现 Mozc）在其词典模块中采用了 DAT 或其变体。\u003c/p\u003e\n\u003ch3\u003eDarts-java 实现\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/komiya-atsushi/darts-java\"\u003eDarts-java\u003c/a\u003e 是 DAT 的一个经典 Java 实现，也是 HanLP 等工具的底层依赖之一。其核心代码结构清晰，值得参考：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic class DoubleArrayTrie {\n    private int[] base;   // BASE 数组\n    private int[] check;  // CHECK 数组\n\n    /**\n     * 精确匹配查询\n     * @param key 查询字符串\n     * @return 匹配结果（词条编号），-1 表示未找到\n     */\n    public int exactMatchSearch(String key) {\n        int result = -1;\n        int b = base[0];       // 从根节点出发\n        int p;\n\n        for (int i = 0; i \u0026lt; key.length(); i++) {\n            p = b + (int)(key.charAt(i)) + 1;\n            if (b == check[p]) {\n                b = base[p];\n            } else {\n                return result;  // 转移失败，快速返回\n            }\n        }\n\n        p = b;                 // 检查是否为完整词条\n        int n = base[p];\n        if (b == check[p] \u0026amp;\u0026amp; n \u0026lt; 0) {\n            result = -n - 1;\n        }\n        return result;\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e此外，Darts-java 还提供了 \u003ccode\u003ecommonPrefixSearch\u003c/code\u003e 方法，用于查找查询串的所有前缀匹配结果，这是中文分词中构建词图（word lattice）的关键操作。\u003c/p\u003e\n\u003cp\u003e在实际工程中，DAT 通常与其他技术组合使用：与 AC 自动机结合实现多模式匹配，与维特比算法结合实现最优分词路径选择，与概率语言模型结合实现统计分词。DAT 扮演的角色始终是最底层的高效词典检索引擎——不起眼但不可或缺。\u003c/p\u003e\n"])</script><script>self.__next_f.push([1,"17:T4fc0,"])</script><script>self.__next_f.push([1,"\u003cp\u003e你有没有遇到过因为没有打印SQL导致问题排查困难？如果你使用了成熟ORM框架，那么很容易支撑SQL的拦截和监控，例如Mybatis的Interceptor或JOOQ的Listener都支持SQL执行过程的跟踪监控，但是，如果你的ORM框架不支持SQL监控，那么很不幸，你就只能在代码中手动打印日志了。然而，为了防SQL注入，应用中的SQL语句都是参数化的，直接打印的话，SQL语句未绑定参数，ORM框架一般都提供了SQL参数绑定的功能，原生的JDBC这样就失去了一定的监控价值。\u003c/p\u003e\n\u003cp\u003e另外，在TOB的业务中，有些场景SQL参数超长，如大IN查询，SQL语句会长达到几万甚至十几万，此时，我们又需要对SQL语句进行缩略打印。注意，这里的SQL缩略打印不是简单的对SQL语句进行截断，而是对SQL语句中的参数列表进行截断，例如下面的SQL\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sql\"\u003eselect * from user \nwhere id in (1001,1001, 1002, 1003, 1004, 1005, 1006, 1007) \nand name in(sql\nselect name from whitelist \nwhere name in(\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;d\u0026#39;,\u0026#39;e\u0026#39;,\u0026#39;f\u0026#39;,\u0026#39;g\u0026#39;,\u0026#39;h\u0026#39;,\u0026#39;i\u0026#39;,\u0026#39;j\u0026#39;,\u0026#39;k\u0026#39;,\u0026#39;l\u0026#39;,\u0026#39;m\u0026#39;)\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e缩略下印如下：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sql\"\u003eselect * from user \nwhere id in (1001,1001, 1002, 1003, 1004,...) \nand name in(\nselect name from whitelist \nwhere name in(\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;d\u0026#39;,\u0026#39;e\u0026#39;,...)\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e既然SQL 监控很重要，那么对于应用层的SQL监控都有哪些手段呢？一个SQL请求的执行链路，一般从DAO层开始：DAO -\u0026gt; ORM -\u0026gt; DataSource  -\u0026gt; Connection -\u0026gt; Driver -\u0026gt; DB，那么在这个链路上有哪些环节可以切入监控呢？ DAO层是数据访问层的入口，而我们的目标是应用层监控，因此，能够实现SQL监控的环节只有：ORM -\u0026gt; DataSource  -\u0026gt; Connection -\u0026gt; Driver，而要实现通用的非侵入式监控，则应该独立于ORM，因此我们可以从\u003cstrong\u003eDataSource  -\u0026gt; Connection -\u0026gt; Driver\u003c/strong\u003e三个环节进行入手：\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003e一、SQL Profile监控\u003c/strong\u003e\u003c/h3\u003e\n\u003ch4\u003e\u003cstrong\u003e1、驱动层监控\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003e如果Driver层支持日志监控，则最方便，例如MySQL，可以在jdbc url中添加logger：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-properties\"\u003ejdbc:mysql://localhost:3306/test?useUnicode=true\u0026amp;characterEncoding=utf8\u0026amp;useSSL=false\u0026amp;serverTimezone=UTC\u0026amp;logger=Slf4JLogger\u0026amp;profileSQL=true\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e基于Driver监控的问题在于：一方面强依赖于DB，和ORM层面临一样的问题，不具有通用性上述的问题，且需要厂商的支持，例如Oracle Driver就不支持日志监控；另一方面SQL格式固定，无法进行定制化输出。\u003c/p\u003e\n\u003ch4\u003e\u003cstrong\u003e2、连接层监控\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003e如果厂商驱动不支持SQL日志，可以Driver进行代理实现SQL监控功能，常用的开源组件如\u003ca href=\"https://p6spy.readthedocs.io/en/latest/\"\u003eP6Spy\u003c/a\u003e、\u003ca href=\"https://github.com/arthurblake/log4jdbc\"\u003elog4jdbc\u003c/a\u003e 等，其原理都是代理了厂商的驱动，因此只需要修改jdbc url：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003epyspy\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"language-properties\"\u003ejdbc:p6spy:mysql://localhost:3306/test?useUnicode=true\u0026amp;characterEncoding=utf8\u0026amp;useSSL=false\u0026amp;serverTimezone=UTC\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003elog4jdbc\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"language-properties\"\u003ejdbc:log4jdbc:mysql://localhost:3306/test?useUnicode=true\u0026amp;characterEncoding=utf8\u0026amp;useSSL=false\u0026amp;serverTimezone=UTC\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\u003cstrong\u003e3、数据源层监控\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003e可以通过对DataSource进行代理实现SQL监控\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eP6Spy：\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e@Bean\n@Primary\npublic DataSource spyDataSource(@Autowired DataSource dataSource) {\n  // wrap a datasource using P6SpyDataSource\n  return new P6DataSource(dataSource);\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003elog4jdbc\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic DataSource spyDataSource(DataSource dataSource) {\n    // wrap the provided dataSource\n  return new DataSource() {\n    @Override\n    public Connection getConnection() throws SQLException {\n      // wrap the connection with log4jdbc\n      return new ConnectionSpy(dataSource.getConnection());\n    }\n      \n    @Override\n    public Connection getConnection(String username, String password) throws SQLException {\n       // wrap the connection with log4jdbc\n      return new ConnectionSpy(dataSource.getConnection(username, password));\n     }\n      //...\n  };\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e上述三种方案都可以实现SQL监控，那么在实际应用场景中选择哪种方式更好呢？这和实际的生产方式有关。在我手，数据库是基于KDB的，Java应用是基于KsBoot，其中，数据库连接是在KDB平台配置的，底层的数据源是使用ShardingSphere+HikariDataSource进行魔改的。\u003c/p\u003e\n\u003cp\u003e第一种方案，由于数据库连接是由DBA维护的，升级需求修改数据库连接，因此不建议。\u003c/p\u003e\n\u003cp\u003e第二种方案，同理需要修改数据库连接，且比第一种更容易配错，因此也不建议。\u003c/p\u003e\n\u003cp\u003e排除上述两种方式，剩下的只有第三种方案了，但是第三种方案有很大的挑战，原因在于需要兼容快手kuaishou-framework奇葩的JdbcTemplate使用方式。确切地说，在于使用了DataSourceConfig。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic interface DataSourceConfig extends HasBizDef {\n\n    /**\n     * 数据源名称，必须与KDB申请时填写的一致\n     */String bizName();\n\n    /**\n     * 获取当前可用区单库只读的JdbcTemplate\n     */\n    default NamedParameterJdbcTemplate read() {\n        return InternalDatasourceConfig.readForceAz(this, currentAz(), currentPaz(), \u0026quot;read\u0026quot;);\n    }   \n\n    /**\n     * 获取当前可用区单库读写的JdbcTemplate\n     */\n    default NamedParameterJdbcTemplate write() {\n        return InternalDatasourceConfig.writeForceAz(this, currentAz(), currentPaz(), \u0026quot;write\u0026quot;);\n    }\t\n  //....\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDefaultDataSourceConfig是一个接口类，默认封装了NamedParameterJdbcTemplate的创建，业务方通过继承该接口来定义数据源:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-kotlin\"\u003eenum class AdDataSources(\n    private val bizDef: BizDef,\n    private val forTest: AdDataSources? = null,\n    private val usingNewZk: Boolean = false\n) : DataSourceConfig{\n    adFansTopProfileDashboardTest,\n    adFansTopProfileDashboard,\n    adChargeTest,\n    adCharge,\n    adChargeReadOnly,\n    adDspReadOnlyTest,\n    adDspReadOnly;\n    public open fun bizName(): String {\n        return bizDef.bizName\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e如果在业务中直接使用了DataSourceConfig创建的NamedParameterJdbcTemplate，那么我们就需要修改过程中创建的DataSource对象。那么，这里的DataSource究竟是怎么创建的呢？\u003c/p\u003e\n\u003cp\u003e具体扒代码的过程就不赘述了，直接说结果吧，kuaishou-framework的数据源最终是通过DataSourceFactory进行创建的，具体代码如下：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic static ListenableDataSource\u0026lt;Failover\u0026lt;Instance\u0026gt;\u0026gt; create(Instance i) {\n   //...\n   try {\n       return supplyWithRetry(\n        DATA_SOURCE_BUILD_RETRY,\n        DATA_SOURCE_BUILD_RETRY_DELAY,\n        () -\u0026gt; new ListenableDataSource\u0026lt;\u0026gt;(\n              bizName, \n              new HikariDataSource(config), ds -\u0026gt; i.toString(), i),\n              DataSourceFactory::needRetry);\n                               \n  } catch (Throwable e) {/**/}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e由代码可以看到，这里的数据源实际上是通过new HikariDataSource(config)手动创建的，而DataSourceConfig又没有对外暴露创建的数据源，所以，我们该如何对DataSource代理呢?\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003e二、动态修改加载类\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e成本最低的方式就是直接修改这段代码，将其中\u0026#x7684;\u003cem\u003e\u0026#x6E;ew HikariDataSource(config)\u003c/em\u003e\u0026#x4FEE;改\u0026#x6210;\u003cem\u003e\u0026#x6E;ew P6DataSource(new HikariDataSource(config))，\u003c/em\u003e\u0026#x90A3;么问题来了，这段代码属于基础组件包中的代码，基础架构组没有动力去修改，而我们又没有修改的权限，要想动这块代码，只能使用黑科技了。黑科技的手段有很多，那么问题又来了，哪种手段更合适呢？\u003c/p\u003e\n\u003cp\u003e首先我们来分析一下，有哪些手段可以修改Java字节码？\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e方案一、编译时修改，需要开发maven插件\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e（不使用maven插件的同学咋办？）\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e方案二、加载时修改，重写类加载器\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e需要在代码中指定特定的类加载器，用有一定的侵入式\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e方案三、运行时修改，使用JavaAgent\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e需要修改应用启动参数，运维成本有点高\u003c/p\u003e\n\u003cp\u003e首先要说明的是，这里不是对类方法进行增强，所以想使用cglib动态代理的想法是不可行的。前面三种方案都有一定的局限性：方案一比较麻烦，方案二侵入性强，方案三则需要使用JavaAgent技术，那有没有方案不使用Agent就可以动态修改已经加载的字节码呢？答案是没有，至少理论上没有。不过，好在天无绝人之路，JDK9之后，可以动态启动JavaAgent，这样就不用修改启动参数了。这里，我们选择使用byte-buddy进行字节码重写。\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e下面是对动态启动Java Agent技术的解释\u003c/em\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote that starting with Java 9, there is the Launcher-Agent-Class manifest attribute for jar files that can specify the class of a Java Agent to start before the class specified with the Main-Class is launched. That way, you can easily have your Agent collaborating with your application code in your JVM, without the need for any additional command line options. The Agent can be as simple as having an agentmain method in your main class storing the Instrumentation reference in a static variable.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSee \u003ca href=\"https://docs.oracle.com/en/java/javase/15/docs/api/java.instrument/java/lang/instrument/package-summary.html#package.description\"\u003ethe java.lang.instrument package documentation\u003c/a\u003e…\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003eGetting hands on an Instrumentation instance when the JVM has not been started with Agents is trickier. It must support launching Agents after startup in general, e.g. via the Attach API. \u003ca href=\"https://stackoverflow.com/a/19912148/2711488\"\u003eThis answer\u003c/a\u003e demonstrates at its end such a self-attach to get hands on the Instrumentation. When you have the necessary manifest attribute in your application jar file, you could even use that as agent jar and omit the creation of a temporary stub file.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003eHowever, recent JVMs forbid self-attaching unless -Djdk.attach.allowAttachSelf=true has been specified at startup, but I suppose, taking additional steps at startup time, is precisely what you don’t want to do. One way to circumvent this, is to use another process. All this process has to to, is to attach to your original process and tell the JVM to start the Agent. Then, it may already terminate and everything else works the same way as before the introduction of this restriction.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAs mentioned in \u003ca href=\"https://stackoverflow.com/questions/56787777/?noredirect=1\u0026lq=1#comment100160373_56787777\"\u003ethis comment\u003c/a\u003e, Byte-Buddy has already implemented those necessary steps and the stripped-down Byte-Buddy-Agent contains that logic only, so you can use it to build your own logic atop it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003e字节码工具对比\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://static.yximgs.com/udata/pkg/EE-KSTACK/4223630ea14c6367968188fd52cafa26.png\" alt=\"图片\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e使用bytebuddy修改字节码\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在实现代码之前，我们回过头来再看一下快手的数据源生成：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003enew ListenableDataSource\u0026lt;\u0026gt;(bizName, new HikariDataSource(config), ds -\u0026gt; i.toString());\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这里实际生成的数据源类型是ListenableDataSource，而ListenableDataSource刚好继承了DelegatingDataSource类，而DelegatingDataSource的构造方法如下：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic class DelegatingDataSource implements DataSource {\n   //...\n  public DelegatingDataSource(DataSource targetDataSource) {\n    this.setTargetDataSource(targetDataSource);\n   }\n\n  public void setTargetDataSource(@Nullable DataSource targetDataSource) {\n      this.targetDataSource = targetDataSource;\n  }\n  //...\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e因此，我们可以通过改写DelegatingDataSource#setTargetDataSource方法，实现同样的效果，修改后的方法应该如下：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic void setTargetDataSource(@Nullable DataSource targetDataSource) {\n        this.targetDataSource = new P6DataSource(targetDataSource;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e那么具体如何修改字节码呢？这里是\u003ca href=\"https://bytebuddy.net/#/tutorial\"\u003e官方文档\u003c/a\u003e，原理我们不做赘述，直接介绍实现了。实现方式有三种：\u003c/p\u003e\n\u003ch4\u003e\u003cstrong\u003e1、类文件替换\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003e假设你已经通过Java代码编译了新的类，现在要替换JVM中类的定义，代码如下：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003enew ByteBuddy()\n  .redefine(NewDelegatingDataSource.class)\n  .name(DelegatingDataSource.class.getName())\n  .make()\n  .load(Thread.currentThread().getContextClassLoader(), \n        ClassReloadingStrategy.fromInstalledAgent());\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\u003cstrong\u003e2、操作字节码：\u003c/strong\u003e\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003enew ByteBuddy()\n    .redefine(DelegatingDataSource.class)\n    //重写DelegatingDataSource#setTargetDataSource方法\n    .method(named(\u0026quot;setTargetDataSource\u0026quot;))\n    .intercept(MyImplementation.INSTANCE)\n    .make()\n    .load(Thread.currentThread().getContextClassLoader(),\n          ClassReloadingStrategy.fromInstalledAgent());\n\nenum MyImplementation implements Implementation {\n\nINSTANCE; // singleton\n\n  @Override\n  public InstrumentedType prepare(InstrumentedType instrumentedType) {\n  return instrumentedType;\n  }\n  \n  @Override\n  public ByteCodeAppender appender(Target implementationTarget) {\n  return MyAppender.INSTANCE;\n  }\n  \n}\n//字节码定义\nenum MyAppender implements ByteCodeAppender {\n\nINSTANCE; // singleton\n\n@Override\npublic Size apply(MethodVisitor methodVisitor,\n        Implementation.Context implementationContext,\n        MethodDescription instrumentedMethod) {\n  Label label0 = new Label();\n  methodVisitor.visitLabel(label0);\n  methodVisitor.visitLineNumber(70, label0);\n  methodVisitor.visitVarInsn(ALOAD, 0);\n  methodVisitor.visitTypeInsn(NEW, \u0026quot;com/p6spy/engine/spy/P6DataSource\u0026quot;);\n  methodVisitor.visitInsn(DUP);\n  methodVisitor.visitVarInsn(ALOAD, 1);\n  methodVisitor.visitMethodInsn(INVOKESPECIAL, \u0026quot;com/p6spy/engine/spy/P6DataSource\u0026quot;, \u0026quot;\u0026lt;init\u0026gt;\u0026quot;, \u0026quot;(Ljavax/sql/DataSource;)V\u0026quot;, false);\n  methodVisitor.visitFieldInsn(PUTFIELD, \u0026quot;org/springframework/jdbc/datasource/DelegatingDataSource\u0026quot;, \u0026quot;targetDataSource\u0026quot;, \u0026quot;Ljavax/sql/DataSource;\u0026quot;);\n  Label label1 = new Label();\n  methodVisitor.visitLabel(label1);\n  methodVisitor.visitLineNumber(71, label1);\n  methodVisitor.visitInsn(RETURN);\n  Label label2 = new Label();\n  methodVisitor.visitLabel(label2);\n  methodVisitor.visitLocalVariable(\u0026quot;this\u0026quot;, \u0026quot;Lorg/springframework/jdbc/datasource/DelegatingDataSource;\u0026quot;, null, label0, label2, 0);\n  methodVisitor.visitLocalVariable(\u0026quot;targetDataSource\u0026quot;, \u0026quot;Ljavax/sql/DataSource;\u0026quot;, null, label0, label2, 1);\n  methodVisitor.visitMaxs(4, 2);\n  return new Size(4, 2);\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e上述代码的核心思想是字节操作字节码，操作字节码是非常复杂和繁重的事情，且无法debug，那么有没有比较方便的方式呢？\u003c/p\u003e\n\u003cp\u003e我们可以手动改写Java代码，然后利用插件生成对应的字节码，然后在其基础上进行修改，研发成本会低很多。这里推荐IDEA的一个插件：Byte-Code-Analyzer，使用该插件可以查看类对应的ASM字节码:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://static.yximgs.com/udata/pkg/EE-KSTACK/e31962a90f6598880e78d8254d6c74d9\" alt=\"图片\"\u003e\u003c/p\u003e\n\u003ch4\u003e\u003cstrong\u003e3、利用byte-buddy的Advice\u003c/strong\u003e\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e public static void redefine() {\n   new ByteBuddy()\n     .redefine(DelegatingDataSource.class)\n     .visit(Advice.to(Decorator.class)\n            .on(ElementMatchers.named(\u0026quot;setTargetDataSource\u0026quot;)))\n     .make()\n     .load(Thread.currentThread().getContextClassLoader(),\n           ClassReloadingStrategy.fromInstalledAgent()).getLoaded();\n }\n\nstatic class Decorator {\n\n  //在方法开始插入代码\n  @Advice.OnMethodEnter\n    public static void enter(@Advice.Argument(value = 0, readOnly = false) DataSource dataSource) {\n    dataSource = new P6DataSource(dataSource);\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ebyte-buddy的Advisor和动态代理的原理不一样，他是直接修改方法体的字节码，上面的方法就是表示在方法开始插入一行，其效果如下：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic void setTargetDataSource(@Nullable DataSource targetDataSource) {\n  //插入的代码\n  targetDataSource = new P6DataSource(targetDataSource);\n  this.targetDataSource = targetDataSource;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e注：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e动态修改已加载的类，是有限制条件的，不能添加方法或者字段，因此通过byte-buddy的Methoddelegation方法修改字节码是不可行的。\u003c/li\u003e\n\u003cli\u003e使用byte-buddy的Advice，可以对非Spring托管的类进行动态增强，因为是直接修改字节码，性能更好。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\u003cstrong\u003e三、自动生效\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e前面我们讲了如何修改字节码，以提供SQL监控功能，那么如何让SQL监控自动生效呢？我们的目标是非侵入式解决方案：既不能修改业务代码，也不能更改系统配置。鉴于Java世界的事实标准，我们利用了SpringBoot-Starter功能，只需增加一个maven依赖，就自动提供了SQL监控能力。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-xml\"\u003e\u0026lt;dependency\u0026gt;\n  \u0026lt;groupId\u0026gt;com.kuaishou.ad\u0026lt;/groupId\u0026gt;\n  \u0026lt;artifactId\u0026gt;sqllog-spring-boot-starter\u0026lt;/artifactId\u0026gt;\n  \u0026lt;version\u0026gt;制品库查询最新版\u0026lt;/version\u0026gt;\n\u0026lt;/dependency\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e至于SpringBoot-Starter的实现原理，网上资料很多，核心思想就是提供默认配置，开箱即用。需要注意的是，Spring6.0自动配置的方案有了调整，原来基于spring.factories的配置改成了org.springframework.boot.autoconfigure.AutoConfiguration.imports，原有的方式还支持，这对应普通应用没有影响，但是在实现Spring多容器隔离的方案上有一定的影响，后面有时间会展开讲一下。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003eprivate static String[] getConfigurations(File file) {\n  @EnableAutoConfiguration\n  class NoScan {\n    //用于扫描META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports,该类定义在方法中,是为了避免扫描当前类时被加载\n  }\n  FileClassLoader classLoader = new FileClassLoader(file);\n  AutoConfigurationImportSelector selector = new AutoConfigurationImportSelector();\n  selector.setBeanClassLoader(classLoader);\n  selector.setResourceLoader(new ClassLoaderResourcePatternResolver(classLoader));\n  selector.setEnvironment(new StandardEnvironment());\n  String[] configurations = selector.selectImports(new StandardAnnotationMetadata(NoScan.class));\n  return configurations;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\u003cstrong\u003e四、SQL打印效果\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003esqllog-spring-boot-starter默认基于p6spy，并对SQL输出提供了扩展，打印SQL日志如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://static.yximgs.com/udata/pkg/EE-KSTACK/28cd44d1451c960cfb982773aab6ec44\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eSQL的打印内容分为三部分：\u003c/p\u003e\n\u003cp\u003e第一行，显示执行时间、耗时、SQL操作、数据库连接等信息\u003c/p\u003e\n\u003cp\u003e第二行，显示参数化SQL\u003c/p\u003e\n\u003cp\u003e第三行，显示绑定参数后的实际执行的SQL\u003c/p\u003e\n\u003cp\u003e通过日志看到，当SQL语句超长时，系统会对参数化SQL进行个性化缩略，而对实际执行的SQL，则保持原样输出，这样可以检索关键信息。\u003c/p\u003e\n"])</script><script>self.__next_f.push([1,"18:T2144,"])</script><script>self.__next_f.push([1,"\u003ch2\u003e一个缺失的词\u003c/h2\u003e\n\u003cp\u003e在读这本书之前，我和大多数人一样，把世界分成两类：脆弱的和坚固的。系统要么经不起冲击，要么扛得住冲击。风险管理的目标就是把脆弱的东西变得坚固。\u003c/p\u003e\n\u003cp\u003e塔勒布指出，这个二分法缺了最关键的一类。有些东西不仅扛得住冲击，而且\u003cstrong\u003e在冲击中变得更强\u003c/strong\u003e。他找遍了所有语言，发现没有现成的词来描述这种特性，于是造了一个：Antifragile，反脆弱。\u003c/p\u003e\n\u003cp\u003e脆弱的反义词不是坚固，就像消极的反义词不是中性。坚固只是光谱的中间位置——它抵抗冲击但不从中获益。反脆弱在坚固的另一端，它需要波动、需要压力、需要混乱，才能保持活力。\u003c/p\u003e\n\u003cp\u003e人体就是最典型的反脆弱系统。骨骼在承受压力后变得更致密，肌肉在撕裂后变得更强壮，免疫系统在接触病原体后变得更高效。如果你把一个人关在无菌、无重力、无压力的环境中「保护」起来，你会得到一个极度脆弱的人。\u003c/p\u003e\n\u003cp\u003e这个框架一旦建立，你会发现它无处不在。作为一个长期做系统架构的人，我发现它对我的专业领域和个人生活都产生了深刻影响。\u003c/p\u003e\n\u003ch2\u003e三元组：脆弱、坚固、反脆弱\u003c/h2\u003e\n\u003cp\u003e塔勒布用一个三元组来分析几乎所有事物：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e脆弱\u003c/th\u003e\n\u003cth\u003e坚固\u003c/th\u003e\n\u003cth\u003e反脆弱\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e大型集中式系统\u003c/td\u003e\n\u003ctd\u003e冗余备份系统\u003c/td\u003e\n\u003ctd\u003e分布式自适应系统\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e单一收入来源\u003c/td\u003e\n\u003ctd\u003e稳定的工资\u003c/td\u003e\n\u003ctd\u003e杠铃式收入结构\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e精确预测\u003c/td\u003e\n\u003ctd\u003e保险对冲\u003c/td\u003e\n\u003ctd\u003e从错误中学习的机制\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e优化效率\u003c/td\u003e\n\u003ctd\u003e增加冗余\u003c/td\u003e\n\u003ctd\u003e保留可选择性\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e这个三元组的价值在于，它让你\u003cstrong\u003e诊断自己的系统处于光谱的哪个位置\u003c/strong\u003e，然后有意识地向反脆弱方向移动。\u003c/p\u003e\n\u003cp\u003e更深的洞见是：我们的文化几乎总在推动我们走向脆弱端。追求效率最大化、消除所有冗余、精确预测未来——这些看似理性的行为，恰恰是脆弱性的来源。\u003c/p\u003e\n\u003ch2\u003e可选择性：反脆弱的核心机制\u003c/h2\u003e\n\u003cp\u003e反脆弱的底层机制是什么？塔勒布给出了一个精炼的答案：\u003cstrong\u003e可选择性（Optionality）\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e可选择性意味着：你的下行风险有限，但上行收益没有上限。这不是赌博——赌博是下行风险无限。可选择性是一种精心设计的不对称结构：坏的情况损失很小，好的情况获益很大。用金融术语说，你拥有的不是期货合约（被锁定），而是期权（有权利但没有义务）。\u003c/p\u003e\n\u003cp\u003e这个概念改变了我看待技术决策的方式。过去做架构设计，我习惯性地追求「最优方案」。现在我意识到，\u003cstrong\u003e最优方案往往是最脆弱的方案\u003c/strong\u003e，因为它对初始假设的依赖最大。一旦环境变化，优化过的系统最先崩溃。\u003c/p\u003e\n\u003cp\u003e更好的策略是保留可选择性：不要过早锁定技术栈，不要把所有逻辑耦合在一起，不要为了当前的效率牺牲未来的灵活性。软件工程中很多最佳实践——接口抽象、松耦合、插件化——本质上都是在创造可选择性，只是我们通常不用这个词来描述。\u003c/p\u003e\n\u003ch2\u003e杠铃策略：极端保守 + 极端冒险\u003c/h2\u003e\n\u003cp\u003e塔勒布最具操作性的建议是\u003cstrong\u003e杠铃策略（Barbell Strategy）\u003c/strong\u003e：不要走中间路线，而是同时做两个极端。\u003c/p\u003e\n\u003cp\u003e把 85-90% 的资源放在极端保守的位置（零风险或近似零风险），然后把 10-15% 放在极端冒险的位置（高风险高回报）。完全跳过中间地带。\u003c/p\u003e\n\u003cp\u003e为什么中间地带反而危险？因为中等风险给你一种虚假的安全感——你既没有真正的安全，也没有获得不对称收益的机会。\u003c/p\u003e\n\u003cp\u003e这个思路映射到分布式系统设计非常直接。与其对所有服务采用统一的「中等容错」方案，不如对核心链路做到极端可靠（多机房多活、强一致性），对非核心链路采用极端简化（允许失败、最终一致性、快速降级）。在关键点做到极致，在其余点保持轻量。\u003c/p\u003e\n\u003cp\u003e混沌工程（Chaos Engineering）也是杠铃策略的体现。Netflix 的 Chaos Monkey 在生产环境中随机杀死服务实例，看似制造了风险，实际上是在用可控的小压力来训练系统的反脆弱能力——主动引入波动，让系统在小规模失败中学习。\u003c/p\u003e\n\u003cp\u003e在职业规划上，杠铃策略同样适用。与其追求「还不错」的中间态路径，不如让收入结构变成杠铃形：一端是极度稳定的基本收入（技术咨询、稳定合同），另一端是极度不确定但上行空间巨大的探索（开源项目、技术创业、内容创作）。即使探索端全部失败，稳定端保证你不会陷入困境；但只要有一个成功，回报可能远超预期。\u003c/p\u003e\n\u003ch2\u003e切身利害：系统纠错的前提条件\u003c/h2\u003e\n\u003cp\u003e塔勒布在后续的著作中进一步发展了一个概念：\u003cstrong\u003eSkin in the Game（切身利害）\u003c/strong\u003e。这个概念在《反脆弱》中已有雏形——他认为，一个系统要具备反脆弱性，决策者必须承担自己决策的后果。\u003c/p\u003e\n\u003cp\u003e没有切身利害的决策系统是危险的。银行家用别人的钱冒险，成功了自己拿奖金，失败了纳税人买单——这就是结构性脆弱。决策者和风险承担者之间的分离，是脆弱性最深层的来源之一。\u003c/p\u003e\n\u003cp\u003e这个观察对技术团队的启示很深。当架构师不需要参与运维，当产品经理不需要处理线上故障，当管理者不需要为技术债务付出代价时，系统就自然地滑向脆弱端。「谁设计，谁运维」不仅仅是 DevOps 的口号，它的深层逻辑是通过切身利害来驱动反脆弱性。亚马逊的「You build it, you run it」原则，本质上就在解决这个问题：让做决策的人承担决策的后果。\u003c/p\u003e\n\u003ch2\u003e对个人生活的重新审视\u003c/h2\u003e\n\u003cp\u003e读完这本书后，我开始重新审视自己的生活结构。\u003c/p\u003e\n\u003cp\u003e我发现自己在很多方面都不自觉地追求「坚固」：稳定的工作、固定的收入、可预测的日程、熟悉的技术栈。这些不是坏事，但当所有的稳定性都依赖外部环境不变，我实际上是在和时间对赌。\u003c/p\u003e\n\u003cp\u003e真正改变我思维的是塔勒布关于「压力源」的态度翻转。反脆弱思维认为，\u003cstrong\u003e适度的压力源是系统保持活力的必要条件\u003c/strong\u003e。没有压力的系统不是健康的，而是一个正在慢慢退化的系统。\u003c/p\u003e\n\u003cp\u003e我开始有意识地给自己引入「可控的不确定性」：每年学一门新的编程语言或技术范式，定期换一种工作方式，尝试自己不擅长的领域。这不是为了「充电」或「自我提升」这种鸡汤式的理由，而是一种刻意的系统维护——通过小剂量的波动来避免大规模的脆弱性积累。\u003c/p\u003e\n\u003ch2\u003e反脆弱的局限\u003c/h2\u003e\n\u003cp\u003e公允地说，反脆弱框架在分析层面极其强大，但在操作层面有时过于模糊。「保留可选择性」说起来容易，具体到每一个决策点，什么算可选择性、代价多大才值得、什么时候该锁定而不是继续保留灵活性——这些塔勒布没有给出足够精确的回答。\u003c/p\u003e\n\u003cp\u003e另外，并非所有系统都需要反脆弱——有些场景（核电站的安全系统、航天器的关键组件）需要的就是极致的坚固，而不是在波动中进化。\u003c/p\u003e\n\u003ch2\u003e结语\u003c/h2\u003e\n\u003cp\u003e《反脆弱》给我最大的收获不是一套方法论，而是一种\u003cstrong\u003e认知框架的升级\u003c/strong\u003e——从「如何避免风险」的防御性思维，转向「如何让风险为我所用」的设计性思维。\u003c/p\u003e\n\u003cp\u003e作为一个做系统设计的人，我现在评估架构方案时会多问一个问题：\u003cstrong\u003e这个系统在遇到意外冲击时，是会崩溃、仅仅存活、还是变得更强？\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e这也许就是塔勒布最核心的洞见：在一个根本无法预测的世界中，比预测更重要的是\u003cstrong\u003e体质\u003c/strong\u003e。不是你能不能预见下一场风暴，而是你的系统是否能在风暴中进化。\u003c/p\u003e\n\u003cp\u003e风会熄灭蜡烛，却能使火越烧越旺。你要做的不是预测风的方向，而是把自己变成火。\u003c/p\u003e\n"])</script><script>self.__next_f.push([1,"19:T6336,"])</script><script>self.__next_f.push([1,"\u003ch2\u003e精确计算的代价与概率方法的价值\u003c/h2\u003e\n\u003cp\u003e在处理大规模数据时，我们经常面临一个根本性矛盾：精确计算所需的时间和空间资源，随数据量的增长而急剧膨胀，往往超出单机甚至集群的承载能力。判断一个元素是否属于一个十亿级集合，精确方案需要数十 GB 的 HashSet；计算两个百万文档集合之间的相似度，朴素的两两比较需要万亿次集合运算。\u003c/p\u003e\n\u003cp\u003e概率数据结构（Probabilistic Data Structures）提供了一条务实的出路：\u003cstrong\u003e用可控的、极小的错误率，换取数量级的空间和时间节省。\u003c/strong\u003e 布隆过滤器用不到传统 HashSet 十分之一的内存完成集合判重，MinHash 将文档相似度计算从集合运算降维为签名比较，Bitmap 用一个 bit 代替一个元素完成存在性标记。这些结构的共同特征是：错误率可以通过参数调节精确控制，且在工程实践中通常可以接受。\u003c/p\u003e\n\u003cp\u003e本文将系统讲解布隆过滤器、MinHash/LSH 两大概率数据结构的数学原理与工程应用，并在此基础上总结海量数据处理的核心方法论与经典问题解法。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e布隆过滤器（Bloom Filter）\u003c/h2\u003e\n\u003ch3\u003e数据结构与基本操作\u003c/h3\u003e\n\u003cp\u003e布隆过滤器由 Burton Howard Bloom 于 1970 年提出，其核心结构极其简洁：一个长度为 m 的位数组（bit array），配合 k 个相互独立的哈希函数。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e插入操作：\u003c/strong\u003e 对于待插入元素 x，分别计算 k 个哈希函数的值 h1(x), h2(x), ..., hk(x)，将位数组中对应的 k 个位置置为 1。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e查询操作：\u003c/strong\u003e 对于待查询元素 y，计算同样的 k 个哈希值，检查位数组中对应的 k 个位置：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若任意一个位置为 0，则 y \u003cstrong\u003e一定不在\u003c/strong\u003e 集合中（确定性否定）\u003c/li\u003e\n\u003cli\u003e若所有位置均为 1，则 y \u003cstrong\u003e可能在\u003c/strong\u003e 集合中（概率性肯定）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这种不对称性是布隆过滤器最关键的特性：\u003cstrong\u003eFalse Negative 永远不会发生，但 False Positive 以可控的概率存在。\u003c/strong\u003e 直觉上很容易理解——如果一个元素确实被插入过，它对应的 k 个位一定已经被置 1，不可能漏报；但多个不同元素的哈希值可能恰好覆盖了某个未插入元素的所有 k 个位置，导致误报。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e插入元素 x:\n  h1(x)=3, h2(x)=7, h3(x)=11\n  位数组: [0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0]\n\n查询元素 y (未插入):\n  h1(y)=3, h2(y)=7, h3(y)=11  → 所有位均为 1 → 误报 (False Positive)\n\n查询元素 z (未插入):\n  h1(z)=3, h2(z)=5, h3(z)=11  → 第 5 位为 0 → 确定不存在\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e错误率的数学分析\u003c/h3\u003e\n\u003cp\u003e假设位数组长度为 m，哈希函数个数为 k，已插入元素个数为 n。在插入一个元素后，某个特定位仍然为 0 的概率为：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eP(某位为0) = (1 - 1/m)^k\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e插入 n 个元素后，该位仍为 0 的概率为：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eP(某位为0) = (1 - 1/m)^(kn) ≈ e^(-kn/m)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e当 m 足够大时，上述近似成立（利用极限 (1-1/m)^m -\u0026gt; e^(-1)）。\u003c/p\u003e\n\u003cp\u003eFalse Positive 发生的条件是：一个不在集合中的元素，其 k 个哈希位置恰好全部为 1。因此误判率为：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ef ≈ (1 - e^(-kn/m))^k\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这个公式揭示了三个参数之间的制约关系：位数组越长（m 越大），误判率越低；哈希函数越多（k 越大），每次插入设置的位越多，位数组填满得越快；已插入元素越多（n 越大），误判率越高。\u003c/p\u003e\n\u003ch3\u003e最优参数选择\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e最优哈希函数个数。\u003c/strong\u003e 对 f 关于 k 求导并令其为零，可以得到使误判率最小的 k 值：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ek_opt = ln2 * (m/n) ≈ 0.693 * (m/n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e在最优 k 值下，位数组中 0 和 1 的比例恰好各占一半。这个结论具有优美的直觉意义：如果 1 太少，说明哈希函数不够多，没有充分利用位数组的空间；如果 1 太多，说明位数组已经过度饱和，碰撞概率急剧上升。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e位数组大小的确定。\u003c/strong\u003e 给定允许的最大误判率 epsilon 和预期插入元素数 n，位数组的最小长度为：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003em \u0026gt;= n * log2(1/epsilon) * (1/ln2) ≈ 1.44 * n * log2(1/epsilon)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e具体数值示例：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e误判率 epsilon\u003c/th\u003e\n\u003cth\u003e每元素所需位数 m/n\u003c/th\u003e\n\u003cth\u003e最优哈希函数数 k\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e1% (0.01)\u003c/td\u003e\n\u003ctd\u003e≈ 9.6 (取 10)\u003c/td\u003e\n\u003ctd\u003e≈ 7\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e0.1% (0.001)\u003c/td\u003e\n\u003ctd\u003e≈ 14.4 (取 15)\u003c/td\u003e\n\u003ctd\u003e≈ 10\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e0.01% (0.0001)\u003c/td\u003e\n\u003ctd\u003e≈ 19.2 (取 20)\u003c/td\u003e\n\u003ctd\u003e≈ 14\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e以常见的 1% 误判率为例，每个元素大约需要 10 个 bit，对于一个包含 1 亿元素的集合，布隆过滤器仅需约 120 MB 内存，而等价的 HashSet 可能需要数 GB。\u003c/p\u003e\n\u003ch3\u003e变种与改进\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eCounting Bloom Filter。\u003c/strong\u003e 标准布隆过滤器的一个显著缺陷是不支持删除操作。如果直接将某个元素对应的位置 0，可能会影响其他元素的判断，因为多个元素可能共享同一个位。Counting Bloom Filter 的思路是将位数组中的每个 bit 扩展为一个计数器（通常 4 bit 即可），插入时计数器加 1，删除时计数器减 1。代价是空间占用扩大为原来的 4 倍左右。需要注意计数器溢出的问题——当计数器达到最大值时不再递增，这会引入少量的 False Negative 可能性。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCuckoo Filter。\u003c/strong\u003e Fan 等人于 2014 年提出的 Cuckoo Filter 在多个维度上优于标准布隆过滤器：支持动态删除、在相同误判率下空间效率更高（尤其在误判率低于 3% 时）、查询性能更好（缓存友好的内存访问模式）。其原理基于 Cuckoo Hashing（布谷鸟哈希），每个元素存储其指纹（fingerprint）而非原始值，通过两个候选桶位置实现插入和驱逐。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSpectral Bloom Filter。\u003c/strong\u003e 在 Counting Bloom Filter 的基础上进一步扩展，不仅记录元素是否存在，还关联元素的出现次数。适用于需要频率估计的场景，如网络流量中各 IP 的访问频次估计。\u003c/p\u003e\n\u003ch3\u003e工程应用\u003c/h3\u003e\n\u003cp\u003e布隆过滤器在工业系统中有广泛应用，以下列举几个典型场景：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRedis Bloom Module。\u003c/strong\u003e Redis 4.0 起通过模块机制支持布隆过滤器（\u003ccode\u003eBF.ADD\u003c/code\u003e、\u003ccode\u003eBF.EXISTS\u003c/code\u003e 等命令）。典型应用是分布式缓存穿透防护：将所有合法 Key 写入布隆过滤器，查询时先经过过滤器判断，对于确定不存在的 Key 直接返回，避免大量无效请求穿透到数据库层。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHBase BlockCache。\u003c/strong\u003e HBase 使用布隆过滤器加速行键查找。在读取 HFile 的数据块之前，先通过布隆过滤器判断目标行键是否可能存在于该数据块中，避免不必要的磁盘 I/O。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e分布式爬虫 URL 去重。\u003c/strong\u003e 对于需要爬取数十亿网页的大规模爬虫系统，使用布隆过滤器判断 URL 是否已被抓取过。少量 False Positive 仅意味着个别 URL 被跳过（可以通过定期全量重爬弥补），而空间节省极为可观。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e网络安全与黑名单。\u003c/strong\u003e Chrome 浏览器早期版本使用布隆过滤器存储恶意 URL 黑名单，在本地快速判断用户访问的 URL 是否可能有害，仅对\u0026quot;可能有害\u0026quot;的 URL 才请求远程服务器做精确验证。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eMinHash 与局部敏感哈希（LSH）\u003c/h2\u003e\n\u003ch3\u003eJaccard 相似度与集合比较的挑战\u003c/h3\u003e\n\u003cp\u003e在推荐系统、文档去重、抄袭检测等场景中，核心操作是衡量两个集合之间的相似程度。Jaccard 相似度是最经典的集合相似度度量：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eJ(A, B) = |A ∩ B| / |A ∪ B|\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eJaccard 相似度的值域为 [0, 1]，完全相同的集合为 1，完全不相交的集合为 0。\u003c/p\u003e\n\u003cp\u003e朴素方法的计算代价是巨大的。假设有 N 个文档需要两两比较相似度，总共需要 C(N,2) = N(N-1)/2 次比较。当 N = 100 万时，这意味着近 5000 亿次比较，每次比较还涉及集合的交集和并集运算。即使单次比较只需 1 微秒，总耗时也超过 5 天。\u003c/p\u003e\n\u003cp\u003eMinHash 与 LSH 的组合提供了一个近似但高效的解决方案：先用 MinHash 将集合压缩为固定长度的签名，再用 LSH 快速筛选出候选相似对，最后仅对候选对做精确比较。\u003c/p\u003e\n\u003ch3\u003eMinHash 的数学原理与正确性证明\u003c/h3\u003e\n\u003cp\u003eMinHash 的核心思想可以通过一个矩阵视角来理解。假设全集 U = {e1, e2, ..., eN}，有若干集合 S1, S2, ...，构造一个 0-1 特征矩阵，其中行对应全集中的元素，列对应各集合，矩阵元素表示该元素是否属于该集合。\u003c/p\u003e\n\u003cp\u003e对矩阵的行施加一个随机排列（permutation）pi，定义集合 S 的 MinHash 值为：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eh_pi(S) = min{ pi(i) : i 属于 S }\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e即在随机排列下，集合 S 中元素被映射到的最小值。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e核心定理：\u003c/strong\u003e 对于任意两个集合 A 和 B：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eP[ h_pi(A) = h_pi(B) ] = J(A, B)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e即两个集合的 MinHash 值相等的概率，恰好等于它们的 Jaccard 相似度。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e证明。\u003c/strong\u003e 考察全集中与 A 或 B 相关的元素，可以分为三类：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e类型 X：同时属于 A 和 B（即 A ∩ B 中的元素）\u003c/li\u003e\n\u003cli\u003e类型 Y：仅属于 A\u003c/li\u003e\n\u003cli\u003e类型 Z：仅属于 B\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在随机排列下，|A ∪ B| = |X| + |Y| + |Z| 个相关元素的顺序是完全随机的。h_pi(A) = h_pi(B) 当且仅当在这些相关元素中，排列值最小的那个属于类型 X（即同时属于 A 和 B）。由于排列是完全随机的，最小值落在类型 X 上的概率为 |X| / (|X| + |Y| + |Z|) = |A ∩ B| / |A ∪ B| = J(A, B)。证毕。\u003c/p\u003e\n\u003ch3\u003e签名矩阵的高效构建\u003c/h3\u003e\n\u003cp\u003e直接对全集的行做随机排列在工程上是不可行的——当全集包含数亿元素时，存储和应用一个完整排列的代价过高。实际做法是使用多个独立的哈希函数来模拟随机排列。\u003c/p\u003e\n\u003cp\u003e具体算法如下：选取 t 个哈希函数 h1, h2, ..., ht，每个哈希函数的形式通常为：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eh_i(x) = (a_i * x + b_i) mod p\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e其中 p 是一个大素数，a_i 和 b_i 是随机选取的系数。\u003c/p\u003e\n\u003cp\u003e对于每个集合 S 和每个哈希函数 h_i，计算签名值：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esig_i(S) = min{ h_i(x) : x 属于 S }\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e最终每个集合被压缩为一个 t 维的签名向量。两个集合签名向量中相同分量的比例，即为 Jaccard 相似度的无偏估计。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport numpy as np\n\ndef build_signature_matrix(sets, universe_size, num_hashes):\n    \u0026quot;\u0026quot;\u0026quot;\n    构建 MinHash 签名矩阵\n\n    参数:\n        sets: 集合列表，每个集合包含整数元素\n        universe_size: 全集大小（用于确定哈希函数的模数）\n        num_hashes: 哈希函数个数（签名维度）\n    返回:\n        签名矩阵，shape = (num_hashes, len(sets))\n    \u0026quot;\u0026quot;\u0026quot;\n    p = next_prime(universe_size)  # 取大于全集大小的最小素数\n    # 随机生成哈希函数系数\n    a = np.random.randint(1, p, size=num_hashes)\n    b = np.random.randint(0, p, size=num_hashes)\n\n    num_sets = len(sets)\n    sig_matrix = np.full((num_hashes, num_sets), np.inf)\n\n    for col, s in enumerate(sets):\n        for elem in s:\n            # 计算该元素在每个哈希函数下的值\n            hashes = (a * elem + b) % p\n            # 更新签名矩阵：取最小值\n            sig_matrix[:, col] = np.minimum(sig_matrix[:, col], hashes)\n\n    return sig_matrix.astype(int)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e签名维度 t 的选择取决于精度要求。根据大数定律，估计的标准误差约为 1/sqrt(t)。t = 100 时标准误差约 10%，t = 400 时约 5%。\u003c/p\u003e\n\u003ch3\u003eLSH 的分桶策略与候选对筛选\u003c/h3\u003e\n\u003cp\u003eMinHash 签名将集合比较的代价从集合运算降低为向量比较，但仍未解决 O(N^2) 的两两比较问题。局部敏感哈希（Locality-Sensitive Hashing, LSH）通过分桶策略，将签名相似的集合映射到同一个桶中，只对同桶内的集合对做精确比较。\u003c/p\u003e\n\u003cp\u003e具体方法是将 t 维签名向量分割为 b 个 band（段），每个 band 包含 r 行（t = b * r）。对于每个 band，将该 band 内的 r 个签名值组合后哈希到桶中。两个集合只要在任意一个 band 中被哈希到同一个桶，就成为候选对。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e概率分析。\u003c/strong\u003e 假设两个集合的真实 Jaccard 相似度为 s，则：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e在某一个哈希函数上签名相同的概率为 s\u003c/li\u003e\n\u003cli\u003e在某个 band（r 行）中所有 r 个签名都相同的概率为 s^r\u003c/li\u003e\n\u003cli\u003e在某个 band 中至少有一个签名不同的概率为 1 - s^r\u003c/li\u003e\n\u003cli\u003e在所有 b 个 band 中都不完全相同（即不成为候选对）的概率为 (1 - s^r)^b\u003c/li\u003e\n\u003cli\u003e成为候选对的概率为 1 - (1 - s^r)^b\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这个概率函数呈现出 S 型曲线的特征，存在一个\u0026quot;阈值\u0026quot;相似度 s* ≈ (1/b)^(1/r)，在该阈值附近概率急剧变化。通过调节 b 和 r 的值，可以精确控制这个阈值：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eb (bands)\u003c/th\u003e\n\u003cth\u003er (rows/band)\u003c/th\u003e\n\u003cth\u003et = b*r\u003c/th\u003e\n\u003cth\u003e阈值 s* ≈ (1/b)^(1/r)\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e20\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e100\u003c/td\u003e\n\u003ctd\u003e≈ 0.55\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e50\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e100\u003c/td\u003e\n\u003ctd\u003e≈ 0.14\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e10\u003c/td\u003e\n\u003ctd\u003e10\u003c/td\u003e\n\u003ctd\u003e100\u003c/td\u003e\n\u003ctd\u003e≈ 0.80\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eb 越大（band 越多），越容易将低相似度的集合对也纳入候选，召回率高但精确率低；r 越大（每个 band 行数越多），阈值越高，只有高相似度的集合对才会成为候选。\u003c/p\u003e\n\u003ch3\u003e工程应用\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e近似文档去重。\u003c/strong\u003e 在搜索引擎的网页去重、新闻聚合等场景中，将文档表示为 shingle（连续 k 个词的子序列）的集合，通过 MinHash+LSH 快速发现近似重复的文档对。Google 的 SimHash 和 MinHash 是这一领域最经典的两个方案。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e推荐系统。\u003c/strong\u003e 在协同过滤推荐中，将\u0026quot;用户-商品\u0026quot;交互矩阵中的每个用户视为一个商品集合（用户购买/浏览过的商品），通过 MinHash 计算用户之间的 Jaccard 相似度，高效找到相似用户群体。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e基因组学。\u003c/strong\u003e 在生物信息学中，MinHash 被广泛用于基因组序列的快速比较。Mash 工具利用 MinHash 将基因组压缩为固定长度的 sketch，使得数万个基因组之间的距离计算在分钟级完成。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e海量数据处理方法论\u003c/h2\u003e\n\u003cp\u003e概率数据结构是海量数据处理工具箱中的重要组成部分，但远非全部。下面系统梳理海量数据处理的核心方法论。\u003c/p\u003e\n\u003ch3\u003e分治策略：Hash 分割与子问题求解\u003c/h3\u003e\n\u003cp\u003e当数据量超出单机内存时，最普遍的策略是\u003cstrong\u003e先分割，再分别处理，最后归并结果\u003c/strong\u003e。Hash 分割是最常用的分割手段：对数据的某个 Key 做哈希，按哈希值取模分配到不同的小文件或分区中。\u003c/p\u003e\n\u003cp\u003e这一策略的核心保证是：\u003cstrong\u003e相同的 Key 一定会被分配到同一个分区。\u003c/strong\u003e 这意味着每个分区可以独立地完成统计、去重或比较操作，不会遗漏。\u003c/p\u003e\n\u003cp\u003e典型流程：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e原始大文件\n    ↓ hash(key) % N\n分成 N 个小文件 (file_0, file_1, ..., file_{N-1})\n    ↓ 各自独立处理\nN 个局部结果\n    ↓ 归并\n全局结果\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这个模式贯穿了海量数据处理的绝大多数问题。当面对\u0026quot;内存不够\u0026quot;的约束时，第一反应应该是 Hash 分割。\u003c/p\u003e\n\u003ch3\u003e位图法：Bitmap 与扩展 Bitmap\u003c/h3\u003e\n\u003cp\u003e标准 Bitmap 用 1 个 bit 表示一个元素的存在性，适用于元素值域有限且密集的场景。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e经典应用：40 亿个 unsigned int 中判断某个数是否存在。\u003c/strong\u003e unsigned int 的值域为 [0, 2^32)，一个覆盖完整值域的 Bitmap 需要 2^32 / 8 = 512 MB 内存。遍历一次数据将所有出现过的数对应位置 1，之后任意查询的时间复杂度为 O(1)。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e扩展 Bitmap（2-Bitmap）。\u003c/strong\u003e 当需要区分\u0026quot;未出现\u0026quot;、\u0026quot;出现一次\u0026quot;和\u0026quot;出现多次\u0026quot;三种状态时，可以用 2 个 bit 表示每个元素，编码为 00（未出现）、01（出现一次）、10（出现多次）。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e应用实例：2.5 亿个整数中找出不重复的整数。\u003c/strong\u003e 使用 2-Bitmap，遍历数据：首次出现标记为 01，再次出现标记为 10。遍历完成后，所有标记为 01 的即为不重复整数。2.5 亿个整数的 2-Bitmap 仅需约 60 MB 内存（若值域为 2^32 则需 1 GB）。\u003c/p\u003e\n\u003ch3\u003e堆与优先队列：Top-K 问题\u003c/h3\u003e\n\u003cp\u003e\u0026quot;从海量数据中找出最大的 K 个元素\u0026quot;是最高频的面试题型之一。核心方法是维护一个大小为 K 的\u003cstrong\u003e最小堆\u003c/strong\u003e：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e取前 K 个元素构建最小堆\u003c/li\u003e\n\u003cli\u003e遍历剩余元素，若当前元素大于堆顶，则替换堆顶并调整堆\u003c/li\u003e\n\u003cli\u003e遍历完成后堆中即为最大的 K 个元素\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e时间复杂度 O(N * logK)，空间复杂度 O(K)。当 K 远小于 N 时，这个方法的效率极高。\u003c/p\u003e\n\u003cp\u003e对于分布式场景，可以先在各节点上分别求出局部 Top-K，再对所有局部结果做一次全局 Top-K 归并。\u003c/p\u003e\n\u003ch3\u003e外排序与多路归并\u003c/h3\u003e\n\u003cp\u003e当数据量远超内存时，外排序（External Sort）是排序和去重的标准方案：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e分割阶段：\u003c/strong\u003e 将数据分割为可以装入内存的小块，每块在内存中排序后写回磁盘\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e归并阶段：\u003c/strong\u003e 使用多路归并（k-way merge），同时打开 k 个有序文件，维护一个大小为 k 的最小堆，每次取堆顶元素输出，再从对应文件读入下一个元素\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e多路归并的磁盘 I/O 次数为 O(N/B * log_k(N/M))，其中 N 为数据总量，B 为磁盘块大小，M 为可用内存，k 为归并路数。\u003c/p\u003e\n\u003ch3\u003eTrie 树与倒排索引\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eTrie 树（前缀树）\u003c/strong\u003e 特别适合处理大量字符串的统计和查询。其优势在于：公共前缀只存储一次，天然支持前缀匹配，插入和查询的时间复杂度仅与字符串长度相关，不受数据量影响。典型场景包括搜索引擎的自动补全、词频统计等。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e倒排索引（Inverted Index）\u003c/strong\u003e 是搜索引擎的核心数据结构。传统的正排索引是\u0026quot;文档 -\u0026gt; 词列表\u0026quot;，倒排索引反转为\u0026quot;词 -\u0026gt; 文档列表\u0026quot;。给定一个查询词，可以在 O(1) 时间内定位到包含该词的所有文档，再通过交集运算处理多词查询。\u003c/p\u003e\n\u003ch3\u003e分布式计算：MapReduce 范式\u003c/h3\u003e\n\u003cp\u003e当单机的分治策略仍然无法应对数据量时，MapReduce 将分治推广到集群级别：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMap 阶段：\u003c/strong\u003e 每个 Mapper 处理输入数据的一个分片，输出 (key, value) 对\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eShuffle 阶段：\u003c/strong\u003e 框架按 key 做哈希分区，将相同 key 的数据发送到同一个 Reducer\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReduce 阶段：\u003c/strong\u003e 每个 Reducer 处理一组具有相同 key 的 value，输出最终结果\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMapReduce 本质上是分治策略的分布式版本，Hash 分割对应 Shuffle，子问题求解对应 Reduce，自然归并对应最终输出的汇总。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e经典问题与解法\u003c/h2\u003e\n\u003ch3\u003e海量日志中提取访问次数最多的 IP\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e问题：\u003c/strong\u003e 有一个包含百亿条访问日志的文件，每行一个 IP 地址，内存限制 1 GB，找出访问次数最多的 IP。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e分析：\u003c/strong\u003e IP 地址最多有 2^32 ≈ 43 亿种，如果用 HashMap 直接统计，最坏情况下需要数十 GB 内存。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e解法：\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eHash 分割。\u003c/strong\u003e 对 IP 地址做哈希，按 hash(IP) % 1000 分配到 1000 个小文件中。由于哈希的均匀性，每个小文件大约包含原始数据的 1/1000，且相同 IP 一定在同一个小文件中。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e分别统计。\u003c/strong\u003e 对每个小文件，使用 HashMap 统计各 IP 的出现次数，记录该文件中出现次数最多的 IP 及其计数。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e全局归并。\u003c/strong\u003e 比较 1000 个局部最大值，取全局最大值即为结果。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e如果某个小文件仍然超出内存限制（极端哈希倾斜），可以对该文件换一个哈希函数再次分割。\u003c/p\u003e\n\u003ch3\u003e50 亿 URL 文件求共同 URL\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e问题：\u003c/strong\u003e A、B 两个文件各包含 50 亿个 URL，可用内存 4 GB，找出两个文件中共同的 URL。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e分析：\u003c/strong\u003e 50 亿个 URL 的原始数据量在 TB 级别，远超内存。但如果将两个文件用相同的哈希函数分割为对应的小文件，则只需比较对应分区。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e解法：\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e使用同一个哈希函数，将 A 文件中的 URL 按 hash(URL) % 1000 分配到 a_0, a_1, ..., a_999 共 1000 个小文件。\u003c/li\u003e\n\u003cli\u003e用同样的方法将 B 文件分配到 b_0, b_1, ..., b_999。\u003c/li\u003e\n\u003cli\u003e对于每一对 (a_i, b_i)，将 a_i 中的 URL 加载到 HashSet 中，遍历 b_i 中的 URL 做查找。输出所有在 HashSet 中找到的 URL 即为该分区的共同 URL。\u003c/li\u003e\n\u003cli\u003e合并所有分区的结果。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e关键在于：相同的 URL 一定会被分配到编号相同的小文件对中，因此只需比较对应分区，不需要交叉比较。\u003c/p\u003e\n\u003cp\u003e另一种方案是使用布隆过滤器：将 A 文件中的所有 URL 构建布隆过滤器（50 亿元素，1% 误判率，约需 6 GB——超出内存限制），或者结合分治策略，先 Hash 分割再在每个分区内使用布隆过滤器。\u003c/p\u003e\n\u003ch3\u003e1 GB 文件 1 MB 内存找频率最高的 100 个词\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e问题：\u003c/strong\u003e 一个 1 GB 的文本文件，可用内存仅 1 MB，找出出现频率最高的 100 个词。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e解法：\u003c/strong\u003e 这是分治、Trie 树和堆三种方法的综合应用。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eHash 分割。\u003c/strong\u003e 对文件中的每个词做哈希，按 hash(word) % 5000 分配到 5000 个小文件中。每个小文件平均约 200 KB，可以装入 1 MB 内存。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTrie 树统计。\u003c/strong\u003e 对每个小文件，构建 Trie 树统计各词的出现次数。同时维护一个大小为 100 的最小堆，记录该文件中频率最高的 100 个词。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e全局归并。\u003c/strong\u003e 将 5000 个文件各自的 Top-100 结果（共 50 万个词频对）做最终的 Top-100 归并。由于相同的词一定在同一个小文件中，局部 Top-100 的并集一定包含全局 Top-100。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e2.5 亿整数中找出不重复的整数\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e问题：\u003c/strong\u003e 2.5 亿个整数（值域为 int 范围），内存有限，找出所有只出现一次的整数。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e解法：\u003c/strong\u003e 使用 2-Bitmap 方案。\u003c/p\u003e\n\u003cp\u003e用 2 个 bit 表示每个整数的状态：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e00：未出现\u003c/li\u003e\n\u003cli\u003e01：出现一次\u003c/li\u003e\n\u003cli\u003e10：出现多次\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e对于 int 值域（2^32 个可能值），2-Bitmap 需要 2^32 * 2 / 8 = 1 GB 内存。如果内存不足 1 GB，可以分两次处理：先处理正整数，再处理负整数，各需 512 MB。\u003c/p\u003e\n\u003cp\u003e遍历所有 2.5 亿个整数，对于每个整数 x：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若 bitmap[x] == 00，置为 01\u003c/li\u003e\n\u003cli\u003e若 bitmap[x] == 01，置为 10\u003c/li\u003e\n\u003cli\u003e若 bitmap[x] == 10，不变\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e遍历完成后，扫描 Bitmap，所有状态为 01 的位置对应的整数即为不重复的整数。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e总结\u003c/h2\u003e\n\u003cp\u003e概率数据结构和海量数据处理方法共同构成了大规模系统的算法基础。回顾全文，可以提炼出几个核心原则：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e空间-精度权衡。\u003c/strong\u003e 布隆过滤器、MinHash、HyperLogLog 等概率结构的本质都是用可控的精度损失换取数量级的空间节省。在工程实践中，1% 的误判率通常是完全可接受的，但内存从 10 GB 降到 100 MB 可能决定了方案是否可行。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e分治是万能钥匙。\u003c/strong\u003e 当数据量超出单机资源时，Hash 分割 + 子问题求解 + 结果归并几乎是唯一的通用解法。这个模式从单机的文件分割到分布式的 MapReduce，形式不同但思想一致。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e选择正确的数据结构。\u003c/strong\u003e Bitmap 适合值域有限的存在性查询，Trie 适合字符串统计，堆适合 Top-K，倒排索引适合关键词检索，布隆过滤器适合集合判重，MinHash 适合相似度计算。没有万能的数据结构，只有与问题匹配的选择。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e参数化思维。\u003c/strong\u003e 布隆过滤器的 m 和 k、MinHash 的签名维度 t、LSH 的 b 和 r——这些参数的选择直接决定了系统的性能和准确度。理解参数背后的数学关系，才能做出合理的工程决策。\u003c/p\u003e\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"article\",null,{\"className\":\"min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"nav\",null,{\"className\":\"flex items-center gap-1 text-sm mb-4\",\"children\":[[\"$\",\"$L13\",null,{\"href\":\"/blog/page/1\",\"className\":\"text-gray-500 hover:text-blue-600 transition-colors\",\"children\":\"博客\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"/\"}],[\"$\",\"$L13\",null,{\"href\":\"/blog/category/engineering/page/1\",\"className\":\"text-gray-500 hover:text-blue-600 transition-colors\",\"children\":\"Engineering\"}],\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"flex items-center mb-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"w-4 h-4 mr-2 text-gray-400\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z\"}]}],[\"$\",\"time\",null,{\"dateTime\":\"2024-04-18\",\"children\":\"2024年04月18日\"}]]}]}],[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold text-gray-900 mb-6 text-center\",\"children\":\"Double Array Trie：高效字典树的压缩与检索实现\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2 mb-6 justify-center\",\"children\":[[\"$\",\"$L13\",\"数据结构\",{\"href\":\"/blog/tag/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/page/1/\",\"className\":\"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors\",\"children\":\"数据结构\"}],[\"$\",\"$L13\",\"Trie\",{\"href\":\"/blog/tag/Trie/page/1/\",\"className\":\"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors\",\"children\":\"Trie\"}],[\"$\",\"$L13\",\"Double Array Trie\",{\"href\":\"/blog/tag/Double%20Array%20Trie/page/1/\",\"className\":\"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors\",\"children\":\"Double Array Trie\"}],[\"$\",\"$L13\",\"中文分词\",{\"href\":\"/blog/tag/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/page/1/\",\"className\":\"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors\",\"children\":\"中文分词\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"max-w-5xl mx-auto\",\"children\":[\"$\",\"$L14\",null,{\"content\":\"$15\"}]}],[\"$\",\"$10\",null,{\"fallback\":[\"$\",\"div\",null,{\"className\":\"mt-12 pt-8 border-t border-gray-200\",\"children\":\"加载导航中...\"}],\"children\":[\"$\",\"$L16\",null,{\"globalNav\":{\"prev\":{\"slug\":\"engineering/middleware/非侵入式SQL监控\",\"title\":\"非侵入式SQL监控\",\"description\":\"你有没有因为应用程序没有打印SQL而导致问题排查困难？有没有因为SQL没有显示参数而导致日志毫无意义？有没有因为SQL超长而导致查看痛苦？有没有因为缺少SQL性能监控而导致无法报警？...\",\"pubDate\":\"2024-04-07\",\"tags\":[\"SQL监控\",\"Java\",\"非侵入式\"],\"heroImage\":\"$undefined\",\"content\":\"$17\"},\"next\":{\"slug\":\"life/reading/反脆弱：从不确定性中获益的系统设计\",\"title\":\"《反脆弱》：从不确定性中获益的系统设计\",\"description\":\"塔勒布的核心洞见不是「如何抵抗风险」，而是「如何让波动成为养分」。反脆弱不是坚固，而是在压力下变得更强。这个框架对系统架构、职业规划和个人生活都有深刻启示。\",\"pubDate\":\"2024-05-10\",\"tags\":[\"读书笔记\",\"反脆弱\",\"塔勒布\",\"系统设计\",\"风险管理\"],\"heroImage\":\"$undefined\",\"content\":\"$18\"}},\"tagNav\":{\"数据结构\":{\"prev\":{\"slug\":\"engineering/algorithm/概率数据结构与海量数据处理：从布隆过滤器到MinHash\",\"title\":\"概率数据结构与海量数据处理：从布隆过滤器到MinHash\",\"description\":\"系统讲解布隆过滤器、MinHash/LSH等概率数据结构的数学原理与工程应用，并总结海量数据处理的核心方法论与经典问题解法\",\"pubDate\":\"2024-01-12\",\"tags\":[\"数据结构\",\"布隆过滤器\",\"MinHash\",\"海量数据\"],\"heroImage\":\"$undefined\",\"content\":\"$19\"},\"next\":null},\"Trie\":{\"prev\":null,\"next\":null},\"Double Array Trie\":{\"prev\":null,\"next\":null},\"中文分词\":{\"prev\":null,\"next\":null}}}]}],[\"$\",\"$L1a\",null,{}]]}]}]}]\n"])</script><script>self.__next_f.push([1,"8:null\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n7:null\n"])</script><script>self.__next_f.push([1,"a:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Double Array Trie：高效字典树的压缩与检索实现 - Skyfalling Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"深入解析Double Array Trie的DFA建模、BASE/CHECK双数组构建算法、动态更新策略及其在中文分词与信息检索中的工程应用\"}],[\"$\",\"meta\",\"2\",{\"property\":\"og:title\",\"content\":\"Double Array Trie：高效字典树的压缩与检索实现\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:description\",\"content\":\"深入解析Double Array Trie的DFA建模、BASE/CHECK双数组构建算法、动态更新策略及其在中文分词与信息检索中的工程应用\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"5\",{\"property\":\"article:published_time\",\"content\":\"2024-04-18\"}],[\"$\",\"meta\",\"6\",{\"property\":\"article:author\",\"content\":\"Skyfalling\"}],[\"$\",\"meta\",\"7\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:title\",\"content\":\"Double Array Trie：高效字典树的压缩与检索实现\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:description\",\"content\":\"深入解析Double Array Trie的DFA建模、BASE/CHECK双数组构建算法、动态更新策略及其在中文分词与信息检索中的工程应用\"}],[\"$\",\"link\",\"10\",{\"rel\":\"shortcut icon\",\"href\":\"/favicon.png\"}],[\"$\",\"link\",\"11\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"link\",\"12\",{\"rel\":\"icon\",\"href\":\"/favicon.png\"}],[\"$\",\"link\",\"13\",{\"rel\":\"apple-touch-icon\",\"href\":\"/favicon.png\"}]],\"error\":null,\"digest\":\"$undefined\"}\n12:{\"metadata\":\"$a:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>