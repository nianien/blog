1:"$Sreact.fragment"
2:I[10616,["6874","static/chunks/6874-7791217feaf05c17.js","7177","static/chunks/app/layout-142e67ac4336647c.js"],"default"]
3:I[87555,[],""]
4:I[31295,[],""]
6:I[59665,[],"OutletBoundary"]
9:I[74911,[],"AsyncMetadataOutlet"]
b:I[59665,[],"ViewportBoundary"]
d:I[59665,[],"MetadataBoundary"]
f:I[26614,[],""]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/66b421ed9771e9de.css","style"]
0:{"P":null,"b":"C33gYo3klV3feVWcJcf5W","p":"","c":["","blog","engineering","data","%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%B8%E7%94%A8%E5%8E%BB%E9%87%8D%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B9%8BBitmap%E7%AF%87",""],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","engineering/data/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%B8%E7%94%A8%E5%8E%BB%E9%87%8D%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B9%8BBitmap%E7%AF%87","c"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/66b421ed9771e9de.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"zh-CN","children":["$","body",null,{"className":"__className_f367f3","children":["$","div",null,{"className":"min-h-screen flex flex-col","children":[["$","$L2",null,{}],["$","main",null,{"className":"flex-1","children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"bg-[var(--background)]","children":["$","div",null,{"className":"mx-auto max-w-7xl px-6 py-12 lg:px-8","children":["$","p",null,{"className":"text-center text-xs leading-5 text-gray-400","children":["© ",2026," Skyfalling"]}]}]}]]}]}]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","engineering/data/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%B8%E7%94%A8%E5%8E%BB%E9%87%8D%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B9%8BBitmap%E7%AF%87","c"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L5",null,["$","$L6",null,{"children":["$L7","$L8",["$","$L9",null,{"promise":"$@a"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","k7aL5vCqUy02fyjHU_R_uv",{"children":[["$","$Lb",null,{"children":"$Lc"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$Ld",null,{"children":"$Le"}]]}],false]],"m":"$undefined","G":["$f","$undefined"],"s":false,"S":true}
10:"$Sreact.suspense"
11:I[74911,[],"AsyncMetadata"]
13:I[6874,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],""]
14:I[32923,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
16:I[40780,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
19:I[85300,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
e:["$","div",null,{"hidden":true,"children":["$","$10",null,{"fallback":null,"children":["$","$L11",null,{"promise":"$@12"}]}]}]
15:T2bf1,<p>去重分析在企业日常分析中的使用频率非常高，如何在大数据场景下快速地进行去重分析一直是一大难点。在近期的 Apache Kylin Meetup 北京站上，我们邀请到 Kyligence 大数据研发工程师陶加涛为大家揭开了大数据分析常用去重算法的神秘面纱。</p>
<p>Apache Kylin 作为目前唯一一个同时支持精确与非精确去重查询的 OLAP 引擎，非常好地覆盖了大数据上的去重需求。本次分享讲解了 Kylin 这两种去重方式背后用到的算法，希望能让大家从源头上理解为什么 Kylin 的去重查询有着如此优异的性能。此次分享的回顾将分为两期，本篇首先为大家介绍精确去重算法 Bitmap 。</p>
<p>首先，请大家思考一个问题：在大数据处理领域中，什么环节是你最不希望见到的？以我的观点来看，shuffle 是我最不愿意见到的环节，因为一旦出现了非常多的 shuffle，就会占用大量的磁盘和网络 IO，从而导致任务进行得非常缓慢。而今天我们所讨论的去重分析，就是一个会产生非常多 shuffle 的场景，先来看以下场景：</p>
<p><img src="/images/blog/engineering/bigdata-image_1_1.png" alt="image_1_1.png"></p>
<p>我们有一张商品访问表，表上有 item 和 user_id 两个列，我们希望求商品的 UV，这是去重非常典型的一个场景。我们的数据是存储在分布式平台上的，分别在数据节点 1 和 2 上。</p>
<p>我们从物理执行层面上想一下这句 SQL 背后会发生什么故事：首先分布式计算框架启动任务, 从两个节点上去拿数据, 因为 SQL group by 了 item 列, 所以需要以 item 为 key 对两个表中的原始数据进行一次 shuffle。我们来看看需要 shuffle 哪些数据：因为 select/group by了 item，所以 item 需要 shuffle 。但是，user_id 我们只需要它的一个统计值，能不能不 shuffle 整个 user_id 的原始值呢？</p>
<p>如果只是简单的求 count 的话, 每个数据节点分别求出对应 item 的 user_id 的 count, 然后只要 shuffle 这个 count 就行了，因为count 只是一个数字, 所以 shuffle 的量非常小。但是由于分析的指标是 count distinct，我们不能简单相加两个节点user_id 的 count distinct 值，我们只有得到一个 key 对应的所有 user_id 才能统计出正确的 count distinct值，而这些值原先可能分布在不同的节点上，所以我们只能通过 shuffle 把这些值收集到同一个节点上再做去重。而当 user_id 这一列的数据量非常大的时候，需要 shuffle 的数据量也会非常大。我们其实最后只需要一个 count 值，那么有办法可以不 shuffle 整个列的原始值吗？我下面要介绍的两种算法就提供了这样的一种思路，使用更少的信息位，同样能够求出该列不重复元素的个数（基数）。</p>
<p><strong>精确算法: Bitmap</strong></p>
<p><img src="/images/blog/engineering/bigdata-image_1_2.png" alt="image_1_2.png"></p>
<p>第一种要介绍的算法是一种精确的去重算法，主要利用了 Bitmap 的原理。Bitmap 也称之为 Bitset，它本质上是定义了一个很大的 bit 数组，每个元素对应到 bit 数组的其中一位。例如有一个集合［2，3，5，8］对应的 Bitmap 数组是［001101001］，集合中的 2 对应到数组 index 为 2 的位置，3 对应到 index 为 3 的位置，下同，得到的这样一个数组，我们就称之为 Bitmap。很直观的，数组中 1 的数量就是集合的基数。追本溯源，我们的目的是用更小的存储去表示更多的信息，而在计算机最小的信息单位是 bit，如果能够用一个 bit 来表示集合中的一个元素，比起原始元素，可以节省非常多的存储。</p>
<p>这就是最基础的 Bitmap，我们可以把 Bitmap 想象成一个容器，我们知道一个 Integer 是32位的，如果一个 Bitmap 可以存放最多 Integer.MAX_VALUE 个值，那么这个 Bitmap 最少需要 32 的长度。一个 32 位长度的 Bitmap 占用的空间是512 M （2^32/8/1024/1024），这种 Bitmap 存在着非常明显的问题：这种 Bitmap 中不论只有 1 个元素或者有 40 亿个元素，它都需要占据 512 M 的空间。回到刚才求 UV 的场景，不是每一个商品都会有那么多的访问，一些爆款可能会有上亿的访问，但是一些比较冷门的商品可能只有几个用户浏览，如果都用这种 Bitmap，它们占用的空间都是一样大的，这显然是不可接受的。</p>
<p><strong>升级版 Bitmap: Roaring Bitmap</strong></p>
<p><img src="/images/blog/engineering/bigdata-image_1_3.png" alt="image_1_3.png"></p>
<p>对于上节说的问题，有一种设计的非常的精巧 Bitmap，叫做 Roaring Bitmap，能够很好地解决上面说的这个问题。我们还是以存放 Integer 值的 Bitmap 来举例，Roaring Bitmap 把一个 32 位的 Integer 划分为高 16 位和低 16 位，取高 16 位找到该条数据所对应的 key，每个 key 都有自己的一个 Container。我们把剩余的低 16 位放入该 Container 中。依据不同的场景，有 3 种不同的 Container，分别是 Array Container、Bitmap Container 和 Run Container，下文将一一介绍。</p>
<p><img src="/images/blog/engineering/bigdata-image_1_4.png" alt="image_1_4.png"></p>
<p>首先第一种，是 Roaring Bitmap 初始化时默认的 Container，叫做 Array Container。Array Container 适合存放稀疏的数据，Array Container 内部的数据结构是一个 short array，这个 array 是有序的，方便查找。数组初始容量为 4，数组最大容量为 4096。超过最大容量 4096 时，会转换为 Bitmap Container。这边举例来说明数据放入一个 Array Container 的过程：有 0xFFFF0000 和 0xFFFF0001 两个数需要放到 Bitmap 中, 它们的前 16 位都是 FFFF，所以他们是同一个 key，它们的后 16 位存放在同一个 Container 中; 它们的后 16 位分别是 0 和 1, 在 Array Container 的数组中分别保存 0 和 1 就可以了，相较于原始的 Bitmap 需要占用 512M 内存来存储这两个数，这种存放实际只占用了 2+4=6 个字节（key 占 2 Bytes，两个 value 占 4 Bytes，不考虑数组的初始容量）。</p>
<p><img src="/images/blog/engineering/bigdata-image_1_5.png" alt="image_1_5.png"></p>
<p>第二种 Container 是 Bitmap Container，其原理就是上文说的 Bitmap。它的数据结构是一个 long 的数组，数组容量固定为 1024，和上文的 Array Container 不同，Array Container 是一个动态扩容的数组。这边推导下 1024 这个值：由于每个 Container 还需处理剩余的后 16 位数据，使用 Bitmap 来存储需要 8192 Bytes（2^16/8）, 而一个 long 值占 8 个 Bytes，所以一共需要 1024（8192/8）个 long 值。所以一个 Bitmap container 固定占用内存 8 KB（1024 * 8 Byte）。当 Array Container 中元素到 4096 个时，也恰好占用 8 k（4096*2Bytes）的空间，正好等于 Bitmap 所占用的 8 KB。而当你存放的元素个数超过 4096 的时候，Array Container 的大小占用还是会线性的增长，但是 Bitmap Container 的内存空间并不会增长，始终还是占用 8 K，所以当 Array Container 超过最大容量（DEFAULT_MAX_SIZE）会转换为 Bitmap Container。</p>
<p>我们自己在 Kylin 中实践使用 Roaring Bitmap 时，我们发现 Array Container 随着数据量的增加会不停地 resize 自己的数组，而 Java 数组的 resize 其实非常消耗性能，因为它会不停地申请新的内存，同时老的内存在复制完成前也不会释放，导致内存占用变高，所以我们建议把 DEFAULT_MAX_SIZE 调得低一点，调成 1024 或者 2048，减少 Array Container 后期 reszie 数组的次数和开销。</p>
<p><img src="/images/blog/engineering/bigdata-image_1_6.png" alt="image_1_6.png"></p>
<p>最后一种 Container 叫做Run Container，这种 Container 适用于存放连续的数据。比如说 1 到 100，一共 100 个数，这种类型的数据称为连续的数据。这边的Run指的是Run Length Encoding（RLE），它对连续数据有比较好的压缩效果。原理是对于连续出现的数字, 只记录初始数字和后续数量。例如: 对于 [11, 12, 13, 14, 15, 21, 22]，会被记录为 11, 4, 21, 1。很显然，该 Container 的存储占用与数据的分布紧密相关。最好情况是如果数据是连续分布的，就算是存放 65536 个元素，也只会占用 2 个 short。而最坏的情况就是当数据全部不连续的时候，会占用 128 KB 内存。</p>
<p><img src="/images/blog/engineering/bigdata-image_1_7.png" alt="image_1_7.png"></p>
<p>总结：用一张图来总结3种 Container 所占的存储空间，可以看到元素个数达到 4096 之前，选用 Array Container 的收益是最好的，当元素个数超过了 4096 时，Array Container 所占用的空间还是线性的增长，而 Bitmap Container 的存储占用则与数据量无关，这个时候 Bitmap Container 的收益就会更好。而 Run Container 占用的存储大小完全看数据的连续性, 因此只能画出一个上下限范围 [4 Bytes, 128 KB]。</p>
<p><strong>在 Kylin 中的应用</strong></p>
<p><img src="/images/blog/engineering/bigdata-image_1_8.png" alt="image_1_8.png"></p>
<p>我们再来看一下Bitmap 在 Kylin 中的应用，Kylin 中编辑 measure 的时候，可以选择 Count Distinct，且Return Type 选为 Precisely，点保存就可以了。但是事情没有那么简单，刚才上文在讲 Bitmap 时，一直都有一个前提，放入的值都是数值类型，但是如果不是数值类型的值，它们不能够直接放入 Bitmap，这时需要构建一个全区字典，做一个值到数值的映射，然后再放入 Bitmap 中。</p>
<p><img src="/images/blog/engineering/bigdata-image_1_9.png" alt="image_1_9.png"></p>
<p>在 Kylin 中构建全局字典，当列的基数非常高的时候，全局字典会成为一个性能的瓶颈。针对这种情况，社区也一直在努力做优化，这边简单介绍几种优化的策略，更详细的优化策略可以见文末的参考链接。</p>
<p><img src="/images/blog/engineering/bigdata-image_1_10.png" alt="image_1_10.png"></p>
<p>1）当一个列的值完全被另外一个列包含，而另一个列有全局字典，可以复用另一个列的全局字典。</p>
<p><img src="/images/blog/engineering/bigdata-image_1_11.png" alt="image_1_11.png"></p>
<p>2）当精确去重指标不需要跨 Segment 聚合的时候，可以使用这个列的 Segment 字典代替（这个列需要字典编码）。在 Kylin 中，Segment 就相当于时间分片的概念。当不会发生跨 Segments 的分析时，这个列的 Segment 字典就可以代替这个全局字典。</p>
<p><img src="/images/blog/engineering/bigdata-image_1_12.png" alt="image_1_12.png"></p>
<p>3）如果你的 cube 包含很多的精确去重指标，可以考虑将这些指标放到不同的列族上。不止是精确去重，像一些复杂 measure，我们都建议使用多个列族去存储，可以提升查询的性能。</p>
17:T1ce1,<p>上篇介绍了利用 Roaring Bitmap 来进行精确去重。虽然这种算法能大大地减少存储开销，但是随着数据量的增大，它依然面临着存储上的压力。在本篇推送中将要介绍的 HyperLogLog（下称 HLL）是一种非精确的去重算法，它的特点是具有非常优异的空间复杂度（几乎可以达到常数级别）。</p>
<p><img src="/images/blog/engineering/bigdata-image_2_1.png" alt="image_2_1.png"></p>
<p>HLL 算法需要完整遍历所有元素一次，而非多次或采样；该算法只能计算集合中有多少个不重复的元素，不能给出每个元素的出现次数或是判断一个元素是否之前出现过；多个使用 HLL 统计出的基数值可以融合。</p>
<p><img src="/images/blog/engineering/bigdata-image_2_2.png" alt="image_2_2.png"></p>
<p><img src="/images/blog/engineering/bigdata-image_2_3.png" alt="image_2_3.png"></p>
<p>HLL 算法有着非常优异的空间复杂度，可以看到它的空间占用随着基数值的增长并没有变化。HLL 后面不同的数字代表着不同的精度，数字越大，精度越高，占用的空间也越大，可以认为 HLL 的空间占用只和精度成正相关。</p>
<p><strong>HLL算法原理感性认知</strong></p>
<p><img src="/images/blog/engineering/bigdata-image_2_4.png" alt="image_2_4.png"></p>
<p>HLL 算法的原理会涉及到比较多的数学知识，这边对这些数学原理和证明不会展开。举一个生活中的例子来帮助大家理解HLL算法的原理：比如你在进行一个实验，内容是不停地抛硬币，记录你连续抛到正面的次数（这是数学中的伯努利过程，感兴趣同学可以自行研究下）；如果你最多的连抛正面记录是3次，那可以想象你并没有做这个实验太多次，如果你最长的连抛正面记录是 20 次，那你可能进行了这个实验上千次。</p>
<p>一种理论上存在的情况是，你非常幸运，第一次进行这个实验就连抛了 20 次正面，我们也会认为你进行了很多次这个实验才得到了这个记录，这就会导致错误的预估；改进的方式是请 10 位同学进行这项实验，这样就可以观察到更多的样本数据，降低出现上述情况的概率。这就是 HLL 算法的核心思想。</p>
<p><strong>HLL算法具体实现</strong></p>
<p><img src="/images/blog/engineering/bigdata-image_2_5.png" alt="image_2_5.png"></p>
<p>HLL 会通过一个 hash 函数来求出集合中所有元素的 hash 值（二进制表示的 hash 值，就可以理解为一串抛硬币正反面结果的序列），得到一个 hash 值的集合，然后找出该 hash 值集合中，第一个 1 出现的最晚的位置。例如有集合为 [010, 100, 001], 集合中元素的第一个 1 出现的位置分别为 2, 1, 3，可以得到里面最大的值为 3，故该集合中第一个1出现的最晚的位置为 3。因为每个位置上出现1的概率都是 1/2，所以我们可以做一个简单的推断，该集合中有 8 个不重复的元素。</p>
<p>可以看到这种简单的推断计算出来集合的基数值是有较大的偏差的，那如何来减少偏差呢？正如我上面的例子里说的一样，HLL 通过多次的进行试验来减少误差。那它是如何进行多次的实验的呢？这里 HLL 使用了分桶的思想，上文中我们一直有提到一个精度的概念，比如说 HLL(10)，这个 10 代表的就是取该元素对应 Hash 值二进制的后 10 位，计算出记录对应的桶，桶中会记录一个数字，代表对应到该桶的 hash 值的第一个 1 出现的最晚的位置。如上图，该 hash 值的后 10 位的 hash 值是 0000001001，转成 10 进制是 9，对应第 9 号桶，而该 hash 值第一个 1 出现的位置是第 6 位，比原先 9 号桶中的数字大，故把 9 号桶中的数字更新为 6。可以看到桶的个数越多，HLL 算法的精度就越高，HLL(10) 有 1024(210) 个桶，HLL(16)有 65536(216) 个桶。同样的，桶的个数越多，所占用的空间也会越大。</p>
<p><img src="/images/blog/engineering/bigdata-image_2_6.png" alt="image_2_6.png"></p>
<p>刚才的例子我们省略了一些细节，为了让大家不至于迷失在细节中而忽视了重点，真实的 HLL 算法的完整描述见上图，这边的重点是计算桶中平均数时使用调和平均数。调和平均数的优点是可以过滤掉不健康的统计值，使用算术平均值容易受到极值的影响（想想你和马云的平均工资），而调和平均数的结果会倾向于集合中比较小的元素。HLL 论文中还有更多的细节和参数，这边就不一一细举，感兴趣的同学可以自己阅读下论文。</p>
<p><strong>HLL评估</strong></p>
<p><img src="/images/blog/engineering/bigdata-image_2_7.png" alt="image_2_7.png"></p>
<p>HLL 的误差分布服从正态分布，它的空间复杂度: O(m log2log2N), Ｎ 为基数, m 为桶个数。这边给大家推导一下它的空间复杂度，我有 264 个的不重复元素(Long. MAX_VALUE)，表达为二进制一个数是 64 位，这是第一重 log2, 那么第一个1最晚可能出现在第 64 位。64 需要 6 个 bit (26=64) 就可以存储，这是第二重 log2。如果精度为 10，则会有 1024 个桶，所以最外面还要乘以桶的个数。由于需要完整的遍历元素一遍，所以它的时间复杂度是一个线性的时间复杂度。</p>
<p><strong>在Kylin中的应用</strong></p>
<p><img src="/images/blog/engineering/bigdata-image_2_8.png" alt="image_2_8.png"></p>
<p>在 Kylin 中使用 HLL 非常简单，在编辑度量的页面选择 COUNT DISTINCT，Return Type 选为非 Precisely 的其他选项，大家根据自己的需求选择不同的精度就可以愉快地使用了。</p>
<p><strong>总结</strong></p>
<p><img src="/images/blog/engineering/bigdata-image_2_9.png" alt="image_2_9.png"></p>
<p>我们回到最开始的去重场景，看看使用了 Bitmap 和 HLL 会给我们带来什么增益：无优化 case 下，每个 item 对应的 user_id 就可以看成存储原始值的一个集合；在使用 Bitmap 优化的case 下，每个 item 对应的 user_id 就可以看成一个 Bitmap 实例，同理 HLL就是一个 HLL 的实例，Bitmap/HLL 实例占用的空间都会比直接存储原始值的集合要小，这就达到了我们开始提的减少 shuffle 数据量的需求。</p>
<p><strong>Q&amp;A</strong></p>
<p>Q：您好，问一下关于精确去重的问题， 我选择了非精确去重，最后的误差率有时候会比界面上提示的值要高一些，这是为什么？</p>
<p>A：首先 HLL 的误差分布服从正态分布，也就是说是在99%的情况下是这个误差，同时 HLL 对于基数比较低的情况，误差会偏高。如果你的基数比较低的话，我推荐使用精确去重。</p>
<p>Q：我想要了解一下 Bitmap 在 Kylin 中，它最终落盘在 HBase 里面是什么样子的？</p>
<p>A：在 HBase 中存储的当然都是 Bytes。这个问题其实就是 Bitmap 的序列化的形式，Roaring Bitmap提供了序列化和反序列化的实现，你也可以写自己的序列化/反序列化的实现。</p>
<p>Q：Roaring Bitmap 里这些 container 要我们自己手动的指定吗。</p>
<p>A：不需要，Roaring Bitmap 会自动选择使用哪个 Container。</p>
18:T2d8f,<h2>一个被忽略的问题</h2>
<p>我们从小被训练回答问题，却很少被训练<strong>判断自己是否真的理解了问题</strong>。</p>
<p>考试拿了高分，说明你记住了答案。工作中能复述方案，说明你读过文档。在技术讨论中抛出几个术语，说明你的信息输入渠道没有堵塞。但这些都不等于理解。</p>
<p><strong>「知道」和「理解」之间隔着一道鸿沟</strong>，大多数人终其一生都在鸿沟的这一侧，却误以为自己已经站在了那一侧。</p>
<p>理查德-费曼和埃隆-马斯克分别从不同的方向触碰了同一个问题的核心：如何确认自己真正理解了一件事？费曼给出的路径是「用最朴素的语言重新讲一遍」，马斯克反复强调的则是「回到最基本的事实，从那里开始推演」。</p>
<p>这两种方法看似属于不同的领域——一个是学习技巧，一个是决策框架——但它们的底层逻辑完全一致：<strong>拒绝在别人的结论上搭积木，而是自己从地基开始砌</strong>。</p>
<h2>费曼方法：用「教」来检验「懂」</h2>
<p>费曼方法的核心流程并不复杂，四步而已：</p>
<ol>
<li><strong>选择一个概念</strong>，写在纸上。</li>
<li><strong>假装你在教一个完全不懂的人</strong>，用最简单的语言把它解释清楚。</li>
<li><strong>发现卡住的地方</strong>——那些你不得不诉诸术语、模糊带过、或者干脆跳过的环节。</li>
<li><strong>回到原始材料</strong>，重新学习那些卡住的部分，然后再教一遍。</li>
</ol>
<p>这个方法之所以有效，是因为它利用了一个认知规律：<strong>语言是思维的探针</strong>。当你试图用简单的话描述一个概念时，你的大脑被迫进行一次完整的重建——不是从记忆中检索一个打包好的答案，而是从底层重新组装这个概念的逻辑链。</p>
<p>一个经典的例子：什么是温度？</p>
<p>大多数受过教育的人会回答「温度是衡量冷热程度的物理量」。这不能算错，但它是一个<strong>循环定义</strong>——你用「冷热」来定义温度，又用温度来定义冷热。如果你试着向一个十岁的孩子解释得再深一层，你会被迫触碰分子运动、动能、统计平均这些概念。你会发现，你对「温度」的理解可能止步于初中物理课本上的一句话，而那句话本身并没有让你真正理解任何东西。</p>
<p>费曼本人在教学中反复演示这一点。他在康奈尔和加州理工的讲座之所以成为传奇，不是因为他讲得简单，而是因为他能在「简单」和「准确」之间找到那个极其狭窄的通道。这条通道只有真正理解了底层原理的人才走得通。</p>
<h2>第一性原理：拆到不能再拆</h2>
<p>第一性原理思维的历史可以追溯到亚里士多德。他在《形而上学》中将其定义为「认识事物的最基本命题或假设，不能被省略或删除，也不能被违反」。</p>
<p>在现代语境中，这个概念被马斯克反复引用并推广。他的表述更直接：<strong>不要用类比来推理，要从最基本的物理事实出发，然后从那里一层一层推上来</strong>。</p>
<p>类比推理是人类大脑的默认模式。它高效、节能、在大多数日常场景中足够用。你看到别人做一件事成功了，你照搬过来，大概率也能过得去。但类比推理的问题在于，它<strong>继承了原始结论中的所有隐含假设</strong>，包括那些可能已经过时、错误、或者根本不适用于你当前情境的假设。</p>
<p>一个工程领域的例子：电池成本。</p>
<p>在 SpaceX 创立初期，火箭发射的市场价格是每公斤载荷约 10000 美元。如果用类比推理，结论就是「火箭发射就是这么贵」。但马斯克的做法是回到第一性原理：火箭的原材料是什么？铝合金、碳纤维、钛合金、燃料。这些材料在大宗商品市场上值多少钱？计算下来，原材料成本大约只占市场价格的 2%。那其余 98% 的成本来自哪里？来自低效的制造流程、一次性使用的设计、以及几十年来没有被挑战过的行业惯例。</p>
<p>这就是第一性原理的力量：<strong>当你拆到最底层，你会发现很多「不可能」其实只是「没人试过」</strong>。</p>
<h2>为什么它们是同一件事</h2>
<p>费曼方法和第一性原理看起来一个朝内（学习）、一个朝外（决策），但它们的内核完全相同。</p>
<p><strong>费曼方法的本质是：强迫你把知识拆解到最基本的组件，然后从那些组件重新组装。</strong> 你在「教」的过程中，实际上是在做一次知识的「逆向工程」——把打包好的结论拆回零件，检查每个零件是否你真的持有，还是只是以为自己持有。</p>
<p><strong>第一性原理的本质是：拒绝使用别人组装好的模块，坚持自己从原材料开始建造。</strong> 这个「建造」过程中的每一步推演，都要求你像费曼方法要求的那样——确认自己能用最简单的逻辑把这一步讲清楚。</p>
<p>换个角度说：</p>
<ul>
<li>费曼方法是<strong>第一性原理在学习场景下的操作手册</strong>。</li>
<li>第一性原理是<strong>费曼方法在决策场景下的哲学表达</strong>。</li>
</ul>
<p>它们共享同一个敌人：<strong>未经检验的继承性假设</strong>。无论是你从教科书上记住的公式，还是你从行业经验中继承的「最佳实践」，只要你没有亲自验证过它的每一个环节，它就可能是你认知结构中的一颗定时炸弹。</p>
<h2>「知道」与「理解」的断裂带</h2>
<p>为了更清楚地说明这个问题，有必要拆解一下「知道」和「理解」之间的结构性差异。</p>
<p><strong>「知道」是对结论的持有</strong>。你知道 E=mc^2，你知道水在 100 度沸腾，你知道微服务架构要做服务发现。这些都是别人推导出来的结论，你把它们存进了记忆。</p>
<p><strong>「理解」是对推导过程的持有</strong>。你不仅知道 E=mc^2，你还能解释为什么质量和能量之间存在等价关系，为什么系数恰好是光速的平方，这个公式是从哪些更基本的假设中推导出来的。你不仅知道水在 100 度沸腾，你还知道这个温度取决于大气压，知道沸腾的微观机制是什么，知道为什么在高原上水不到 100 度就开了。</p>
<p>两者的区别在日常生活中不明显，但在三个场景下会暴露出来：</p>
<p><strong>场景一：遇到异常。</strong> 当系统出现教科书上没写过的问题时，「知道」的人束手无策，因为他们的知识库里没有匹配的条目。「理解」的人可以从原理出发，推演出可能的原因。</p>
<p><strong>场景二：需要迁移。</strong> 当你需要把一个领域的知识应用到另一个领域时，「知道」的人只能做表面类比。「理解」的人能识别出底层结构的同构性，做出深层迁移。</p>
<p><strong>场景三：需要创新。</strong> 创新几乎必然意味着打破现有结论。如果你只是持有结论，打破它就等于失去一切。如果你持有推导过程，打破旧结论只是修改了某个中间环节，整个知识结构依然稳固。</p>
<h2>如何在实践中应用</h2>
<p>理论讲得再多，不转化为可操作的行为就是空谈。以下是几个经过验证的实践路径。</p>
<h3>写作即学习</h3>
<p>写文章是费曼方法最自然的实现形式。你不需要真的找一个人来「教」——把一个概念写成一篇文章，就是在强迫自己完成从拆解到重建的全过程。</p>
<p>关键不在于文章的文采，而在于<strong>逻辑链的完整性</strong>。每写一段，问自己：如果读者在这里问「为什么」，我能不能不查资料地回答？如果不能，说明这里有一个你尚未真正理解的环节。</p>
<h3>对每个「最佳实践」追问三层为什么</h3>
<p>在技术工作中，我们被大量的「最佳实践」和「设计模式」包围。这些东西不是不好，但它们是别人在特定上下文中推导出的结论。你需要追问：</p>
<ul>
<li><strong>第一层</strong>：这个实践要解决什么问题？</li>
<li><strong>第二层</strong>：这个问题为什么会存在？它的根因是什么？</li>
<li><strong>第三层</strong>：有没有可能从根因层面消除这个问题，使得这个实践本身变得不必要？</li>
</ul>
<p>大多数人停在第一层。能到第二层的人已经是少数。到第三层的人，往往就是那些能做出架构级创新的人。</p>
<h3>建立「解释清单」</h3>
<p>给自己列一份清单，写下你工作中经常使用但无法从零解释的概念。比如：</p>
<ul>
<li>什么是 TCP 的三次握手？为什么是三次而不是两次？</li>
<li>什么是数据库索引？B+树为什么比哈希表更适合做索引？</li>
<li>什么是分布式一致性？CAP 定理的证明过程是什么？</li>
</ul>
<p>这份清单就是你的「知识债务表」。每周花时间偿还一两笔债务，用费曼方法的标准来检验——如果你能向一个非技术人员把这个概念解释清楚（不丢失核心准确性），这笔债务就可以划掉。</p>
<h3>警惕「熟悉感陷阱」</h3>
<p>认知心理学中有一个现象叫「流畅性错觉」：当你重复接触某个信息时，你会对它产生熟悉感，而大脑会把这种熟悉感误判为「理解」。</p>
<p>你读了三遍设计模式的书，觉得自己「懂了」。但让你在白板上从零画出一个观察者模式的完整实现，你可能画不出来。这不是记忆力的问题，而是你从未真正拥有过那个知识——你只是和它见过面。</p>
<p>对抗这个陷阱的方法只有一个：<strong>主动检验</strong>。不要默读，要默写。不要点头，要动手。不要觉得「我看过」就等于「我会了」。</p>
<h2>理解是一种建造</h2>
<p>费曼晚年接受采访时说过一段话，大意是：这个世界上有两种知识，一种是「知道一个事物的名字」，一种是「真正理解这个事物」。前者是标签，后者是结构。</p>
<p>这个区分放在今天比以往任何时候都更重要。在信息过载的时代，我们比任何一代人都更容易「知道」——搜索引擎和大语言模型可以在几秒内给你任何问题的「答案」。但这恰恰让「理解」变得更稀缺、更有价值。</p>
<p><strong>理解不是接收，是建造。</strong> 就像你不能通过看别人砌墙来学会砌墙，你也不能通过阅读别人的结论来获得理解。你必须自己拿起砖块，感受灰浆的粘度，体会水平线的意义，亲手一层一层砌上去。</p>
<p>费曼方法和第一性原理给你的不是知识本身，而是<strong>一种确认「我是否真的理解了」的检验工具</strong>。它们不能替你学习，但它们能防止你在自欺中浪费时间。</p>
<p>在一个越来越依赖「快速获取答案」的世界里，愿意慢下来拆解、重建、验证的人，反而拥有了一种稀缺的竞争优势。因为大多数人的知识结构像是一堆借来的积木——看着像那么回事，但一推就倒。而从第一性原理出发、用费曼方法验证过的知识，是你自己浇筑的钢筋混凝土。</p>
<p>它不华丽，但它扛得住。</p>
5:["$","article",null,{"className":"min-h-screen","children":["$","div",null,{"className":"mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12","children":[["$","header",null,{"className":"mb-8","children":[["$","nav",null,{"className":"flex items-center gap-1 text-sm mb-4","children":[["$","$L13",null,{"href":"/blog/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"博客"}],["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"Engineering"}],[["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/data/page/1","className":"text-blue-600 hover:text-blue-700 transition-colors","children":"数据工程"}]]]}],["$","div",null,{"className":"flex items-center mb-6","children":["$","div",null,{"className":"inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal","children":[["$","svg",null,{"className":"w-4 h-4 mr-2 text-gray-400","fill":"none","stroke":"currentColor","viewBox":"0 0 24 24","children":["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","strokeWidth":2,"d":"M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z"}]}],["$","time",null,{"dateTime":"2025-03-26","children":"2025年03月26日"}]]}]}],["$","h1",null,{"className":"text-4xl font-bold text-gray-900 mb-6 text-center","children":"大数据分析常用去重算法分析之Bitmap篇"}],["$","div",null,{"className":"flex flex-wrap gap-2 mb-6 justify-center","children":[["$","$L13","大数据",{"href":"/blog/tag/%E5%A4%A7%E6%95%B0%E6%8D%AE/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"大数据"}],["$","$L13","去重算法",{"href":"/blog/tag/%E5%8E%BB%E9%87%8D%E7%AE%97%E6%B3%95/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"去重算法"}],["$","$L13","Bitmap",{"href":"/blog/tag/Bitmap/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"Bitmap"}]]}]]}],["$","div",null,{"className":"max-w-5xl mx-auto","children":["$","$L14",null,{"content":"$15"}]}],["$","$10",null,{"fallback":["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":"加载导航中..."}],"children":["$","$L16",null,{"globalNav":{"prev":{"slug":"engineering/data/大数据分析常用去重算法分析之HyperLogLog篇","title":"大数据分析常用去重算法分析之HyperLogLog篇","description":"上篇介绍了利用 Roaring Bitmap 来进行精确去重。虽然这种算法能大大地减少存储开销，但是随着数据量的增大，它依然面临着存储上的压力。在本篇推送中将要介绍的 HyperLogLog（下称 HLL）是一种非精确的去重算法，它的特点是具有非常优异的空间复杂度（几乎可以达到常数级别）。 ","pubDate":"2025-03-25","tags":["大数据","去重算法","HyperLogLog"],"heroImage":"$undefined","content":"$17"},"next":{"slug":"science/cognition/费曼方法与第一性原理：如何真正理解一件事","title":"费曼方法与第一性原理：如何真正理解一件事","description":"大多数人的学习停留在「记住结论」的层面，而真正的理解需要拆到不能再拆。费曼方法和第一性原理，本质上是同一种思维方式的两个切面。","pubDate":"2025-04-05","tags":["第一性原理","费曼方法","学习方法","认知科学"],"heroImage":"$undefined","content":"$18"}},"tagNav":{"大数据":{"prev":"$5:props:children:props:children:props:children:2:props:children:props:globalNav:prev","next":null},"去重算法":{"prev":"$5:props:children:props:children:props:children:2:props:children:props:globalNav:prev","next":null},"Bitmap":{"prev":null,"next":null}}}]}],["$","$L19",null,{}]]}]}]}]
8:null
c:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
7:null
a:{"metadata":[["$","title","0",{"children":"大数据分析常用去重算法分析之Bitmap篇 - Skyfalling Blog"}],["$","meta","1",{"name":"description","content":"去重分析在企业日常分析中的使用频率非常高，如何在大数据场景下快速地进行去重分析一直是一大难点。在近期的 Apache Kylin Meetup 北京站上，我们邀请到 Kyligence 大数据研发工程师陶加涛为大家揭开了大数据分析常用去重算法的神秘面纱。 Apache Kylin 作为目前唯一一个同..."}],["$","meta","2",{"property":"og:title","content":"大数据分析常用去重算法分析之Bitmap篇"}],["$","meta","3",{"property":"og:description","content":"去重分析在企业日常分析中的使用频率非常高，如何在大数据场景下快速地进行去重分析一直是一大难点。在近期的 Apache Kylin Meetup 北京站上，我们邀请到 Kyligence 大数据研发工程师陶加涛为大家揭开了大数据分析常用去重算法的神秘面纱。 Apache Kylin 作为目前唯一一个同..."}],["$","meta","4",{"property":"og:type","content":"article"}],["$","meta","5",{"property":"article:published_time","content":"2025-03-26"}],["$","meta","6",{"property":"article:author","content":"Skyfalling"}],["$","meta","7",{"name":"twitter:card","content":"summary"}],["$","meta","8",{"name":"twitter:title","content":"大数据分析常用去重算法分析之Bitmap篇"}],["$","meta","9",{"name":"twitter:description","content":"去重分析在企业日常分析中的使用频率非常高，如何在大数据场景下快速地进行去重分析一直是一大难点。在近期的 Apache Kylin Meetup 北京站上，我们邀请到 Kyligence 大数据研发工程师陶加涛为大家揭开了大数据分析常用去重算法的神秘面纱。 Apache Kylin 作为目前唯一一个同..."}],["$","link","10",{"rel":"shortcut icon","href":"/favicon.png"}],["$","link","11",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","link","12",{"rel":"icon","href":"/favicon.png"}],["$","link","13",{"rel":"apple-touch-icon","href":"/favicon.png"}]],"error":null,"digest":"$undefined"}
12:{"metadata":"$a:metadata","error":null,"digest":"$undefined"}
