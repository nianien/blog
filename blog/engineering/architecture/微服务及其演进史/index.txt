1:"$Sreact.fragment"
2:I[10616,["6874","static/chunks/6874-7791217feaf05c17.js","7177","static/chunks/app/layout-142e67ac4336647c.js"],"default"]
3:I[87555,[],""]
4:I[31295,[],""]
6:I[59665,[],"OutletBoundary"]
9:I[74911,[],"AsyncMetadataOutlet"]
b:I[59665,[],"ViewportBoundary"]
d:I[59665,[],"MetadataBoundary"]
f:I[26614,[],""]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/fffdcdb4fb651185.css","style"]
0:{"P":null,"b":"Dhq92JtvZPpMJAPdZEYU7","p":"","c":["","blog","engineering","architecture","%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%8F%8A%E5%85%B6%E6%BC%94%E8%BF%9B%E5%8F%B2",""],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","engineering/architecture/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%8F%8A%E5%85%B6%E6%BC%94%E8%BF%9B%E5%8F%B2","c"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/fffdcdb4fb651185.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"zh-CN","children":["$","body",null,{"className":"__className_f367f3","children":["$","div",null,{"className":"min-h-screen flex flex-col","children":[["$","$L2",null,{}],["$","main",null,{"className":"flex-1","children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"bg-[var(--background)]","children":["$","div",null,{"className":"mx-auto max-w-7xl px-6 py-12 lg:px-8","children":["$","p",null,{"className":"text-center text-xs leading-5 text-gray-400","children":["© ",2026," Skyfalling"]}]}]}]]}]}]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","engineering/architecture/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%8F%8A%E5%85%B6%E6%BC%94%E8%BF%9B%E5%8F%B2","c"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L5",null,["$","$L6",null,{"children":["$L7","$L8",["$","$L9",null,{"promise":"$@a"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","Yx1cMjUCBXF929a934VKrv",{"children":[["$","$Lb",null,{"children":"$Lc"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$Ld",null,{"children":"$Le"}]]}],false]],"m":"$undefined","G":["$f","$undefined"],"s":false,"S":true}
10:"$Sreact.suspense"
11:I[74911,[],"AsyncMetadata"]
13:I[6874,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],""]
14:I[32923,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
16:I[40780,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
19:I[85300,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
e:["$","div",null,{"hidden":true,"children":["$","$10",null,{"fallback":null,"children":["$","$L11",null,{"promise":"$@12"}]}]}]
15:T3ba4,<h3>1 传统单体系统介绍 <a href="#scroller-1" id="scroller-1"></a></h3>
<p>在很多项目的业务初期阶段，高速迭代上线是首要考虑的事情，对后期的容量预估、可扩展性和系统健壮性、高可用一般没有那么重视。但随着业务的发展，用户量、请求量的暴增，</p>
<p>发现原来的单体系统已经远远不满足需求了，特别是随着互联网整体的高速发展，对系统的要求越来越高。</p>
<p>但是物理服务器的CPU、内存、存储器、连接数等资源有限，单体系统能够承受的的QPS也是有限的，某个时段大量连接同时执行操作，会导致web服务和数据库服务在处理上遇到性能瓶颈。</p>
<p>为了解决这个问题，伟大的前辈们发扬了分而治之的思想，对大数据库、大表进行分割，可以参考我的《<a href="https://www.cnblogs.com/wzh2010/p/15049878.html">分库分表</a>》，以便实施更好的控制和管理。</p>
<p>同时创建多个服务实例，使用多台服务机进行CPU、内存、存储的分摊，提供更好的性能。</p>
<h4>1.1 单体系统的问题 <a href="#scroller-2" id="scroller-2"></a></h4>
<p>1、复杂性高：由于是一个单体的系统，所以整个系统的模块是耦合在一起的，模块的边界比较模糊、依赖关系错综复杂。功能的调整，容易带来不可知的影响和潜在的bug风险。</p>
<p>2、服务性能问题：单体系统遇到性能瓶颈问题，只能横向扩展，增加服务实例，进行负载均衡分担压力。无法纵向扩展，做模块拆分。</p>
<p>3、扩缩容能力受限：单体应用只能作为一个整体进行扩展，影响范围大，无法根据业务模块的需要进行单个模块的伸缩。</p>
<p>4、无法做故障隔离：当所有的业务功能模块都聚集在一个程序集当中，如果其中的某一个小的功能模块出现问题（如某个请求堵塞），那么都有可能会造成整个系统的崩溃。</p>
<p>5、发布的影响范围较大：每次发布都是整个系统进行发布，发布会导致整个系统的重启，对于大型的综合系统挑战比较大，如果将各个模块拆分，哪个部分做了修改，只发布哪个部分所在的模块即可。</p>
<h4>&#x20;<a href="#scroller-3" id="scroller-3"></a></h4>
<h4>1.2 单体系统的优点 <a href="#scroller-4" id="scroller-4"></a></h4>
<p>1、系统的简易性：系统语言风格、业务结构，接口格式均具有一致性，服务都是耦合在一起的，不存在各个业务通信问题。</p>
<p>2、易于测试：单体应用一旦部署，所有的服务或特性就都可以使用了，简化了测试过程，无需额外测试服务间的依赖，测试均可在部署完成后开始。</p>
<p>3、易于部署与升级：相对于微服务架构中的每个服务独立部署，单体系统只需将单个目录下的服务程序统一部署和升级。</p>
<p>4、较低的维护成本：只需维护单个系统即可。运维主要包括配置、部署、监控与告警和日志收集四大方面。相对于单体系统，微服务架构中的每个服务都需要独立地配置、部署、监控和日志收集，成本呈指数级增长。</p>
<h4>&#x20;<a href="#scroller-5" id="scroller-5"></a></h4>
<h4>1.3 单体服务到微服务的发展过程 <a href="#scroller-6" id="scroller-6"></a></h4>
<p>EUREKA的注册中心逐渐被ZooKeeper和Nacos等替代了。</p>
<p><img src="/images/blog/engineering/microservice-image_2_1.png" alt="image_2_1.png"></p>
<h3>2 关于微服务 <a href="#scroller-7" id="scroller-7"></a></h3>
<p>微服务是一种架构模式，是面向服务的体系结构（SOA）软件架构模式的一种演变，它提倡将单一应用程序划分成一组松散耦合的细粒度小型服务，辅助轻量级的协议，互相协调、互相配合，为用户提供最终价值。所以，微服务（或微服务架构）是一种云原生架构方法，其中单个应用程序由许多松散耦合且可独立部署的较小组件或服务组成。这些服务通常包含如下特点：</p>
<h4>2.1 单一职责 <a href="#scroller-8" id="scroller-8"></a></h4>
<p>微服务架构中的每个节点高度服务化，都是具有业务逻辑的，符合高内聚、低耦合原则以及单一职责原则的单元，包括数据库和数据模型；不同的服务通过“管道”的方式灵活组合，从而构建出庞大的系统。</p>
<h4>2.2 轻量级通信 <a href="#scroller-9" id="scroller-9"></a></h4>
<p>通过REST API模式或者RPC框架，实现服务间互相协作的轻量级通信机制。</p>
<h4>2.3 独立性 <a href="#scroller-10" id="scroller-10"></a></h4>
<p>在微服务架构中，每个服务都是独立的业务单元，与其他服务高度解耦，只需要改变当前服务本身，就可以完成独立的开发、测试、部署、运维。</p>
<h4>2.4 进程隔离 <a href="#scroller-11" id="scroller-11"></a></h4>
<p>在微服务架构中，应用程序由多个服务组成，每个服务都是高度自治的独立业务实体，可以运行在独立的进程中，不同的服务能非常容易地部署到不同的主机上，实现高度自治和高度隔离。进程的隔离，还能保证服务达到动态扩缩容的能力，业务高峰期自动增加服务资源以提升并发能力，业务低谷期则可自动释放服务资源以节省开销。</p>
<h4>2.5 混合技术栈和混合部署方式 <a href="#scroller-12" id="scroller-12"></a></h4>
<p>团队可以为不同的服务组件使用不同的技术栈和不同的部署方式（公有云、私有云、混合云）。</p>
<h4>2.6 简化治理 <a href="#scroller-13" id="scroller-13"></a></h4>
<p>组件可以彼此独立地进行扩缩容和治理，从而减少了因必须缩放整个应用程序而产生的浪费和成本，因为单个功能可能面临过多的负载。</p>
<h4>2.7 安全可靠，可维护。 <a href="#scroller-14" id="scroller-14"></a></h4>
<p>从架构上对运维提供友好的支撑，在安全、可维护的基础上规范化发布流程，支持数据存储容灾、业务模块隔离、访问权限控制、编码安全检测等。</p>
<h3>3 微服务演进史 <a href="#scroller-15" id="scroller-15"></a></h3>
<p>我们前面已经了解了微服务的概念，通过百度指数可以看出，从2012年之后，微服务的发展有显著的发展趋势。</p>
<p><img src="/images/blog/engineering/microservice-image_2_2.png" alt="image_2_2.png"></p>
<p>目前业内的微服务相关开发平台和框架还是比较多的，比如较早的Spring Cloud（使用Eureke做服务注册与发现，Ribbon做服务间负载均衡，Hystrix做服务容错保护），</p>
<p>阿里的Dubbo，微软的.Net体系微服务框架 Service Fabric，再到后来进阶的服务网格(Service Mesh,如 Istio、Linkerd）。</p>
<p>那从12年开始到现在，微服务到底发展到哪个阶段了，在各个阶段的进阶过程中，又有哪些的变化。所以我们需要了解微服务技术的历史发展脉络。</p>
<p>下面的内容参考了 <a href="https://philcalcado.com/">Phil Calçado</a>的文章<a href="https://philcalcado.com/2017/08/03/pattern_service_mesh.html">《Pattern: Service Mesh》</a>，从开发者的视角，详细分析了从微服务到Service Mesh技术的演进过程，这边做了进一步的整理和总结。</p>
<h4>3.1 第一阶：简单服务通信模块 <a href="#scroller-16" id="scroller-16"></a></h4>
<p>这是最初的模样，开发人员最开始的时候想象的两个服务间简单的通信模式，抽象表示如下，两个服务之间直接进行通信：</p>
<p><img src="/images/blog/engineering/microservice-image_2_3.png" alt="image_2_3.png"></p>
<p>3.2 第二阶：原始通信时代</p>
<p>上面的方式非常简单，但实际情况远比想象的复杂很多，通信需要底层字节码传输和电子信号的物理层来完成，在TCP协议出现之前，</p>
<p>服务需要自己处理网络通信所面临的丢包、错误、乱序、重试等一系列流控问题，因此服务实现中，除了业务逻辑外，还包含对网络传输问题的处理逻辑。</p>
<p><img src="/images/blog/engineering/microservice-image_2_4.png" alt="image_2_4.png"></p>
<h4>3.3 第三阶：TCP时代 <a href="#scroller-18" id="scroller-18"></a></h4>
<p>TCP协议的出现，避免了每个服务自己实现一套相似的网络传输处理逻辑，解决网络传输中通用的流量控制问题。</p>
<p>这时候我们把处理网络传输的能力下沉，从服务的实现中抽离出来，成为操作系统网络层的一部分。</p>
<p><img src="/images/blog/engineering/microservice-image_2_5.png" alt="image_2_5.png"></p>
<h4>3.4 第四阶：第一代微服务（Spring Cloud/RPC） <a href="#scroller-19" id="scroller-19"></a></h4>
<p>TCP出现之后，服务间的网络通信已经不是一个难题了，所以 GFS/BigTable/MapReduce 为代表的分布式系统得到了蓬勃的发展。</p>
<p>这时，分布式系统特有的通信语义又出现了，如服务注册与发现、负载均衡、熔断降级策略、认证和授权、端到端trace、日志与监控等，因此根据业务需求,完成一些通信语义的实现。</p>
<p><img src="/images/blog/engineering/microservice-image_2_6.png" alt="image_2_6.png"></p>
<h4>3.5 第五阶：第二代微服务 <a href="#scroller-20" id="scroller-20"></a></h4>
<p>为了避免每个服务都需要自己实现一套分布式系统通信的语义功能，随着技术的发展，一些面向微服务架构的通用开发框架出现了，如Twitter的<a href="https://finagle.github.io/">Finagle</a>、Facebook的<a href="https://code.facebook.com/posts/1503205539947302">Proxygen</a>以及Spring Cloud等，</p>
<p>这些框架实现了分布式系统通信需要的各种通用语义功能：如负载均衡和服务发现等，因此一定程度上屏蔽了这些通信细节，使得开发人员使用较少的框架代码就能开发出健壮的分布式系统。</p>
<p><img src="/images/blog/engineering/microservice-image_2_7.png" alt="image_2_7.png"></p>
<h4>3.6 第六阶：第一代Service Mesh <a href="#scroller-21" id="scroller-21"></a></h4>
<p>上面的第二代微服务框架目前看着挺完美了，但整套微服务框架其实是很复杂的，比如Spring Cloud，聚合了很多组件。所以在实践过程中，会发现有如下诸多问题：</p>
<ul>
<li>**侵入性强。**想要集成SDK的能力，除了需要添加相关依赖，业务层中入侵的代码、注解、配置，与治理层界限不清晰。</li>
<li>**升级成本高。**每次升级都需要业务应用修改SDK版本，重新进行功能回归测试，并对每一台服务进行部署上线，与快速迭代开发相悖。</li>
<li>**版本碎片化严重。**由于升级成本高，而中间件版本更新快，导致线上不同服务引用的SDK版本不统一、能力参差不齐，造成很难统一治理。</li>
<li>**中间件演变困难。**由于版本碎片化严重，导致中间件向前演进的过程中就需要在代码中兼容各种各样的老版本逻辑，带着&quot;枷锁”前行，无法实现快速迭代。</li>
<li>**内容多、门槛高。**依赖组件多，学习成本高，即使通用分布式系统屏蔽了很多的实现细节，我们引入微服务框架并熟练使用也是要花费巨大的精力的。</li>
<li>**治理功能不全。**不同于RPC框架，SpringCloud作为治理全家桶的典型，也不是万能的，诸如协议转换支持、多重授权机制、动态请求路由、故障注入、灰度发布等高级功能并没有覆盖到。</li>
<li>**无法实现真正意义上的语言无关性。**提供的框架一般只支持一种或几种语言，要将框架不支持的语言研发的服务也纳入微服务架构中，是比较有难度的。</li>
</ul>
<p>所以，第一代微服务架构 Service Mesh就产生了，它作为一个基础设施层，能够与业务解耦，主要解决复杂网络拓扑下微服务与微服务之间的通信，其实现形态一般为轻量级网络代理，并与应用以边车代理（SideCar）模式部署，同时对业务应用透明。</p>
<p><img src="/images/blog/engineering/microservice-image_2_8.png" alt="image_2_8.png"></p>
<p>SideCar将分布式服务的通信抽象为单独一层，需要和服务部署在一起，接管服务的流量，通过代理之间的通信间接完成服务之间的通信请求。</p>
<p>所以在这一层中它能够实现负载均衡、服务发现、认证授权、监控追踪、流量控制等分布式系统所需要的功能。</p>
<p><img src="/images/blog/engineering/microservice-image_2_9.png" alt="image_2_9.png"></p>
<p>如果我们从一个全局视角来看，绿色的为应用服务，蓝色的为SideCar，就会得到如下部署图：</p>
<p><img src="/images/blog/engineering/microservice-image_2_10.png" alt="image_2_10.png"></p>
<p>如果我们省略去服务，只看Service Mesh的代理边车的网格应该是这样的：</p>
<p><img src="/images/blog/engineering/microservice-image_2_11.png" alt="image_2_11.png"></p>
<p>流量经过的时候，会先被代理边车所劫持，然后再进入服务，所以它就是一个由若干服务代理所组成的错综复杂的网格。</p>
<h4>3.7 第七阶：第二代Service Mesh <a href="#scroller-22" id="scroller-22"></a></h4>
<p>第一代Service Mesh由一系列独立运行的单机代理服务构成，为了提供统一的上层运维入口，演化出了集中式的控制面板，我们称之为控制面（control plane）。</p>
<p>控制面和所有的数据面（data plane，即代理边车）进行交互，比如策略下发、数据采集等。这就是以Istio为代表的第二代Service Mesh。</p>
<p><img src="/images/blog/engineering/microservice-image_2_12.png" alt="image_2_12.png"></p>
<p>只包含控制面和数据面的 Service Mesh 服务网格全局结构图 如下：</p>
<p><img src="/images/blog/engineering/microservice-image_2_13.png" alt="image_2_13.png"></p>
<p>从上面的结构图可以看出，Service Mesh 的基础设施层主要分为两部分：控制平面与数据平面。当前流行的开源服务网格 Istio 和 Linkerd 都是这种构造。</p>
<p>控制平面的特点：</p>
<ul>
<li>不直接解析数据包。</li>
<li>与控制平面中的代理通信，下发策略和配置。</li>
<li>负责网络行为的可视化。</li>
<li>通常提供 API 或者命令行工具可用于配置版本化管理，便于持续集成和部署。</li>
</ul>
<p>数据平面的特点：</p>
<ul>
<li>通常是按照无状态目标设计的，但实际上为了提高流量转发性能，需要缓存一些数据，因此无状态也是有争议的。</li>
<li>直接处理入站和出站数据包，转发、路由、健康检查、负载均衡、认证、鉴权、产生监控数据等。</li>
<li>对应用来说透明，即可以做到无感知部署。</li>
</ul>
<p>到这一步我们大概了解了微服务架构的演进过程，也初步了解Service Mesh技术比较于传统的微服务架构有哪些优势。</p>
17:T6ae1,<h1>架构设计模板</h1>
<p>架构设计文档的价值不在于文档本身，而在于写文档的过程——它迫使我们在动手之前系统性地思考。一份好的设计文档能回答三个问题：<strong>为什么要做、怎么做、做到什么程度算完</strong>。</p>
<p>然而实际工作中，设计文档常见两类问题：要么太空——通篇架构图但缺乏落地细节；要么有遗漏——上线后才发现没考虑容灾、没定义回滚方案。根本原因是缺少一个结构化的思考框架。</p>
<p>本文提供一套经过实践验证的架构设计模板，包含 11 个维度。它的设计思路遵循一条主线：</p>
<blockquote>
<p><strong>问题驱动 → 方案设计 → 工程落地</strong></p>
<ul>
<li>问题驱动（第 1 章）：搞清楚为什么要做，边界在哪</li>
<li>方案设计（第 2-9 章）：从架构到细节，把方案想透</li>
<li>工程落地（第 10-11 章）：怎么部署、怎么分期交付</li>
</ul>
</blockquote>
<p>这 11 个维度既可以作为写设计文档的提纲，也可以当作评审时的 Checklist。每个维度给出<strong>要回答的关键问题</strong>和<strong>具体交付物</strong>，文末附可直接复用的 Markdown 模板。</p>
<hr>
<h3>1. 需求介绍</h3>
<p>需求介绍的核心任务是把「为什么要做」讲清楚。它不是产品需求文档（PRD）的复述，而是从技术视角回答：现状有什么问题、我们打算怎么解决、做到什么程度算成功。</p>
<p><strong>要回答的关键问题：</strong></p>
<ul>
<li><strong>现状与痛点</strong>：当前系统/流程存在什么问题？对业务造成了哪些可量化的影响（故障频率、延迟、人工成本等）？</li>
<li><strong>目标与范围</strong>：新方案要解决哪些问题？同样重要的是——不解决哪些问题？明确的边界能防止需求蔓延。</li>
<li><strong>核心场景</strong>：列出 3-5 个最重要的使用场景。场景是连接需求与设计的桥梁——拆得越细，后面的设计越不容易遗漏。</li>
<li><strong>干系人</strong>：谁是用户？谁会被改动影响？谁需要配合？</li>
<li><strong>约束条件</strong>：时间窗口、预算、技术栈限制、合规要求等。</li>
<li><strong>验收标准</strong>：用可量化的指标定义「做完了」，如 P99 延迟 &lt; 200ms、可用性 &gt; 99.95%、数据一致性延迟 &lt; 1s。</li>
</ul>
<p><strong>实践建议：</strong></p>
<p>用「场景走查」来验证需求完整性——把每个核心场景从头到尾走一遍，记录每一步涉及的系统、数据和人员。走查过程中自然会暴露出遗漏的约束和边界条件。</p>
<p><strong>交付物：</strong> 需求背景文档（含场景列表、干系人矩阵、约束条件、验收标准）</p>
<hr>
<h3>2. 架构总览</h3>
<p>架构总览是整个设计文档的「地图」。评审者和后续加入的开发人员，通常最先看的就是这一章。它需要回答：系统长什么样、分几块、各块之间怎么协作。</p>
<p><strong>多视角描述架构：</strong></p>
<p>业界常用 <a href="https://en.wikipedia.org/wiki/4%2B1_architectural_view_model">4+1 视图模型</a> 或 <a href="https://c4model.com/">C4 模型</a> 来组织架构描述。对于多数项目，以下三个视角已经够用：</p>
<ul>
<li><strong>概念模型</strong>：系统中有哪些核心领域概念？它们之间的关系是什么？概念模型是整个设计的骨架。看似简单的概念定义——比如「部署包 = 介质包 + 配置」——往往直接决定了后续的技术设计。建议用 UML 类图或 ER 图表达。</li>
<li><strong>逻辑架构图</strong>：系统分几层？每层有哪些模块？模块之间的依赖方向是什么？建议按能力分层（接入层 → 业务逻辑层 → 领域服务层 → 基础设施层），并标注每个模块的核心职责。</li>
<li><strong>系统上下文图</strong>（System Context）：聚焦系统边界——哪些能力自研，哪些依赖外部系统？与周边系统的交互协议和数据格式是什么？这张图对于跨团队协作尤其关键。</li>
</ul>
<p><strong>画图原则：</strong></p>
<p>架构图的唯一标准是<strong>易懂</strong>。一些实用建议：</p>
<ul>
<li>每张图只表达一个层次的信息，避免在同一张图中混合部署细节和业务逻辑</li>
<li>用颜色/形状区分不同类型的组件（自研服务、外部依赖、中间件、数据存储）</li>
<li>标注关键数据流的方向和协议</li>
<li>推荐工具：Excalidraw（轻量手绘风）、draw.io（标准流程图）、PlantUML（文本生成图）</li>
</ul>
<p><strong>实践建议：</strong></p>
<p>好的架构图是改出来的，不是一次画对的。建议在正式评审前做一次小范围宣讲，一是统一理解，二是通过反馈优化设计。</p>
<p><strong>交付物：</strong> 概念模型图、逻辑架构图、系统上下文图</p>
<hr>
<h3>3. 核心流程</h3>
<p>架构总览展示了系统的静态结构，核心流程则展示系统的动态行为——各组件如何协作完成具体业务场景。<strong>架构图 + 时序图是设计评审中最有价值的两张图</strong>，前者回答「是什么」，后者回答「怎么运转」。</p>
<p><strong>场景驱动的梳理方法：</strong></p>
<ol>
<li><strong>列出核心场景</strong>：从用户/调用方的视角，挑出最重要的 3-5 个场景（通常就是需求介绍中的核心场景）</li>
<li><strong>画出 Happy Path</strong>：每个场景走一遍完整调用链路，用时序图（Sequence Diagram）标注参与方、调用顺序、数据流向</li>
<li><strong>标注关键路径</strong>：在时序图上标记性能瓶颈点、状态变更点、数据持久化点</li>
<li><strong>补充异常流程</strong>：这是最容易被忽略但最重要的部分——下游超时怎么办？重试是否幂等？消息丢了怎么补偿？数据不一致怎么修复？</li>
</ol>
<p><strong>常见陷阱：</strong></p>
<p>很多设计文档只画了「晴天场景」，对异常路径一笔带过。但线上故障绝大多数发生在异常分支。建议对每个核心流程至少补充以下异常场景：</p>
<ul>
<li>依赖服务不可用</li>
<li>网络超时 / 部分失败</li>
<li>数据不一致（如消息乱序、重复投递）</li>
<li>资源耗尽（连接池满、磁盘满、内存 OOM）</li>
</ul>
<p><strong>交付物：</strong> 核心场景的时序图（含 Happy Path 和关键异常流程）</p>
<hr>
<h3>4. 详细设计</h3>
<p>详细设计是对架构中复杂组件的「放大镜」。不需要面面俱到，但对核心模块和高风险模块必须写清楚。</p>
<p><strong>通常涵盖以下几类：</strong></p>
<p><strong>数据模型</strong></p>
<ul>
<li>核心表结构设计（字段、类型、约束）</li>
<li>索引策略（查询模式决定索引设计，而非反过来）</li>
<li>数据生命周期：冷热分离策略、归档/清理规则、数据保留期限</li>
<li>数据量评估：初始数据量、增长速率、单表上限</li>
</ul>
<p><strong>接口契约</strong></p>
<ul>
<li>对外 API 定义：路径、方法、入参、出参、错误码、版本策略</li>
<li>如涉及多系统协作，还需定义 SPI（扩展点接口）——即「我提供框架，你来实现具体逻辑」的扩展机制</li>
<li>接口幂等性设计：哪些接口需要幂等？幂等 Key 怎么生成？</li>
<li>建议遵循 <a href="https://www.openapis.org/">OpenAPI</a> 规范，便于自动生成文档和客户端代码</li>
</ul>
<p><strong>状态机</strong></p>
<ul>
<li>如业务有复杂状态流转（订单、审批、工单等），一张状态机图比大段文字清晰得多</li>
<li>明确每个状态转换的触发条件、执行动作和失败回退</li>
</ul>
<p><strong>关键算法/策略</strong></p>
<ul>
<li>路由策略（一致性 Hash、权重轮询等）</li>
<li>调度算法（优先级队列、公平调度等）</li>
<li>限流算法（令牌桶、滑动窗口等）</li>
</ul>
<p><strong>实践建议：</strong></p>
<p>详细设计不必一次写完，可以在开发过程中迭代补充。但有两样东西必须在写代码之前定好：<strong>接口契约</strong>和<strong>数据模型</strong>——它们的变更成本最高，影响面最广。</p>
<p><strong>交付物：</strong> 数据模型设计、接口文档（API/SPI）、状态机图（如有）、关键算法说明</p>
<hr>
<h3>5. 高可用设计</h3>
<p>高可用设计回答一个核心问题：<strong>系统的某个部分挂了，整体还能不能用？</strong> 这是从「能跑」到「能扛」的关键一步。</p>
<p><strong>冗余与容灾</strong></p>
<ul>
<li>服务层：是否多实例部署？是否跨可用区（AZ）部署？单个 AZ 故障时服务是否仍然可用？</li>
<li>数据层：数据库是否有主从/多副本？故障切换是自动还是手动？RPO（数据丢失量）和 RTO（恢复时间）的目标是多少？</li>
<li>降级方案：核心链路和非核心链路是否隔离？当非核心依赖不可用时，核心功能是否能继续运行？降级是自动触发还是手动开关？</li>
</ul>
<p><strong>故障检测与自愈</strong></p>
<ul>
<li>健康检查：Liveness Probe（进程是否存活）和 Readiness Probe（是否可接收流量）分别怎么设计？</li>
<li>熔断策略：使用什么熔断器（如 Sentinel、Resilience4j）？熔断阈值和恢复策略如何配置？</li>
<li>限流策略：在哪一层限流（网关层 / 应用层）？限流粒度是什么（全局 / 租户 / 接口）？</li>
<li>隔离机制：线程池隔离、信号量隔离还是进程隔离？</li>
</ul>
<p><strong>数据一致性</strong></p>
<ul>
<li>一致性模型选择：强一致（CP）还是最终一致（AP）？在什么场景下可以接受最终一致？</li>
<li>跨服务一致性方案：Saga、TCC、本地消息表、事务消息等，各有适用场景。选择依据是什么？</li>
<li>补偿机制：当一致性被破坏时，如何检测和修复？是否需要对账任务？</li>
</ul>
<p><strong>可观测性</strong></p>
<ul>
<li>监控三支柱：Metrics（指标）、Logging（日志）、Tracing（链路追踪）各自的方案是什么？</li>
<li>关键监控指标按 <a href="https://grafana.com/blog/2018/08/02/the-red-method-how-to-instrument-your-services/">RED 方法</a> 分类：Rate（请求速率）、Errors（错误率）、Duration（延迟分布）</li>
<li>告警规则：分级（P0/P1/P2/P3）、阈值、通知渠道、响应 SLA</li>
<li>故障定位：如何从告警快速定位到根因？是否有 Runbook（故障手册）？</li>
</ul>
<p><strong>交付物：</strong> 高可用方案说明（含冗余策略、故障恢复流程、可观测性方案、告警清单）</p>
<hr>
<h3>6. 高性能设计</h3>
<p>高性能设计的核心原则是<strong>先定目标，再找瓶颈，最后谈优化</strong>。没有量化目标的优化是盲目的。</p>
<p><strong>性能目标</strong></p>
<ul>
<li>QPS/TPS 目标：峰值多少？日常多少？需要预留多少 Buffer？</li>
<li>延迟目标：P50、P95、P99 分别是多少？（只看平均值会掩盖长尾问题）</li>
<li>数据量级：当前数据量多大？未来 1-3 年的增长预期？</li>
</ul>
<p><strong>瓶颈分析</strong></p>
<ul>
<li>识别系统是 CPU 密集型还是 IO 密集型</li>
<li>找出关键路径上的瓶颈点：数据库查询、外部 API 调用、序列化/反序列化、锁竞争等</li>
<li>使用 <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl 定律</a> 评估优化收益——优化非瓶颈环节收效甚微</li>
</ul>
<p><strong>分层优化策略</strong></p>
<table>
<thead>
<tr>
<th>层次</th>
<th>常用手段</th>
</tr>
</thead>
<tbody><tr>
<td>接入层</td>
<td>CDN 加速、负载均衡、连接复用、协议优化（HTTP/2、gRPC）</td>
</tr>
<tr>
<td>应用层</td>
<td>本地缓存（Caffeine）、分布式缓存（Redis）、异步化（MQ）、批量合并、并行调用</td>
</tr>
<tr>
<td>数据层</td>
<td>读写分离、分库分表、索引优化、热点数据隔离、查询结果缓存</td>
</tr>
<tr>
<td>基础设施</td>
<td>水平扩容、弹性伸缩（HPA）、资源池化、JVM/Runtime 调优</td>
</tr>
</tbody></table>
<p><strong>压测验证</strong></p>
<ul>
<li>工具选择：JMeter（全功能）、wrk/hey（轻量 HTTP）、k6（脚本化场景）</li>
<li>压测策略：阶梯加压找出拐点，而非直接打满</li>
<li>压测环境与生产环境的差异要记录清楚（机器规格、数据量、网络拓扑）</li>
<li>压测报告要包含：吞吐量曲线、延迟分布、资源利用率、瓶颈定位</li>
</ul>
<p><strong>交付物：</strong> 性能目标定义、瓶颈分析、分层优化方案、压测计划</p>
<hr>
<h3>7. 可扩展性设计</h3>
<p>可扩展性回答两个问题：<strong>加功能容不容易（业务扩展性）<strong>和</strong>加机器扛不扛得住更多量（容量扩展性）</strong>。</p>
<p><strong>业务扩展性</strong></p>
<p>好的扩展性设计遵循 <a href="https://en.wikipedia.org/wiki/Open%E2%80%93closed_principle">开闭原则</a>——对扩展开放，对修改关闭。具体评判标准：</p>
<blockquote>
<p>新增一类需求时，是加配置就行，还是要写代码？写代码的话，是新增代码就行，还是要改已有代码？越往前者靠，扩展性越好。</p>
</blockquote>
<p>常见的扩展性手段：</p>
<ul>
<li><strong>插件化 / SPI 机制</strong>：通过接口抽象 + 实现注册，新增场景只需新增实现类</li>
<li><strong>策略模式 + 配置驱动</strong>：将业务规则外化为配置，通过策略分发路由到不同处理逻辑</li>
<li><strong>事件驱动</strong>：核心流程产出事件，扩展功能订阅事件，彼此解耦</li>
<li><strong>合理的领域划分</strong>：按业务能力（而非技术层次）划分模块，模块间通过明确的接口通信</li>
</ul>
<p><strong>容量扩展性</strong></p>
<ul>
<li>服务层：是否无状态？能否直接水平扩容？如果有状态（如本地缓存、WebSocket 长连接），扩容时如何处理？</li>
<li>数据层：数据库如何扩展？是否预留了分片键？分片策略是什么？</li>
<li>消息队列：Partition 数量是否支持后续扩展？Consumer Group 的 Rebalance 策略是什么？</li>
<li>单点瓶颈：系统中是否存在不可水平扩展的单点？如何规避或缓解？</li>
</ul>
<p><strong>交付物：</strong> 扩展点清单、领域划分图、容量扩展方案</p>
<hr>
<h3>8. 安全设计</h3>
<p>安全设计即使当前没有明确需求，也应作为 Checklist 在评审中显式确认。写「经评估，本期暂不涉及」远好过完全不提——前者是有意识的决策，后者是遗漏。</p>
<p><strong>认证与授权</strong></p>
<ul>
<li>认证方案：JWT、OAuth 2.0、Session、OIDC？Token 的签发、刷新和吊销机制？</li>
<li>授权模型：RBAC（基于角色）、ABAC（基于属性）？权限粒度到什么级别（菜单/按钮/数据行）？</li>
<li>服务间认证：内部服务间调用是否需要认证？方案是什么（mTLS、服务账号、JWT 传递）？</li>
</ul>
<p><strong>数据安全</strong></p>
<ul>
<li>敏感数据识别：哪些字段属于 PII（个人可识别信息）？如密码、手机号、身份证号、银行卡号</li>
<li>存储加密：敏感字段是否加密存储？加密算法和密钥管理方案？</li>
<li>数据脱敏：日志、监控、非生产环境中的敏感数据是否脱敏？脱敏规则是什么？</li>
<li>数据合规：是否涉及 GDPR、个人信息保护法等合规要求？数据跨境传输策略？</li>
</ul>
<p><strong>传输安全</strong></p>
<ul>
<li>是否全链路 HTTPS？TLS 版本和加密套件？</li>
<li>内部服务通信是否加密（mTLS）？证书管理方案？</li>
</ul>
<p><strong>审计与防护</strong></p>
<ul>
<li>审计日志：哪些关键操作需要记录？日志包含哪些字段（who/when/what/where）？日志的保留期限？</li>
<li>防攻击：SQL 注入、XSS、CSRF、SSRF 的防护措施？是否使用 WAF？</li>
<li>限流防刷：敏感接口（登录、短信验证码、支付）是否有专门的限流策略？</li>
</ul>
<p><strong>交付物：</strong> 安全设计说明（含认证方案、数据分级与保护策略、审计要求）</p>
<hr>
<h3>9. 技术选型</h3>
<p>技术选型是影响最深远的决策之一——选错了，后续所有人都在还债。好的选型不追求「最先进」，而追求「最合适」。</p>
<p><strong>要回答的关键问题：</strong></p>
<ul>
<li>核心语言和框架的选择依据是什么？</li>
<li>中间件的选择（消息队列、缓存、数据库、搜索引擎等）基于什么考量？</li>
<li>是否做过技术预研或 PoC 验证？结论是什么？</li>
</ul>
<p><strong>选型的评估维度：</strong></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>要点</th>
</tr>
</thead>
<tbody><tr>
<td>功能匹配度</td>
<td>能否满足当前和可预见的未来需求？</td>
</tr>
<tr>
<td>生产成熟度</td>
<td>是否有大规模生产验证？社区活跃度和生态完善度？</td>
</tr>
<tr>
<td>团队匹配度</td>
<td>团队是否熟悉？学习曲线和上手成本？——技术再好，团队用不起来也白搭</td>
</tr>
<tr>
<td>运维成本</td>
<td>部署复杂度、监控支持、故障排查难度、升级迁移成本</td>
</tr>
<tr>
<td>许可证合规</td>
<td>开源许可证是否满足商业需求（注意 AGPL、SSPL 等传染性许可证）？</td>
</tr>
</tbody></table>
<p><strong>实践建议：</strong></p>
<ul>
<li>用 <a href="https://adr.github.io/">ADR（Architecture Decision Records）</a> 记录每个关键技术决策的上下文、选项、决策和后果，方便后续团队理解「当初为什么这么选」</li>
<li>如有多个候选方案，列对比表时不要超过 5 个维度，聚焦最关键的差异点</li>
<li>团队编码规范、Git 工作流、Code Review 流程等工程规范也可以写在这一节</li>
</ul>
<p><strong>交付物：</strong> 技术栈清单、关键选型对比表（或 ADR）、工程规范说明</p>
<hr>
<h3>10. 部署方案</h3>
<p>部署方案不只是「怎么把服务跑起来」，更要回答：怎么安全地发布变更、出了问题怎么快速回滚。</p>
<p><strong>环境规划</strong></p>
<ul>
<li>环境定义：开发（Dev）→ 测试（Test/QA）→ 预发（Staging）→ 生产（Production）</li>
<li>环境隔离：各环境之间如何隔离（独立集群 / Namespace 隔离 / 标签路由）？</li>
<li>配置管理：配置与代码是否分离？环境差异（副本数、资源配额、域名、Feature Flag）通过什么机制管理？推荐 ConfigMap + 密钥管理服务（如 Vault）</li>
</ul>
<p><strong>发布策略</strong></p>
<ul>
<li>滚动更新（Rolling Update）：适合大多数无状态服务，K8s 原生支持</li>
<li>蓝绿部署（Blue-Green）：适合需要快速切换和回滚的场景，需双倍资源</li>
<li>灰度发布（Canary）：适合风险较高的变更，按流量比例 / 用户标签 / 地域逐步放量</li>
<li>发布过程中的健康检查（Readiness Gate）和自动回滚（基于错误率 / 延迟的 Rollback 策略）</li>
</ul>
<p><strong>回滚方案</strong></p>
<ul>
<li>代码回滚流程：谁触发？如何执行？回滚后是否需要通知下游？</li>
<li>数据库回滚：Schema 变更是否向下兼容？是否准备了回滚脚本？建议采用 Expand-Contract 模式处理不兼容变更</li>
<li>配置回滚：配置变更是否有版本化和快速回滚能力？</li>
</ul>
<p><strong>资源规划</strong></p>
<ul>
<li>每个服务的 CPU / 内存 Request 和 Limit 如何设定？建议基于压测数据而非经验估算</li>
<li>是否需要 HPA（Horizontal Pod Autoscaler）？扩缩容的指标和阈值？</li>
<li>存储方案：云盘（持久化）、对象存储（文件/图片）、本地盘（临时缓存）分别用在哪里？</li>
</ul>
<p><strong>交付物：</strong> 部署架构图（物理视图）、环境配置清单、发布策略说明、回滚 Runbook</p>
<hr>
<h3>11. 架构演进规划</h3>
<p>大型项目不可能一步到位，分阶段交付是常态。架构演进规划的目标是让团队在每个阶段都知道做什么、为什么先做这个、后面还要做什么。</p>
<p><strong>MVP 定义</strong></p>
<ul>
<li>最小可用版本的范围是什么？用<strong>场景</strong>定义 MVP——用户能跑通哪些核心场景，就是 MVP</li>
<li>建议在 MVP 之前做一次<strong>架构原型验证</strong>（Walking Skeleton）：用最小的端到端场景跑通整个架构，验证核心技术方案的可行性。这一步能在早期暴露架构层面的问题，避免后期大面积返工</li>
</ul>
<p><strong>里程碑规划</strong></p>
<ul>
<li>按阶段拆分：每期交付什么功能？交付标准是什么？</li>
<li>阶段间的技术依赖：前一期没完成是否会阻塞后一期？有没有可以并行的工作？</li>
<li>建议用甘特图或里程碑表格可视化，让进度一目了然</li>
</ul>
<p><strong>技术债务管理</strong></p>
<ul>
<li>当前设计中有哪些已知的妥协和 TODO？为什么现在不做？</li>
<li>每笔技术债务的「利息」是什么——不偿还会导致什么后果？</li>
<li>计划在什么时间点偿还？建议将技术债务纳入迭代计划，而非无限期搁置</li>
</ul>
<p><strong>团队分工</strong></p>
<ul>
<li>各模块由谁负责？模块间的接口由谁定义、谁联调、谁验收？</li>
<li>团队能力与分工是否匹配？是否需要安排技术预研或培训？</li>
<li>再好的架构，如果不考虑团队的实际能力，也未必落得了地</li>
</ul>
<p><strong>交付物：</strong> MVP 范围定义、里程碑计划表、技术债务台账、团队分工矩阵</p>
<hr>
<h2>附：可复用的架构设计文档模板</h2>
<p>以下模板可直接复制使用，按实际情况填写或删除不需要的章节。</p>
<pre><code class="language-markdown"># [系统名称] 架构设计文档

&gt; 作者：xxx | 日期：yyyy-MM-dd | 版本：v1.0 | 状态：Draft / In Review / Approved

---

## 1. 需求介绍

### 1.1 现状与痛点
&lt;!-- 当前系统存在什么问题？可量化的业务影响？ --&gt;

### 1.2 目标与范围
&lt;!-- 要解决什么？不解决什么（明确边界）？ --&gt;

### 1.3 核心场景
| # | 场景名称 | 场景描述 | 优先级 |
|---|----------|----------|--------|
| 1 |          |          |        |

### 1.4 干系人
| 角色 | 人员 | 职责 |
|------|------|------|
|      |      |      |

### 1.5 约束条件
&lt;!-- 时间、预算、技术栈、合规等 --&gt;

### 1.6 验收标准
| 指标 | 目标值 | 度量方式 |
|------|--------|----------|
|      |        |          |

---

## 2. 架构总览

### 2.1 概念模型
&lt;!-- 核心领域概念及其关系（附图） --&gt;

### 2.2 逻辑架构图
&lt;!-- 系统分层、模块划分、依赖关系（附图） --&gt;

### 2.3 系统上下文
&lt;!-- 与周边系统的交互：协议、数据格式、调用方向（附图） --&gt;

---

## 3. 核心流程

### 3.1 场景一：[场景名称]

**Happy Path：**
&lt;!-- 时序图 --&gt;

**异常流程：**
&lt;!-- 超时 / 下游不可用 / 数据不一致 的处理方式 --&gt;

### 3.2 场景二：[场景名称]
&lt;!-- 同上 --&gt;

---

## 4. 详细设计

### 4.1 数据模型
&lt;!-- 核心表结构、索引策略、数据生命周期 --&gt;

### 4.2 接口契约
&lt;!-- API / SPI 定义：路径、方法、入参、出参、错误码 --&gt;

### 4.3 状态机
&lt;!-- 状态流转图（如有） --&gt;

### 4.4 关键算法
&lt;!-- 核心算法/策略的描述 --&gt;

---

## 5. 高可用设计

### 5.1 冗余与容灾
&lt;!-- 多实例 / 跨 AZ / 主从切换 / 降级方案 --&gt;

### 5.2 故障检测与自愈
&lt;!-- 健康检查 / 熔断 / 限流 / 隔离 --&gt;

### 5.3 数据一致性
&lt;!-- CP vs AP 选择 / 跨服务一致性方案 / 补偿机制 --&gt;

### 5.4 可观测性
| 类型 | 指标/工具 | 告警阈值 | 响应 SLA |
|------|-----------|----------|----------|
| Metrics |        |          |          |
| Logging |        |          |          |
| Tracing |        |          |          |

---

## 6. 高性能设计

### 6.1 性能目标
| 指标 | 目标值 |
|------|--------|
| QPS  |        |
| P99  |        |
| 数据量级 |    |

### 6.2 瓶颈分析
&lt;!-- 关键路径上的瓶颈点及根因分析 --&gt;

### 6.3 优化方案
&lt;!-- 按接入层 / 应用层 / 数据层 / 基础设施分层说明 --&gt;

### 6.4 压测计划
&lt;!-- 工具、场景、环境差异、通过标准 --&gt;

---

## 7. 可扩展性设计

### 7.1 业务扩展性
&lt;!-- 扩展点清单 / SPI 机制 / 领域划分 --&gt;

### 7.2 容量扩展性
&lt;!-- 无状态服务扩容 / 有状态组件扩展 / 单点瓶颈规避 --&gt;

---

## 8. 安全设计

- **认证与授权**：
- **数据安全**：
- **传输安全**：
- **审计日志**：
- **防攻击**：

&lt;!-- 如本期不涉及，请注明「经评估，本期暂不涉及」并说明原因 --&gt;

---

## 9. 技术选型

| 类别 | 选型 | 备选方案 | 选择依据 |
|------|------|----------|----------|
|      |      |          |          |

### 关键决策记录（ADR）
&lt;!-- 对于有争议的选型，记录上下文、选项、决策和后果 --&gt;

---

## 10. 部署方案

### 10.1 环境规划
| 环境 | 集群/NS | 副本数 | 资源配额 | 域名 |
|------|---------|--------|----------|------|
| Dev  |         |        |          |      |
| Test |         |        |          |      |
| Staging |      |        |          |      |
| Prod |         |        |          |      |

### 10.2 发布策略
&lt;!-- 滚动更新 / 蓝绿 / 灰度，以及健康检查和自动回滚机制 --&gt;

### 10.3 回滚方案
&lt;!-- 代码回滚流程 / 数据库兼容性 / 配置回滚 --&gt;

---

## 11. 架构演进规划

### 11.1 MVP 定义
&lt;!-- 第一个版本的最小可用范围（用场景定义） --&gt;

### 11.2 里程碑
| 阶段 | 时间 | 交付内容 | 验收标准 | 依赖 |
|------|------|----------|----------|------|
|      |      |          |          |      |

### 11.3 技术债务
| 债务 | 产生原因 | 影响（利息） | 计划偿还时间 |
|------|----------|--------------|--------------|
|      |          |              |              |

### 11.4 团队分工
| 模块 | 负责人/团队 | 上下游依赖 |
|------|-------------|------------|
|      |             |            |

---

## 附录

### 术语表
| 术语 | 定义 |
|------|------|
|      |      |

### 参考文档
&lt;!-- 相关 PRD、技术预研报告、竞品分析等 --&gt;

### 变更记录
| 版本 | 日期 | 变更人 | 变更内容 |
|------|------|--------|----------|
| v1.0 |      |        | 初稿     |
</code></pre>
18:T4fc2,<h3>1 微服务优势与挑战 <a href="#scroller-1" id="scroller-1"></a></h3>
<h4>1.1 微服务的优势 <a href="#scroller-2" id="scroller-2"></a></h4>
<p><strong>1.1.1 单一职责</strong></p>
<p>微服务架构中的每个节点高度服务化，都是具有业务逻辑的，符合高内聚、低耦合原则以及单一职责原则的单元，包括数据库和数据模型；不同的服务通过“管道”的方式灵活组合，从而构建出庞大的系统。</p>
<p><strong>1.1.2 轻量级通信</strong></p>
<p>通过REST API模式或者RPC框架，事件流和消息代理的组合相互通信，实现服务间互相协作的轻量级通信机制。</p>
<p><strong>1.1.3 独立性</strong></p>
<p>在微服务架构中，每个服务都是独立的业务单元，与其他服务高度解耦，只需要改变当前服务本身，就可以完成独立的开发、测试、部署、运维。</p>
<p><strong>1.1.4 进程隔离</strong></p>
<p>在微服务架构中，应用程序由多个服务组成，每个服务都是高度自治的独立业务实体，可以运行在独立的进程中，不同的服务能非常容易地部署到不同的主机上，实现高度自治和高度隔离。进程的隔离，还能保证服务达到动态扩缩容的能力，业务高峰期自动增加服务资源以提升并发能力，业务低谷期则可自动释放服务资源以节省开销。</p>
<p><strong>1.1.5 混合技术栈和混合部署方式</strong></p>
<p>团队可以为不同的服务组件使用不同的技术栈和不同的部署方式（公有云、私有云、混合云）。</p>
<p><strong>1.1.6 简化治理</strong></p>
<p>组件可以彼此独立地进行缩放，从而减少了因必须缩放整个应用程序而产生的浪费和成本，独立的发布、服务治理。</p>
<p><strong>1.1.7 安全可靠，可维护。</strong></p>
<p>从架构上对运维提供友好的支撑，在安全、可维护的基础上规范化发布流程，支持数据存储容灾、业务模块隔离、访问权限控制、编码安全检测等。</p>
<h4>1.2 面临的挑战 <a href="#scroller-10" id="scroller-10"></a></h4>
<p><strong>1.2.1 分布式固有复杂性</strong></p>
<p>微服务架构是基于分布式的系统，而构建分布式系统必然会带来额外的开销。</p>
<ul>
<li>性能： 分布式系统是跨进程、跨网络的调用，受网络延迟和带宽的影响。</li>
<li>可靠性： 由于高度依赖于网络状况，任何一次的远程调用都有可能失败，随着服务的增多还会出现更多的潜在故障点。因此，如何提高系统的可靠性、降低因网络引起的故障率，是系统构建的一大挑战。</li>
<li>分布式通信： 分布式通信大大增加了功能实现的复杂度，并且伴随着定位难、调试难等问题。</li>
<li>数据一致性： 需要保证分布式系统的数据强一致性，即在 C（一致性）A（可用性）P（分区容错性） 三者之间做出权衡。这块可以参考我的这篇《<a href="https://www.cnblogs.com/wzh2010/p/15311142.html">分布式事务</a>》。</li>
</ul>
<p><strong>1.2.2 服务的依赖管理和测试</strong></p>
<p>在单体应用中，通常使用集成测试来验证依赖是否正常。而在微服务架构中，服务数量众多，每个服务都是独立的业务单元，服务主要通过接口进行交互，如何保证它的正常，是测试面临的主要挑战。</p>
<p>所以单元测试和单个服务链路的可用性非常重要。</p>
<p><strong>1.2.3 有效的配置版本管理</strong></p>
<p>在单体系统中，配置可以写在yaml文件，分布式系统中需要统一进行配置管理，同一个服务在不同的场景下对配置的值要求还可能不一样，所以需要引入配置的版本管理、环境管理。</p>
<p><strong>1.2.4 自动化的部署流程</strong></p>
<p>在微服务架构中，每个服务都独立部署，交付周期短且频率高，人工部署已经无法适应业务的快速变化。有效地构建自动化部署体系，配合服务网格、容器技术，是微服务面临的另一个挑战。</p>
<p><strong>1.2.5 对于DevOps更高的要求</strong></p>
<p>在微服务架构的实施过程中，开发人员和运维人员的角色发生了变化，开发者也将承担起整个服务的生命周期的责任，包括部署、链路追踪、监控；因此，按需调整组织架构、构建全功能的团队，也是一个不小的挑战。</p>
<p><strong>1.2.6 运维成本</strong></p>
<p>运维主要包括配置、部署、监控与告警和日志收集四大方面。微服务架构中，每个服务都需要独立地配置、部署、监控和收集日志，成本呈指数级增长。</p>
<p>服务化粒度越细，运维成本越高。</p>
<p>怎样去解决这些问题，是微服务架构必须面临的挑战。</p>
<h3>2 微服务全景架构 <a href="#scroller-17" id="scroller-17"></a></h3>
<p><img src="/images/blog/engineering/microservice-image_1_1.png" alt="image_1_1.png"></p>
<h3>3 微服务核心组件 <a href="#scroller-19" id="scroller-19"></a></h3>
<p>微服务架构核心组件包括：</p>
<table>
<thead>
<tr>
<th><strong>组件名</strong></th>
</tr>
</thead>
<tbody><tr>
<td>服务注册与发现</td>
</tr>
<tr>
<td>API 网关服务</td>
</tr>
<tr>
<td>分布式配置中心</td>
</tr>
<tr>
<td>服务通信</td>
</tr>
<tr>
<td>服务治理</td>
</tr>
<tr>
<td>服务监控</td>
</tr>
<tr>
<td>分布式服务追踪</td>
</tr>
</tbody></table>
<h4>3.1 服务注册与发现 <a href="#scroller-20" id="scroller-20"></a></h4>
<p><strong>3.1.1 原理图</strong></p>
<p><img src="/images/blog/engineering/microservice-image_1_2.png" alt="image_1_2.png"></p>
<p>服务注册与发现三要素：</p>
<ul>
<li>Provider：服务的提供方</li>
<li>Consumer：调用远程服务的服务消费方</li>
<li>Registry：服务注册和发现的注册中心</li>
</ul>
<p><strong>3.1.2 注册中心的原理、流程</strong></p>
<p>1、 Provider(服务提供者)绑定指定端口并启动服务</p>
<p>2、提供者连接注册中心，并发本机 IP、端口、应用信息和服务信息发送至注册中心存储</p>
<p>3、Consumer(消费者），连接注册中心 ，并发送应用信息、所求服务信息至注册中心</p>
<p>4、注册中心根据消费者所求服务信息匹配对应的提供者列表发送至Consumer 应用缓存。</p>
<p>5、Consumer 在发起远程调用时基于缓存的消费者列表择其一发起调用。</p>
<p>6、Provider 状态变更会实时通知注册中心、在由注册中心实时推送至Consumer设计的原因：</p>
<p>Consumer 与 Provider 解偶，双方都可以横向增减节点数。注册中心对本身可做对等集群，可动态增减节点，并且任意一台宕掉后，将自动切换到另一台</p>
<p>7、去中心化，双方不直接依赖注册中心，即使注册中心全部宕机短时间内也不会影响服务的调用（Consumer应用缓存中保留提供者 Provider 列表）</p>
<p>8、服务提供者无状态，任意一台宕掉后，不影响使用</p>
<p>注册中心包含如下功能：注册中心、服务注册和反注册、心跳监测与汇报、服务订阅、服务变更查询、集群部署、服务健康状态检测、服务状态变更通知 等</p>
<p>我们有很多种注册中心的技术，Zookeeper、Etcd、Consul、Eureka 4种比较常用，如下</p>
<table>
<thead>
<tr>
<th></th>
<th>Zookeeper</th>
<th>Etcd</th>
<th>Consul</th>
<th>Eureka</th>
</tr>
</thead>
<tbody><tr>
<td>CAP模型</td>
<td>CP</td>
<td>CP</td>
<td>CP</td>
<td>AP</td>
</tr>
<tr>
<td>数据一致性算法</td>
<td>ZAB</td>
<td>Raft</td>
<td>Raft</td>
<td>❌</td>
</tr>
<tr>
<td>多数据中心</td>
<td>❌</td>
<td>❌</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr>
<td>多语言支持</td>
<td>客户端</td>
<td>Http/gRPC</td>
<td>Http/DNS</td>
<td>Http</td>
</tr>
<tr>
<td>Watch</td>
<td>TCP</td>
<td>Long Polling</td>
<td>Long Polling</td>
<td>Long Polling</td>
</tr>
<tr>
<td>KV存储</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr>
<td>服务健康检查</td>
<td>心跳</td>
<td>心跳</td>
<td><p>服务状态，<br>内存，硬盘等</p></td>
<td>自定义</td>
</tr>
<tr>
<td>自身监控</td>
<td>❌</td>
<td>metrics</td>
<td>metrics</td>
<td>metrics</td>
</tr>
<tr>
<td>SpringCloud 支持</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>自身开发语言</td>
<td>Java</td>
<td>Go</td>
<td>Go</td>
<td>Java</td>
</tr>
</tbody></table>
<p>分布式系统中CAP模型3者不可兼得。由于网络的原因，分布式系统中P是必备的，意味着只能选择 AP 或者 CP。CP 代表数据一致性是第一位的，AP 代表可用性是第一位的。</p>
<p>Zookeeper、Etcd、Consul 是 CP 型注册中心，牺牲可用性来保证数据强一致性</p>
<p>Eureka 是 AP 型注册中心，牺牲一致性来保证可用性</p>
<h4>3.2 API 网关服务 <a href="#scroller-23" id="scroller-23"></a></h4>
<p><img src="/images/blog/engineering/microservice-image_1_3.png" alt="image_1_3.png"></p>
<p>上面是Api网关服务的基本架构：用户的请求经过统一的Api网关来访问微服务里具体的服务颗粒，并且可能产生串联的链路服务调用。</p>
<p>有很多耳熟能详的API网关技术，比如 Zuul、Kong、Tyk等，提供了服务路由在内的很多通用功能，后面会有专门的章节来说这个。</p>
<p>Tyk：Tyk是一个开放源码的API网关，它是快速、可扩展和现代的。Tyk提供了一个API管理平台，其中包括API网关、API分析、开发人员门户和API管理面板。Try 是一个基于Go实现的网关服务。</p>
<p>Kong：Kong是一个可扩展的开放源码API Layer(也称为API网关或API中间件)。Kong 在任何RESTful API的前面运行，通过插件扩展，它提供了超越核心平台的额外功能和服务。</p>
<p>Netflix zuul：Zuul是一种提供动态路由、监视、弹性、安全性等功能的边缘服务。Zuul是Netflix出品的一个基于JVM路由和服务端的负载均衡器。</p>
<p>除了路由之外，Api网关服务还包含：认证和授权，重试、熔断、降级，负载均衡，日志、监控、链路追踪，灰度发布，ABTesting 等功能。</p>
<h4>3.3 配置中心 <a href="#scroller-24" id="scroller-24"></a></h4>
<p><img src="/images/blog/engineering/microservice-image_1_4.png" alt="image_1_4.png"></p>
<p>上面这个是携程的开源配置中心Apollo系统的架构设计，我们从下往上进行分析：</p>
<p>1、Config Service提供配置的读取、推送等功能，服务对象是Apollo客户端</p>
<p>2、Admin Service提供配置的修改、发布等功能，服务对象是Apollo Portal（管理界面）</p>
<p>3、Config Service和Admin Service都是多实例、无状态部署，所以需要将自己注册到Eureka中并保持心跳，支持注册、更新、删除能力</p>
<p>4、在Eureka之上我们架了一层Meta Server用于封装Eureka的服务发现接口</p>
<p>5、Client通过域名访问Meta Server获取Config Service服务列表（IP+Port），而后直接通过IP+Port访问服务，同时在Client侧会做load balance、错误重试</p>
<p>6、Portal通过域名访问Meta Server获取Admin Service服务列表（IP+Port），而后直接通过IP+Port访问服务，同时在Portal侧会做load balance、错误重试</p>
<p>7、为了简化部署，我们实际上会把Config Service、Eureka和Meta Server三个逻辑角色部署在同一个JVM进程中</p>
<p>上面的架构体现了如下特点：</p>
<p>•高可用：配置服务为多实例部署，访问层保证 load balance、错误重试 •弱依赖：使用了Eureka来做配置中心的服务注册，如果出现问题或者网络出现问题的时候，服务应该可以依赖于它本身所缓存的配置来提供正常的服务</p>
<h4>3.4 服务通信 <a href="#scroller-25" id="scroller-25"></a></h4>
<p>分布式系统一般是由多个微服务颗粒组成的，微服务与微服务之前存在互相调用，甚至多个链路访问的情况。所以他们之间是需要通信的，通信方式继承于SOA，包含同步与异步两种模式。</p>
<p><strong>3.4.1 同步访问方式</strong></p>
<p>1、RPC 访问模式</p>
<p>Remote Procedure Call Protocol，远程过程调用协议，一般使用在分布式业务或者微服务架构风格中。像调用本地函数一样，去调用一个远端服务。本质上是请求链的底层，维护同一个端口，进行socket通信。常见的RPC技术包含 gRPC、Dubbo、Thrift 等。</p>
<p><img src="/images/blog/engineering/microservice-image_1_5.png" alt="image_1_5.png"></p>
<p>2、REST 访问模式</p>
<p>这个应该大家最常用，可以通过一套统一风格的接口模式，为Web，iOS和Android等提供接口服务。</p>
<p><strong>3.4.2 异步访问方式</strong></p>
<p>消息中间件：RabbitMQ、Kafka、RocketMQ之类，对于实时性要求不那么严格的服务请求和计算。</p>
<h4>3.5 服务治理 <a href="#scroller-28" id="scroller-28"></a></h4>
<p>常见的服务治理手段有如下几种：</p>
<p><strong>3.5.1 节点管理</strong></p>
<p>服务调用失败时可能是服务提供者自身出现，也可能是网络发生故障，我们一般有两种处理手段。</p>
<p>1. 注册中心主动摘除机制 这种机制要求服务提供者定时向注册中心汇报心跳，如果超时，就认为服务提供者出现问题，并将节点从服务列表中摘除。</p>
<p>2. 服务消费者摘除机制 当服务提供者网络出现异常，服务消费者调用就会失败，如果持续错误就可以将它从服务提供者节点列表中移除。</p>
<p><strong>3.5.2 负载均衡</strong></p>
<p>服务消费者在从服务列表中选取可用节点时，如果能让性能较好的服务机多承担一些流量的话，就能充分利用机器的性能。这就需要对负载均衡算法做一些调整。</p>
<p>常用的负载均衡算法主要包括以下几种：</p>
<p>1. Radom 随机算法 从可用的服务节点中随机选取一个节点。一般情况下，随机算法是均匀的，也就是说后端服务节点无论配置好坏，最终得到的调用量都差不多。</p>
<p>2. Round Robin 轮询算法（加权重） 就是按照固定的权重，对可用服务节点进行轮询。如果所有服务节点的权重都是相同的，则每个节点的调用量也是差不多的。但可以给性能较好的节点的权重调大些，充分发挥其性能优势，提高整体调用的平均性能。</p>
<p>3. Least Conn 最少活跃调用算法 这种算法是在服务消费者这一端的内存里动态维护着同每一个服务节点之间的连接数，选择连接数最小的节点发起调用，也就是选择了调用量最小的服务节点，性能理论上也是最优的。</p>
<p>4. 一致性 Hash 算法 指相同参数的请求总是发到同一服务节点。当某一个服务节点出现故障时，原本发往该节点的请求，基于虚拟节点机制，平摊到其他节点上，不会引起剧烈变动。</p>
<p><strong>3.5.3 服务路由</strong></p>
<p>所谓的路由规则，就是通过一定的规则如条件表达式或者正则表达式来限定服务节点的选择范围。</p>
<p>制定路由规则主要有两个原因。</p>
<p>1. 业务存在灰度发布、多版本ABTesting的需求</p>
<p>功能逐步开放发布或者灰度测试的场景。</p>
<p>2. 多机房就近访问的需求</p>
<p>一般可以通过 IP 段规则来控制访问，在选择服务节点时，优先选择同一 IP 段的节点。这个也是算力靠近的优先原则。</p>
<p><strong>3.5.4 服务容错</strong></p>
<p>在分布式系统中，分区容错性是很重要的一个话题，要知道，服务间的调用调用并不总是成功，服务提供者程序bug、异常退出 或者 消费者与提供者之间的网络故障。而服务调用失败之后，我们需要一些方法来保证调用的正常。</p>
<p>常用的方式有以下几种：</p>
<p>FailOver 失败自动切换。就是服务消费者发现调用失败或者超时后，自动从可用的服务节点列表中选择下一个节点重新发起调用，也可以设置重试的次数。</p>
<p>FailBack 失败通知。就是服务消费者调用失败或者超时后，不再重试，而是根据失败的详细信息，来决定后续的执行策略。</p>
<p>FailCache 失败缓存。就是服务消费者调用失败或者超时后，不立即发起重试，而是隔一段时间后再次尝试发起调用。</p>
<p>FailFast 快速失败。就是服务消费者调用一次失败后，不再重试。</p>
<p>服务治理的手段是从不同角度来确保服务调用的成功率。节点管理是从服务节点健康状态角度来考虑，负载均衡和服务路由是从服务节点访问优先级角度来考虑，而服务容错是从调用的健康状态角度来考虑。</p>
<h4>3.6 服务监控 <a href="#scroller-33" id="scroller-33"></a></h4>
<p><img src="/images/blog/engineering/microservice-image_1_6.png" alt="image_1_6.png"></p>
<p>常见的开发监控报警技术有 ELK、InfluxData的TICK、Promethues 等。</p>
<p>在分布式系统中，微服务一般都具有复杂的链路调用，对于链路之间的状态、服务可用性、调用情况的监控，是需要一套完整的服务监控系统去保障的。</p>
<p>如我们上面的那个图所示， 服务系统主要由哪几部分构成：</p>
<p>1、数据采集部分，包含性能指标信息、日志信息（一般是服务埋点日志或者sidecar的inbound、outbound信息）、端到端的Trace信息。</p>
<p>2、采集上来的监控数据通过传输系统，或者使用消息中间件来异步传输，或者调用服务端接口推送监控数据。并把这些数据持久化到我们的数据服务层中。</p>
<p>3、制定一套规则，对于采集到的数据进行清理、计算、分级等，处理好的数据，通过提前设置好的报警策略，来判断它是否触发了这些报警。</p>
<p>4、梳理完的数据可以进行查询展示（有一个日志查询界面）、分级报警、分析趋势报表推送等。</p>
<h4>3.7 服务追踪 <a href="#scroller-34" id="scroller-34"></a></h4>
<p>服务追踪的原理主要包括下面两个关键点。</p>
<p>1、为了实现请求跟踪，当请求发送到分布式系统的入口端点时，只需要服务跟踪框架为该请求创建一个唯一的跟踪标识，同时在分布式系统内部流转的时候，框架始终保持传递该唯一标识，直到返回给请求方为止，这个唯一标识就是前文中提到的 Trace ID。</p>
<p>通过 Trace ID 的记录，我们就能将所有请求过程的日志关联起来。</p>
<p>2、为了统计各处理单元的时间延迟，当请求到达各个服务组件时，或是处理逻辑到达某个状态时，也通过一个唯一标识来标记它的开始、具体过程以及结束，该标识就是前文中提到的 Span ID。对于每个 Span 来说，它必须有开始和结束两个节点，</p>
<p>通过记录开始 Span 和结束 Span 的时间戳，就能统计出该 Span 的时间延迟，除了时间戳记录之外，它还可以包含一些其他元数据，比如事件名称、请求信息等。</p>
<p><img src="/images/blog/engineering/microservice-image_1_7.png" alt="image_1_7.png"></p>
<p>上图显示了Trace ID 和 Spand ID 在链路中的传输过程，它把服务调用的一个时序结构给展现出来了。</p>
<p>常见的服务链路追踪的技术有Zipkin、Pinpoint、SkyWalking 等。后面讲到Service Mesh的时候会详细说下Zipkin的x-b3 header头传递，以及流量染色的使用，非常给力。</p>
<h3>4 总结 <a href="#scroller-35" id="scroller-35"></a></h3>
<p>微服务架构提倡的单一应用程序划分成一组松散耦合的细粒度小型服务，辅助轻量级的协议，互相协调、互相配合，实现高效的应用价值，符合我们应用服务开发的发展趋势。</p>
<p>后续我们围绕它的核心模块：服务注册与发现、API 网关服务、分布式配置中心、服务通信、服务治理、分布式服务追踪与监控等，从原理到实践，一步步展开来研究。</p>
5:["$","article",null,{"className":"min-h-screen","children":["$","div",null,{"className":"mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12","children":[["$","header",null,{"className":"mb-8","children":[["$","nav",null,{"className":"flex items-center gap-1 text-sm mb-4","children":[["$","$L13",null,{"href":"/blog/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"博客"}],["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"Engineering"}],[["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/architecture/page/1","className":"text-blue-600 hover:text-blue-700 transition-colors","children":"架构设计"}]]]}],["$","div",null,{"className":"flex items-center mb-6","children":["$","div",null,{"className":"inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal","children":[["$","svg",null,{"className":"w-4 h-4 mr-2 text-gray-400","fill":"none","stroke":"currentColor","viewBox":"0 0 24 24","children":["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","strokeWidth":2,"d":"M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z"}]}],["$","time",null,{"dateTime":"2024-03-19","children":"2024年03月19日"}]]}]}],["$","h1",null,{"className":"text-4xl font-bold text-gray-900 mb-6 text-center","children":"微服务及其演进史"}],["$","div",null,{"className":"flex flex-wrap gap-2 mb-6 justify-center","children":[["$","$L13","微服务",{"href":"/blog/tag/%E5%BE%AE%E6%9C%8D%E5%8A%A1/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"微服务"}],["$","$L13","架构演进",{"href":"/blog/tag/%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"架构演进"}],["$","$L13","分布式系统",{"href":"/blog/tag/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"分布式系统"}]]}]]}],["$","div",null,{"className":"max-w-5xl mx-auto","children":["$","$L14",null,{"content":"$15"}]}],["$","$10",null,{"fallback":["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":"加载导航中..."}],"children":["$","$L16",null,{"globalNav":{"prev":{"slug":"engineering/architecture/架构设计模板","title":"架构设计模板","description":"一套可落地的架构设计文档模板，涵盖需求分析、架构总览、核心流程、详细设计等 11 个关键维度，附可直接复用的 Markdown 模板。","pubDate":"2024-03-16","tags":["架构设计","设计模板","方法论"],"heroImage":"$undefined","content":"$17"},"next":{"slug":"engineering/architecture/微服务全景架构","title":"微服务全景架构","description":"微服务架构提倡的单一应用程序划分成一组松散耦合的细粒度小型服务，辅助轻量级的协议，互相协调、互相配合，实现高效的应用价值，符合我们应用服务开发的发展趋势。","pubDate":"2024-03-20","tags":["微服务","全景架构","分布式系统"],"heroImage":"$undefined","content":"$18"}},"tagNav":{"微服务":{"prev":null,"next":"$5:props:children:props:children:props:children:2:props:children:props:globalNav:next"},"架构演进":{"prev":null,"next":null},"分布式系统":{"prev":null,"next":"$5:props:children:props:children:props:children:2:props:children:props:globalNav:next"}}}]}],["$","$L19",null,{}]]}]}]}]
8:null
c:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
7:null
a:{"metadata":[["$","title","0",{"children":"微服务及其演进史 - Skyfalling Blog"}],["$","meta","1",{"name":"description","content":"在很多项目的业务初期阶段，高速迭代上线是首要考虑的事情，对后期的容量预估、可扩展性和系统健壮性、高可用一般没有那么重视。但随着业务的发展，用户量、请求量的暴增， 发现原来的单体系统已经远远不满足需求了，特别是随着互联网整体的高速发展，对系统的要求越来越高。 但是物理服务器的CPU、内存、存储器、连接..."}],["$","meta","2",{"property":"og:title","content":"微服务及其演进史"}],["$","meta","3",{"property":"og:description","content":"在很多项目的业务初期阶段，高速迭代上线是首要考虑的事情，对后期的容量预估、可扩展性和系统健壮性、高可用一般没有那么重视。但随着业务的发展，用户量、请求量的暴增， 发现原来的单体系统已经远远不满足需求了，特别是随着互联网整体的高速发展，对系统的要求越来越高。 但是物理服务器的CPU、内存、存储器、连接..."}],["$","meta","4",{"property":"og:type","content":"article"}],["$","meta","5",{"property":"article:published_time","content":"2024-03-19"}],["$","meta","6",{"property":"article:author","content":"Skyfalling"}],["$","meta","7",{"name":"twitter:card","content":"summary"}],["$","meta","8",{"name":"twitter:title","content":"微服务及其演进史"}],["$","meta","9",{"name":"twitter:description","content":"在很多项目的业务初期阶段，高速迭代上线是首要考虑的事情，对后期的容量预估、可扩展性和系统健壮性、高可用一般没有那么重视。但随着业务的发展，用户量、请求量的暴增， 发现原来的单体系统已经远远不满足需求了，特别是随着互联网整体的高速发展，对系统的要求越来越高。 但是物理服务器的CPU、内存、存储器、连接..."}],["$","link","10",{"rel":"shortcut icon","href":"/favicon.png"}],["$","link","11",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","link","12",{"rel":"icon","href":"/favicon.png"}],["$","link","13",{"rel":"apple-touch-icon","href":"/favicon.png"}]],"error":null,"digest":"$undefined"}
12:{"metadata":"$a:metadata","error":null,"digest":"$undefined"}
