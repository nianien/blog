1:"$Sreact.fragment"
2:I[10616,["6874","static/chunks/6874-7791217feaf05c17.js","7177","static/chunks/app/layout-142e67ac4336647c.js"],"default"]
3:I[87555,[],""]
4:I[31295,[],""]
6:I[59665,[],"OutletBoundary"]
9:I[74911,[],"AsyncMetadataOutlet"]
b:I[59665,[],"ViewportBoundary"]
d:I[59665,[],"MetadataBoundary"]
f:I[26614,[],""]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/fffdcdb4fb651185.css","style"]
0:{"P":null,"b":"wlOkUxTzHfxl8sQA11M8Z","p":"","c":["","blog","engineering","architecture","%E5%BC%82%E5%9C%B0%E5%A4%9A%E6%B4%BB%E6%9E%B6%E6%9E%84%EF%BC%9A%E4%BB%8E%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E8%B7%B5%E7%9A%84%E6%BC%94%E8%BF%9B%E4%B9%8B%E8%B7%AF",""],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","engineering/architecture/%E5%BC%82%E5%9C%B0%E5%A4%9A%E6%B4%BB%E6%9E%B6%E6%9E%84%EF%BC%9A%E4%BB%8E%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E8%B7%B5%E7%9A%84%E6%BC%94%E8%BF%9B%E4%B9%8B%E8%B7%AF","c"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/fffdcdb4fb651185.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"zh-CN","children":["$","body",null,{"className":"__className_f367f3","children":["$","div",null,{"className":"min-h-screen flex flex-col","children":[["$","$L2",null,{}],["$","main",null,{"className":"flex-1","children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"bg-[var(--background)]","children":["$","div",null,{"className":"mx-auto max-w-7xl px-6 py-12 lg:px-8","children":["$","p",null,{"className":"text-center text-xs leading-5 text-gray-400","children":["© ",2026," Skyfalling"]}]}]}]]}]}]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","engineering/architecture/%E5%BC%82%E5%9C%B0%E5%A4%9A%E6%B4%BB%E6%9E%B6%E6%9E%84%EF%BC%9A%E4%BB%8E%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E8%B7%B5%E7%9A%84%E6%BC%94%E8%BF%9B%E4%B9%8B%E8%B7%AF","c"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L5",null,["$","$L6",null,{"children":["$L7","$L8",["$","$L9",null,{"promise":"$@a"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","37UMO5TkSn7eDTSvMy9bkv",{"children":[["$","$Lb",null,{"children":"$Lc"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$Ld",null,{"children":"$Le"}]]}],false]],"m":"$undefined","G":["$f","$undefined"],"s":false,"S":true}
10:"$Sreact.suspense"
11:I[74911,[],"AsyncMetadata"]
13:I[6874,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],""]
14:I[32923,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
16:I[40780,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
1c:I[85300,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
e:["$","div",null,{"hidden":true,"children":["$","$10",null,{"fallback":null,"children":["$","$L11",null,{"promise":"$@12"}]}]}]
15:T5b1d,<blockquote>
<p>多地多机房部署是互联网系统的必然发展方向。一个系统要走到这一步，必然要面对流量调配、数据拆分、网络延时、架构升级等一系列问题。本文从最简单的单机架构出发，沿着可用性不断提升的脉络，逐步推演出异地多活架构的完整面貌，并结合阿里单元化方案解析工业级落地实践。</p>
</blockquote>
<h2>为什么需要异地多活？</h2>
<p>一个好的软件架构应当遵循三个核心原则：<strong>高性能、高可用、易扩展</strong>。其中，高可用通常用两个指标来衡量：</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><strong>MTBF</strong>（Mean Time Between Failure）</td>
<td>两次故障的间隔时间，越长说明系统越稳定</td>
</tr>
<tr>
<td><strong>MTTR</strong>（Mean Time To Repair）</td>
<td>故障恢复时间，越短说明对用户影响越小</td>
</tr>
</tbody></table>
<p>可用性的计算公式为：</p>
<pre><code>可用性（Availability）= MTBF / (MTBF + MTTR) × 100%
</code></pre>
<p>通常用&quot;N 个 9&quot;来描述系统的可用性等级：</p>
<table>
<thead>
<tr>
<th>可用性</th>
<th>年故障时间</th>
<th>日均故障时间</th>
</tr>
</thead>
<tbody><tr>
<td>99%（2 个 9）</td>
<td>3.65 天</td>
<td>~14.4 分钟</td>
</tr>
<tr>
<td>99.9%（3 个 9）</td>
<td>8.76 小时</td>
<td>~86.4 秒</td>
</tr>
<tr>
<td>99.99%（4 个 9）</td>
<td>52.6 分钟</td>
<td>~8.6 秒</td>
</tr>
<tr>
<td>99.999%（5 个 9）</td>
<td>5.26 分钟</td>
<td>~0.86 秒</td>
</tr>
</tbody></table>
<p>要达到 4 个 9 以上的可用性，平均每天的故障时间必须控制在 10 秒以内。每提升 1 个 9，都对系统设计提出更高的要求。</p>
<p>然而故障是不可避免的，主要来自三个方面：</p>
<ul>
<li><strong>硬件故障</strong>：交换机、路由器、磁盘等硬件损坏</li>
<li><strong>软件问题</strong>：代码 Bug、配置错误、依赖服务异常</li>
<li><strong>不可抗力</strong>：地震、水灾、火灾、停电、光缆被挖断</li>
</ul>
<p>历史上不乏惨痛的教训：</p>
<table>
<thead>
<tr>
<th>时间</th>
<th>事件</th>
<th>影响</th>
</tr>
</thead>
<tbody><tr>
<td>2013.07</td>
<td>微信因市政施工导致光缆被挖断</td>
<td>宕机数小时</td>
</tr>
<tr>
<td>2015.05</td>
<td>杭州光纤被挖断</td>
<td>近 3 亿用户约 5 小时无法访问支付宝</td>
</tr>
<tr>
<td>2021.07</td>
<td>B站部分服务器机房故障</td>
<td>整站持续 3 小时无法访问</td>
</tr>
<tr>
<td>2021.10</td>
<td>富途证券机房电力闪断</td>
<td>用户 2 小时无法登录和交易</td>
</tr>
</tbody></table>
<p><strong>不同体量的系统关注的重点不同</strong>：体量小时关注用户增长，体量上来后关注性能体验，体量再大到一定规模后，可用性就变得尤为重要。对于全民级应用而言，再小概率的风险也不能忽视——这就是异地多活存在的根本原因。</p>
<h2>部署架构的演进历程</h2>
<h3>第一阶段：单机架构</h3>
<p>最简单的模型：客户端请求 → 业务应用 → 单机数据库 → 返回结果。</p>
<p>数据库单机部署，一旦遭遇意外，所有数据全部丢失。即使做了定期备份，也存在两个问题：</p>
<ul>
<li><strong>恢复需要时间</strong>：停机恢复，时间取决于数据量</li>
<li><strong>数据不完整</strong>：备份存在时间差，不是最新数据</li>
</ul>
<p>数据库越大，故障恢复时间越长，这种方案可能连 1 个 9 都达不到。</p>
<h3>第二阶段：主从副本</h3>
<p>在另一台机器上部署数据库从库（slave），与主库（master）保持实时同步。</p>
<table>
<thead>
<tr>
<th>优势</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>数据完整性高</td>
<td>主从实时同步，数据差异极小</td>
</tr>
<tr>
<td>抗故障能力提升</td>
<td>主库异常时从库可切换为主库</td>
</tr>
<tr>
<td>读性能提升</td>
<td>业务可直接读从库，分担主库压力</td>
</tr>
</tbody></table>
<blockquote>
<p>提升系统可用性的关键就是<strong>冗余</strong>——担心一个实例故障就部署多个实例，担心一台机器宕机就部署多台机器。</p>
</blockquote>
<h3>第三阶段：同城灾备</h3>
<p>机房级别的风险虽然概率小，但一旦发生影响巨大。应对方案就不能局限在一个机房内了——需要在同城再搭建一个机房，用专线网络连通。</p>
<h4>冷备</h4>
<p>B 机房只做数据备份，不提供实时服务，只在 A 机房故障时才启用。</p>
<ul>
<li>优点：数据有异地备份</li>
<li>缺点：数据不完整、恢复期间业务不可用</li>
</ul>
<h4>热备</h4>
<p>B 机房完整镜像 A 机房：接入层、业务应用、数据存储（从库）全部部署就位，处于待命状态。</p>
<p>A 机房故障时只需做两件事：</p>
<ol>
<li>B 机房所有从库提升为主库</li>
<li>DNS 指向 B 机房，接入流量</li>
</ol>
<p><strong>热备相比冷备最大的优点是：随时可切换。</strong></p>
<p>无论冷备还是热备，B 机房都处于备用状态，统称为<strong>同城灾备</strong>。它解决了机房级别的故障问题，可用性再次提升，但有一个隐患——B 机房从未经历过真实流量的考验，切换时不敢百分百保证能正常工作。</p>
<h3>第四阶段：同城双活</h3>
<p>让 B 机房也接入流量、实时提供服务，好处有二：</p>
<ol>
<li><strong>实时训练后备军</strong>：让 B 机房达到与 A 机房相同的&quot;作战水平&quot;，随时可切换</li>
<li><strong>分担流量压力</strong>：B 机房接入流量后，减轻 A 机房的负载</li>
</ol>
<p>但 B 机房的存储是 A 机房的从库，默认不可写。解决方案是在<strong>业务应用层做读写分离改造</strong>：</p>
<table>
<thead>
<tr>
<th>操作</th>
<th>路由策略</th>
</tr>
</thead>
<tbody><tr>
<td>读请求</td>
<td>可读任意机房的存储</td>
</tr>
<tr>
<td>写请求</td>
<td>只允许写 A 机房（主库所在）</td>
</tr>
</tbody></table>
<p>所有存储（MySQL、Redis 等）都需要区分读写请求，有一定的业务改造成本。A 机房为<strong>主机房</strong>，B 机房为<strong>从机房</strong>。</p>
<p>两个机房部署在同城，物理距离近，专线网络延迟可接受。B 机房可以从 10% → 30% → 50% → 100% 逐步接入流量，持续验证其工作能力。</p>
<p><strong>同城双活</strong>比灾备更进一步：B 机房实时接入流量，且能应对随时的故障切换，系统弹性大大增强。</p>
<blockquote>
<p>但两个机房在物理上仍处于同一城市。如果整个城市发生自然灾害（如 2021 年河南水灾），两个机房依旧存在全局覆没的风险。</p>
</blockquote>
<h3>第五阶段：两地三中心</h3>
<p>为了应对城市级别的灾难，需要在<strong>异地</strong>（通常建议距离 1000 公里以上）再部署一个机房。</p>
<ul>
<li>A、B 机房在同一城市，同时提供服务（同城双活）</li>
<li>C 机房部署在异地，只做数据灾备</li>
</ul>
<p>这就是<strong>两地三中心</strong>架构，常用于银行、金融、政企项目。但问题依旧：启用灾备机房需要时间，且启用后的服务不确定能否如期工作。</p>
<h2>异地双活：跨越延迟的鸿沟</h2>
<h3>为什么&quot;简单异地部署&quot;行不通？</h3>
<p>如果把同城双活的架构直接搬到异地（例如 A 在北京、B 在上海），会遇到一个致命问题——<strong>网络延迟</strong>。</p>
<p>北京到上海约 1300 公里，即使光纤以光速传输，一个来回也需要近 10ms。加上路由器、交换机等设备，实际延迟可达 <strong>30ms 左右</strong>。更关键的是，远距离专线的质量远不如机房内网——延迟波动、丢包、甚至中断都是常态。</p>
<p>一个页面可能访问后端几十个 API，如果每次都跨机房访问，整个页面的响应延迟可能达到<strong>秒级</strong>——这是不可接受的。</p>
<blockquote>
<p>虽然机房按同城双活的模型部署在了异地，但这本质上是一种<strong>伪异地双活</strong>。</p>
</blockquote>
<h3>真正的异地双活：机房内闭环</h3>
<p>既然跨机房延迟是客观存在的物理限制，核心思路就是<strong>尽量避免跨机房调用</strong>——每个机房的请求在本机房内完成闭环。</p>
<p>这意味着每个机房都需要拥有独立的读写能力：</p>
<table>
<thead>
<tr>
<th>改造项</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>数据库双主</strong></td>
<td>两个机房的数据库都是主库，支持本地读写</td>
</tr>
<tr>
<td><strong>双向数据同步</strong></td>
<td>任一机房写入的数据，自动同步到另一个机房</td>
</tr>
<tr>
<td><strong>全量数据</strong></td>
<td>两个机房都拥有全量数据，支持任意切换</td>
</tr>
</tbody></table>
<h4>数据双向同步</h4>
<p>MySQL 本身支持双主架构和双向复制。但 Redis、消息队列（Kafka、RocketMQ 等）这些有状态服务并不原生支持，需要<strong>开发专用的数据同步中间件</strong>。</p>
<p>数据同步中间件的核心作用：</p>
<pre><code>北京机房写入 order=AAAAA → 中间件同步到上海
上海机房写入 order=BBBBB → 中间件同步到北京
最终：两个机房都有 order=AAAAA 和 order=BBBBB
</code></pre>
<p>使用中间件同步数据可以容忍专线的不稳定——专线出问题时中间件自动重试直到成功，达到<strong>数据最终一致性</strong>。</p>
<h4>数据冲突问题</h4>
<p>两个机房都可写，如果修改的是<strong>同一条数据</strong>，就会发生冲突：</p>
<pre><code>用户短时间内发起两个修改请求：
  → 请求 A 落在北京机房，修改 order=AAAAA（尚未同步到上海）
  → 请求 B 落在上海机房，修改 order=BBBBB（尚未同步到北京）
  → 两个机房以谁为准？
</code></pre>
<blockquote>
<p>系统发生故障并不可怕，可怕的是<strong>数据发生错误</strong>，因为修正数据的成本极高。</p>
</blockquote>
<h3>解决数据冲突：路由分片</h3>
<p>核心思想是：<strong>同一个用户的所有请求，只在一个机房内完成业务闭环</strong>，从根源上避免冲突。</p>
<p>需要在接入层之上部署<strong>路由层</strong>，根据规则将用户分流到不同机房。常见的分片策略有两种：</p>
<h4>策略一：哈希分片</h4>
<p>根据用户 userId 计算哈希值取模，从路由表中找到对应机房。</p>
<pre><code>用户 0~700   → 北京机房
用户 701~999 → 上海机房
</code></pre>
<p>对于未登录用户：</p>
<ul>
<li>方案 A：全部路由到固定机房</li>
<li>方案 B：根据设备 ID 进行哈希取模</li>
</ul>
<h4>策略二：地理位置分片</h4>
<p>非常适合与地理位置密切相关的业务（打车、外卖等）。</p>
<pre><code>北京、河北、内蒙古 → 北京机房
上海、浙江、江苏   → 上海机房
</code></pre>
<p>以外卖为例，商家、用户、骑手都在相同的地理范围内，天然适合按地域分片。</p>
<h4>全局数据的特殊处理</h4>
<p>有一类数据无法做分片——<strong>全局强一致数据</strong>，典型如商品库存。这类数据只能采用&quot;写主机房、读从机房&quot;的方案，无法真正双活。</p>
<p>这意味着在交易链路中，虽然全链路都做了机房内闭环，到了库存扣减这一步又回到了中心机房，单元化闭环被打破了。</p>
<p><strong>一种解决思路是库存分摊</strong>：将一个商品的库存拆分到不同机房，每个机房独立扣减本地库存，再通过<strong>库存调拨程序</strong>在机房间进行库存共享和再平衡。</p>
<table>
<thead>
<tr>
<th>场景</th>
<th>方案</th>
</tr>
</thead>
<tbody><tr>
<td>普通交易</td>
<td>库存分摊 + 库存调拨程序保证机房间库存共享</td>
</tr>
<tr>
<td>秒杀场景</td>
<td>各机房独立扣减，无需调拨（库存本就要被快速消耗完）</td>
</tr>
</tbody></table>
<h2>异地多活：从双活到 N 活</h2>
<p>按照单元化的方式，每个机房可以部署在任意地区，随时扩展新机房，只需在最上层定义好分片规则。但随着机房数量增多，数据同步的复杂度急剧上升——每个机房写入数据后需要同步到所有其他机房，网状拓扑的复杂度为 O(N²)。</p>
<h3>从网状到星状</h3>
<p>业界的优化方案是将<strong>网状架构升级为星状</strong>：确立一个<strong>中心机房</strong>，所有数据同步都以中心机房为枢纽。</p>
<pre><code>   ┌──────────┐
   │ 单元机房A │──┐
   └──────────┘  │
   ┌──────────┐  │  ┌──────────┐
   │ 单元机房B │──┼──│ 中心机房  │
   └──────────┘  │  └──────────┘
   ┌──────────┐  │
   │ 单元机房C │──┘
   └──────────┘
</code></pre>
<table>
<thead>
<tr>
<th>对比项</th>
<th>网状同步</th>
<th>星状同步</th>
</tr>
</thead>
<tbody><tr>
<td>同步复杂度</td>
<td>O(N²)，每增一个机房所有机房都需改造</td>
<td>O(N)，只需同步到中心机房</td>
</tr>
<tr>
<td>扩展性</td>
<td>差</td>
<td>好，新机房只需和中心建立同步关系</td>
</tr>
<tr>
<td>中心依赖</td>
<td>无</td>
<td>中心机房稳定性要求高</td>
</tr>
<tr>
<td>容灾</td>
<td>任一机房可接管</td>
<td>中心故障时可提升任一机房为新中心</td>
</tr>
</tbody></table>
<p><strong>星状架构的优势</strong>：</p>
<ul>
<li>一个机房写入数据只需同步到中心机房，中心再同步至其他机房</li>
<li>不需要关心一共部署了多少机房，扩展新机房的成本极低</li>
<li>中心机房故障时，可将任一单元机房提升为新中心，继续服务</li>
</ul>
<p>至此，系统真正实现了<strong>异地多活</strong>——多个机房同时对外提供服务，任意机房故障可快速切换，系统具备极强的扩展能力。</p>
<h2>阿里单元化实践</h2>
<p>阿里在实施单元化时，根据业务特点采用了两种模式：</p>
<h3>交易单元化 vs 导购单元化</h3>
<table>
<thead>
<tr>
<th>对比维度</th>
<th>交易单元化</th>
<th>导购单元化</th>
</tr>
</thead>
<tbody><tr>
<td>入口流量</td>
<td>入口清晰（商品详情→购物车→下单→支付）</td>
<td>入口分散，大促时增加各种场景和玩法</td>
</tr>
<tr>
<td>链路特征</td>
<td>以<strong>写</strong>为主</td>
<td>大部分是<strong>读</strong></td>
</tr>
<tr>
<td>数据库模式</td>
<td><strong>WRITE 模式</strong>（本地读写，双向同步）</td>
<td><strong>COPY 模式</strong>（中心写入，单元只读）</td>
</tr>
<tr>
<td>单元化范围</td>
<td>全链路必须做单元化（对用户下单有直接影响）</td>
<td>仅 C 端服务做单元化，商家后台中心化部署</td>
</tr>
<tr>
<td>资源成本</td>
<td>较高（每个单元完整部署）</td>
<td>较低（商家后台等只部署在中心）</td>
</tr>
</tbody></table>
<p>导购单元化采用 COPY 模式的原因：商家后台服务的可用性要求相对较低，故障恢复后继续操作即可，对大盘交易影响不大。中心化部署能<strong>大幅节省资源成本和维护成本</strong>，也能降低开发人员的开发成本。</p>
<h3>单元化路由透传机制</h3>
<p>单元化的核心在于路由信息的全链路透传——从接入层到最底层的数据层，每一层都需要能够正确识别和传递路由参数。</p>
<table>
<thead>
<tr>
<th>层次</th>
<th>路由机制</th>
</tr>
</thead>
<tbody><tr>
<td><strong>接入层</strong></td>
<td>解析 HTTP 请求中的路由参数（cookie/header/body），路由到正确的应用 SLB</td>
</tr>
<tr>
<td><strong>应用层</strong></td>
<td>中间件从 HTTP 请求中提取路由参数保存到上下文，供后续 RPC 和消息使用</td>
</tr>
<tr>
<td><strong>RPC 层</strong></td>
<td>RPC 客户端从上下文取出路由参数，随 RPC 请求传递到远程 Provider</td>
</tr>
<tr>
<td><strong>消息层</strong></td>
<td>MQ 客户端发送消息时从上下文获取路由参数添加到消息属性，消费时还原到上下文</td>
</tr>
<tr>
<td><strong>数据层</strong></td>
<td>保证数据落库到正确单元的 DB，防止数据脏写</td>
</tr>
</tbody></table>
<h3>单元协同与单元保护</h3>
<p>在单元化演进过程中，有两个关键问题需要解决：</p>
<p><strong>单元协同</strong>：某些特定业务场景需要保证数据强一致性（如库存扣减），这类服务只能在中心单元提供服务。所有对中心服务的调用都会直接路由到中心单元完成。</p>
<p><strong>单元保护</strong>：系统自上而下各层都要具备<strong>纠错保护能力</strong>，保证业务按单元化规则正确流转：</p>
<table>
<thead>
<tr>
<th>保护层</th>
<th>纠错机制</th>
</tr>
</thead>
<tbody><tr>
<td>接入层纠偏</td>
<td>流量进入接入层后，通过路由参数判断归属单元，非本单元流量代理到正确的目标单元</td>
</tr>
<tr>
<td>RPC 纠偏</td>
<td>RPC Consumer 端根据请求的单元信息进行路由选址，错误流量会被重定向到正确单元</td>
</tr>
<tr>
<td>数据层保护</td>
<td>数据库层面的最后防线，防止数据写入错误的单元</td>
</tr>
</tbody></table>
<h2>异地多活落地的关键挑战</h2>
<p>落地异地多活远不止架构设计，还需要在多个维度做好准备：</p>
<h3>数据一致性保障</h3>
<table>
<thead>
<tr>
<th>挑战</th>
<th>应对策略</th>
</tr>
</thead>
<tbody><tr>
<td>同步延迟导致的数据不一致</td>
<td>接受最终一致性，业务层做好容错设计</td>
</tr>
<tr>
<td>数据冲突（双写同一条数据）</td>
<td>通过路由分片从源头避免，辅以冲突检测和仲裁机制</td>
</tr>
<tr>
<td>同步中断（专线故障）</td>
<td>中间件自动重试 + 断点续传，恢复后自动追数据</td>
</tr>
<tr>
<td>数据校验</td>
<td>定期对账程序比对两地数据，发现差异自动修复</td>
</tr>
</tbody></table>
<h3>机房切换策略</h3>
<table>
<thead>
<tr>
<th>切换类型</th>
<th>触发条件</th>
<th>操作</th>
</tr>
</thead>
<tbody><tr>
<td>计划内切换</td>
<td>机房维护、演练</td>
<td>逐步调整路由权重，平滑迁移流量</td>
</tr>
<tr>
<td>故障切换</td>
<td>机房故障</td>
<td>DNS 切换 + 路由规则调整，将故障机房流量转移到其他机房</td>
</tr>
<tr>
<td>回切</td>
<td>故障恢复</td>
<td>先同步恢复期间的增量数据，再逐步回切流量</td>
</tr>
</tbody></table>
<h3>业务分级与取舍</h3>
<p>并非所有业务都需要做异地多活，需要根据业务重要程度进行分级：</p>
<table>
<thead>
<tr>
<th>级别</th>
<th>业务类型</th>
<th>多活策略</th>
</tr>
</thead>
<tbody><tr>
<td>P0</td>
<td>核心交易链路（下单、支付）</td>
<td>必须做单元化，机房内完全闭环</td>
</tr>
<tr>
<td>P1</td>
<td>重要辅助（购物车、搜索）</td>
<td>做单元化部署，允许短时降级</td>
</tr>
<tr>
<td>P2</td>
<td>一般功能（商家后台、运营工具）</td>
<td>中心化部署，故障时暂时不可用</td>
</tr>
<tr>
<td>P3</td>
<td>非核心（日志、统计）</td>
<td>不做多活，故障后补数据</td>
</tr>
</tbody></table>
<h3>配套基础设施</h3>
<p>异地多活的落地还依赖一系列配套设施：</p>
<ul>
<li><strong>全局流量调度</strong>：DNS + HTTP DNS + 接入层路由，支持按规则精细分流</li>
<li><strong>数据同步中间件</strong>：覆盖 MySQL、Redis、MQ 等所有有状态服务</li>
<li><strong>统一配置中心</strong>：支持多机房配置的统一管理和快速下发</li>
<li><strong>全链路监控</strong>：跨机房的调用链追踪、数据同步延迟监控、一致性校验报告</li>
<li><strong>演练平台</strong>：定期进行故障演练，验证切换流程的有效性</li>
</ul>
<h2>架构演进全景对比</h2>
<table>
<thead>
<tr>
<th>阶段</th>
<th>方案</th>
<th>机房数</th>
<th>可用性</th>
<th>核心特点</th>
<th>主要局限</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>单机架构</td>
<td>1</td>
<td>&lt; 99%</td>
<td>最简单</td>
<td>单点故障，数据丢失</td>
</tr>
<tr>
<td>2</td>
<td>主从副本</td>
<td>1</td>
<td>~99.9%</td>
<td>数据冗余</td>
<td>机房级故障无法应对</td>
</tr>
<tr>
<td>3</td>
<td>同城灾备</td>
<td>2（同城）</td>
<td>~99.95%</td>
<td>机房级冗余</td>
<td>备用机房未经验证</td>
</tr>
<tr>
<td>4</td>
<td>同城双活</td>
<td>2（同城）</td>
<td>~99.99%</td>
<td>双机房实时服务</td>
<td>无法应对城市级灾难</td>
</tr>
<tr>
<td>5</td>
<td>两地三中心</td>
<td>3（两城）</td>
<td>~99.99%</td>
<td>异地数据备份</td>
<td>灾备机房启用慢</td>
</tr>
<tr>
<td>6</td>
<td>异地双活</td>
<td>2（异地）</td>
<td>~99.99%+</td>
<td>机房内闭环，双主同步</td>
<td>需要大量中间件和业务改造</td>
</tr>
<tr>
<td>7</td>
<td>异地多活</td>
<td>N（多地）</td>
<td>~99.999%</td>
<td>星状同步，任意扩展</td>
<td>实施复杂度高，需要强大的基础设施支撑</td>
</tr>
</tbody></table>
<h2>总结</h2>
<p>异地多活的演进，本质上是一部<strong>用冗余换可用性</strong>的发展史。从中可以提炼出以下核心认知：</p>
<ol>
<li><strong>冗余是高可用的基石</strong>：从主从副本到多机房部署，每一次演进都是在更大的维度上做冗余</li>
<li><strong>延迟是异地部署的核心矛盾</strong>：跨城网络延迟是客观物理限制，必须通过&quot;机房内闭环&quot;来规避</li>
<li><strong>数据一致性是最大的技术挑战</strong>：双向同步、冲突避免、最终一致性保障，每一环都需要精心设计</li>
<li><strong>路由分片是解决冲突的根本手段</strong>：通过哈希分片或地理分片，确保同一用户的请求在同一机房内闭环</li>
<li><strong>星状拓扑是多活扩展的最优解</strong>：相比网状同步的 O(N²) 复杂度，星状拓扑将复杂度降为 O(N)</li>
<li><strong>不是所有业务都需要多活</strong>：根据业务重要程度分级，P0 核心链路做完整单元化，非核心业务中心化部署节省成本</li>
<li><strong>架构设计是技术与成本的平衡</strong>：异地多活需要路由层、数据同步中间件、监控体系、演练平台等大量基础设施支撑，没有足够的人力物力很难落地</li>
</ol>
<blockquote>
<p>好的架构不是一步到位的，而是随着业务体量的增长逐步演进的。理解每一步演进背后的驱动力和技术挑战，比直接套用某个方案更加重要。</p>
</blockquote>
17:T4fc0,<p>你有没有遇到过因为没有打印SQL导致问题排查困难？如果你使用了成熟ORM框架，那么很容易支撑SQL的拦截和监控，例如Mybatis的Interceptor或JOOQ的Listener都支持SQL执行过程的跟踪监控，但是，如果你的ORM框架不支持SQL监控，那么很不幸，你就只能在代码中手动打印日志了。然而，为了防SQL注入，应用中的SQL语句都是参数化的，直接打印的话，SQL语句未绑定参数，ORM框架一般都提供了SQL参数绑定的功能，原生的JDBC这样就失去了一定的监控价值。</p>
<p>另外，在TOB的业务中，有些场景SQL参数超长，如大IN查询，SQL语句会长达到几万甚至十几万，此时，我们又需要对SQL语句进行缩略打印。注意，这里的SQL缩略打印不是简单的对SQL语句进行截断，而是对SQL语句中的参数列表进行截断，例如下面的SQL</p>
<pre><code class="language-sql">select * from user 
where id in (1001,1001, 1002, 1003, 1004, 1005, 1006, 1007) 
and name in(sql
select name from whitelist 
where name in(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;,&#39;g&#39;,&#39;h&#39;,&#39;i&#39;,&#39;j&#39;,&#39;k&#39;,&#39;l&#39;,&#39;m&#39;)
)
</code></pre>
<p>缩略下印如下：</p>
<pre><code class="language-sql">select * from user 
where id in (1001,1001, 1002, 1003, 1004,...) 
and name in(
select name from whitelist 
where name in(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,...)
)
</code></pre>
<p>既然SQL 监控很重要，那么对于应用层的SQL监控都有哪些手段呢？一个SQL请求的执行链路，一般从DAO层开始：DAO -&gt; ORM -&gt; DataSource  -&gt; Connection -&gt; Driver -&gt; DB，那么在这个链路上有哪些环节可以切入监控呢？ DAO层是数据访问层的入口，而我们的目标是应用层监控，因此，能够实现SQL监控的环节只有：ORM -&gt; DataSource  -&gt; Connection -&gt; Driver，而要实现通用的非侵入式监控，则应该独立于ORM，因此我们可以从<strong>DataSource  -&gt; Connection -&gt; Driver</strong>三个环节进行入手：</p>
<h3><strong>一、SQL Profile监控</strong></h3>
<h4><strong>1、驱动层监控</strong></h4>
<p>如果Driver层支持日志监控，则最方便，例如MySQL，可以在jdbc url中添加logger：</p>
<pre><code class="language-properties">jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=UTC&amp;logger=Slf4JLogger&amp;profileSQL=true
</code></pre>
<p>基于Driver监控的问题在于：一方面强依赖于DB，和ORM层面临一样的问题，不具有通用性上述的问题，且需要厂商的支持，例如Oracle Driver就不支持日志监控；另一方面SQL格式固定，无法进行定制化输出。</p>
<h4><strong>2、连接层监控</strong></h4>
<p>如果厂商驱动不支持SQL日志，可以Driver进行代理实现SQL监控功能，常用的开源组件如<a href="https://p6spy.readthedocs.io/en/latest/">P6Spy</a>、<a href="https://github.com/arthurblake/log4jdbc">log4jdbc</a> 等，其原理都是代理了厂商的驱动，因此只需要修改jdbc url：</p>
<ul>
<li>pyspy</li>
</ul>
<pre><code class="language-properties">jdbc:p6spy:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=UTC
</code></pre>
<ul>
<li>log4jdbc</li>
</ul>
<pre><code class="language-properties">jdbc:log4jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=UTC
</code></pre>
<h4><strong>3、数据源层监控</strong></h4>
<p>可以通过对DataSource进行代理实现SQL监控</p>
<ul>
<li>P6Spy：</li>
</ul>
<pre><code class="language-java">@Bean
@Primary
public DataSource spyDataSource(@Autowired DataSource dataSource) {
  // wrap a datasource using P6SpyDataSource
  return new P6DataSource(dataSource);
}
</code></pre>
<ul>
<li>log4jdbc</li>
</ul>
<pre><code class="language-java">public DataSource spyDataSource(DataSource dataSource) {
    // wrap the provided dataSource
  return new DataSource() {
    @Override
    public Connection getConnection() throws SQLException {
      // wrap the connection with log4jdbc
      return new ConnectionSpy(dataSource.getConnection());
    }
      
    @Override
    public Connection getConnection(String username, String password) throws SQLException {
       // wrap the connection with log4jdbc
      return new ConnectionSpy(dataSource.getConnection(username, password));
     }
      //...
  };
}
</code></pre>
<p>上述三种方案都可以实现SQL监控，那么在实际应用场景中选择哪种方式更好呢？这和实际的生产方式有关。在我手，数据库是基于KDB的，Java应用是基于KsBoot，其中，数据库连接是在KDB平台配置的，底层的数据源是使用ShardingSphere+HikariDataSource进行魔改的。</p>
<p>第一种方案，由于数据库连接是由DBA维护的，升级需求修改数据库连接，因此不建议。</p>
<p>第二种方案，同理需要修改数据库连接，且比第一种更容易配错，因此也不建议。</p>
<p>排除上述两种方式，剩下的只有第三种方案了，但是第三种方案有很大的挑战，原因在于需要兼容快手kuaishou-framework奇葩的JdbcTemplate使用方式。确切地说，在于使用了DataSourceConfig。</p>
<pre><code class="language-java">public interface DataSourceConfig extends HasBizDef {

    /**
     * 数据源名称，必须与KDB申请时填写的一致
     */String bizName();

    /**
     * 获取当前可用区单库只读的JdbcTemplate
     */
    default NamedParameterJdbcTemplate read() {
        return InternalDatasourceConfig.readForceAz(this, currentAz(), currentPaz(), &quot;read&quot;);
    }   

    /**
     * 获取当前可用区单库读写的JdbcTemplate
     */
    default NamedParameterJdbcTemplate write() {
        return InternalDatasourceConfig.writeForceAz(this, currentAz(), currentPaz(), &quot;write&quot;);
    }	
  //....
}
</code></pre>
<p>DefaultDataSourceConfig是一个接口类，默认封装了NamedParameterJdbcTemplate的创建，业务方通过继承该接口来定义数据源:</p>
<pre><code class="language-kotlin">enum class AdDataSources(
    private val bizDef: BizDef,
    private val forTest: AdDataSources? = null,
    private val usingNewZk: Boolean = false
) : DataSourceConfig{
    adFansTopProfileDashboardTest,
    adFansTopProfileDashboard,
    adChargeTest,
    adCharge,
    adChargeReadOnly,
    adDspReadOnlyTest,
    adDspReadOnly;
    public open fun bizName(): String {
        return bizDef.bizName
    }
}
</code></pre>
<p>如果在业务中直接使用了DataSourceConfig创建的NamedParameterJdbcTemplate，那么我们就需要修改过程中创建的DataSource对象。那么，这里的DataSource究竟是怎么创建的呢？</p>
<p>具体扒代码的过程就不赘述了，直接说结果吧，kuaishou-framework的数据源最终是通过DataSourceFactory进行创建的，具体代码如下：</p>
<pre><code class="language-java">public static ListenableDataSource&lt;Failover&lt;Instance&gt;&gt; create(Instance i) {
   //...
   try {
       return supplyWithRetry(
        DATA_SOURCE_BUILD_RETRY,
        DATA_SOURCE_BUILD_RETRY_DELAY,
        () -&gt; new ListenableDataSource&lt;&gt;(
              bizName, 
              new HikariDataSource(config), ds -&gt; i.toString(), i),
              DataSourceFactory::needRetry);
                               
  } catch (Throwable e) {/**/}
}
</code></pre>
<p>由代码可以看到，这里的数据源实际上是通过new HikariDataSource(config)手动创建的，而DataSourceConfig又没有对外暴露创建的数据源，所以，我们该如何对DataSource代理呢?</p>
<h3><strong>二、动态修改加载类</strong></h3>
<p>成本最低的方式就是直接修改这段代码，将其中&#x7684;<em>&#x6E;ew HikariDataSource(config)</em>&#x4FEE;改&#x6210;<em>&#x6E;ew P6DataSource(new HikariDataSource(config))，</em>&#x90A3;么问题来了，这段代码属于基础组件包中的代码，基础架构组没有动力去修改，而我们又没有修改的权限，要想动这块代码，只能使用黑科技了。黑科技的手段有很多，那么问题又来了，哪种手段更合适呢？</p>
<p>首先我们来分析一下，有哪些手段可以修改Java字节码？</p>
<ul>
<li>方案一、编译时修改，需要开发maven插件</li>
</ul>
<p>（不使用maven插件的同学咋办？）</p>
<ul>
<li>方案二、加载时修改，重写类加载器</li>
</ul>
<p>需要在代码中指定特定的类加载器，用有一定的侵入式</p>
<ul>
<li>方案三、运行时修改，使用JavaAgent</li>
</ul>
<p>需要修改应用启动参数，运维成本有点高</p>
<p>首先要说明的是，这里不是对类方法进行增强，所以想使用cglib动态代理的想法是不可行的。前面三种方案都有一定的局限性：方案一比较麻烦，方案二侵入性强，方案三则需要使用JavaAgent技术，那有没有方案不使用Agent就可以动态修改已经加载的字节码呢？答案是没有，至少理论上没有。不过，好在天无绝人之路，JDK9之后，可以动态启动JavaAgent，这样就不用修改启动参数了。这里，我们选择使用byte-buddy进行字节码重写。</p>
<p><em>下面是对动态启动Java Agent技术的解释</em></p>
<blockquote>
<p>Note that starting with Java 9, there is the Launcher-Agent-Class manifest attribute for jar files that can specify the class of a Java Agent to start before the class specified with the Main-Class is launched. That way, you can easily have your Agent collaborating with your application code in your JVM, without the need for any additional command line options. The Agent can be as simple as having an agentmain method in your main class storing the Instrumentation reference in a static variable.</p>
</blockquote>
<blockquote>
<p>See <a href="https://docs.oracle.com/en/java/javase/15/docs/api/java.instrument/java/lang/instrument/package-summary.html#package.description">the java.lang.instrument package documentation</a>…</p>
</blockquote>
<blockquote>
<p>Getting hands on an Instrumentation instance when the JVM has not been started with Agents is trickier. It must support launching Agents after startup in general, e.g. via the Attach API. <a href="https://stackoverflow.com/a/19912148/2711488">This answer</a> demonstrates at its end such a self-attach to get hands on the Instrumentation. When you have the necessary manifest attribute in your application jar file, you could even use that as agent jar and omit the creation of a temporary stub file.</p>
</blockquote>
<blockquote>
<p>However, recent JVMs forbid self-attaching unless -Djdk.attach.allowAttachSelf=true has been specified at startup, but I suppose, taking additional steps at startup time, is precisely what you don’t want to do. One way to circumvent this, is to use another process. All this process has to to, is to attach to your original process and tell the JVM to start the Agent. Then, it may already terminate and everything else works the same way as before the introduction of this restriction.</p>
</blockquote>
<blockquote>
<p>As mentioned in <a href="https://stackoverflow.com/questions/56787777/?noredirect=1&lq=1#comment100160373_56787777">this comment</a>, Byte-Buddy has already implemented those necessary steps and the stripped-down Byte-Buddy-Agent contains that logic only, so you can use it to build your own logic atop it.</p>
</blockquote>
<ul>
<li>字节码工具对比</li>
</ul>
<p><img src="https://static.yximgs.com/udata/pkg/EE-KSTACK/4223630ea14c6367968188fd52cafa26.png" alt="图片"></p>
<ul>
<li>使用bytebuddy修改字节码</li>
</ul>
<p>在实现代码之前，我们回过头来再看一下快手的数据源生成：</p>
<pre><code class="language-java">new ListenableDataSource&lt;&gt;(bizName, new HikariDataSource(config), ds -&gt; i.toString());
</code></pre>
<p>这里实际生成的数据源类型是ListenableDataSource，而ListenableDataSource刚好继承了DelegatingDataSource类，而DelegatingDataSource的构造方法如下：</p>
<pre><code class="language-java">public class DelegatingDataSource implements DataSource {
   //...
  public DelegatingDataSource(DataSource targetDataSource) {
    this.setTargetDataSource(targetDataSource);
   }

  public void setTargetDataSource(@Nullable DataSource targetDataSource) {
      this.targetDataSource = targetDataSource;
  }
  //...
}
</code></pre>
<p>因此，我们可以通过改写DelegatingDataSource#setTargetDataSource方法，实现同样的效果，修改后的方法应该如下：</p>
<pre><code class="language-java">public void setTargetDataSource(@Nullable DataSource targetDataSource) {
        this.targetDataSource = new P6DataSource(targetDataSource;
}
</code></pre>
<p>那么具体如何修改字节码呢？这里是<a href="https://bytebuddy.net/#/tutorial">官方文档</a>，原理我们不做赘述，直接介绍实现了。实现方式有三种：</p>
<h4><strong>1、类文件替换</strong></h4>
<p>假设你已经通过Java代码编译了新的类，现在要替换JVM中类的定义，代码如下：</p>
<pre><code class="language-java">new ByteBuddy()
  .redefine(NewDelegatingDataSource.class)
  .name(DelegatingDataSource.class.getName())
  .make()
  .load(Thread.currentThread().getContextClassLoader(), 
        ClassReloadingStrategy.fromInstalledAgent());
</code></pre>
<h4><strong>2、操作字节码：</strong></h4>
<pre><code class="language-java">new ByteBuddy()
    .redefine(DelegatingDataSource.class)
    //重写DelegatingDataSource#setTargetDataSource方法
    .method(named(&quot;setTargetDataSource&quot;))
    .intercept(MyImplementation.INSTANCE)
    .make()
    .load(Thread.currentThread().getContextClassLoader(),
          ClassReloadingStrategy.fromInstalledAgent());

enum MyImplementation implements Implementation {

INSTANCE; // singleton

  @Override
  public InstrumentedType prepare(InstrumentedType instrumentedType) {
  return instrumentedType;
  }
  
  @Override
  public ByteCodeAppender appender(Target implementationTarget) {
  return MyAppender.INSTANCE;
  }
  
}
//字节码定义
enum MyAppender implements ByteCodeAppender {

INSTANCE; // singleton

@Override
public Size apply(MethodVisitor methodVisitor,
        Implementation.Context implementationContext,
        MethodDescription instrumentedMethod) {
  Label label0 = new Label();
  methodVisitor.visitLabel(label0);
  methodVisitor.visitLineNumber(70, label0);
  methodVisitor.visitVarInsn(ALOAD, 0);
  methodVisitor.visitTypeInsn(NEW, &quot;com/p6spy/engine/spy/P6DataSource&quot;);
  methodVisitor.visitInsn(DUP);
  methodVisitor.visitVarInsn(ALOAD, 1);
  methodVisitor.visitMethodInsn(INVOKESPECIAL, &quot;com/p6spy/engine/spy/P6DataSource&quot;, &quot;&lt;init&gt;&quot;, &quot;(Ljavax/sql/DataSource;)V&quot;, false);
  methodVisitor.visitFieldInsn(PUTFIELD, &quot;org/springframework/jdbc/datasource/DelegatingDataSource&quot;, &quot;targetDataSource&quot;, &quot;Ljavax/sql/DataSource;&quot;);
  Label label1 = new Label();
  methodVisitor.visitLabel(label1);
  methodVisitor.visitLineNumber(71, label1);
  methodVisitor.visitInsn(RETURN);
  Label label2 = new Label();
  methodVisitor.visitLabel(label2);
  methodVisitor.visitLocalVariable(&quot;this&quot;, &quot;Lorg/springframework/jdbc/datasource/DelegatingDataSource;&quot;, null, label0, label2, 0);
  methodVisitor.visitLocalVariable(&quot;targetDataSource&quot;, &quot;Ljavax/sql/DataSource;&quot;, null, label0, label2, 1);
  methodVisitor.visitMaxs(4, 2);
  return new Size(4, 2);
  }
}
</code></pre>
<p>上述代码的核心思想是字节操作字节码，操作字节码是非常复杂和繁重的事情，且无法debug，那么有没有比较方便的方式呢？</p>
<p>我们可以手动改写Java代码，然后利用插件生成对应的字节码，然后在其基础上进行修改，研发成本会低很多。这里推荐IDEA的一个插件：Byte-Code-Analyzer，使用该插件可以查看类对应的ASM字节码:</p>
<p><img src="https://static.yximgs.com/udata/pkg/EE-KSTACK/e31962a90f6598880e78d8254d6c74d9" alt="图片"></p>
<h4><strong>3、利用byte-buddy的Advice</strong></h4>
<pre><code class="language-java"> public static void redefine() {
   new ByteBuddy()
     .redefine(DelegatingDataSource.class)
     .visit(Advice.to(Decorator.class)
            .on(ElementMatchers.named(&quot;setTargetDataSource&quot;)))
     .make()
     .load(Thread.currentThread().getContextClassLoader(),
           ClassReloadingStrategy.fromInstalledAgent()).getLoaded();
 }

static class Decorator {

  //在方法开始插入代码
  @Advice.OnMethodEnter
    public static void enter(@Advice.Argument(value = 0, readOnly = false) DataSource dataSource) {
    dataSource = new P6DataSource(dataSource);
  }
}
</code></pre>
<p>byte-buddy的Advisor和动态代理的原理不一样，他是直接修改方法体的字节码，上面的方法就是表示在方法开始插入一行，其效果如下：</p>
<pre><code class="language-java">public void setTargetDataSource(@Nullable DataSource targetDataSource) {
  //插入的代码
  targetDataSource = new P6DataSource(targetDataSource);
  this.targetDataSource = targetDataSource;
}
</code></pre>
<p>注：</p>
<ol>
<li>动态修改已加载的类，是有限制条件的，不能添加方法或者字段，因此通过byte-buddy的Methoddelegation方法修改字节码是不可行的。</li>
<li>使用byte-buddy的Advice，可以对非Spring托管的类进行动态增强，因为是直接修改字节码，性能更好。</li>
</ol>
<h3><strong>三、自动生效</strong></h3>
<p>前面我们讲了如何修改字节码，以提供SQL监控功能，那么如何让SQL监控自动生效呢？我们的目标是非侵入式解决方案：既不能修改业务代码，也不能更改系统配置。鉴于Java世界的事实标准，我们利用了SpringBoot-Starter功能，只需增加一个maven依赖，就自动提供了SQL监控能力。</p>
<pre><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;com.kuaishou.ad&lt;/groupId&gt;
  &lt;artifactId&gt;sqllog-spring-boot-starter&lt;/artifactId&gt;
  &lt;version&gt;制品库查询最新版&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>至于SpringBoot-Starter的实现原理，网上资料很多，核心思想就是提供默认配置，开箱即用。需要注意的是，Spring6.0自动配置的方案有了调整，原来基于spring.factories的配置改成了org.springframework.boot.autoconfigure.AutoConfiguration.imports，原有的方式还支持，这对应普通应用没有影响，但是在实现Spring多容器隔离的方案上有一定的影响，后面有时间会展开讲一下。</p>
<pre><code class="language-java">private static String[] getConfigurations(File file) {
  @EnableAutoConfiguration
  class NoScan {
    //用于扫描META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports,该类定义在方法中,是为了避免扫描当前类时被加载
  }
  FileClassLoader classLoader = new FileClassLoader(file);
  AutoConfigurationImportSelector selector = new AutoConfigurationImportSelector();
  selector.setBeanClassLoader(classLoader);
  selector.setResourceLoader(new ClassLoaderResourcePatternResolver(classLoader));
  selector.setEnvironment(new StandardEnvironment());
  String[] configurations = selector.selectImports(new StandardAnnotationMetadata(NoScan.class));
  return configurations;
}
</code></pre>
<h3><strong>四、SQL打印效果</strong></h3>
<p>sqllog-spring-boot-starter默认基于p6spy，并对SQL输出提供了扩展，打印SQL日志如下：</p>
<p><img src="https://static.yximgs.com/udata/pkg/EE-KSTACK/28cd44d1451c960cfb982773aab6ec44" alt=""></p>
<p>SQL的打印内容分为三部分：</p>
<p>第一行，显示执行时间、耗时、SQL操作、数据库连接等信息</p>
<p>第二行，显示参数化SQL</p>
<p>第三行，显示绑定参数后的实际执行的SQL</p>
<p>通过日志看到，当SQL语句超长时，系统会对参数化SQL进行个性化缩略，而对实际执行的SQL，则保持原样输出，这样可以检索关键信息。</p>
18:T51ba,<blockquote>
<p>微服务架构的核心难题不是技术选型，而是<strong>如何找到正确的服务边界</strong>。拆分得太粗，和单体无异；拆分得太细，分布式的复杂性会吞噬所有收益。领域驱动设计（DDD）提供了一套系统性的方法论，帮助我们从业务本质出发，找到合理的拆分边界。本文将从 DDD 的核心概念出发，结合电商领域的实例，完整展示如何基于 DDD 构建微服务。</p>
</blockquote>
<h2>微服务的本质：不是&quot;小&quot;，而是&quot;界限清晰&quot;</h2>
<p>微服务中的&quot;微&quot;虽然表示服务的规模，但它并不是微服务架构的核心标准。Adrian Cockcroft 对微服务有一个精炼的定义：</p>
<blockquote>
<p>&quot;面向服务的架构由具有<strong>界限上下文</strong>、<strong>松散耦合</strong>的元素组成。&quot;</p>
</blockquote>
<p>一个真正的微服务架构应当具备以下特征：</p>
<table>
<thead>
<tr>
<th>特征</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>业务边界清晰</td>
<td>服务以业务上下文为中心，而非技术抽象</td>
</tr>
<tr>
<td>实现细节隐藏</td>
<td>通过意图接口暴露功能，不泄露内部实现</td>
</tr>
<tr>
<td>数据独立</td>
<td>服务不共享数据库，每个服务拥有自己的数据存储</td>
</tr>
<tr>
<td>故障快速恢复</td>
<td>具备容错和弹性能力</td>
</tr>
<tr>
<td>独立部署</td>
<td>团队可以自主、频繁地发布变更</td>
</tr>
<tr>
<td>自动化文化</td>
<td>自动化测试、持续集成、持续交付</td>
</tr>
</tbody></table>
<p>归纳起来：<strong>松散耦合的面向服务架构，每个服务封装在定义良好的界限上下文中，支持快速、频繁且可靠的交付。</strong></p>
<p>微服务的强大之处在于：<strong>边界内建立高内聚，边界外建立低耦合</strong>——倾向于一起改变的事物应该放在一起。但说起来容易做起来难，业务在不断发展，设想也随之改变。因此，<strong>重构能力</strong>是设计系统时必须考虑的关键问题。</p>
<h2>DDD 核心概念速览</h2>
<p>领域驱动设计（Domain-Driven Design）因 Eric Evans 的同名著作而闻名，它是一组思想、原则和模式，帮助我们基于业务领域的底层模型来设计软件系统。</p>
<h3>基本术语</h3>
<table>
<thead>
<tr>
<th>概念</th>
<th>定义</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td><strong>领域（Domain）</strong></td>
<td>组织所从事的业务范围</td>
<td>零售、电子商务</td>
</tr>
<tr>
<td><strong>子域（Subdomain）</strong></td>
<td>领域下的业务单元，一个领域由多个子域组成</td>
<td>目录、购物车、履约、支付</td>
</tr>
<tr>
<td><strong>统一语言（Ubiquitous Language）</strong></td>
<td>开发人员与领域专家共同使用的、表达业务模型的语言</td>
<td>&quot;商品&quot;、&quot;订单&quot;、&quot;履约&quot;</td>
</tr>
<tr>
<td><strong>界限上下文（Bounded Context）</strong></td>
<td>模型的有效边界，同一术语在不同上下文中含义不同</td>
<td>见下文详述</td>
</tr>
</tbody></table>
<h3>界限上下文：同一个词，不同的含义</h3>
<p>以电商系统中的 <strong>&quot;Item&quot;（商品）</strong> 为例，它在不同的上下文中有着截然不同的含义：</p>
<table>
<thead>
<tr>
<th>上下文</th>
<th>&quot;Item&quot; 的含义</th>
<th>关注的属性</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Catalog（目录）</strong></td>
<td>可出售的产品</td>
<td>名称、描述、价格、图片、分类</td>
</tr>
<tr>
<td><strong>Cart（购物车）</strong></td>
<td>客户添加到购物车的商品选项</td>
<td>SKU、数量、选中状态</td>
</tr>
<tr>
<td><strong>Fulfillment（履约）</strong></td>
<td>将要运送给客户的仓库物料</td>
<td>仓库位置、重量、物流单号</td>
</tr>
</tbody></table>
<p>通过将这些模型分离并隔离在各自的边界内，我们可以自由地表达这些模型而不产生歧义。</p>
<blockquote>
<p><strong>子域 vs 界限上下文</strong>：子域属于<strong>问题空间</strong>（业务如何看待问题），界限上下文属于<strong>解决方案空间</strong>（如何实现问题的解决方案）。理论上一个子域可以有多个界限上下文，但我们努力做到每个子域只有一个。</p>
</blockquote>
<h2>从界限上下文到微服务</h2>
<h3>界限上下文 ≠ 微服务</h3>
<p>每个界限上下文都能直接映射为一个微服务吗？<strong>不一定</strong>。</p>
<p>以&quot;定价&quot;界限上下文为例，它可能包含三个不同的模型：</p>
<table>
<thead>
<tr>
<th>模型（聚合）</th>
<th>职责</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Price（价格）</strong></td>
<td>管理目录商品的价格</td>
</tr>
<tr>
<td><strong>Priced Items（定价项）</strong></td>
<td>计算商品列表的总价</td>
</tr>
<tr>
<td><strong>Discounts（折扣）</strong></td>
<td>管理和应用各类折扣规则</td>
</tr>
</tbody></table>
<p>如果把这三个模型放在一个服务中，随着时间推移，界限可能变得模糊，职责开始重叠，最终退化为&quot;大泥球&quot;。</p>
<h3>聚合（Aggregate）：更精细的拆分单元</h3>
<p>DDD 中的<strong>聚合</strong>是由相关模型组成的自包含单元，是<strong>数据变更的原子边界</strong>。</p>
<blockquote>
<p>聚合是关联对象的集群，被视为数据变更的单元。外部引用仅限于指定聚合的一个成员——<strong>聚合根（Aggregate Root）</strong>。在聚合的边界内需应用一组一致性规则。</p>
</blockquote>
<p>聚合的核心约束：</p>
<ul>
<li><strong>一致性在单个聚合内保证</strong>：跨聚合的一致性只能做到最终一致</li>
<li><strong>只能通过已发布的接口修改聚合</strong>：外部不能绕过聚合根直接操作内部对象</li>
<li><strong>任何违反这些规则的行为都有让应用退化为大泥球的风险</strong></li>
</ul>
<h3>拆分策略：从保守到激进</h3>
<table>
<thead>
<tr>
<th>策略</th>
<th>适用场景</th>
<th>优势</th>
<th>风险</th>
</tr>
</thead>
<tbody><tr>
<td>一个界限上下文 = 一个微服务</td>
<td>领域模糊、业务初期</td>
<td>保守安全，避免过早拆分</td>
<td>服务可能过大</td>
</tr>
<tr>
<td>一个聚合 = 一个微服务</td>
<td>领域清晰、边界确定</td>
<td>粒度精细，独立演进</td>
<td>分布式复杂度高</td>
</tr>
<tr>
<td>一个界限上下文 = 多个微服务</td>
<td>上下文内聚合边界清晰</td>
<td>兼顾灵活与可控</td>
<td>需要精确的聚合划分</td>
</tr>
</tbody></table>
<blockquote>
<p>对于不完全了解的业务领域，建议从<strong>保守策略</strong>开始：将整个界限上下文及其聚合组成单个微服务。确保聚合之间通过接口充分隔离，后续再拆分的成本会低得多。<strong>将两个微服务合并为一个的成本远高于将一个微服务拆分为两个</strong>。</p>
</blockquote>
<h2>上下文映射：精确划分服务边界</h2>
<p>上下文映射（Context Mapping）用于识别和定义各种界限上下文和聚合之间的关系。它帮助我们回答一个关键问题：<strong>这些服务之间应该如何协作？</strong></p>
<h3>一个错误的设计示例</h3>
<p>以电商支付场景为例，假设有三个服务都需要处理支付：</p>
<table>
<thead>
<tr>
<th>服务</th>
<th>支付相关操作</th>
</tr>
</thead>
<tbody><tr>
<td>购物车服务</td>
<td>在线支付授权</td>
</tr>
<tr>
<td>订单服务</td>
<td>订单履约后结算</td>
</tr>
<tr>
<td>联络中心服务</td>
<td>支付重试、变更支付方式</td>
</tr>
</tbody></table>
<p>如果每个服务都内嵌支付聚合并直接对接支付网关，会产生严重问题：</p>
<ul>
<li><strong>一致性不可保证</strong>：支付聚合分散在多个服务中，无法强制执行不变性</li>
<li><strong>并发冲突</strong>：联络中心更改支付方式时，订单服务可能正在用旧方式结算</li>
<li><strong>变更扩散</strong>：支付网关的任何变更都要改动多个服务、多个团队</li>
</ul>
<h3>重新定义服务边界</h3>
<p>通过上下文映射，将支付聚合收拢到一个独立的<strong>支付服务</strong>中：</p>
<table>
<thead>
<tr>
<th>改造项</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>支付服务独立</strong></td>
<td>支付聚合有了专属的界限上下文，不变量在单个服务边界内管理</td>
</tr>
<tr>
<td><strong>反腐层（ACL）</strong></td>
<td>在支付服务和支付网关之间加入适配层，隔离核心领域模型与第三方数据模型</td>
</tr>
<tr>
<td><strong>购物车→支付</strong></td>
<td>同步 API 调用，因为下单时需要即时的支付授权反馈</td>
</tr>
<tr>
<td><strong>订单→支付</strong></td>
<td>异步事件驱动，订单服务发出域事件，支付服务监听并完成结算</td>
</tr>
<tr>
<td><strong>联络中心→支付</strong></td>
<td>异步事件驱动，变更支付方式时发出事件，支付服务撤销旧卡、处理新卡</td>
</tr>
</tbody></table>
<p>核心原则：<strong>微服务架构的成败取决于聚合之间的低耦合以及聚合之内的高内聚。</strong></p>
<h2>事件风暴：协作式的服务边界发现</h2>
<p>事件风暴（Event Storming）是 Alberto Brandolini 提出的一种轻量级的协作建模技术，它是识别聚合和微服务边界的另一种必不可少的工具。</p>
<h3>什么是事件风暴？</h3>
<p>简单来说，事件风暴是团队在一起进行的头脑风暴，目标是识别系统中发生的各种<strong>领域事件</strong>和<strong>业务流程</strong>。</p>
<p>工作方式：</p>
<ol>
<li>所有相关团队在同一个房间（物理或虚拟）</li>
<li>在白板上用不同颜色的便利贴标记事件、命令、聚合和策略</li>
<li>识别重叠概念、模糊的领域语言和冲突的业务流程</li>
<li>对相关模型进行分组，重新定义聚合边界</li>
</ol>
<h3>便利贴颜色约定</h3>
<table>
<thead>
<tr>
<th>颜色</th>
<th>含义</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>橙色</td>
<td><strong>领域事件</strong>（已发生的事实）</td>
<td>&quot;订单已创建&quot;、&quot;支付已完成&quot;</td>
</tr>
<tr>
<td>蓝色</td>
<td><strong>命令</strong>（触发事件的动作）</td>
<td>&quot;创建订单&quot;、&quot;取消订单&quot;</td>
</tr>
<tr>
<td>黄色</td>
<td><strong>聚合</strong>（命令作用的对象）</td>
<td>&quot;订单&quot;、&quot;支付&quot;、&quot;库存&quot;</td>
</tr>
<tr>
<td>紫色</td>
<td><strong>策略/规则</strong>（事件触发的后续逻辑）</td>
<td>&quot;支付完成后发送确认邮件&quot;</td>
</tr>
<tr>
<td>红色</td>
<td><strong>热点/问题</strong>（需要讨论的疑问）</td>
<td>&quot;退款流程和订单取消是否耦合？&quot;</td>
</tr>
</tbody></table>
<h3>事件风暴的产出</h3>
<p>一次成功的事件风暴通常会产出：</p>
<ul>
<li><strong>重新定义的聚合列表</strong>：这些可能成为新的微服务</li>
<li><strong>领域事件清单</strong>：需要在微服务之间流动的事件</li>
<li><strong>命令清单</strong>：外部用户或其他服务直接调用的操作</li>
<li><strong>团队共识</strong>：对领域、统一语言和精确服务边界的共同理解</li>
</ul>
<h2>微服务间的通信：拥抱最终一致性</h2>
<h3>从单体到微服务的一致性挑战</h3>
<p>在单体应用中，多个聚合在同一个进程边界内，可以在一个事务中完成：客户下单 → 扣减库存 → 发送邮件。所有操作要么都成功，要么都失败。</p>
<p>但微服务化后，这些聚合分散到了不同的分布式系统中。根据 <strong>CAP 定理</strong>：</p>
<blockquote>
<p>一个分布式系统只能同时满足三个特性中的两个：<strong>一致性（C）</strong>、<strong>可用性（A）</strong>、<strong>分区容错（P）</strong>。</p>
</blockquote>
<p>在现实系统中，分区容错（P）是不可协商的——网络不可靠、虚拟机可以宕机、区域延迟可能恶化。因此我们只能在<strong>可用性</strong>和<strong>一致性</strong>之间选择。而在现代互联网应用中，牺牲可用性通常也不可接受。</p>
<p><strong>结论：基于最终一致性设计应用程序。</strong></p>
<h3>事件驱动架构</h3>
<p>微服务可以将聚合上发生的重要变更以<strong>领域事件（Domain Event）</strong> 的形式发出，感兴趣的服务监听这些事件并在自己的领域内执行相应操作。</p>
<p>以&quot;订单取消&quot;为例：</p>
<pre><code>订单服务发布事件：OrderCancelled
  → 支付服务监听 → 执行退款
  → 库存服务监听 → 调整商品库存
  → 通知服务监听 → 发送取消确认邮件
</code></pre>
<p>这种方式避免了两种耦合：</p>
<table>
<thead>
<tr>
<th>耦合类型</th>
<th>事件驱动如何避免</th>
</tr>
</thead>
<tbody><tr>
<td><strong>行为耦合</strong></td>
<td>一个领域无需规定其他领域应该做什么</td>
</tr>
<tr>
<td><strong>时间耦合</strong></td>
<td>一个流程的完成不依赖于所有系统同时可用</td>
</tr>
</tbody></table>
<h3>事件驱动的可靠性保障</h3>
<table>
<thead>
<tr>
<th>角色</th>
<th>保障措施</th>
</tr>
</thead>
<tbody><tr>
<td><strong>生产者</strong></td>
<td>确保事件<strong>至少发出一次</strong>（At Least Once），失败时有回退机制重新触发</td>
</tr>
<tr>
<td><strong>消费者</strong></td>
<td>以<strong>幂等方式</strong>消费事件，同一事件重复到达不产生副作用</td>
</tr>
<tr>
<td><strong>事件排序</strong></td>
<td>事件可能乱序到达，消费者用时间戳或版本号保证正确性</td>
</tr>
</tbody></table>
<h3>何时仍需同步调用？</h3>
<p>并非所有场景都适合事件驱动。当需要<strong>即时反馈</strong>时（如购物车→支付授权），仍需同步 API 调用。但要注意：</p>
<ul>
<li>同步调用引入了<strong>行为耦合</strong>和<strong>时间耦合</strong></li>
<li>被调用服务不可用时，调用方也会受影响</li>
</ul>
<p><strong>缓解策略</strong>：同步调用作为主路径，辅以基于事件或批处理的异步重试作为降级方案。在用户体验、系统弹性和运营成本之间做好权衡。</p>
<blockquote>
<p><strong>何时应该合并而非拆分？</strong> 如果发现两个聚合之间需要强 ACID 事务，这是一个强烈的信号——它们可能应该属于同一个聚合。在拆分之前，事件风暴和上下文映射可以帮助我们及早识别这些依赖关系。</p>
</blockquote>
<h2>BFF 模式：解耦前端与领域服务</h2>
<h3>问题：服务为了迎合调用者而变形</h3>
<p>微服务架构中一个常见的反模式是：<strong>域服务为了满足前端的特定数据需求而编排其他服务</strong>。</p>
<p>以&quot;订单详情页&quot;为例，页面需要同时展示订单信息和退款信息。如果让订单服务调用退款服务来组装复合响应：</p>
<ul>
<li>订单服务的自治性降低：退款聚合的变更会影响订单服务</li>
<li>增加故障点：退款服务宕机时订单服务也受影响</li>
<li>变更成本高：前端需求变化时需要两个团队同时改动</li>
</ul>
<h3>解决方案：Backend for Frontends（BFF）</h3>
<p>BFF 是由<strong>消费者团队</strong>（前端团队）创建和维护的后端服务，负责：</p>
<ul>
<li>对多个域服务进行集成和编排</li>
<li>为前端提供定制化的数据契约</li>
<li>根据不同终端（Web/Mobile）优化响应格式和体积</li>
</ul>
<table>
<thead>
<tr>
<th>对比</th>
<th>无 BFF</th>
<th>有 BFF</th>
</tr>
</thead>
<tbody><tr>
<td>数据编排</td>
<td>域服务互相调用，或前端直接调多个服务</td>
<td>BFF 统一编排，域服务保持纯粹</td>
</tr>
<tr>
<td>变更自主性</td>
<td>前端需求变化要改多个域服务</td>
<td>前端团队自主改 BFF</td>
</tr>
<tr>
<td>性能优化</td>
<td>移动端可能获取过多冗余数据</td>
<td>可按终端定制负载大小</td>
</tr>
<tr>
<td>技术选型</td>
<td>受域服务 API 限制</td>
<td>BFF 可采用 GraphQL 等灵活方案</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>尽早构建 BFF 服务</strong>，可以避免两种不良后果：域服务被迫支持跨域编排，或前端不得不直接调用多个后端服务。</p>
</blockquote>
<h2>从单体到微服务：拆分路线图</h2>
<p>将以上所有工具整合，从单体拆分到微服务的推荐路径：</p>
<h3>第一步：战略设计（Strategic Design）</h3>
<ol>
<li><strong>识别子域</strong>：与领域专家一起梳理业务，划分子域</li>
<li><strong>定义界限上下文</strong>：为每个子域确定解决方案的边界</li>
<li><strong>建立统一语言</strong>：在每个上下文内建立一致的业务术语</li>
</ol>
<h3>第二步：战术发现（Tactical Discovery）</h3>
<ol start="4">
<li><strong>事件风暴</strong>：跨团队协作，识别领域事件、命令、聚合和热点问题</li>
<li><strong>上下文映射</strong>：绘制上下文之间的依赖关系和协作模式</li>
<li><strong>识别聚合</strong>：在每个上下文内找到自包含的数据变更单元</li>
</ol>
<h3>第三步：服务划分（Service Decomposition）</h3>
<ol start="7">
<li><strong>确定服务边界</strong>：根据聚合和上下文映射，确定每个微服务的边界</li>
<li><strong>设计通信方式</strong>：区分同步调用和异步事件，优先使用事件驱动</li>
<li><strong>规划 BFF 层</strong>：为不同终端设计专属的后端聚合层</li>
</ol>
<h3>第四步：渐进式拆分（Incremental Migration）</h3>
<ol start="10">
<li><strong>从边缘开始</strong>：先拆分耦合最少、边界最清晰的服务</li>
<li><strong>绞杀者模式</strong>：新功能用微服务实现，老功能逐步迁移</li>
<li><strong>持续验证</strong>：每拆分一个服务，验证边界是否正确，必要时调整</li>
</ol>
<h2>DDD 战略设计与战术设计的关系</h2>
<p>很多团队在实践 DDD 时过度关注<strong>战术设计</strong>（实体、值对象、聚合根、仓储等代码层面的模式），而忽视了<strong>战略设计</strong>（子域、界限上下文、上下文映射）。对于微服务架构而言，战略设计的价值远大于战术设计：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>战略设计</th>
<th>战术设计</th>
</tr>
</thead>
<tbody><tr>
<td>关注点</td>
<td>服务边界、团队协作、系统结构</td>
<td>代码结构、领域模型、设计模式</td>
</tr>
<tr>
<td>影响范围</td>
<td>整个系统架构</td>
<td>单个服务内部</td>
</tr>
<tr>
<td>决策成本</td>
<td>错误的边界划分代价极高</td>
<td>内部重构成本相对可控</td>
</tr>
<tr>
<td>适用阶段</td>
<td>架构设计初期</td>
<td>服务实现阶段</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>先做对战略设计（找到正确的边界），再做好战术设计（在边界内写好代码）。</strong> 边界划错了，代码写得再漂亮也是徒劳。</p>
</blockquote>
<h2>总结</h2>
<p>基于 DDD 构建微服务的核心认知：</p>
<ol>
<li><strong>微服务的本质是界限清晰</strong>，不是规模小。边界内高内聚，边界外低耦合</li>
<li><strong>界限上下文是服务拆分的起点</strong>，但不是终点——聚合才是更精细的拆分单元</li>
<li><strong>上下文映射揭示服务间的真实依赖</strong>，帮助我们避免聚合被错误地分散到多个服务中</li>
<li><strong>事件风暴是最有效的协作式建模工具</strong>，它能让团队在分解前就达成共识</li>
<li><strong>拥抱最终一致性</strong>，优先使用事件驱动架构，减少服务间的行为耦合和时间耦合</li>
<li><strong>BFF 模式解耦前端与域服务</strong>，让域服务专注于核心业务逻辑</li>
<li><strong>先保守后激进</strong>：不确定时将整个上下文作为一个服务，确保聚合间接口隔离，后续再拆分</li>
<li><strong>合并的成本远高于拆分</strong>：将两个数据库合并为一个，远比将一个数据库拆为两个要困难</li>
</ol>
<blockquote>
<p>DDD 不是银弹，它是一种思考方式。它引导我们从业务本质出发，用结构化的方法找到正确的服务边界。在微服务架构中，<strong>找到正确的边界比选择正确的技术栈重要十倍</strong>。</p>
</blockquote>
19:T5070,<h2>1 微服务的注册与发现 <a href="#scroller-1" id="scroller-1"></a></h2>
<p>我们前面在全景架构中对服务注册与发现做了大致的说明，本章我们着重详细说明微服务下注册与发现的这个能力。</p>
<p>微服务注册与发现类似于生活中的&quot;电话通讯录&quot;的概念，它记录了通讯录服务和电话的映射关系。在分布式架构中，服务会注册进去，当服务需要调用其它服务时，就这里找到服务的地址，进行调用。</p>
<p>步骤如下：</p>
<p>1、你先要把&quot;好友某某&quot;记录在通讯录中。</p>
<p>2、拨打电话的时候通过通讯录中找到&quot;好友某某&quot;，并拨通回电话。</p>
<p>3、当好友某某电话号码更新的时候，需要通知到你，并修改通讯录服务中的号码。</p>
<p>从这个过程中我们看到了一些特点：</p>
<p>1、把 &quot;好友某某&quot; 的电话号码写入通讯录中，统一在通讯录中维护，后续号码变更也是更新到通讯录中，这个过程就是服务注册的过程。</p>
<p>2、后续我们通过&quot;好友某某&quot;就可以定位到通讯录中的电话号码，并拨通电话，这个过程理解为服务发现的过程。</p>
<p>而我们微服务架构中的服务注册与发现结构如下图所示：</p>
<p><img src="/images/blog/engineering/microservice-image_6_1.png" alt="image_6_1.png"></p>
<p>图片中是一个典型的微服务架构，这个结构中主要涉及到三大角色：</p>
<p>provider - 服务提供者</p>
<p>consumer - 服务消费者</p>
<p>register center - 注册中心</p>
<p>它们之间的关系大致如下：</p>
<p>1、每个微服务在启动时，将自己的网络地址等信息（微服务的ServiceName、IP、Port、MetaData等）注册到注册中心，注册中心存储这些数据。</p>
<p>2、服务消费者从注册中心查询服务提供者的地址，并通过该地址调用服务提供者的接口。</p>
<p>3、各个微服务与注册中心使用一定机制（例如心跳）通信。如果注册中心与某微服务长时间无法通信，就会注销该实例。</p>
<p>优点如下：</p>
<p>1、解耦：服务消费者跟服务提供者解耦，各自变化，不互相影响</p>
<p>2、扩展：服务消费者和服务提供者增加和删除新的服务，对于双方没有任何影响</p>
<p>3、中介者设计模式：用一个中介对象来封装一系列的对象交互，这是一种多对多关系的中介者模式。</p>
<p>从功能上拆开主要有三块：服务注册、服务发现，和注册中心。我们一个一个来看。</p>
<h3>1.1 服务注册 <a href="#scroller-2" id="scroller-2"></a></h3>
<p>如图中，为Register注册中心注册一个服务信息，会将服务的信息：ServiceName、IP、Port以及服务实例MetaData元数据信息写入到注册中心。当服务发生变化的时候，也可以更新到注册中心。</p>
<p><img src="/images/blog/engineering/microservice-image_6_2.png" alt="image_6_2.png"></p>
<p>服务提供者（服务实例） 的服务注册模型是一种简单、容易理解、流行的服务注册模型，其在多种技术生态中都有所体现：</p>
<p>1、在K8S生态中，通过 K8S Service服务信息，和Pod的 endpoint（用来记录service对应的pod的访问地址）来进行注册。</p>
<p>2、在Spring Cloud生态中，应用名 对应 服务Service，实例 IP + Port 对应 Instance实例。比较典型的就是A服务，后面对应有多个实例做负载均衡。</p>
<p>3、在其他的注册组件中，比如 Eureka、Consul，服务模型也都是 服务→ 服务实例。</p>
<p>可以认为服务实例是一个真正的实体的载体，服务是对这些相同能力或者相同功能服务实例的一个抽象。</p>
<p><img src="/images/blog/engineering/microservice-image_6_3.png" alt="image_6_3.png"></p>
<h3>1.2 服务发现 <a href="#scroller-3" id="scroller-3"></a></h3>
<p>服务发现实际就是我们查询已经注册好的服务提供者，比如 p-&gt;p.queryService(serviceName)，通过服务名称查询某个服务是否存在，如果存在，</p>
<p>返回它的所有实例信息，即一组包含ip 、 port 、metadata元数据信息的endpoints信息。</p>
<p>这一组endpoints信息一般会被缓存在本地，如果注册中心挂掉，可保证段时间内依旧可用，这是去中心化的做法。对于单个 Service 后面有多个 Instance的情况（如上图），做 load balance。</p>
<p>服务发现的方式一般有两种：</p>
<p>1、拉取的方式：服务消费方（Consumer）主动向注册中心发起服务查询的请求。</p>
<p>2、推送的方式：服务订阅/通知变更（下发）：服务消费方（Consumer）主动向注册中心订阅某个服务，当注册中心中该服务信息发生变更时，注册中心主动通知消费者。</p>
<h3>1.3 注册中心 <a href="#scroller-4" id="scroller-4"></a></h3>
<p>注册中心提供的基本能力包括：提供服务注册、服务发现 以及 健康检查。</p>
<p>服务注册跟服务发现上面已经详细介绍了， 健康检查指的是指注册中心能够感知到微服务实例的健康状况，便于上游微服务实例及时发现下游微服务实例的健康状况。采取必备的访问措施，如避免访问不健康的实例。</p>
<p>主要的检查方式包括：</p>
<p>1、服务Provider 进行 TTL 健康汇报（Time To Live，微服务Provider定期向注册中心汇报健康状态）。</p>
<p>2、注册中心主动检查服务Provider接口。</p>
<p>综合我们前面的内容，可以总结下注册中心有如下几种能力：</p>
<p>1、高可用</p>
<p>这个主要体现在两个方面。一个方面是，注册中心本身作为基础设施层，具备高可用；第二种是就是前面我们说到的去中心化，极端情况下的故障，短时间内是不影响微服务应用的调用的</p>
<p>2、可视化操作</p>
<p>常用的注册中心，类似 Eureka、Consul 都有比较丰富的管理界面，对配置、服务注册、服务发现进行可视化管理。</p>
<p>3、高效运维</p>
<p>注册中心的文档丰富，对运维的支持比较好，并且对于服务的注册是动态感知获取的，方便动态扩容。</p>
<p>4、权限控制</p>
<p>数据是具有敏感性，无论是服务信息注册或服务是调用，需要具备权限控制能力，避免侵入或越权请求</p>
<p>5、服务注册推、拉能力</p>
<p>这个前面说过了，微服务应用程序（服务的Consumer），能够快速感知到服务实例的变化情况，使用拉取或者注册中心下发的方式进行处理。</p>
<p><img src="/images/blog/engineering/microservice-image_6_4.png" alt="image_6_4.png"></p>
<h2>2 现下的主流注册中心 <a href="#scroller-5" id="scroller-5"></a></h2>
<h3>2.1 Eureka <a href="#scroller-6" id="scroller-6"></a></h3>
<h4>2.1.1 介绍 <a href="#scroller-7" id="scroller-7"></a></h4>
<p>Eureka是Netflix OSS套件中关于服务注册和发现的解决方案。因为Spring Cloud 在它的微服务解决方案中对Eureka进行了集成，并作为优先推荐方案进行宣传，所以早期有用 Spring Cloud 来建设微服务系统的同学会比较熟悉。</p>
<p>目前大量公司的微服务系统中依旧使用Eureka作为注册中心，它的核心设计思想也被后续大量注册中心产品借鉴。但目前 <a href="https://github.com/Netflix/eureka/wiki">Eureka 2.0已经停止维护</a>，所以新的微服务架构设计中，不再建议使用。</p>
<p>Spring Cloud Netflix主要分为两个部分：</p>
<p>1、Eureka Server： 作为注册中心Server端，向微服务应用程序提供服务注册、发现、健康检查等能力。</p>
<p>2、Eureka Client： 微服务应用程序Client端，用以和Eureka Server进行通信。</p>
<p><img src="/images/blog/engineering/microservice-image_6_5.png" alt="image_6_5.png"></p>
<p>Eureka有比较友好的管理界面，如上图所示：</p>
<p>1、System Status：显示当前Eureka Server信息。</p>
<p>2、Instances Current registered with Eureka：在Eureka Server当前注册的数据，在Spring Cloud生态中，被注册的服务可以呗发现并罗列在这个地方。</p>
<p>3、General Info：基本信息，如cpu、内存、环境等。</p>
<h4>2.1.2 整体架构 <a href="#scroller-8" id="scroller-8"></a></h4>
<p><img src="/images/blog/engineering/microservice-image_6_6.png" alt="image_6_6.png"></p>
<p>Eureka Server可以运行多个实例来构建集群，解决单点问题，但不同于ZooKeeper的选举leader的过程，Eureka Server采用的是Peer to Peer对等通信。</p>
<p>所以他有如下特点：</p>
<p>1、去中心化的架构：无master/slave区分，每一个Peer都是对等的。在这种架构中，节点通过彼此互相注册来提高可用性，每个节点需要添加一个或多个有效的serviceUrl指向其他节点。每个节点都可被视为其他节点的副本。</p>
<p>2、故障转移/故障恢复：如果某台Eureka Server宕机，Eureka Client的请求会自动切换到新的Eureka Server节点，当宕机的服务器重新恢复后，Eureka会再次将其纳入到服务器集群管理之中。</p>
<p>3、节点复制：当节点开始接受客户端请求时，所有的操作都会进行replicateToPeer（节点间复制）操作，将请求复制到其他Eureka Server当前所知的所有节点中。</p>
<p>同理，一个新的Eureka Server节点启动后，会首先尝试从邻近节点获取所有实例注册表信息，完成初始化。</p>
<p>4、CAP模式：复制算法非强一致性算法，而是当有数据写入时，Eureka Server将数据同步给其他的节点，因此Eureka在CAP提系统（一致性、可用性、分区容错性）是典型的AP系统。</p>
<h4>2.1.3 接入Spring Cloud <a href="#scroller-9" id="scroller-9"></a></h4>
<p><img src="/images/blog/engineering/microservice-image_6_7.png" alt="image_6_7.png"></p>
<p>如上图所示：</p>
<p>1、Provider 服务提供者：服务向注册中心注册服务信息，即 服务 -&gt; 服务实例 数据模型， 同时定时向注册中心汇报健康检查，如果一定时间内（一般90s）没有进行心跳汇报，则会被注册中心剔除。</p>
<p>所以这边注意，注册中心感知到应用下线并进行剔除这个过程可能比较长。</p>
<p>2、Consumer 服务消费者：服务向注册中心获取所需服务对应的服务实例信息。这边需要注意，Eureka不支持订阅，因此在Spring Cloud生态中，通过定时拉取方式从注册中心中获取所需的服务实例信息。</p>
<p>3、Remote Call 远程调用：Consumer从注册中心获取的Provider的实例信息，通过 Load Balance的策略，确定一个实际的实例，发起远程调用。</p>
<h3>2.2 ZooKeeper <a href="#scroller-10" id="scroller-10"></a></h3>
<h4>2.2.1 介绍 <a href="#scroller-11" id="scroller-11"></a></h4>
<p>作为一个分布式的、开源的协调服务，ZooKeeper实现了一系列基础功能，包括简单易用的接口。</p>
<p>这些接口被用来实现服务的注册与发现功能。并实现一些高级功能，如数据同步、分布式锁、配置中心、集群选举、命名服务等。</p>
<p><img src="/images/blog/engineering/microservice-image_6_8.png" alt="image_6_8.png"></p>
<p>在数据模型上，类似于传统的文件系统，节点类型分为：</p>
<p>1、持久节点：节点创建后，就一直存在，除非执行删除操作，主动删掉这个节点。</p>
<p>2、临时节点（注册中心场景下的主要实现机制）：临时节点的生命周期和客户端会话绑定。也就是说，如果客户端会话失效，那么这个节点就会自动被清除掉。</p>
<p>在实际场景下，微服务启动的时候，会创建一个服务临时节点，等把服务停止，短时间后节点就没有了。</p>
<p><img src="/images/blog/engineering/microservice-image_6_9.png" alt="image_6_9.png"></p>
<p>Zookeeper有如下特点：</p>
<p>1、最终一致性：为客户端展示同一视图，这是zookeeper最重要的功能。2、可靠性：如果消息被到一台服务器接受，那么它将被所有的服务器接受。3、实时性：Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。4、等待无关（wait-free）：慢的或者失效的client不干预快速的client的请求。5、原子性：更新只能成功或者失败，没有中间状态。6、顺序性：所有Server，同一消息发布顺序一致。</p>
<h4>2.2.2 整体架构 <a href="#scroller-12" id="scroller-12"></a></h4>
<p><img src="/images/blog/engineering/microservice-image_6_10.png" alt="image_6_10.png"></p>
<p>上图是Zookeeper 的服务架构，他有如下流程：</p>
<p>1、 多个节点组成分布式架构，每个Server在内存中存储一份数据；</p>
<p>2、通过选举产生leader，通过 Paxos(帕克索斯)强一致性算法 进行保证，是典型的CP结构。</p>
<p>3、Leader负责处理数据更新等操作（Zab协议）；</p>
<h4>2.2.3 接入Dubbo生态 <a href="#scroller-13" id="scroller-13"></a></h4>
<p><img src="/images/blog/engineering/microservice-image_6_11.png" alt="image_6_11.png"></p>
<p>上图中的角色如下：</p>
<p>Provider：提供者,服务发布方</p>
<p>Consumer：消费者, 调用服务方</p>
<p>Container：Dubbo容器.依赖于Spring容器</p>
<p>Registry：注册中心，当Container启动时把所有可以提供的服务列表上Registry中进行注册，告诉Consumer提供了什么服务，以及服务方的位置</p>
<p>Monitor:监听器</p>
<p>说明：ZooKeeper在注册中心方面对Dubbo生态支持的比较好。服务提供者Providerzai Container启动时主动向注册中心Registry ZooKeeper中注册信息。</p>
<p>服务消费者Consumer启动时向注册中心Registry ZooKeeper中订阅注册中心，当Provider的信息发生变化时，注册中心ZooKeeper会主动向Consumer进行推送通知变更。</p>
<p>这边注意与Eureka的区别，这是主动推送通知，是注册中心下发的操作。</p>
<h3>2.3 Consul <a href="#scroller-14" id="scroller-14"></a></h3>
<h4>2.3.1 介绍 <a href="#scroller-15" id="scroller-15"></a></h4>
<p>Consul是HashiCorp推出的一款软件，是一个Service Mesh解决方案，提供了功能丰富的控制面功能：</p>
<p>1、Service Discovery（服务发现）</p>
<p>2、Configuration（配置化）</p>
<p>3、Segmentation Functionality</p>
<p>这些功能可以根据需要独立使用，或者将它们一起使用用来构建完整的Service Mesh。</p>
<p>Consul提供的关键功能如下：</p>
<p>1、Service Discovery：服务注册/发现功能。</p>
<p>2、Health Checking：健康检查，丰富的健康检查方式；</p>
<p>3、KV Store：KV存储功能，可应用多种场景，如动态配置存储，分布式协调、leader选举等。</p>
<p>4、Multi DataCenter：多数据中心。</p>
<h4>2.3.2 整体架构 <a href="#scroller-16" id="scroller-16"></a></h4>
<p><img src="/images/blog/engineering/microservice-image_6_12.png" alt="image_6_12.png"></p>
<p>如上图为Consul的架构，这边对技术点做一下说明：</p>
<p>1、Raft: 一种分布式一致性算法，Consul使用该算法保持强一致性，所以也是典型的CP模式</p>
<p>2、Client：Client是一种agent，其将会重定向所有的RPC 请求到Server。Client是无状态的，其主要参与LAN Gossip协议池。其占用很少的资源，并且消耗很少的网络带宽。</p>
<p>3、Server：Server是一种agent，其包含了一系列的责任包括：参与Raft协议写半数（Raft Quorum）、维护集群状态、响应RPC响应、和其他Datacenter通过WAN gossip交换信息和重定向查询请求至leader或者远端Datacenter。</p>
<p>4、Datacenter: Datacenter其是私有的、低延迟、高带宽的网络环境，去除了在公共网络上的网络交互。</p>
<p>5、Consensus: Consensus一致性在leader 选举、顺序执行transaction 上。当这些事务已经提交至有限状态机（finite-state machine）中，Consul定义consensus作为复制状态机的一致性。本质上使用实现了Raft协议，对于具体实现细节可参考 Consensus Protocol。</p>
<p>6、Gossip：Consul使用了Serf，其提供了Gossip协议多种用途，Serf提供成员关系、失败检查和事件广播。</p>
<p>7、LAN Gossip: Local Area Network Gossip其包含在同一个网络环境或Datacenter的节点。</p>
<p>8、WAN Gossip: Wide Area Network Gossip 其只包含Server节点，这些server分布在不同的datacenter中，其主要通过因特网或广域网相互交流。</p>
<p>9、RPC: 远程过程调用，用于服务之间的通信。</p>
<p>10、CAP抉择：在高可用方面，Consul使用Raft协议作为其分布式一致性协议，本身对故障节点有一定的容忍性，在单个DataCenter中Consul集群中节点的数量控制在2*n + 1个节点，其中n为可容忍的宕机个数，通常为3个节点。</p>
<p>所以是典型的CP模式。</p>
<p><img src="/images/blog/engineering/microservice-image_6_13.png" alt="image_6_13.png"></p>
<p>根据Consul 的选举机制和服务原理，我们有两个注意点 ：</p>
<p>1、部署Consul Service 节点应该奇数为宜，因为+1的偶数节点和奇数节点可容忍的故障数是一样的，比如上图3和4，另一方面，偶数个节点在选主节点的时候可能会出现二分选票的情况，还得重新选举。</p>
<p>2、Consul Service 节点数不是越多越好，虽然Server数量越多可容忍的故障数越多，但是Raft进行日志复制也是很耗时间的，而且Server数量越多，性能越低，所以结合实际场景，一般建议Server部署3个即可。</p>
<p>有兴趣的同学可以去Consul官网看看它的选举机制，还可以对比下Redis中Sentinel模式。</p>
<h4>2.3.3 生态对接 <a href="#scroller-17" id="scroller-17"></a></h4>
<p><strong>对接Spring Cloud生态</strong></p>
<p><img src="/images/blog/engineering/microservice-image_6_14.png" alt="image_6_14.png"></p>
<p>Consul作为注册中心，集成在Spring Cloud生态。可以看出，跟Eureka对接到Spring Cloud 生态的过程很像。</p>
<p>但是这边的健康检查更丰富，可以有多种不同的的Check方式：</p>
<ul>
<li>Script check（Script+ Interval）</li>
<li>基于HTTP请求</li>
<li>基于tcp请求</li>
<li>基于grpc请求</li>
</ul>
<h3>2.4 总结对比 <a href="#scroller-19" id="scroller-19"></a></h3>
<table>
<thead>
<tr>
<th><strong>指标</strong></th>
<th><strong>Eureka</strong></th>
<th><strong>Zookeeper</strong></th>
<th><strong>Consul</strong></th>
<th><strong>Etcd</strong></th>
</tr>
</thead>
<tbody><tr>
<td>一致性协议</td>
<td>AP</td>
<td>CP（Paxos算法）</td>
<td>CP（Raft算法）</td>
<td>CP（Raft算法）</td>
</tr>
<tr>
<td>健康检查</td>
<td>TTL(Time To Live)</td>
<td>TCP Keep Alive</td>
<td>TTL\HTTP\TCP\Script</td>
<td>Lease TTL KeepAlive</td>
</tr>
<tr>
<td>watch/long polling</td>
<td>不支持</td>
<td>watch</td>
<td>long polling</td>
<td>watch</td>
</tr>
<tr>
<td>雪崩保护</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>安全与权限</td>
<td>不支持</td>
<td>ACL</td>
<td>ACL</td>
<td>RBAC</td>
</tr>
<tr>
<td>是否支持多数据中心</td>
<td>是</td>
<td>否</td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td>是否有管理界面</td>
<td>是</td>
<td>否（可用第三方ZkTools）</td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td>Spring Cloud 集成</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>Dubbo 集成</td>
<td>不支持</td>
<td>支持</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>K8S 集成</td>
<td>不支持</td>
<td>不支持</td>
<td>支持</td>
<td>支持</td>
</tr>
</tbody></table>
<p>这边是对业内4种注册中心各纬度上的对比，Eureka是典型的AP类型，Zookeeper和Consul是典型的CP类型。如何选择取决你的业务是倾向A：高可用性 还是 C：强一致性。</p>
<p>当然，业务是复杂的，在真正的技术选型时，还是要根据自己的实际业务现状来判断。有一些倾向，比如你的系统是Spring Cloud体系下，那优先选择Eureka、Consul。</p>
<p>如果业务会更多向云原生对齐，则Consul、Etcd会是比较优先的选择。</p>
1a:T72c6,<h1>SET化架构：从单元化原理到大规模落地实践</h1>
<blockquote>
<p>当系统规模突破单机房、单集群的承载极限，当一次机房故障就可能导致全站不可用时，SET 化架构就成为了必然选择。它不是一种特定的技术方案，而是一种<strong>将系统划分为独立自治单元，实现水平扩展和故障隔离</strong>的架构思想。</p>
</blockquote>
<p>互联网业务的高速增长给架构带来了两个根本性挑战：<strong>容量的天花板</strong>和<strong>可用性的脆弱性</strong>。传统的垂直扩展（Scale-up）终有极限，而简单的水平扩展（Scale-out）在数据一致性、服务依赖、运维复杂度等方面又面临诸多困难。</p>
<p>SET 化架构（也称为单元化架构、Cell-based Architecture）正是为了系统性地解决这些问题而诞生的。本文将从原理到实践，全面解析 SET 化架构的设计与落地。</p>
<h2>什么是 SET 化架构？</h2>
<h3>概念定义</h3>
<p>SET（Scalable Elastic Topology，可扩展弹性拓扑）化架构是一种<strong>将系统按照某个维度（通常是用户 ID）划分为多个独立、自包含的部署单元</strong>的架构模式。每个 SET 都是一个&quot;小型完整系统&quot;，拥有独立的应用服务、缓存、数据库等全套基础设施，能够独立处理分配给它的流量。</p>
<pre><code>SET 化的核心思想：

传统架构：         所有用户 → 一套系统
                    （纵向扩展，存在单点瓶颈）

SET 化架构：       用户按规则分组 → 每组对应一个 SET
                    SET-1: 用户 0~999W    → 独立的一套完整系统
                    SET-2: 用户 1000W~1999W → 独立的一套完整系统
                    SET-3: 用户 2000W~2999W → 独立的一套完整系统
                    （水平扩展，理论上无上限）
</code></pre>
<h3>SET 的核心特征</h3>
<table>
<thead>
<tr>
<th>特征</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>自包含</strong></td>
<td>每个 SET 拥有完整的服务栈（应用、缓存、DB），能独立处理请求</td>
</tr>
<tr>
<td><strong>对等部署</strong></td>
<td>所有 SET 的架构相同，只是处理的数据分片不同</td>
</tr>
<tr>
<td><strong>故障隔离</strong></td>
<td>单个 SET 的故障不会影响其他 SET</td>
</tr>
<tr>
<td><strong>水平扩展</strong></td>
<td>通过增加 SET 数量实现容量扩展</td>
</tr>
<tr>
<td><strong>流量可调度</strong></td>
<td>通过路由规则灵活调度流量在 SET 间的分配</td>
</tr>
</tbody></table>
<h3>SET 化与传统分布式的区别</h3>
<table>
<thead>
<tr>
<th>维度</th>
<th>传统分布式架构</th>
<th>SET 化架构</th>
</tr>
</thead>
<tbody><tr>
<td>扩展方式</td>
<td>各层独立扩展（加应用节点、加 DB 从库）</td>
<td>整体作为一个单元扩展</td>
</tr>
<tr>
<td>故障影响</td>
<td>某一层故障影响全局</td>
<td>故障隔离在单个 SET 内</td>
</tr>
<tr>
<td>数据分片</td>
<td>数据库层分片，应用层无感知</td>
<td>从入口到数据库全链路分片</td>
</tr>
<tr>
<td>部署单元</td>
<td>按服务部署</td>
<td>按 SET（单元）部署</td>
</tr>
<tr>
<td>容量规划</td>
<td>各组件独立评估</td>
<td>按 SET 整体评估</td>
</tr>
</tbody></table>
<h2>SET 化架构演进历程</h2>
<p>SET 化不是一步到位的设计，而是随着业务规模增长逐步演化的结果。</p>
<h3>阶段一：单体架构</h3>
<pre><code>用户 → 应用服务器 → 数据库
</code></pre>
<p>所有功能在一个应用中，单库单表。适用于初创期，简单高效。</p>
<p><strong>瓶颈</strong>：单机容量有限，数据库成为瓶颈。</p>
<h3>阶段二：读写分离 + 缓存</h3>
<pre><code>用户 → 应用集群 → 缓存 → 主库（写）/ 从库（读）
</code></pre>
<p>通过读写分离缓解数据库压力，引入缓存降低 DB 负载。</p>
<p><strong>瓶颈</strong>：写入瓶颈无法解决，主库仍是单点。</p>
<h3>阶段三：分库分表</h3>
<pre><code>用户 → 应用集群 → 数据库中间件 → DB 分片 1 / DB 分片 2 / DB 分片 N
</code></pre>
<p>数据库水平拆分，解决写入瓶颈。但分片逻辑散落在各处，跨分片查询复杂。</p>
<p><strong>瓶颈</strong>：应用层无分片感知，缓存与 DB 分片不对齐，运维复杂。</p>
<h3>阶段四：服务化（微服务）</h3>
<pre><code>用户 → API 网关 → 微服务 A / 微服务 B / ... → 各自的 DB
</code></pre>
<p>按业务域拆分为独立服务，各服务独立部署和扩展。</p>
<p><strong>瓶颈</strong>：服务间调用复杂，全链路缺乏统一的分片和隔离机制。</p>
<h3>阶段五：SET 化（单元化）</h3>
<pre><code>用户 → 统一路由层 → SET-1（完整服务栈）/ SET-2 / SET-N
                       ↕ 数据同步
</code></pre>
<p>全链路按统一维度分片，每个 SET 自包含完整服务栈，实现真正的水平扩展和故障隔离。</p>
<p><strong>这就是 SET 化架构的终态。</strong> 下面详细介绍每个核心组件的设计。</p>
<h2>核心设计一：流量路由</h2>
<p>流量路由是 SET 化架构的&quot;大脑&quot;，它决定了每个请求应该被路由到哪个 SET。</p>
<h3>路由键的选择</h3>
<p>路由键（Sharding Key）是 SET 化的核心决策之一，选择不当会导致严重的跨 SET 调用问题。</p>
<table>
<thead>
<tr>
<th>路由键</th>
<th>优点</th>
<th>缺点</th>
<th>适用业务</th>
</tr>
</thead>
<tbody><tr>
<td><strong>用户 ID</strong></td>
<td>用户维度天然隔离，覆盖面广</td>
<td>用户间交互需跨 SET</td>
<td>电商、社交、O2O</td>
</tr>
<tr>
<td><strong>商户 ID</strong></td>
<td>商户维度隔离</td>
<td>用户下单需跨 SET</td>
<td>B 端平台</td>
</tr>
<tr>
<td><strong>地理区域</strong></td>
<td>天然的流量隔离</td>
<td>跨区域业务需特殊处理</td>
<td>本地生活、物流</td>
</tr>
<tr>
<td><strong>订单 ID</strong></td>
<td>订单维度隔离</td>
<td>需要提前生成带路由信息的 ID</td>
<td>交易系统</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>实践经验</strong>：绝大多数 C 端业务选择<strong>用户 ID</strong> 作为路由键，因为用户是最核心的业务实体，以用户为维度分片可以最大程度地减少跨 SET 调用。</p>
</blockquote>
<h3>路由架构设计</h3>
<p>SET 化的路由通常分为三层：</p>
<p><strong>第一层：接入路由（DNS / LB 层）</strong></p>
<p>在最外层通过 DNS 或负载均衡器将流量分配到对应的 SET。</p>
<pre><code>用户请求 → DNS 解析 → 全局负载均衡（GSLB）
                            ↓
                    根据用户 ID 哈希路由
                    ↓           ↓           ↓
                 SET-1 LB    SET-2 LB    SET-3 LB
</code></pre>
<p><strong>第二层：网关路由（API Gateway 层）</strong></p>
<p>API 网关根据请求中的路由键（如 Header、Cookie、Token 中的用户 ID）将请求路由到正确的 SET。</p>
<pre><code>请求 → API Gateway → 提取路由键 → 查询路由表 → 转发到目标 SET
</code></pre>
<p><strong>第三层：服务路由（RPC 层）</strong></p>
<p>服务间调用时，RPC 框架自动根据上下文中的路由键将请求路由到同 SET 的服务实例。</p>
<pre><code>Service A (SET-1) → RPC Framework → 自动路由到 → Service B (SET-1)
                    （通过上下文传递 SET 标识）
</code></pre>
<h3>路由表设计</h3>
<p>路由表是映射用户到 SET 的核心数据结构：</p>
<pre><code>路由表结构：
┌──────────────┬──────────┬──────────┐
│  分片范围      │  SET ID  │  状态     │
├──────────────┼──────────┼──────────┤
│  0 ~ 999      │  SET-1   │  Active  │
│  1000 ~ 1999  │  SET-2   │  Active  │
│  2000 ~ 2999  │  SET-3   │  Active  │
│  3000 ~ 3999  │  SET-1   │  Active  │  ← 同一个 SET 可承载多个分片
└──────────────┴──────────┴──────────┘
</code></pre>
<p>路由策略的关键设计要点：</p>
<ol>
<li><strong>虚拟分片</strong>：不直接将用户映射到物理 SET，而是先映射到虚拟分片（如 1024 个），再将虚拟分片映射到物理 SET。这样扩容时只需调整虚拟分片的映射关系</li>
<li><strong>路由缓存</strong>：路由表在网关和服务端本地缓存，避免每次请求都查询路由服务</li>
<li><strong>路由一致性</strong>：路由表变更时需要保证全链路一致性，避免请求被路由到错误的 SET</li>
</ol>
<h2>核心设计二：数据分片与同步</h2>
<p>数据层是 SET 化最复杂的部分，需要解决数据分片、跨 SET 数据访问、数据同步等问题。</p>
<h3>数据分类</h3>
<p>SET 化架构中的数据按照与路由键的关系分为三类：</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>定义</th>
<th>存储方式</th>
<th>举例</th>
</tr>
</thead>
<tbody><tr>
<td><strong>SET 内数据</strong></td>
<td>与路由键强绑定的数据</td>
<td>仅存储在对应 SET</td>
<td>用户订单、用户资产、购物车</td>
</tr>
<tr>
<td><strong>全局数据</strong></td>
<td>所有 SET 共享的数据</td>
<td>全局存储 + 各 SET 只读副本</td>
<td>商品信息、配置数据、类目</td>
</tr>
<tr>
<td><strong>跨 SET 数据</strong></td>
<td>涉及多个路由键的数据</td>
<td>全局存储或冗余存储</td>
<td>商户维度的聚合数据、排行榜</td>
</tr>
</tbody></table>
<h3>SET 内数据</h3>
<p>SET 内数据遵循&quot;谁的数据谁存储&quot;原则，每个 SET 只处理和存储自己分片内的数据：</p>
<pre><code>SET-1 数据库：只存储 UserID 0~999 的数据
SET-2 数据库：只存储 UserID 1000~1999 的数据

用户 A (ID=500) 下单 → 请求路由到 SET-1 → 订单写入 SET-1 DB
用户 B (ID=1500) 下单 → 请求路由到 SET-2 → 订单写入 SET-2 DB
</code></pre>
<h3>全局数据</h3>
<p>全局数据（如商品信息）需要所有 SET 都能访问，通常采用以下方案：</p>
<table>
<thead>
<tr>
<th>方案</th>
<th>原理</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>全局服务</strong></td>
<td>独立部署的全局服务 + 数据库</td>
<td>数据一致性好</td>
<td>全局服务成为依赖瓶颈</td>
</tr>
<tr>
<td><strong>数据广播</strong></td>
<td>写入全局库后异步同步到各 SET</td>
<td>本地读取性能好</td>
<td>数据有延迟，存储冗余</td>
</tr>
<tr>
<td><strong>缓存分发</strong></td>
<td>全局数据写入后推送到各 SET 缓存</td>
<td>读取极快</td>
<td>缓存一致性需要保障</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>实践建议</strong>：高频读取的全局数据（如商品详情）采用&quot;数据广播 + 本地缓存&quot;方案；低频但要求强一致的全局数据（如配置变更）采用&quot;全局服务&quot;方案。</p>
</blockquote>
<h3>数据同步机制</h3>
<p>SET 间的数据同步是保证业务连续性的关键，特别是在故障切换场景下：</p>
<pre><code>                     主 SET                          备 SET
                 ┌──────────┐                    ┌──────────┐
                 │  应用层    │                    │  应用层    │
                 │  缓存层    │                    │  缓存层    │
                 │  数据库    │ ── Binlog 同步 ──→ │  数据库    │
                 └──────────┘                    └──────────┘

        同步方式：MySQL Binlog → Canal/DTS → 目标 SET 数据库
        同步延迟：通常 &lt; 1s，需要监控告警
</code></pre>
<p>数据同步的关键指标：</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>目标值</th>
<th>监控方式</th>
</tr>
</thead>
<tbody><tr>
<td>同步延迟</td>
<td>&lt; 1 秒</td>
<td>Binlog 位点差监控</td>
</tr>
<tr>
<td>数据一致性</td>
<td>99.99%</td>
<td>定期全量对账</td>
</tr>
<tr>
<td>同步可用性</td>
<td>99.99%</td>
<td>同步链路健康检查</td>
</tr>
</tbody></table>
<h2>核心设计三：全局服务</h2>
<p>有些服务天然不能被 SET 化，它们需要作为全局服务为所有 SET 提供能力。</p>
<h3>全局 ID 生成</h3>
<p>在 SET 化架构中，ID 生成必须保证全局唯一且带有路由信息：</p>
<pre><code>ID 结构设计：
┌────────────┬──────────┬───────────┬──────────┐
│  时间戳      │  SET ID  │  机器 ID   │  序列号   │
│  41 bits    │  5 bits  │  5 bits   │  12 bits │
└────────────┴──────────┴───────────┴──────────┘

总长度：63 bits（Long 类型）
</code></pre>
<table>
<thead>
<tr>
<th>生成方案</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>全局 ID 服务</strong></td>
<td>全局唯一性保证最强</td>
<td>依赖外部服务，存在可用性风险</td>
<td>核心业务（订单、支付）</td>
</tr>
<tr>
<td><strong>本地 Snowflake</strong></td>
<td>无外部依赖，性能最高</td>
<td>需要解决时钟回拨问题</td>
<td>非核心业务</td>
</tr>
<tr>
<td><strong>号段模式</strong></td>
<td>批量获取减少调用</td>
<td>号段用尽时有短暂延迟</td>
<td>通用场景</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>兜底策略</strong>：本地 ID 生成作为兜底方案，当全局 ID 服务不可用时自动降级为本地生成，确保业务不中断。</p>
</blockquote>
<h3>全局配置中心</h3>
<p>配置中心负责管理所有 SET 的路由规则、业务配置和开关：</p>
<pre><code>配置中心架构：
                  ┌─────────────────┐
                  │   配置中心集群     │
                  │  (ZK/Nacos/etcd) │
                  └────────┬────────┘
                     ↙     ↓     ↘
            SET-1 Agent  SET-2 Agent  SET-3 Agent
               ↓            ↓            ↓
            本地缓存      本地缓存      本地缓存

推送机制：配置变更 → 配置中心 → 推送给各 SET Agent → 更新本地缓存
</code></pre>
<h3>全局调度中心</h3>
<p>负责 SET 的健康监控、故障检测和流量调度：</p>
<table>
<thead>
<tr>
<th>功能</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>健康检查</td>
<td>定期探测各 SET 的健康状态</td>
</tr>
<tr>
<td>故障检测</td>
<td>发现 SET 异常时触发告警</td>
</tr>
<tr>
<td>流量切换</td>
<td>故障 SET 的流量自动切换到备用 SET</td>
</tr>
<tr>
<td>容量管理</td>
<td>监控各 SET 的容量使用率</td>
</tr>
<tr>
<td>扩缩容编排</td>
<td>新增或下线 SET 时的流量编排</td>
</tr>
</tbody></table>
<h2>核心设计四：故障隔离与切换</h2>
<p>故障隔离是 SET 化架构最核心的价值之一。</p>
<h3>故障域划分</h3>
<p>SET 化架构将故障影响范围从&quot;全站&quot;缩小到&quot;单个 SET&quot;：</p>
<pre><code>传统架构故障：
  DB 主库宕机 → 全站不可用 → 影响 100% 用户

SET 化架构故障：
  SET-2 DB 宕机 → 仅 SET-2 不可用 → 影响约 33% 用户（假设 3 个 SET）
                    ↓ 自动切换
                 SET-2 流量切换到备用 → 影响时间 &lt; 分钟级
</code></pre>
<h3>故障切换策略</h3>
<table>
<thead>
<tr>
<th>策略</th>
<th>切换速度</th>
<th>数据风险</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>主备切换</strong></td>
<td>秒级~分钟级</td>
<td>可能丢失未同步数据</td>
<td>SET 内部 DB 主备切换</td>
</tr>
<tr>
<td><strong>SET 间切换</strong></td>
<td>分钟级</td>
<td>依赖数据同步延迟</td>
<td>整个 SET 故障</td>
</tr>
<tr>
<td><strong>跨机房切换</strong></td>
<td>分钟级~小时级</td>
<td>需要全量数据同步</td>
<td>机房级故障</td>
</tr>
</tbody></table>
<h3>故障切换流程</h3>
<pre><code>正常状态：
  用户流量 → 路由层 → SET-2（主）

故障检测：
  健康检查失败 → 确认 SET-2 不可用 → 触发切换流程

切换执行：
  1. 停止 SET-2 的流量接入（路由层摘除）
  2. 等待 SET-2 → SET-2-备 的数据同步完成（或接受部分数据丢失）
  3. 更新路由表：SET-2 的分片 → SET-2-备
  4. 开放 SET-2-备 的流量接入
  5. 验证切换后的业务正确性

恢复状态：
  用户流量 → 路由层 → SET-2-备（新主）
</code></pre>
<h3>容灾等级</h3>
<table>
<thead>
<tr>
<th>等级</th>
<th>容灾范围</th>
<th>实现方式</th>
<th>RTO</th>
</tr>
</thead>
<tbody><tr>
<td><strong>L1</strong></td>
<td>单机故障</td>
<td>应用集群 + DB 主备</td>
<td>秒级</td>
</tr>
<tr>
<td><strong>L2</strong></td>
<td>机架故障</td>
<td>跨机架部署</td>
<td>秒级</td>
</tr>
<tr>
<td><strong>L3</strong></td>
<td>机房故障</td>
<td>同城双机房 SET 互备</td>
<td>分钟级</td>
</tr>
<tr>
<td><strong>L4</strong></td>
<td>城市故障</td>
<td>异地 SET 互备</td>
<td>分钟级~小时级</td>
</tr>
</tbody></table>
<h2>核心设计五：SET 扩缩容</h2>
<p>SET 化架构的一个重要优势是可以通过增减 SET 数量来调整系统容量。</p>
<h3>扩容流程</h3>
<pre><code>扩容场景：当前 3 个 SET 容量不足，需要扩容到 4 个 SET

Step 1: 部署新 SET（SET-4）
  - 部署完整的应用服务、缓存、数据库
  - 从现有 SET 同步全局数据

Step 2: 数据迁移
  - 将 SET-1 的部分虚拟分片的数据迁移到 SET-4
  - 采用双写方案保证迁移过程不中断服务

Step 3: 路由切换
  - 更新路由表：迁移的虚拟分片指向 SET-4
  - 灰度切换流量，逐步验证

Step 4: 清理
  - 验证完成后，清理 SET-1 中已迁移的数据
  - 回收空闲资源
</code></pre>
<h3>虚拟分片的价值</h3>
<p>虚拟分片是实现平滑扩缩容的关键：</p>
<pre><code>初始状态（3 个 SET，1024 个虚拟分片）：
  SET-1: 虚拟分片 0~341
  SET-2: 虚拟分片 342~682
  SET-3: 虚拟分片 683~1023

扩容到 4 个 SET（只需调整虚拟分片映射）：
  SET-1: 虚拟分片 0~255
  SET-2: 虚拟分片 256~511
  SET-3: 虚拟分片 512~767
  SET-4: 虚拟分片 768~1023

优势：用户 → 虚拟分片的映射不变，只调整虚拟分片 → 物理 SET 的映射
</code></pre>
<h2>实践案例：电商交易系统 SET 化</h2>
<p>以一个典型的电商交易系统为例，展示 SET 化的具体落地方案。</p>
<h3>业务分析</h3>
<table>
<thead>
<tr>
<th>服务</th>
<th>路由键关系</th>
<th>SET 化策略</th>
</tr>
</thead>
<tbody><tr>
<td>用户服务</td>
<td>用户 ID（强绑定）</td>
<td>SET 内部署</td>
</tr>
<tr>
<td>订单服务</td>
<td>用户 ID（强绑定）</td>
<td>SET 内部署</td>
</tr>
<tr>
<td>支付服务</td>
<td>用户 ID（强绑定）</td>
<td>SET 内部署</td>
</tr>
<tr>
<td>商品服务</td>
<td>无关（全局数据）</td>
<td>全局部署 + 数据广播</td>
</tr>
<tr>
<td>库存服务</td>
<td>商品维度（跨 SET）</td>
<td>全局部署</td>
</tr>
<tr>
<td>搜索服务</td>
<td>无关（全局数据）</td>
<td>全局部署</td>
</tr>
<tr>
<td>营销服务</td>
<td>活动维度（跨 SET）</td>
<td>全局部署</td>
</tr>
</tbody></table>
<h3>整体架构</h3>
<pre><code>                        ┌──────────────────────────────────┐
                        │          统一接入层（GSLB）         │
                        └───────────────┬──────────────────┘
                                        ↓
                        ┌──────────────────────────────────┐
                        │         API Gateway（路由层）       │
                        │    提取 UserID → 查询路由表 → 转发   │
                        └──┬──────────────┬────────────┬───┘
                           ↓              ↓            ↓
                    ┌─────────────┐ ┌─────────────┐ ┌─────────────┐
                    │   SET-1     │ │   SET-2     │ │   SET-3     │
                    │ ┌─────────┐ │ │ ┌─────────┐ │ │ ┌─────────┐ │
                    │ │用户服务  │ │ │ │用户服务  │ │ │ │用户服务  │ │
                    │ │订单服务  │ │ │ │订单服务  │ │ │ │订单服务  │ │
                    │ │支付服务  │ │ │ │支付服务  │ │ │ │支付服务  │ │
                    │ │Redis    │ │ │ │Redis    │ │ │ │Redis    │ │
                    │ │MySQL    │ │ │ │MySQL    │ │ │ │MySQL    │ │
                    │ └─────────┘ │ │ └─────────┘ │ │ └─────────┘ │
                    └─────────────┘ └─────────────┘ └─────────────┘
                           ↕              ↕            ↕
                    ┌──────────────────────────────────────────┐
                    │              全局服务层                     │
                    │  商品服务 │ 库存服务 │ 搜索服务 │ 营销服务    │
                    │         全局 ID 服务 │ 配置中心              │
                    └──────────────────────────────────────────┘
</code></pre>
<h3>下单流程的 SET 化处理</h3>
<pre><code>用户 A（ID=500）下单购买商品 X：

1. 请求到达 API Gateway
2. Gateway 提取 UserID=500，查路由表 → SET-1
3. 请求转发到 SET-1 的订单服务
4. 订单服务调用全局商品服务查询商品信息
5. 订单服务调用全局库存服务扣减库存
6. 订单服务在 SET-1 本地 DB 创建订单
7. 订单服务调用 SET-1 本地的支付服务发起支付
8. 支付完成后，SET-1 的订单服务更新本地订单状态
</code></pre>
<p>关键点：</p>
<ul>
<li>用户维度的数据操作（创建订单、支付）在 SET 内完成，无跨 SET 调用</li>
<li>商品、库存等全局数据通过全局服务访问</li>
<li>RPC 框架自动将 SET 标识通过上下文传递，保证 SET 内调用的正确性</li>
</ul>
<h2>SET 化实施路线</h2>
<p>SET 化是一个渐进式的过程，不应该一步到位。</p>
<h3>阶段规划</h3>
<table>
<thead>
<tr>
<th>阶段</th>
<th>目标</th>
<th>关键动作</th>
<th>周期</th>
</tr>
</thead>
<tbody><tr>
<td><strong>P0：基础设施准备</strong></td>
<td>具备 SET 化的基础能力</td>
<td>统一 RPC 框架、引入路由组件、改造 ID 生成</td>
<td>1~2 月</td>
</tr>
<tr>
<td><strong>P1：核心链路 SET 化</strong></td>
<td>交易核心链路实现 SET 化</td>
<td>订单、支付、用户服务 SET 化部署</td>
<td>2~3 月</td>
</tr>
<tr>
<td><strong>P2：全链路 SET 化</strong></td>
<td>所有服务完成 SET 化改造</td>
<td>非核心服务 SET 化、全局服务治理</td>
<td>3~6 月</td>
</tr>
<tr>
<td><strong>P3：异地 SET</strong></td>
<td>实现异地多活能力</td>
<td>跨机房 SET 部署、数据同步、故障切换</td>
<td>3~6 月</td>
</tr>
</tbody></table>
<h3>改造清单</h3>
<p><strong>应用层改造</strong>：</p>
<ul>
<li>所有服务支持从请求上下文中提取和传递路由键</li>
<li>RPC 框架支持基于路由键的服务路由</li>
<li>消息队列的生产和消费支持 SET 路由</li>
<li>定时任务支持按 SET 分片执行</li>
</ul>
<p><strong>数据层改造</strong>：</p>
<ul>
<li>数据库按 SET 进行物理隔离</li>
<li>缓存按 SET 进行 namespace 隔离</li>
<li>全局数据的同步机制建设</li>
<li>数据对账和修复工具</li>
</ul>
<p><strong>基础设施改造</strong>：</p>
<ul>
<li>统一路由服务建设</li>
<li>全局 ID 生成服务建设</li>
<li>监控体系支持 SET 维度</li>
<li>发布系统支持按 SET 灰度</li>
</ul>
<h2>SET 化与异地多活的关系</h2>
<p>SET 化架构是异地多活的基础。两者的关系可以这样理解：</p>
<pre><code>SET 化 = 单元化部署 + 流量路由 + 数据分片
异地多活 = SET 化 + 跨地域部署 + 数据同步 + 故障切换
</code></pre>
<table>
<thead>
<tr>
<th>维度</th>
<th>同城 SET 化</th>
<th>异地多活 SET 化</th>
</tr>
</thead>
<tbody><tr>
<td>部署范围</td>
<td>同城多机房</td>
<td>跨城市多机房</td>
</tr>
<tr>
<td>网络延迟</td>
<td>&lt; 1ms</td>
<td>10~50ms</td>
</tr>
<tr>
<td>数据同步</td>
<td>同步/半同步复制</td>
<td>异步复制（最终一致性）</td>
</tr>
<tr>
<td>故障切换</td>
<td>自动秒级切换</td>
<td>手动/半自动分钟级切换</td>
</tr>
<tr>
<td>核心挑战</td>
<td>路由准确性</td>
<td>数据一致性 + 切换决策</td>
</tr>
</tbody></table>
<blockquote>
<p>SET 化架构天然具备&quot;每个 SET 独立自治&quot;的特性，这为异地多活提供了完美的基础。只需将不同的 SET 部署到不同的地域，配合数据同步和流量调度，就能实现异地多活。</p>
</blockquote>
<h2>常见问题与解决方案</h2>
<h3>跨 SET 调用问题</h3>
<p><strong>问题</strong>：部分业务场景不可避免需要跨 SET 访问数据。</p>
<p><strong>解决方案</strong>：</p>
<table>
<thead>
<tr>
<th>场景</th>
<th>解决方案</th>
</tr>
</thead>
<tbody><tr>
<td>用户查看商户信息</td>
<td>商户数据作为全局数据广播</td>
</tr>
<tr>
<td>商户查看所有订单</td>
<td>聚合服务从各 SET 并行查询后合并</td>
</tr>
<tr>
<td>全站排行榜</td>
<td>各 SET 本地计算后汇总到全局服务</td>
</tr>
<tr>
<td>跨用户转账</td>
<td>通过消息队列异步通知目标 SET</td>
</tr>
</tbody></table>
<h3>数据迁移问题</h3>
<p><strong>问题</strong>：扩容时需要在 SET 间迁移数据。</p>
<p><strong>解决方案</strong>：双写方案</p>
<pre><code>Phase 1: 新 SET 开始从旧 SET 同步增量数据（Binlog 订阅）
Phase 2: 同步追上后，开启双写模式（新请求同时写入新旧 SET）
Phase 3: 路由切换，新请求全部路由到新 SET
Phase 4: 验证无误后，停止双写，清理旧数据
</code></pre>
<h3>全局服务瓶颈</h3>
<p><strong>问题</strong>：全局服务成为所有 SET 的共同依赖，可能成为瓶颈。</p>
<p><strong>解决方案</strong>：</p>
<ol>
<li><strong>数据本地化</strong>：全局数据尽可能广播到各 SET 本地，减少全局服务调用</li>
<li><strong>缓存优先</strong>：全局数据走多级缓存，降低对全局 DB 的访问</li>
<li><strong>异步化</strong>：非实时性要求的全局操作通过消息队列异步处理</li>
<li><strong>弹性扩展</strong>：全局服务本身也需要集群化部署和弹性扩展</li>
</ol>
<h2>总结</h2>
<p>SET 化架构是应对互联网业务规模化增长的系统性解决方案。它的核心思想并不复杂——<strong>把一个大系统拆分成多个独立自治的小系统</strong>——但真正的挑战在于落地过程中的每一个细节。</p>
<p>回顾 SET 化的关键设计决策：</p>
<ol>
<li><strong>路由键选择决定了架构的天花板</strong>。选错路由键会导致大量跨 SET 调用，抵消 SET 化的优势</li>
<li><strong>数据分类是 SET 化的基础</strong>。明确哪些是 SET 内数据、哪些是全局数据，才能设计合理的数据架构</li>
<li><strong>虚拟分片是弹性扩展的关键</strong>。不要将用户直接映射到物理 SET，虚拟分片层带来的灵活性至关重要</li>
<li><strong>全局服务的治理不能忽视</strong>。全局服务是所有 SET 的共同依赖，必须做到高可用和高性能</li>
<li><strong>渐进式实施是务实的选择</strong>。从核心链路开始，逐步扩展，而不是试图一步到位</li>
</ol>
<blockquote>
<p><strong>SET 化不是目的，而是手段。</strong> 它服务于两个根本目标：让系统能够水平扩展以承载业务增长，让故障影响可控以保障用户体验。在实施 SET 化之前，先问自己：当前的业务规模真的需要 SET 化吗？</p>
</blockquote>
1b:T7179,<h2>为什么你的系统需要限流</h2>
<p>先看两个真实事故。</p>
<p><strong>事故一：短信轰炸。</strong> 电商大促，运营要向 200 万用户推送促销短信。开发对接了短信服务商 API，写了批量发送任务就上线。活动当天，200 万条请求几乎同时涌向服务商。服务商 API 上限是 400 QPS。没有任何限流措施，前几秒就把接口打崩，后续请求全部超时或静默丢弃。几个小时后才发现，超过一半的短信根本没送达。</p>
<p><strong>事故二：风控反噬。</strong> 某大型互联网公司风控系统，平时运行稳定。双十一流量瞬间飙到日常 10 倍，风控依赖的下游评分服务没做流量保护，直接崩溃。连锁反应：所有经过风控的交易请求因调用超时被拦截——包括完全正常的用户交易。最终损失不是来自欺诈，而是自己的系统把正常用户挡在了门外。</p>
<p>两个事故揭示同一个本质：<strong>限流不是为了&quot;限制&quot;，而是为了&quot;保护&quot;。</strong></p>
<p>在高并发系统设计中，缓存、降级和限流被称为&quot;三大利器&quot;：</p>
<table>
<thead>
<tr>
<th>手段</th>
<th>解决的问题</th>
<th>核心机制</th>
<th>局限性</th>
</tr>
</thead>
<tbody><tr>
<td><strong>缓存</strong></td>
<td>提速</td>
<td>将高频数据放入更快的存储层</td>
<td>对写操作无能为力</td>
</tr>
<tr>
<td><strong>降级</strong></td>
<td>止损</td>
<td>放弃非核心功能保核心链路</td>
<td>前提是有东西可降，秒杀场景无法降级</td>
</tr>
<tr>
<td><strong>限流</strong></td>
<td>控流</td>
<td>主动丢弃/延迟超量请求</td>
<td>需要准确的容量评估，否则误杀或漏放</td>
</tr>
</tbody></table>
<p>三者各有分工，但限流的不可替代性在于：当稀缺资源被争抢、写操作高并发、昂贵查询集中调用时，缓存和降级都帮不了你。</p>
<hr>
<h2>四种限流算法：原理、适用场景与工程取舍</h2>
<h3>漏桶算法（Leaky Bucket）</h3>
<p><strong>核心原理</strong></p>
<p>漏桶的逻辑可以用一句话概括：<strong>无论流入多快，流出永远恒定。</strong></p>
<pre><code>请求流入 → [  桶（有容量上限）  ] → 恒定速率流出 → 下游处理
                    ↓
              桶满则丢弃
</code></pre>
<ul>
<li>请求以任意速率流入桶中</li>
<li>桶底以固定速率流出（处理请求）</li>
<li>桶有容量上限，溢出的请求被直接丢弃</li>
</ul>
<p><strong>核心参数</strong></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
<th>设计考量</th>
</tr>
</thead>
<tbody><tr>
<td>流出速率</td>
<td>下游能承受的恒定处理能力</td>
<td>取决于下游系统的稳态吞吐上限</td>
</tr>
<tr>
<td>桶容量</td>
<td>允许暂存的最大请求数</td>
<td>过大导致延迟积累，过小导致突发流量全被丢弃</td>
</tr>
</tbody></table>
<p><strong>适用场景</strong></p>
<ul>
<li>对接物理设备或硬件接口（严格不允许任何突发）</li>
<li>需要绝对平滑的输出流量（如音视频流的恒定码率传输）</li>
<li>流量整形（traffic shaping）场景</li>
</ul>
<p><strong>不适用场景</strong></p>
<ul>
<li>互联网业务的 API 限流（真实流量天然是突发的，漏桶的死板会浪费系统空闲容量）</li>
<li>需要快速响应突发请求的场景</li>
</ul>
<p><strong>工程实践：Nginx 的 <code>limit_req</code> 就是漏桶实现</strong></p>
<pre><code class="language-nginx"># 定义限流区域：10MB 共享内存，每个 IP 每秒 10 个请求
limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;

server {
    location /api/ {
        # burst=20：桶容量为 20，超出的排队
        # nodelay：排队请求不延迟，立即处理（占用 burst 配额）
        limit_req zone=api burst=20 nodelay;

        # 超限返回 429 而非默认的 503
        limit_req_status 429;
    }
}
</code></pre>
<p>这里有个常见误区：<code>burst=20 nodelay</code> 不是&quot;允许突发 20 个请求&quot;那么简单。<code>nodelay</code> 的含义是突发请求立即转发（不排队等待），但每个突发请求会&quot;占用&quot;一个 burst 槽位，槽位按 <code>rate</code> 的速率恢复。实际效果是：瞬间可以通过 30 个请求（rate + burst），但之后必须等槽位恢复。</p>
<hr>
<h3>令牌桶算法（Token Bucket）</h3>
<p><strong>核心原理</strong></p>
<p>令牌桶的理念与漏桶相反：<strong>在空闲时积蓄能力，在繁忙时释放能力。</strong></p>
<pre><code>令牌生成器 ──恒定速率──→ [  令牌桶（有容量上限）  ]
                                    ↓
                         请求到达 → 取令牌 → 有令牌则通过
                                           → 无令牌则拒绝/等待
</code></pre>
<ul>
<li>系统以恒定速率向桶中放入令牌</li>
<li>每个请求消耗一个（或多个）令牌</li>
<li>令牌充足时请求立即通过</li>
<li>令牌耗尽时请求被拒绝或阻塞等待</li>
<li>桶有容量上限，多余令牌溢出</li>
</ul>
<p><strong>核心参数</strong></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
<th>设计考量</th>
</tr>
</thead>
<tbody><tr>
<td>令牌生成速率</td>
<td>系统的持续处理能力</td>
<td>对应系统稳态吞吐上限</td>
</tr>
<tr>
<td>桶容量</td>
<td>允许的最大突发量</td>
<td>编码了对突发流量的容忍度</td>
</tr>
</tbody></table>
<p><strong>适用场景</strong></p>
<ul>
<li>互联网 API 限流（绝大多数场景的首选）</li>
<li>允许合理突发的业务场景（秒杀、热点事件引发的流量脉冲）</li>
<li>需要区分长期速率和瞬时峰值的场景</li>
</ul>
<p><strong>工程实践：Guava RateLimiter 的两种模式</strong></p>
<p>Guava 提供了两种令牌桶实现，对应两种不同的业务需求：</p>
<pre><code class="language-java">// 模式一：SmoothBursty —— 允许突发
// 以每秒 100 个令牌的速率生成，桶容量等于 1 秒的产量（100）
RateLimiter limiter = RateLimiter.create(100.0);

// 场景：API 网关限流
// 特点：空闲期积累的令牌可以一次性消费，应对突发
if (limiter.tryAcquire()) {
    processRequest();
} else {
    return Response.status(429).build();
}
</code></pre>
<pre><code class="language-java">// 模式二：SmoothWarmingUp —— 冷启动预热
// 速率 100/s，预热期 3 秒
RateLimiter limiter = RateLimiter.create(100.0, 3, TimeUnit.SECONDS);

// 场景：数据库连接池、缓存冷启动
// 特点：系统刚启动时不会全速放量，给下游一个&quot;热身&quot;时间
// 预热期内速率从低到高线性增长，避免冷系统被瞬时流量打垮
</code></pre>
<p><strong>SmoothBursty vs SmoothWarmingUp 的选择</strong></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>SmoothBursty</th>
<th>SmoothWarmingUp</th>
</tr>
</thead>
<tbody><tr>
<td>突发处理</td>
<td>允许消费积累的令牌，支持突发</td>
<td>冷启动期间限制突发</td>
</tr>
<tr>
<td>典型场景</td>
<td>API 限流、消息推送</td>
<td>数据库预热、缓存预热</td>
</tr>
<tr>
<td>核心关注</td>
<td>流量的峰谷平衡</td>
<td>系统的冷热状态转换</td>
</tr>
</tbody></table>
<p><strong>关键注意</strong>：Guava RateLimiter 是<strong>单机限流</strong>。它只能控制当前 JVM 进程的流量，在分布式环境下需要配合 Redis 方案使用。</p>
<hr>
<h3>固定窗口计数器（Fixed Window Counter）</h3>
<p><strong>核心原理</strong></p>
<p>在一个固定时间窗口内维护计数器，超过阈值就拒绝，窗口结束时归零。</p>
<pre><code>|← 窗口1 (0-1s) →|← 窗口2 (1-2s) →|
    count=0→100        count=0→...
    阈值=100           阈值=100
</code></pre>
<p><strong>经典问题：窗口边界的 2 倍峰值</strong></p>
<pre><code>|← 窗口1 →|← 窗口2 →|
      ↑
   最后100ms涌入100个  最前100ms涌入100个

   → 200ms 内实际通过了 200 个请求（2 倍于阈值）
</code></pre>
<p><strong>适用场景</strong></p>
<ul>
<li>精度要求不高的简单限流（大部分业务场景）</li>
<li>需要快速实现的场景</li>
<li>阈值本身留有足够余量（2 倍偶发峰值可承受）</li>
</ul>
<p><strong>工程判断</strong>：在很多场景中，固定窗口的精度已经足够。边界处偶尔的 2 倍峰值，对于留有余量的系统来说不是问题。不要为理论上的完美过度工程化。</p>
<hr>
<h3>滑动窗口计数器（Sliding Window）</h3>
<p><strong>核心原理</strong></p>
<p>将时间窗口划分为更细的子窗口（slot），统计时基于当前时间点向前滑动统计。</p>
<pre><code>子窗口:  |s1|s2|s3|s4|s5|s6|s7|s8|s9|s10|
当前统计范围:          |←————————————→|
</code></pre>
<p><strong>与固定窗口的对比</strong></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>固定窗口</th>
<th>滑动窗口</th>
</tr>
</thead>
<tbody><tr>
<td>精度</td>
<td>存在边界 2 倍峰值</td>
<td>消除边界效应</td>
</tr>
<tr>
<td>实现复杂度</td>
<td>一个计数器</td>
<td>N 个子窗口计数器</td>
</tr>
<tr>
<td>存储开销</td>
<td>O(1)</td>
<td>O(N)，N 为子窗口数</td>
</tr>
<tr>
<td>适用场景</td>
<td>精度要求低、快速实现</td>
<td>精度要求高、阈值接近系统极限</td>
</tr>
</tbody></table>
<p><strong>工程实践：Sentinel 的滑动窗口实现</strong></p>
<p>阿里巴巴的 Sentinel 框架使用 <code>LeapArray</code> 数据结构实现滑动窗口：</p>
<ul>
<li>将 1 秒划分为若干个 <code>WindowWrap</code>（默认 2 个，即 500ms 一个子窗口）</li>
<li>每个子窗口维护独立的 pass/block/exception 等计数器</li>
<li>通过环形数组 + 时间戳判断实现窗口滑动，避免频繁创建销毁对象</li>
</ul>
<hr>
<h3>四种算法对比总结</h3>
<table>
<thead>
<tr>
<th>算法</th>
<th>核心特征</th>
<th>突发处理</th>
<th>实现复杂度</th>
<th>推荐场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>漏桶</strong></td>
<td>恒定输出</td>
<td>不允许突发</td>
<td>低</td>
<td>流量整形、硬件接口</td>
</tr>
<tr>
<td><strong>令牌桶</strong></td>
<td>弹性输出</td>
<td>允许有限突发</td>
<td>中</td>
<td>API 限流（首选）</td>
</tr>
<tr>
<td><strong>固定窗口</strong></td>
<td>简单计数</td>
<td>边界可能 2 倍峰值</td>
<td>最低</td>
<td>快速实现、精度要求低</td>
</tr>
<tr>
<td><strong>滑动窗口</strong></td>
<td>精确计数</td>
<td>平滑</td>
<td>高</td>
<td>精度要求高、阈值紧</td>
</tr>
</tbody></table>
<p><strong>选择策略</strong>：如果没有特殊需求，令牌桶是互联网业务的默认选择。如果需要极致简单，用固定窗口。如果下游绝对不能承受波动，用漏桶。如果阈值非常接近系统极限，用滑动窗口。</p>
<hr>
<h2>从单机到分布式：最关键的认知跃迁</h2>
<h3>单机限流为什么在集群中失效</h3>
<p>一个团队用 Guava RateLimiter 限制短信 API 调用为 400 QPS，本地测试完美。代码部署到 4 个节点后，4 个节点各自以 400 QPS 发送，服务商实际承受 1600 QPS，接口再次崩溃。</p>
<p><strong>根因：单机限流只能控制单个进程的流量，对其他节点一无所知。</strong></p>
<p>直觉的修复是均分配额：4 个节点各分 100 QPS。但这引入新问题：</p>
<pre><code>理想中：
  节点A: 100 QPS → 25%
  节点B: 100 QPS → 25%
  节点C: 100 QPS → 25%
  节点D: 100 QPS → 25%

现实中（负载不均）：
  节点A: 240 QPS → 只放行 100，拒绝 140 ✗
  节点B: 120 QPS → 只放行 100，拒绝  20 ✗
  节点C:  30 QPS → 只用了 30，浪费  70
  节点D:  10 QPS → 只用了 10，浪费  90

  总放行：240 QPS（理论可放 400，实际只放了 240）
  → 系统实际吞吐远低于理论上限
</code></pre>
<p>动态调整配额（根据节点负载实时重新分配）？复杂度爆炸——你需要协调机制感知节点上下线、收集实时负载、计算下发配额，这本身就是一个分布式系统问题。</p>
<p><strong>标准答案：将限流状态提升到共享的集中存储中。</strong></p>
<h3>分布式限流的核心原则</h3>
<blockquote>
<p><strong>限流的粒度决定了它的准确性。</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th>保护对象</th>
<th>限流粒度</th>
<th>方案</th>
</tr>
</thead>
<tbody><tr>
<td>本机 CPU/内存</td>
<td>进程级</td>
<td>Guava RateLimiter、Sentinel</td>
</tr>
<tr>
<td>外部 API 配额</td>
<td>系统级（全集群）</td>
<td>Redis 分布式计数器</td>
</tr>
<tr>
<td>业务规则（如用户发送频率）</td>
<td>用户级</td>
<td>Redis + 用户维度 key</td>
</tr>
</tbody></table>
<hr>
<h2>Redis 分布式限流：为什么是标准答案</h2>
<p>Redis 之所以成为分布式限流的事实标准，是因为它的特性精确匹配了限流的每一个核心需求：</p>
<table>
<thead>
<tr>
<th>限流需求</th>
<th>Redis 特性</th>
<th>为什么匹配</th>
</tr>
</thead>
<tbody><tr>
<td>原子性：&quot;读取-判断-递增&quot;必须原子</td>
<td>INCR 原子命令 + Lua 脚本</td>
<td>单线程模型，天然无并发冲突</td>
</tr>
<tr>
<td>极致性能：每个请求都要过限流</td>
<td>内存操作，亚毫秒级延迟</td>
<td>不成为业务瓶颈</td>
</tr>
<tr>
<td>共享状态：所有节点看到同一个计数器</td>
<td>独立服务，集群可访问</td>
<td>分布式协调问题消失</td>
</tr>
<tr>
<td>自动过期：时间窗口结束后计数器清零</td>
<td>Key 级别 TTL</td>
<td>无需额外清理逻辑</td>
</tr>
</tbody></table>
<h3>工程实践：基于 Redis + Lua 的固定窗口限流</h3>
<p><strong>为什么必须用 Lua 脚本？</strong></p>
<p>不用 Lua 的伪代码：</p>
<pre><code>count = redis.GET(key)          -- 步骤1：读取
if count &lt; threshold:           -- 步骤2：判断
    redis.INCR(key)             -- 步骤3：递增
    return ALLOW
else:
    return REJECT
</code></pre>
<p>并发问题：两个节点同时读到 count=399（阈值 400），都判断&quot;未超限&quot;，都执行 INCR。最终 count=401，但两个请求都通过了。高并发下，这种竞态条件被急剧放大，限流形同虚设。</p>
<p><strong>Lua 脚本实现（原子操作）</strong></p>
<pre><code class="language-lua">-- KEYS[1]: 限流 key，如 &quot;rate_limit:sms_api:1609459200&quot;
-- ARGV[1]: 阈值
-- ARGV[2]: 窗口过期时间（秒）

local key = KEYS[1]
local threshold = tonumber(ARGV[1])
local expire_time = tonumber(ARGV[2])

local current = tonumber(redis.call(&#39;GET&#39;, key) or &quot;0&quot;)

if current + 1 &gt; threshold then
    return 0  -- 拒绝
else
    redis.call(&#39;INCR&#39;, key)
    if current == 0 then
        redis.call(&#39;EXPIRE&#39;, key, expire_time)
    end
    return 1  -- 放行
end
</code></pre>
<p><strong>Key 设计规范</strong></p>
<pre><code>格式：rate_limit:{业务标识}:{维度}:{时间窗口}
示例：
  rate_limit:sms_api:global:1609459200       -- 全局短信 API 限流
  rate_limit:login:user:12345:1609459200     -- 用户维度登录限流
  rate_limit:order:tenant:abc:1609459200     -- 租户维度下单限流
</code></pre>
<h3>工程实践：基于 Redis 的滑动窗口限流</h3>
<p>当固定窗口的边界问题不可接受时，可以用 Redis Sorted Set 实现滑动窗口：</p>
<pre><code class="language-lua">-- KEYS[1]: 限流 key
-- ARGV[1]: 阈值
-- ARGV[2]: 窗口大小（毫秒）
-- ARGV[3]: 当前时间戳（毫秒）
-- ARGV[4]: 唯一请求ID

local key = KEYS[1]
local threshold = tonumber(ARGV[1])
local window = tonumber(ARGV[2])
local now = tonumber(ARGV[3])
local request_id = ARGV[4]

-- 移除窗口外的过期记录
redis.call(&#39;ZREMRANGEBYSCORE&#39;, key, 0, now - window)

-- 统计当前窗口内的请求数
local count = redis.call(&#39;ZCARD&#39;, key)

if count &lt; threshold then
    -- 添加当前请求，score 为时间戳
    redis.call(&#39;ZADD&#39;, key, now, request_id)
    redis.call(&#39;PEXPIRE&#39;, key, window)
    return 1  -- 放行
else
    return 0  -- 拒绝
end
</code></pre>
<p><strong>两种 Redis 方案的对比</strong></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>固定窗口（String + INCR）</th>
<th>滑动窗口（Sorted Set）</th>
</tr>
</thead>
<tbody><tr>
<td>存储开销</td>
<td>O(1)，一个 key 一个计数器</td>
<td>O(N)，N 为窗口内请求数</td>
</tr>
<tr>
<td>时间复杂度</td>
<td>O(1)</td>
<td>O(log N)</td>
</tr>
<tr>
<td>精度</td>
<td>边界可能 2 倍峰值</td>
<td>精确</td>
</tr>
<tr>
<td>适用</td>
<td>大部分场景</td>
<td>阈值紧、精度要求高</td>
</tr>
</tbody></table>
<p><strong>工程建议</strong>：优先用固定窗口方案。只有当阈值非常接近系统极限（余量 &lt; 20%）时，才需要滑动窗口的精度。</p>
<h3>关于时钟同步</h3>
<p>分布式系统中，各节点用本地时间计算 Redis key 中的时间窗口标识，时钟偏移可能导致不同节点在不同窗口中计数。严格做法是用 Redis 服务端时间 <code>redis.call(&#39;TIME&#39;)</code>。但现代服务器通过 NTP 同步后的时钟偏差通常在毫秒级，对秒级窗口几乎无影响。</p>
<p><strong>工程判断</strong>：对于秒级窗口，使用本地时间戳即可。对于百毫秒级窗口或对精度有极端要求的场景，使用 Redis 服务端时间。</p>
<hr>
<h2>多层限流：纵深防御架构</h2>
<p>一个常见误区是试图在某一层解决所有限流问题。良好的限流架构应该是分层的——每一层保护不同的东西，承担不同的职责。</p>
<pre><code>                     请求流入
                        ↓
┌──────────────────────────────────────────┐
│  第一层：接入层（Nginx / CDN）            │  ← 挡住恶意流量和 DDoS
│  基于 IP 的连接数和请求速率限制            │
└──────────────────────────────────────────┘
                        ↓
┌──────────────────────────────────────────┐
│  第二层：API 网关（Gateway）              │  ← 业务感知型限流
│  基于用户/租户/API 维度的差异化限流        │
└──────────────────────────────────────────┘
                        ↓
┌──────────────────────────────────────────┐
│  第三层：业务层                           │  ← 业务规则型限流
│  业务语义的频率控制（发帖/下单/发短信）     │
└──────────────────────────────────────────┘
                        ↓
┌──────────────────────────────────────────┐
│  第四层：数据层                           │  ← 最后一道防线
│  连接池 / 线程池隔离 / 熔断器             │
└──────────────────────────────────────────┘
</code></pre>
<h3>各层详细对比</h3>
<table>
<thead>
<tr>
<th>层级</th>
<th>保护对象</th>
<th>限流维度</th>
<th>典型工具</th>
<th>算法</th>
</tr>
</thead>
<tbody><tr>
<td>接入层</td>
<td>基础设施</td>
<td>IP、连接数</td>
<td>Nginx <code>limit_req</code>/<code>limit_conn</code></td>
<td>漏桶</td>
</tr>
<tr>
<td>API 网关</td>
<td>服务处理能力</td>
<td>用户 ID、API Key、租户</td>
<td>Redis + Lua、Sentinel</td>
<td>令牌桶/滑动窗口</td>
</tr>
<tr>
<td>业务层</td>
<td>业务规则</td>
<td>业务实体（用户行为频率）</td>
<td>Redis + 业务代码</td>
<td>固定窗口</td>
</tr>
<tr>
<td>数据层</td>
<td>存储和依赖</td>
<td>并发连接数</td>
<td>连接池、Hystrix、Resilience4j</td>
<td>信号量/熔断</td>
</tr>
</tbody></table>
<h3>各层工程实践</h3>
<p><strong>接入层：Nginx 配置示例</strong></p>
<pre><code class="language-nginx">http {
    # IP 维度的请求速率限制
    limit_req_zone $binary_remote_addr zone=ip_rate:10m rate=100r/s;

    # IP 维度的并发连接数限制
    limit_conn_zone $binary_remote_addr zone=ip_conn:10m;

    server {
        # API 接口：每 IP 100r/s，突发 50
        location /api/ {
            limit_req zone=ip_rate burst=50 nodelay;
            limit_conn ip_conn 50;
            limit_req_status 429;
        }

        # 登录接口：更严格的限制
        location /api/login {
            limit_req zone=ip_rate burst=5;
            limit_req_status 429;
        }
    }
}
</code></pre>
<p><strong>API 网关层：差异化限流</strong></p>
<pre><code class="language-java">// 不同级别用户的限流配置
public class RateLimitConfig {
    // 免费用户：60 次/分钟
    // 付费用户：600 次/分钟
    // 企业用户：6000 次/分钟

    public int getThreshold(User user) {
        return switch (user.getTier()) {
            case FREE       -&gt; 60;
            case PREMIUM    -&gt; 600;
            case ENTERPRISE -&gt; 6000;
        };
    }

    // 不同 API 端点的限流配置
    // 重查询接口：50 QPS
    // 轻量读接口：5000 QPS
    // 写操作接口：200 QPS

    public int getThreshold(String endpoint) {
        return switch (endpoint) {
            case &quot;/api/report/generate&quot; -&gt; 50;    // 计算密集
            case &quot;/api/user/info&quot;       -&gt; 5000;  // 轻量读
            case &quot;/api/order/create&quot;    -&gt; 200;   // 写操作
            default                     -&gt; 1000;
        };
    }
}
</code></pre>
<p><strong>业务层：业务规则型限流</strong></p>
<pre><code class="language-java">// 业务限流的阈值来自产品需求，不是压测
public class BusinessRateLimiter {

    // 防骚扰：每用户每分钟最多 5 条短信
    public boolean allowSendSms(long userId) {
        String key = &quot;biz:sms:&quot; + userId + &quot;:&quot; + currentMinute();
        return redisRateLimiter.tryAcquire(key, 5, 60);
    }

    // 反垃圾：新账号 24 小时内最多发 10 条帖子
    public boolean allowPost(long userId, boolean isNewAccount) {
        if (!isNewAccount) return true;
        String key = &quot;biz:post:new:&quot; + userId + &quot;:&quot; + today();
        return redisRateLimiter.tryAcquire(key, 10, 86400);
    }

    // 运营策略：商家每天最多创建 100 个促销活动
    public boolean allowCreatePromotion(long merchantId) {
        String key = &quot;biz:promo:&quot; + merchantId + &quot;:&quot; + today();
        return redisRateLimiter.tryAcquire(key, 100, 86400);
    }
}
</code></pre>
<p><strong>数据层：隐式限流</strong></p>
<p>数据层的&quot;限流&quot;通常不以限流的名义出现，但本质上发挥着同样的作用：</p>
<ul>
<li><strong>连接池</strong>：连接池满时新请求排队等待 → 并发度上限</li>
<li><strong>线程池隔离</strong>：为每个下游依赖分配独立线程池 → 故障隔离</li>
<li><strong>熔断器</strong>：错误率超阈值时直接停止调用 → 自适应限流</li>
</ul>
<p><strong>每一层保护不同的东西。</strong> 接入层保护基础设施不被滥用流量冲垮；API 网关保护服务处理能力不被超载；业务层保护业务规则不被绕过；数据层保护最脆弱的存储和依赖。</p>
<hr>
<h2>限流之后：被拒绝的请求去哪了</h2>
<p>大多数限流讨论都集中在&quot;如何拒绝&quot;，很少有人思考&quot;拒绝之后怎么办&quot;。而在真实业务中，后者往往更重要。</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>做法</th>
<th>适用场景</th>
<th>风险</th>
</tr>
</thead>
<tbody><tr>
<td><strong>直接拒绝</strong></td>
<td>返回 429 + Retry-After</td>
<td>开放 API、程序化调用方</td>
<td>用户体验差</td>
</tr>
<tr>
<td><strong>排队等待</strong></td>
<td>写入 MQ，消费者限速消费</td>
<td>异步操作（短信、邮件、报表）</td>
<td>队列积压导致延迟不可控</td>
</tr>
<tr>
<td><strong>降级响应</strong></td>
<td>返回缓存/兜底数据</td>
<td>推荐、搜索、详情页非核心模块</td>
<td>数据时效性降低</td>
</tr>
<tr>
<td><strong>引流分担</strong></td>
<td>导向备用路径（CDN/只读副本）</td>
<td>读多写少的场景</td>
<td>需要备用链路的维护成本</td>
</tr>
</tbody></table>
<p><strong>关键原则：限流策略和拒绝策略必须配套设计。</strong></p>
<p>回到短信发送事故：被限流的短信不能直接丢弃，必须进入重试队列。秒杀请求被限流？直接告知&quot;已售罄&quot;比让用户苦等体验更好。商品详情页被限流？返回缓存数据即可，用户感知的是&quot;数据没那么新&quot;而不是&quot;服务挂了&quot;。</p>
<p>只设计了限流而没考虑拒绝后的处理，就像只安装了闸门却没修泄洪渠——水是拦住了，但迟早会溃坝。</p>
<hr>
<h2>阈值从哪来：限流的度量方法论</h2>
<p>所有限流工程中最难的问题不是技术实现，而是：<strong>阈值应该设多少？</strong></p>
<h3>四步确定阈值</h3>
<table>
<thead>
<tr>
<th>步骤</th>
<th>方法</th>
<th>产出</th>
</tr>
</thead>
<tbody><tr>
<td><strong>1. 压测基线</strong></td>
<td>逐步加压，观察 P99 延迟和错误率的拐点</td>
<td>系统实际容量边界</td>
</tr>
<tr>
<td><strong>2. 安全系数</strong></td>
<td>阈值 = 容量边界 × 70%~80%</td>
<td>留出余量应对突发波动</td>
</tr>
<tr>
<td><strong>3. 持续监控</strong></td>
<td>监控 P99、错误率、CPU、内存</td>
<td>发现容量变化及时调整</td>
</tr>
<tr>
<td><strong>4. 渐进调整</strong></td>
<td>从保守值开始，观察线上表现后逐步放宽</td>
<td>避免上线即翻车</td>
</tr>
</tbody></table>
<h3>自适应限流</h3>
<p>更高级的形态是基于实时指标的自动限流。以 Sentinel 为例：</p>
<pre><code class="language-java">// 基于系统负载的自适应限流
SystemRule rule = new SystemRule();
rule.setHighestCpuUsage(0.8);    // CPU &gt; 80% 时触发限流
rule.setHighestSystemLoad(2.5);   // System Load &gt; 2.5 时触发限流
rule.setAvgRt(200);               // 平均 RT &gt; 200ms 时触发限流

// 优点：省去人为猜测阈值
// 风险：正常流量波动可能触发误限，需仔细调试灵敏度
</code></pre>
<h3>阈值是业务决策</h3>
<blockquote>
<p><strong>限流阈值不是纯技术参数，而是一个业务决策。</strong></p>
</blockquote>
<p>它编码的是&quot;我们愿意承受多大负载，以及拒绝超额流量的业务成本是什么&quot;。</p>
<ul>
<li>面向消费者的核心交易链路：拒绝一个请求 = 损失一笔订单 → 阈值宜宽</li>
<li>内部数据分析任务：晚执行几分钟无损失 → 阈值可严</li>
<li>计算密集的报表接口：单个请求消耗大量资源 → 阈值必须严</li>
</ul>
<p>阈值设定必须综合技术容量和业务容忍度，需要工程团队和产品团队协同决策。</p>
<hr>
<h2>总结：限流是一种系统思维</h2>
<p>限流从表面看是算法选择题，但真正落地到生产环境时，它是一个系统设计问题：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>核心问题</th>
</tr>
</thead>
<tbody><tr>
<td><strong>容量</strong></td>
<td>系统到底能承受多少？需要压测和监控，不是拍脑袋</td>
</tr>
<tr>
<td><strong>优先级</strong></td>
<td>必须拒绝时，拒绝谁？VIP vs 普通、核心 vs 边缘、写 vs 读</td>
</tr>
<tr>
<td><strong>失败模式</strong></td>
<td>限流触发后怎么办？报错、排队、降级还是引流</td>
</tr>
<tr>
<td><strong>权衡</strong></td>
<td>平滑性 vs 响应性、精确性 vs 性能、简单性 vs 灵活性</td>
</tr>
</tbody></table>
<p>最好的限流系统是你感觉不到它存在的系统。流量平稳时安静旁观，突增时默默吸收合理突发，真正超限时优雅拒绝——确保已接受的请求仍能正常处理。它不是一堵墙，而是一个阀门：精确控制流量进出，让系统在极端压力下保持可控、可预测、可依赖。</p>
<p><strong>限流的本质，是对系统能力边界的敬畏，以及在边界之内追求最大价值的工程智慧。</strong></p>
5:["$","article",null,{"className":"min-h-screen","children":["$","div",null,{"className":"mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12","children":[["$","header",null,{"className":"mb-8","children":[["$","nav",null,{"className":"flex items-center gap-1 text-sm mb-4","children":[["$","$L13",null,{"href":"/blog/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"博客"}],["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"Engineering"}],[["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/architecture/page/1","className":"text-blue-600 hover:text-blue-700 transition-colors","children":"架构设计"}]]]}],["$","div",null,{"className":"flex items-center mb-6","children":["$","div",null,{"className":"inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal","children":[["$","svg",null,{"className":"w-4 h-4 mr-2 text-gray-400","fill":"none","stroke":"currentColor","viewBox":"0 0 24 24","children":["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","strokeWidth":2,"d":"M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z"}]}],["$","time",null,{"dateTime":"2024-04-08","children":"2024年04月08日"}]]}]}],["$","h1",null,{"className":"text-4xl font-bold text-gray-900 mb-6 text-center","children":"异地多活架构：从原理到实践的演进之路"}],["$","div",null,{"className":"flex flex-wrap gap-2 mb-6 justify-center","children":[["$","$L13","架构设计",{"href":"/blog/tag/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"架构设计"}],["$","$L13","异地多活",{"href":"/blog/tag/%E5%BC%82%E5%9C%B0%E5%A4%9A%E6%B4%BB/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"异地多活"}],["$","$L13","高可用",{"href":"/blog/tag/%E9%AB%98%E5%8F%AF%E7%94%A8/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"高可用"}],["$","$L13","容灾",{"href":"/blog/tag/%E5%AE%B9%E7%81%BE/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"容灾"}],["$","$L13","单元化",{"href":"/blog/tag/%E5%8D%95%E5%85%83%E5%8C%96/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"单元化"}]]}]]}],["$","div",null,{"className":"max-w-5xl mx-auto","children":["$","$L14",null,{"content":"$15"}]}],["$","$10",null,{"fallback":["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":"加载导航中..."}],"children":["$","$L16",null,{"globalNav":{"prev":{"slug":"engineering/middleware/非侵入式SQL监控","title":"非侵入式SQL监控","description":"你有没有因为应用程序没有打印SQL而导致问题排查困难？有没有因为SQL没有显示参数而导致日志毫无意义？有没有因为SQL超长而导致查看痛苦？有没有因为缺少SQL性能监控而导致无法报警？...","pubDate":"2024-04-07","tags":["SQL监控","Java","非侵入式"],"heroImage":"$undefined","content":"$17"},"next":{"slug":"engineering/domain/基于DDD构建微服务：从战略设计到落地实践","title":"基于DDD构建微服务：从战略设计到落地实践","description":"深入探讨领域驱动设计（DDD）如何指导微服务的拆分与设计。从界限上下文、聚合、上下文映射到事件风暴，系统性地阐述 DDD 的战略设计工具如何帮助我们找到正确的服务边界，并通过事件驱动架构和 BFF 模式解决微服务间的通信与协作问题。","pubDate":"2024-04-15","tags":["DDD","微服务","领域驱动设计","架构设计","事件驱动"],"heroImage":"$undefined","content":"$18"}},"tagNav":{"架构设计":{"prev":{"slug":"engineering/architecture/服务注册与发现","title":"服务注册与发现","description":"我们前面在全景架构中对服务注册与发现做了大致的说明，本章我们着重详细说明微服务下注册与发现的这个能力。微服务注册与发现类似于生活中的电话通讯录的概念，它记录了通讯录服务和电话的映射关系。","pubDate":"2024-03-23","tags":["微服务","服务发现","架构设计"],"heroImage":"$undefined","content":"$19"},"next":"$5:props:children:props:children:props:children:2:props:children:props:globalNav:next"},"异地多活":{"prev":{"slug":"engineering/architecture/SET化架构：从单元化原理到大规模落地实践","title":"SET化架构：从单元化原理到大规模落地实践","description":"深入剖析SET化（单元化）架构的核心原理与设计实践，涵盖流量路由、数据分片、全局服务、故障隔离等关键环节，结合美团、阿里等大厂实践经验，构建可水平扩展的弹性架构体系。","pubDate":"2024-03-15","tags":["架构设计","SET化架构","单元化","异地多活","高可用"],"heroImage":"$undefined","content":"$1a"},"next":null},"高可用":{"prev":"$5:props:children:props:children:props:children:2:props:children:props:tagNav:异地多活:prev","next":{"slug":"engineering/architecture/限流的本质：从令牌桶到分布式流控的架构思考","title":"限流的本质：从令牌桶到分布式流控的架构思考","description":"限流不是一个算法问题，而是一个系统设计问题。从单机令牌桶到分布式 Redis 计数器，从 Nginx 接入层到业务层精细化流控——每一层的限流策略背后，都是对系统容量、业务优先级和降级策略的深度思考。","pubDate":"2025-11-25","tags":["限流","分布式系统","系统架构","高可用"],"heroImage":"$undefined","content":"$1b"}},"容灾":{"prev":null,"next":null},"单元化":{"prev":"$5:props:children:props:children:props:children:2:props:children:props:tagNav:异地多活:prev","next":null}}}]}],["$","$L1c",null,{}]]}]}]}]
8:null
c:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
7:null
a:{"metadata":[["$","title","0",{"children":"异地多活架构：从原理到实践的演进之路 - Skyfalling Blog"}],["$","meta","1",{"name":"description","content":"从单机架构到异地多活，系统性梳理多机房部署架构的演进历程。深入剖析同城灾备、同城双活、异地双活、异地多活的核心原理与技术挑战，并结合阿里单元化方案解析工业级落地实践。"}],["$","meta","2",{"property":"og:title","content":"异地多活架构：从原理到实践的演进之路"}],["$","meta","3",{"property":"og:description","content":"从单机架构到异地多活，系统性梳理多机房部署架构的演进历程。深入剖析同城灾备、同城双活、异地双活、异地多活的核心原理与技术挑战，并结合阿里单元化方案解析工业级落地实践。"}],["$","meta","4",{"property":"og:type","content":"article"}],["$","meta","5",{"property":"article:published_time","content":"2024-04-08"}],["$","meta","6",{"property":"article:author","content":"Skyfalling"}],["$","meta","7",{"name":"twitter:card","content":"summary"}],["$","meta","8",{"name":"twitter:title","content":"异地多活架构：从原理到实践的演进之路"}],["$","meta","9",{"name":"twitter:description","content":"从单机架构到异地多活，系统性梳理多机房部署架构的演进历程。深入剖析同城灾备、同城双活、异地双活、异地多活的核心原理与技术挑战，并结合阿里单元化方案解析工业级落地实践。"}],["$","link","10",{"rel":"shortcut icon","href":"/favicon.png"}],["$","link","11",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","link","12",{"rel":"icon","href":"/favicon.png"}],["$","link","13",{"rel":"apple-touch-icon","href":"/favicon.png"}]],"error":null,"digest":"$undefined"}
12:{"metadata":"$a:metadata","error":null,"digest":"$undefined"}
