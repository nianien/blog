1:"$Sreact.fragment"
2:I[10616,["6874","static/chunks/6874-7791217feaf05c17.js","7177","static/chunks/app/layout-142e67ac4336647c.js"],"default"]
3:I[87555,[],""]
4:I[31295,[],""]
6:I[59665,[],"OutletBoundary"]
9:I[74911,[],"AsyncMetadataOutlet"]
b:I[59665,[],"ViewportBoundary"]
d:I[59665,[],"MetadataBoundary"]
f:I[26614,[],""]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/fffdcdb4fb651185.css","style"]
0:{"P":null,"b":"wlOkUxTzHfxl8sQA11M8Z","p":"","c":["","blog","engineering","architecture","%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%9A%8415%E4%B8%AA%E9%94%A6%E5%9B%8A",""],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","engineering/architecture/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%9A%8415%E4%B8%AA%E9%94%A6%E5%9B%8A","c"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/fffdcdb4fb651185.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"zh-CN","children":["$","body",null,{"className":"__className_f367f3","children":["$","div",null,{"className":"min-h-screen flex flex-col","children":[["$","$L2",null,{}],["$","main",null,{"className":"flex-1","children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"bg-[var(--background)]","children":["$","div",null,{"className":"mx-auto max-w-7xl px-6 py-12 lg:px-8","children":["$","p",null,{"className":"text-center text-xs leading-5 text-gray-400","children":["© ",2026," Skyfalling"]}]}]}]]}]}]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","engineering/architecture/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%9A%8415%E4%B8%AA%E9%94%A6%E5%9B%8A","c"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L5",null,["$","$L6",null,{"children":["$L7","$L8",["$","$L9",null,{"promise":"$@a"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","qXfmo_XPVcZAKKK_cb1IFv",{"children":[["$","$Lb",null,{"children":"$Lc"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$Ld",null,{"children":"$Le"}]]}],false]],"m":"$undefined","G":["$f","$undefined"],"s":false,"S":true}
10:"$Sreact.suspense"
11:I[74911,[],"AsyncMetadata"]
13:I[6874,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],""]
14:I[32923,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
16:I[40780,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
1b:I[85300,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
e:["$","div",null,{"hidden":true,"children":["$","$10",null,{"fallback":null,"children":["$","$L11",null,{"promise":"$@12"}]}]}]
15:T3749,<p>记得很久之前，去面试过<strong>字节跳动</strong>。被三面的面试官问了一道场景设计题目：<strong>如何设计一个高并发系统</strong>。当时我回答得比较粗糙，最近回想起来，所以整理了设计高并发系统的15个锦囊，相信大家看完会有帮助的。</p>
<p><img src="/images/blog/engineering/system-image_3_1.png" alt="image_3_1.png"></p>
<h3>如何理解高并发系统 <a href="#item-0-1" id="item-0-1"></a></h3>
<p>所谓设计<strong>高并发</strong>系统，就是设计一个系统，保证它<strong>整体可用</strong>的同时，能够<strong>处理很高的并发用户请求</strong>，能够承受<strong>很大的流量冲击</strong>。</p>
<p>我们要设计高并发的系统，那就需要处理好一些常见的系统瓶颈问题，如<strong>内存不足、磁盘空间不足，连接数不够，网络宽带不够</strong>等等，以应对突发的流量洪峰。</p>
<h3>1. 分而治之，横向扩展 <a href="#item-0-2" id="item-0-2"></a></h3>
<p>如果你<strong>只部署一个应用，只部署一台服务器</strong>，那抗住的流量请求是非常有限的。并且，单体的应用，有单点的风险，如果它挂了，那服务就不可用了。</p>
<p>因此，设计一个高并发系统，我们可以<strong>分而治之，横向扩展</strong>。也就是说，采用分布式部署的方式，部署多台服务器，把流量分流开，让每个服务器都承担一部分的并发和流量，提升<strong>整体系统的并发能力</strong>。</p>
<h3>2. 微服务拆分（系统拆分） <a href="#item-0-3" id="item-0-3"></a></h3>
<p>要提高系统的吞吐，提高系统的处理并发请求的能力。除了采用<strong>分布式部署的方式</strong>外，还可以做<strong>微服务拆分</strong>，这样就可以达到分摊请求流量的目的，提高了并发能力。</p>
<p>所谓的<strong>微服务拆分</strong>，其实就是把一个单体的应用，按功能单一性，拆分为多个服务模块。<strong>比如一个电商系统，拆分为用户系统、订单系统、商品系统等等</strong>。</p>
<p><img src="/images/blog/engineering/system-image_3_2.png" alt="image_3_2.png"></p>
<h3>3. 分库分表 <a href="#item-0-4" id="item-0-4"></a></h3>
<p>当业务量暴增的话，MySQL单机<strong>磁盘容量会撑爆</strong>。并且，我们知道数据库连接数是有限的。<strong>在高并发的场景下</strong>，大量请求访问数据库，<code>MySQL</code>单机是扛不住的！高并发场景下，会出现<code>too many connections</code>报错。</p>
<p>所以高并发的系统，<strong>需要考虑拆分为多个数据库，来抗住高并发的毒打</strong>。而假如你的单表数据量非常大，存储和查询的性能就会遇到瓶颈了，如果你做了很多优化之后还是无法提升效率的时候，就需要考虑做<strong>分表</strong>了。一般千万级别数据量，就需要<strong>分表</strong>，每个表的数据量少一点，提升SQL查询性能。</p>
<p>当面试官问要求你设计一个高并发系统的时候，一般都要说到<strong>分库分表</strong>这个点。</p>
<h3>4. 池化技术 <a href="#item-0-5" id="item-0-5"></a></h3>
<p>在高并发的场景下，<strong>数据库连接数</strong>可能成为瓶颈，因为连接数是有限的。</p>
<p>我们的请求调用数据库时，都会先获取数据库的连接，然后依靠这个连接来查询数据，搞完收工，最后关闭连接，释放资源。如果我们不用数据库连接池的话，每次执行<code>SQL</code>，都要创建连接和销毁连接，这就会导致每个查询请求都变得更慢了，相应的，系统处理用户请求的能力就降低了。</p>
<p>因此，需要使用池化技术，即<strong>数据库连接池、HTTP 连接池、Redis 连接池</strong>等等。使用数据库连接池，可以避免每次查询都新建连接，减少不必要的资源开销，通过复用连接池，<strong>提高系统处理高并发请求的能力</strong>。</p>
<p>同理，我们使用线程池，也能<strong>让任务并行处理，更高效地完成任务</strong>。</p>
<h3>5. 主从分离 <a href="#item-0-6" id="item-0-6"></a></h3>
<p>通常来说，一台单机的MySQL服务器，可以支持<code>500</code>左右的<code>TPS</code>和<code>10000</code>左右的<code>QPS</code>，即单机支撑的<strong>请求访问是有限</strong>的。因此你做了分布式部署，部署了多台机器，部署了主数据库、从数据库。</p>
<p>但是，如果双十一搞活动，流量肯定会猛增的。如果所有的查询请求，都走主库的话，主库肯定扛不住，因为查询请求量是非常非常大的。因此一般都要求做<strong>主从分离</strong>，然后实时性要求不高的读请求，都去读从库，<strong>写的请求或者实时性要求高的请求，才走主库</strong>。这样就很好保护了主库，也提高了系统的吞吐。</p>
<p>当然，如果回答了主从分离，面试官可能扩展开问你<strong>主从复制原理，问你主从延迟问题</strong>等等，这块大家需要<strong>全方位复习好</strong>哈。</p>
<h3>6. 使用缓存 <a href="#item-0-7" id="item-0-7"></a></h3>
<p>无论是操作系统，浏览器，还是一些复杂的中间件，你都可以看到缓存的影子。我们使用缓存，主要是提升系统接口的性能，这样高并发场景，你的系统就可以支持更多的用户同时访问。</p>
<p>常用的缓存包括：<code>Redis</code>缓存，<code>JVM</code>本地缓存，<code>memcached</code>等等。就拿<code>Redis</code>来说，它单机就能轻轻松松应对几万的并发，你读场景的业务，可以用缓存来抗高并发。</p>
<p>缓存虽然用得爽，但是要<strong>注意缓存使用的一些问题</strong>：</p>
<ul>
<li>缓存与数据库的一致性问题</li>
<li>缓存雪崩</li>
<li>缓存穿透</li>
<li>缓存击穿</li>
</ul>
<h3>7. CDN，加速静态资源访问 <a href="#item-0-8" id="item-0-8"></a></h3>
<p>商品图片，<code>icon</code>等等静态资源，可以对页面做<strong>静态化处理，减少访问服务端的请求</strong>。如果用户分布在全国各地，有的在上海，有的在深圳，地域相差很远，网速也各不相同。为了让用户最快访问到页面，可以使用<code>CDN</code>。<code>CDN</code>可以让用户就近获取所需内容。</p>
<p>什么是CDN？</p>
<blockquote>
<p>Content Delivery Network/Content Distribution Network,翻译过来就是内容分发网络，它表示将静态资源分发到位于多个地理位置机房的服务器，可以做到数据就近访问，加速了静态资源的访问速度，因此让系统更好处理正常别的动态请求。</p>
</blockquote>
<h3>8. 消息队列，削锋 <a href="#item-0-9" id="item-0-9"></a></h3>
<p>我们搞一些双十一、双十二等运营活动时，需要<strong>避免流量暴涨，打垮应用系统的风险</strong>。因此一般会引入消息队列，来应对<strong>高并发的场景</strong>。</p>
<p><img src="/images/blog/engineering/system-image_3_3.png" alt="image_3_3.png"></p>
<p>假设你的应用系统每秒最多可以处理<code>2k</code>个请求，每秒却有<code>5k</code>的请求过来，可以引入消息队列，应用系统每秒从消息队列拉<code>2k</code>请求处理得了。</p>
<p>有些伙伴担心这样可能会出现<strong>消息积压</strong>的问题：</p>
<ul>
<li>首先，搞一些运营活动，不会每时每刻都那么多请求过来你的系统（<strong>除非有人恶意攻击</strong>），高峰期过去后，积压的请求可以慢慢处理；</li>
<li>其次，如果消息队列长度超过最大数量，可以直接抛弃用户请求或跳转到错误页面；</li>
</ul>
<h3>9. ElasticSearch <a href="#item-0-10" id="item-0-10"></a></h3>
<p><code>Elasticsearch</code>，大家都使用得比较多了吧，<strong>一般搜索功能都会用到它</strong>。它是一个分布式、高扩展、高实时的搜索与数据分析引擎，简称为<code>ES</code>。</p>
<p>我们在聊高并发，为啥聊到<code>ES</code>呢？因为<code>ES</code>可以扩容方便，天然支撑高并发。<strong>当数据量大的时候，不用动不动就加机器扩容，分库等等</strong>，可以考虑用<code>ES</code>来支持简单的查询搜索、统计类的操作。</p>
<h3>10. 降级熔断 <a href="#item-0-11" id="item-0-11"></a></h3>
<p><strong>熔断降级</strong>是保护系统的一种手段。当前互联网系统一般都是分布式部署的。而分布式系统中偶尔会出现某个基础服务不可用，最终导致整个系统不可用的情况, 这种现象被称为<strong>服务雪崩效应</strong>。</p>
<p>比如分布式调用链路<code>A-&gt;B-&gt;C....</code>，下图所示：</p>
<p><img src="/images/blog/engineering/system-image_3_4.png" alt="image_3_4.png"></p>
<blockquote>
<p>如果服务<code>C</code>出现问题，比如是因为慢<code>SQL</code>导致调用缓慢，那将导致<code>B</code>也会延迟，从而<code>A</code>也会延迟。堵住的<code>A</code>请求会消耗占用系统的线程、IO、CPU等资源。当请求<code>A</code>的服务越来越多，占用计算机的资源也越来越多，最终会导致系统瓶颈出现，造成其他的请求同样不可用，最后导致业务系统崩溃。</p>
</blockquote>
<p>为了应对服务雪崩, 常见的做法是<strong>熔断和降级</strong>。最简单是加开关控制，当下游系统出问题时，开关打开降级，不再调用下游系统。还可以选用开源组件<code>Hystrix</code>来支持。</p>
<p>你要保证设计的系统能应对<strong>高并发场景</strong>，那肯定要考虑<strong>熔断降级</strong>逻辑进来。</p>
<h3>11. 限流 <a href="#item-0-12" id="item-0-12"></a></h3>
<p>限流也是我们应对高并发的一种方案。我们当然希望，在高并发大流量过来时，系统能全部请求都正常处理。但是有时候没办法，系统的CPU、网络带宽、内存、线程等资源都是有限的。因此，我们要考虑限流。</p>
<p>如果你的系统每秒扛住的请求是一千，<strong>如果一秒钟来了十万请求呢</strong>？换个角度就是说，高并发的时候，流量洪峰来了，超过系统的承载能力，怎么办呢？</p>
<p>这时候，我们可以采取限流方案。就是为了保护系统，多余的请求，直接丢弃。</p>
<blockquote>
<p><strong>什么是限流</strong>：在计算机网络中，限流就是控制网络接口发送或接收请求的速率，它可防止DoS攻击和限制Web爬虫。限流，也称流量控制。是指系统在面临高并发，或者大流量请求的情况下，限制新的请求对系统的访问，从而保证系统的稳定性。</p>
</blockquote>
<p>可以使用<code>Guava</code>的<code>RateLimiter</code>单机版限流，也可以使用<code>Redis</code>分布式限流，还可以使用阿里开源组件<code>sentinel</code>限流。</p>
<p>面试的时候，你说到限流这块的话？面试官很大概率会问你限流的算法，因此，大家在准备面试的时候，需要复习一下这几种经典的限流算法哈</p>
<h3>12. 异步 <a href="#item-0-13" id="item-0-13"></a></h3>
<blockquote>
<p>回忆一下什么是同步，什么是异步呢？以<strong>方法调用</strong>为例，它代表<strong>调用方要阻塞等待被调用方法中的逻辑执行完成</strong>。这种方式下，当被调用方法响应时间较长时，会造成调用方长久的阻塞，在高并发下会造成整体系统性能下降甚至发生雪崩。异步调用恰恰相反，调用方不需要等待方法逻辑执行完成就可以返回执行其他的逻辑，在被调用方法执行完毕后再通过回调、事件通知等方式将结果反馈给调用方。</p>
</blockquote>
<p>因此，设计一个高并发的系统，<strong>需要在恰当的场景使用异步</strong>。如何使用异步呢？后端可以借用消息队列实现。比如在海量秒杀请求过来时，先放到消息队列中，快速响应用户，告诉用户请求正在处理中，这样就可以释放资源来处理更多的请求。秒杀请求处理完后，通知用户秒杀抢购成功或者失败。</p>
<h3>13. 接口的常规优化 <a href="#item-0-14" id="item-0-14"></a></h3>
<p>设计一个高并发的系统，需要设计接口的性能足够好，这样系统在相同时间，就可以处理更多的请求。当说到这里的话，可以跟面试官说说接口优化的一些方案了。</p>
<p><img src="/images/blog/engineering/system-image_3_5.png" alt="image_3_5.png"></p>
<h3>14. 压力测试确定系统瓶颈 <a href="#item-0-15" id="item-0-15"></a></h3>
<p>设计高并发系统，离不开最重要的一环，<strong>就是压力测试</strong>。就是在系统上线前，需要对系统进行压力测试，测清楚你的系统支撑的最大并发是多少，确定系统的瓶颈点，让自己心里有底，最好预防措施。</p>
<p>压测完要分析整个调用链路，性能可能出现问题是网络层（如带宽）、Nginx层、服务层、还是数据路缓存等中间件等等。</p>
<p><code>loadrunner</code>是一款不错的压力测试工具，<code>jmeter</code>则是接口性能测试工具，都可以来做下压测。</p>
<h3>15. 应对突发流量峰值：扩容+切流量 <a href="#item-0-16" id="item-0-16"></a></h3>
<p>如果是突发的流量高峰，除了降级、限流保证系统不跨，我们可以采用这两种方案，保证系统尽可能服务用户请求：</p>
<ul>
<li>扩容：<strong>比如增加从库、提升配置的方式</strong>，提升系统/组件的流量承载能力。比如增加<code>MySQL、Redis</code>从库来处理查询请求。</li>
<li>切流量：<strong>服务多机房部署</strong>，如果高并发流量来了，把流量从一个机房切换到另一个机房。</li>
</ul>
17:T6ae1,<h1>架构设计模板</h1>
<p>架构设计文档的价值不在于文档本身，而在于写文档的过程——它迫使我们在动手之前系统性地思考。一份好的设计文档能回答三个问题：<strong>为什么要做、怎么做、做到什么程度算完</strong>。</p>
<p>然而实际工作中，设计文档常见两类问题：要么太空——通篇架构图但缺乏落地细节；要么有遗漏——上线后才发现没考虑容灾、没定义回滚方案。根本原因是缺少一个结构化的思考框架。</p>
<p>本文提供一套经过实践验证的架构设计模板，包含 11 个维度。它的设计思路遵循一条主线：</p>
<blockquote>
<p><strong>问题驱动 → 方案设计 → 工程落地</strong></p>
<ul>
<li>问题驱动（第 1 章）：搞清楚为什么要做，边界在哪</li>
<li>方案设计（第 2-9 章）：从架构到细节，把方案想透</li>
<li>工程落地（第 10-11 章）：怎么部署、怎么分期交付</li>
</ul>
</blockquote>
<p>这 11 个维度既可以作为写设计文档的提纲，也可以当作评审时的 Checklist。每个维度给出<strong>要回答的关键问题</strong>和<strong>具体交付物</strong>，文末附可直接复用的 Markdown 模板。</p>
<hr>
<h3>1. 需求介绍</h3>
<p>需求介绍的核心任务是把「为什么要做」讲清楚。它不是产品需求文档（PRD）的复述，而是从技术视角回答：现状有什么问题、我们打算怎么解决、做到什么程度算成功。</p>
<p><strong>要回答的关键问题：</strong></p>
<ul>
<li><strong>现状与痛点</strong>：当前系统/流程存在什么问题？对业务造成了哪些可量化的影响（故障频率、延迟、人工成本等）？</li>
<li><strong>目标与范围</strong>：新方案要解决哪些问题？同样重要的是——不解决哪些问题？明确的边界能防止需求蔓延。</li>
<li><strong>核心场景</strong>：列出 3-5 个最重要的使用场景。场景是连接需求与设计的桥梁——拆得越细，后面的设计越不容易遗漏。</li>
<li><strong>干系人</strong>：谁是用户？谁会被改动影响？谁需要配合？</li>
<li><strong>约束条件</strong>：时间窗口、预算、技术栈限制、合规要求等。</li>
<li><strong>验收标准</strong>：用可量化的指标定义「做完了」，如 P99 延迟 &lt; 200ms、可用性 &gt; 99.95%、数据一致性延迟 &lt; 1s。</li>
</ul>
<p><strong>实践建议：</strong></p>
<p>用「场景走查」来验证需求完整性——把每个核心场景从头到尾走一遍，记录每一步涉及的系统、数据和人员。走查过程中自然会暴露出遗漏的约束和边界条件。</p>
<p><strong>交付物：</strong> 需求背景文档（含场景列表、干系人矩阵、约束条件、验收标准）</p>
<hr>
<h3>2. 架构总览</h3>
<p>架构总览是整个设计文档的「地图」。评审者和后续加入的开发人员，通常最先看的就是这一章。它需要回答：系统长什么样、分几块、各块之间怎么协作。</p>
<p><strong>多视角描述架构：</strong></p>
<p>业界常用 <a href="https://en.wikipedia.org/wiki/4%2B1_architectural_view_model">4+1 视图模型</a> 或 <a href="https://c4model.com/">C4 模型</a> 来组织架构描述。对于多数项目，以下三个视角已经够用：</p>
<ul>
<li><strong>概念模型</strong>：系统中有哪些核心领域概念？它们之间的关系是什么？概念模型是整个设计的骨架。看似简单的概念定义——比如「部署包 = 介质包 + 配置」——往往直接决定了后续的技术设计。建议用 UML 类图或 ER 图表达。</li>
<li><strong>逻辑架构图</strong>：系统分几层？每层有哪些模块？模块之间的依赖方向是什么？建议按能力分层（接入层 → 业务逻辑层 → 领域服务层 → 基础设施层），并标注每个模块的核心职责。</li>
<li><strong>系统上下文图</strong>（System Context）：聚焦系统边界——哪些能力自研，哪些依赖外部系统？与周边系统的交互协议和数据格式是什么？这张图对于跨团队协作尤其关键。</li>
</ul>
<p><strong>画图原则：</strong></p>
<p>架构图的唯一标准是<strong>易懂</strong>。一些实用建议：</p>
<ul>
<li>每张图只表达一个层次的信息，避免在同一张图中混合部署细节和业务逻辑</li>
<li>用颜色/形状区分不同类型的组件（自研服务、外部依赖、中间件、数据存储）</li>
<li>标注关键数据流的方向和协议</li>
<li>推荐工具：Excalidraw（轻量手绘风）、draw.io（标准流程图）、PlantUML（文本生成图）</li>
</ul>
<p><strong>实践建议：</strong></p>
<p>好的架构图是改出来的，不是一次画对的。建议在正式评审前做一次小范围宣讲，一是统一理解，二是通过反馈优化设计。</p>
<p><strong>交付物：</strong> 概念模型图、逻辑架构图、系统上下文图</p>
<hr>
<h3>3. 核心流程</h3>
<p>架构总览展示了系统的静态结构，核心流程则展示系统的动态行为——各组件如何协作完成具体业务场景。<strong>架构图 + 时序图是设计评审中最有价值的两张图</strong>，前者回答「是什么」，后者回答「怎么运转」。</p>
<p><strong>场景驱动的梳理方法：</strong></p>
<ol>
<li><strong>列出核心场景</strong>：从用户/调用方的视角，挑出最重要的 3-5 个场景（通常就是需求介绍中的核心场景）</li>
<li><strong>画出 Happy Path</strong>：每个场景走一遍完整调用链路，用时序图（Sequence Diagram）标注参与方、调用顺序、数据流向</li>
<li><strong>标注关键路径</strong>：在时序图上标记性能瓶颈点、状态变更点、数据持久化点</li>
<li><strong>补充异常流程</strong>：这是最容易被忽略但最重要的部分——下游超时怎么办？重试是否幂等？消息丢了怎么补偿？数据不一致怎么修复？</li>
</ol>
<p><strong>常见陷阱：</strong></p>
<p>很多设计文档只画了「晴天场景」，对异常路径一笔带过。但线上故障绝大多数发生在异常分支。建议对每个核心流程至少补充以下异常场景：</p>
<ul>
<li>依赖服务不可用</li>
<li>网络超时 / 部分失败</li>
<li>数据不一致（如消息乱序、重复投递）</li>
<li>资源耗尽（连接池满、磁盘满、内存 OOM）</li>
</ul>
<p><strong>交付物：</strong> 核心场景的时序图（含 Happy Path 和关键异常流程）</p>
<hr>
<h3>4. 详细设计</h3>
<p>详细设计是对架构中复杂组件的「放大镜」。不需要面面俱到，但对核心模块和高风险模块必须写清楚。</p>
<p><strong>通常涵盖以下几类：</strong></p>
<p><strong>数据模型</strong></p>
<ul>
<li>核心表结构设计（字段、类型、约束）</li>
<li>索引策略（查询模式决定索引设计，而非反过来）</li>
<li>数据生命周期：冷热分离策略、归档/清理规则、数据保留期限</li>
<li>数据量评估：初始数据量、增长速率、单表上限</li>
</ul>
<p><strong>接口契约</strong></p>
<ul>
<li>对外 API 定义：路径、方法、入参、出参、错误码、版本策略</li>
<li>如涉及多系统协作，还需定义 SPI（扩展点接口）——即「我提供框架，你来实现具体逻辑」的扩展机制</li>
<li>接口幂等性设计：哪些接口需要幂等？幂等 Key 怎么生成？</li>
<li>建议遵循 <a href="https://www.openapis.org/">OpenAPI</a> 规范，便于自动生成文档和客户端代码</li>
</ul>
<p><strong>状态机</strong></p>
<ul>
<li>如业务有复杂状态流转（订单、审批、工单等），一张状态机图比大段文字清晰得多</li>
<li>明确每个状态转换的触发条件、执行动作和失败回退</li>
</ul>
<p><strong>关键算法/策略</strong></p>
<ul>
<li>路由策略（一致性 Hash、权重轮询等）</li>
<li>调度算法（优先级队列、公平调度等）</li>
<li>限流算法（令牌桶、滑动窗口等）</li>
</ul>
<p><strong>实践建议：</strong></p>
<p>详细设计不必一次写完，可以在开发过程中迭代补充。但有两样东西必须在写代码之前定好：<strong>接口契约</strong>和<strong>数据模型</strong>——它们的变更成本最高，影响面最广。</p>
<p><strong>交付物：</strong> 数据模型设计、接口文档（API/SPI）、状态机图（如有）、关键算法说明</p>
<hr>
<h3>5. 高可用设计</h3>
<p>高可用设计回答一个核心问题：<strong>系统的某个部分挂了，整体还能不能用？</strong> 这是从「能跑」到「能扛」的关键一步。</p>
<p><strong>冗余与容灾</strong></p>
<ul>
<li>服务层：是否多实例部署？是否跨可用区（AZ）部署？单个 AZ 故障时服务是否仍然可用？</li>
<li>数据层：数据库是否有主从/多副本？故障切换是自动还是手动？RPO（数据丢失量）和 RTO（恢复时间）的目标是多少？</li>
<li>降级方案：核心链路和非核心链路是否隔离？当非核心依赖不可用时，核心功能是否能继续运行？降级是自动触发还是手动开关？</li>
</ul>
<p><strong>故障检测与自愈</strong></p>
<ul>
<li>健康检查：Liveness Probe（进程是否存活）和 Readiness Probe（是否可接收流量）分别怎么设计？</li>
<li>熔断策略：使用什么熔断器（如 Sentinel、Resilience4j）？熔断阈值和恢复策略如何配置？</li>
<li>限流策略：在哪一层限流（网关层 / 应用层）？限流粒度是什么（全局 / 租户 / 接口）？</li>
<li>隔离机制：线程池隔离、信号量隔离还是进程隔离？</li>
</ul>
<p><strong>数据一致性</strong></p>
<ul>
<li>一致性模型选择：强一致（CP）还是最终一致（AP）？在什么场景下可以接受最终一致？</li>
<li>跨服务一致性方案：Saga、TCC、本地消息表、事务消息等，各有适用场景。选择依据是什么？</li>
<li>补偿机制：当一致性被破坏时，如何检测和修复？是否需要对账任务？</li>
</ul>
<p><strong>可观测性</strong></p>
<ul>
<li>监控三支柱：Metrics（指标）、Logging（日志）、Tracing（链路追踪）各自的方案是什么？</li>
<li>关键监控指标按 <a href="https://grafana.com/blog/2018/08/02/the-red-method-how-to-instrument-your-services/">RED 方法</a> 分类：Rate（请求速率）、Errors（错误率）、Duration（延迟分布）</li>
<li>告警规则：分级（P0/P1/P2/P3）、阈值、通知渠道、响应 SLA</li>
<li>故障定位：如何从告警快速定位到根因？是否有 Runbook（故障手册）？</li>
</ul>
<p><strong>交付物：</strong> 高可用方案说明（含冗余策略、故障恢复流程、可观测性方案、告警清单）</p>
<hr>
<h3>6. 高性能设计</h3>
<p>高性能设计的核心原则是<strong>先定目标，再找瓶颈，最后谈优化</strong>。没有量化目标的优化是盲目的。</p>
<p><strong>性能目标</strong></p>
<ul>
<li>QPS/TPS 目标：峰值多少？日常多少？需要预留多少 Buffer？</li>
<li>延迟目标：P50、P95、P99 分别是多少？（只看平均值会掩盖长尾问题）</li>
<li>数据量级：当前数据量多大？未来 1-3 年的增长预期？</li>
</ul>
<p><strong>瓶颈分析</strong></p>
<ul>
<li>识别系统是 CPU 密集型还是 IO 密集型</li>
<li>找出关键路径上的瓶颈点：数据库查询、外部 API 调用、序列化/反序列化、锁竞争等</li>
<li>使用 <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl 定律</a> 评估优化收益——优化非瓶颈环节收效甚微</li>
</ul>
<p><strong>分层优化策略</strong></p>
<table>
<thead>
<tr>
<th>层次</th>
<th>常用手段</th>
</tr>
</thead>
<tbody><tr>
<td>接入层</td>
<td>CDN 加速、负载均衡、连接复用、协议优化（HTTP/2、gRPC）</td>
</tr>
<tr>
<td>应用层</td>
<td>本地缓存（Caffeine）、分布式缓存（Redis）、异步化（MQ）、批量合并、并行调用</td>
</tr>
<tr>
<td>数据层</td>
<td>读写分离、分库分表、索引优化、热点数据隔离、查询结果缓存</td>
</tr>
<tr>
<td>基础设施</td>
<td>水平扩容、弹性伸缩（HPA）、资源池化、JVM/Runtime 调优</td>
</tr>
</tbody></table>
<p><strong>压测验证</strong></p>
<ul>
<li>工具选择：JMeter（全功能）、wrk/hey（轻量 HTTP）、k6（脚本化场景）</li>
<li>压测策略：阶梯加压找出拐点，而非直接打满</li>
<li>压测环境与生产环境的差异要记录清楚（机器规格、数据量、网络拓扑）</li>
<li>压测报告要包含：吞吐量曲线、延迟分布、资源利用率、瓶颈定位</li>
</ul>
<p><strong>交付物：</strong> 性能目标定义、瓶颈分析、分层优化方案、压测计划</p>
<hr>
<h3>7. 可扩展性设计</h3>
<p>可扩展性回答两个问题：<strong>加功能容不容易（业务扩展性）<strong>和</strong>加机器扛不扛得住更多量（容量扩展性）</strong>。</p>
<p><strong>业务扩展性</strong></p>
<p>好的扩展性设计遵循 <a href="https://en.wikipedia.org/wiki/Open%E2%80%93closed_principle">开闭原则</a>——对扩展开放，对修改关闭。具体评判标准：</p>
<blockquote>
<p>新增一类需求时，是加配置就行，还是要写代码？写代码的话，是新增代码就行，还是要改已有代码？越往前者靠，扩展性越好。</p>
</blockquote>
<p>常见的扩展性手段：</p>
<ul>
<li><strong>插件化 / SPI 机制</strong>：通过接口抽象 + 实现注册，新增场景只需新增实现类</li>
<li><strong>策略模式 + 配置驱动</strong>：将业务规则外化为配置，通过策略分发路由到不同处理逻辑</li>
<li><strong>事件驱动</strong>：核心流程产出事件，扩展功能订阅事件，彼此解耦</li>
<li><strong>合理的领域划分</strong>：按业务能力（而非技术层次）划分模块，模块间通过明确的接口通信</li>
</ul>
<p><strong>容量扩展性</strong></p>
<ul>
<li>服务层：是否无状态？能否直接水平扩容？如果有状态（如本地缓存、WebSocket 长连接），扩容时如何处理？</li>
<li>数据层：数据库如何扩展？是否预留了分片键？分片策略是什么？</li>
<li>消息队列：Partition 数量是否支持后续扩展？Consumer Group 的 Rebalance 策略是什么？</li>
<li>单点瓶颈：系统中是否存在不可水平扩展的单点？如何规避或缓解？</li>
</ul>
<p><strong>交付物：</strong> 扩展点清单、领域划分图、容量扩展方案</p>
<hr>
<h3>8. 安全设计</h3>
<p>安全设计即使当前没有明确需求，也应作为 Checklist 在评审中显式确认。写「经评估，本期暂不涉及」远好过完全不提——前者是有意识的决策，后者是遗漏。</p>
<p><strong>认证与授权</strong></p>
<ul>
<li>认证方案：JWT、OAuth 2.0、Session、OIDC？Token 的签发、刷新和吊销机制？</li>
<li>授权模型：RBAC（基于角色）、ABAC（基于属性）？权限粒度到什么级别（菜单/按钮/数据行）？</li>
<li>服务间认证：内部服务间调用是否需要认证？方案是什么（mTLS、服务账号、JWT 传递）？</li>
</ul>
<p><strong>数据安全</strong></p>
<ul>
<li>敏感数据识别：哪些字段属于 PII（个人可识别信息）？如密码、手机号、身份证号、银行卡号</li>
<li>存储加密：敏感字段是否加密存储？加密算法和密钥管理方案？</li>
<li>数据脱敏：日志、监控、非生产环境中的敏感数据是否脱敏？脱敏规则是什么？</li>
<li>数据合规：是否涉及 GDPR、个人信息保护法等合规要求？数据跨境传输策略？</li>
</ul>
<p><strong>传输安全</strong></p>
<ul>
<li>是否全链路 HTTPS？TLS 版本和加密套件？</li>
<li>内部服务通信是否加密（mTLS）？证书管理方案？</li>
</ul>
<p><strong>审计与防护</strong></p>
<ul>
<li>审计日志：哪些关键操作需要记录？日志包含哪些字段（who/when/what/where）？日志的保留期限？</li>
<li>防攻击：SQL 注入、XSS、CSRF、SSRF 的防护措施？是否使用 WAF？</li>
<li>限流防刷：敏感接口（登录、短信验证码、支付）是否有专门的限流策略？</li>
</ul>
<p><strong>交付物：</strong> 安全设计说明（含认证方案、数据分级与保护策略、审计要求）</p>
<hr>
<h3>9. 技术选型</h3>
<p>技术选型是影响最深远的决策之一——选错了，后续所有人都在还债。好的选型不追求「最先进」，而追求「最合适」。</p>
<p><strong>要回答的关键问题：</strong></p>
<ul>
<li>核心语言和框架的选择依据是什么？</li>
<li>中间件的选择（消息队列、缓存、数据库、搜索引擎等）基于什么考量？</li>
<li>是否做过技术预研或 PoC 验证？结论是什么？</li>
</ul>
<p><strong>选型的评估维度：</strong></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>要点</th>
</tr>
</thead>
<tbody><tr>
<td>功能匹配度</td>
<td>能否满足当前和可预见的未来需求？</td>
</tr>
<tr>
<td>生产成熟度</td>
<td>是否有大规模生产验证？社区活跃度和生态完善度？</td>
</tr>
<tr>
<td>团队匹配度</td>
<td>团队是否熟悉？学习曲线和上手成本？——技术再好，团队用不起来也白搭</td>
</tr>
<tr>
<td>运维成本</td>
<td>部署复杂度、监控支持、故障排查难度、升级迁移成本</td>
</tr>
<tr>
<td>许可证合规</td>
<td>开源许可证是否满足商业需求（注意 AGPL、SSPL 等传染性许可证）？</td>
</tr>
</tbody></table>
<p><strong>实践建议：</strong></p>
<ul>
<li>用 <a href="https://adr.github.io/">ADR（Architecture Decision Records）</a> 记录每个关键技术决策的上下文、选项、决策和后果，方便后续团队理解「当初为什么这么选」</li>
<li>如有多个候选方案，列对比表时不要超过 5 个维度，聚焦最关键的差异点</li>
<li>团队编码规范、Git 工作流、Code Review 流程等工程规范也可以写在这一节</li>
</ul>
<p><strong>交付物：</strong> 技术栈清单、关键选型对比表（或 ADR）、工程规范说明</p>
<hr>
<h3>10. 部署方案</h3>
<p>部署方案不只是「怎么把服务跑起来」，更要回答：怎么安全地发布变更、出了问题怎么快速回滚。</p>
<p><strong>环境规划</strong></p>
<ul>
<li>环境定义：开发（Dev）→ 测试（Test/QA）→ 预发（Staging）→ 生产（Production）</li>
<li>环境隔离：各环境之间如何隔离（独立集群 / Namespace 隔离 / 标签路由）？</li>
<li>配置管理：配置与代码是否分离？环境差异（副本数、资源配额、域名、Feature Flag）通过什么机制管理？推荐 ConfigMap + 密钥管理服务（如 Vault）</li>
</ul>
<p><strong>发布策略</strong></p>
<ul>
<li>滚动更新（Rolling Update）：适合大多数无状态服务，K8s 原生支持</li>
<li>蓝绿部署（Blue-Green）：适合需要快速切换和回滚的场景，需双倍资源</li>
<li>灰度发布（Canary）：适合风险较高的变更，按流量比例 / 用户标签 / 地域逐步放量</li>
<li>发布过程中的健康检查（Readiness Gate）和自动回滚（基于错误率 / 延迟的 Rollback 策略）</li>
</ul>
<p><strong>回滚方案</strong></p>
<ul>
<li>代码回滚流程：谁触发？如何执行？回滚后是否需要通知下游？</li>
<li>数据库回滚：Schema 变更是否向下兼容？是否准备了回滚脚本？建议采用 Expand-Contract 模式处理不兼容变更</li>
<li>配置回滚：配置变更是否有版本化和快速回滚能力？</li>
</ul>
<p><strong>资源规划</strong></p>
<ul>
<li>每个服务的 CPU / 内存 Request 和 Limit 如何设定？建议基于压测数据而非经验估算</li>
<li>是否需要 HPA（Horizontal Pod Autoscaler）？扩缩容的指标和阈值？</li>
<li>存储方案：云盘（持久化）、对象存储（文件/图片）、本地盘（临时缓存）分别用在哪里？</li>
</ul>
<p><strong>交付物：</strong> 部署架构图（物理视图）、环境配置清单、发布策略说明、回滚 Runbook</p>
<hr>
<h3>11. 架构演进规划</h3>
<p>大型项目不可能一步到位，分阶段交付是常态。架构演进规划的目标是让团队在每个阶段都知道做什么、为什么先做这个、后面还要做什么。</p>
<p><strong>MVP 定义</strong></p>
<ul>
<li>最小可用版本的范围是什么？用<strong>场景</strong>定义 MVP——用户能跑通哪些核心场景，就是 MVP</li>
<li>建议在 MVP 之前做一次<strong>架构原型验证</strong>（Walking Skeleton）：用最小的端到端场景跑通整个架构，验证核心技术方案的可行性。这一步能在早期暴露架构层面的问题，避免后期大面积返工</li>
</ul>
<p><strong>里程碑规划</strong></p>
<ul>
<li>按阶段拆分：每期交付什么功能？交付标准是什么？</li>
<li>阶段间的技术依赖：前一期没完成是否会阻塞后一期？有没有可以并行的工作？</li>
<li>建议用甘特图或里程碑表格可视化，让进度一目了然</li>
</ul>
<p><strong>技术债务管理</strong></p>
<ul>
<li>当前设计中有哪些已知的妥协和 TODO？为什么现在不做？</li>
<li>每笔技术债务的「利息」是什么——不偿还会导致什么后果？</li>
<li>计划在什么时间点偿还？建议将技术债务纳入迭代计划，而非无限期搁置</li>
</ul>
<p><strong>团队分工</strong></p>
<ul>
<li>各模块由谁负责？模块间的接口由谁定义、谁联调、谁验收？</li>
<li>团队能力与分工是否匹配？是否需要安排技术预研或培训？</li>
<li>再好的架构，如果不考虑团队的实际能力，也未必落得了地</li>
</ul>
<p><strong>交付物：</strong> MVP 范围定义、里程碑计划表、技术债务台账、团队分工矩阵</p>
<hr>
<h2>附：可复用的架构设计文档模板</h2>
<p>以下模板可直接复制使用，按实际情况填写或删除不需要的章节。</p>
<pre><code class="language-markdown"># [系统名称] 架构设计文档

&gt; 作者：xxx | 日期：yyyy-MM-dd | 版本：v1.0 | 状态：Draft / In Review / Approved

---

## 1. 需求介绍

### 1.1 现状与痛点
&lt;!-- 当前系统存在什么问题？可量化的业务影响？ --&gt;

### 1.2 目标与范围
&lt;!-- 要解决什么？不解决什么（明确边界）？ --&gt;

### 1.3 核心场景
| # | 场景名称 | 场景描述 | 优先级 |
|---|----------|----------|--------|
| 1 |          |          |        |

### 1.4 干系人
| 角色 | 人员 | 职责 |
|------|------|------|
|      |      |      |

### 1.5 约束条件
&lt;!-- 时间、预算、技术栈、合规等 --&gt;

### 1.6 验收标准
| 指标 | 目标值 | 度量方式 |
|------|--------|----------|
|      |        |          |

---

## 2. 架构总览

### 2.1 概念模型
&lt;!-- 核心领域概念及其关系（附图） --&gt;

### 2.2 逻辑架构图
&lt;!-- 系统分层、模块划分、依赖关系（附图） --&gt;

### 2.3 系统上下文
&lt;!-- 与周边系统的交互：协议、数据格式、调用方向（附图） --&gt;

---

## 3. 核心流程

### 3.1 场景一：[场景名称]

**Happy Path：**
&lt;!-- 时序图 --&gt;

**异常流程：**
&lt;!-- 超时 / 下游不可用 / 数据不一致 的处理方式 --&gt;

### 3.2 场景二：[场景名称]
&lt;!-- 同上 --&gt;

---

## 4. 详细设计

### 4.1 数据模型
&lt;!-- 核心表结构、索引策略、数据生命周期 --&gt;

### 4.2 接口契约
&lt;!-- API / SPI 定义：路径、方法、入参、出参、错误码 --&gt;

### 4.3 状态机
&lt;!-- 状态流转图（如有） --&gt;

### 4.4 关键算法
&lt;!-- 核心算法/策略的描述 --&gt;

---

## 5. 高可用设计

### 5.1 冗余与容灾
&lt;!-- 多实例 / 跨 AZ / 主从切换 / 降级方案 --&gt;

### 5.2 故障检测与自愈
&lt;!-- 健康检查 / 熔断 / 限流 / 隔离 --&gt;

### 5.3 数据一致性
&lt;!-- CP vs AP 选择 / 跨服务一致性方案 / 补偿机制 --&gt;

### 5.4 可观测性
| 类型 | 指标/工具 | 告警阈值 | 响应 SLA |
|------|-----------|----------|----------|
| Metrics |        |          |          |
| Logging |        |          |          |
| Tracing |        |          |          |

---

## 6. 高性能设计

### 6.1 性能目标
| 指标 | 目标值 |
|------|--------|
| QPS  |        |
| P99  |        |
| 数据量级 |    |

### 6.2 瓶颈分析
&lt;!-- 关键路径上的瓶颈点及根因分析 --&gt;

### 6.3 优化方案
&lt;!-- 按接入层 / 应用层 / 数据层 / 基础设施分层说明 --&gt;

### 6.4 压测计划
&lt;!-- 工具、场景、环境差异、通过标准 --&gt;

---

## 7. 可扩展性设计

### 7.1 业务扩展性
&lt;!-- 扩展点清单 / SPI 机制 / 领域划分 --&gt;

### 7.2 容量扩展性
&lt;!-- 无状态服务扩容 / 有状态组件扩展 / 单点瓶颈规避 --&gt;

---

## 8. 安全设计

- **认证与授权**：
- **数据安全**：
- **传输安全**：
- **审计日志**：
- **防攻击**：

&lt;!-- 如本期不涉及，请注明「经评估，本期暂不涉及」并说明原因 --&gt;

---

## 9. 技术选型

| 类别 | 选型 | 备选方案 | 选择依据 |
|------|------|----------|----------|
|      |      |          |          |

### 关键决策记录（ADR）
&lt;!-- 对于有争议的选型，记录上下文、选项、决策和后果 --&gt;

---

## 10. 部署方案

### 10.1 环境规划
| 环境 | 集群/NS | 副本数 | 资源配额 | 域名 |
|------|---------|--------|----------|------|
| Dev  |         |        |          |      |
| Test |         |        |          |      |
| Staging |      |        |          |      |
| Prod |         |        |          |      |

### 10.2 发布策略
&lt;!-- 滚动更新 / 蓝绿 / 灰度，以及健康检查和自动回滚机制 --&gt;

### 10.3 回滚方案
&lt;!-- 代码回滚流程 / 数据库兼容性 / 配置回滚 --&gt;

---

## 11. 架构演进规划

### 11.1 MVP 定义
&lt;!-- 第一个版本的最小可用范围（用场景定义） --&gt;

### 11.2 里程碑
| 阶段 | 时间 | 交付内容 | 验收标准 | 依赖 |
|------|------|----------|----------|------|
|      |      |          |          |      |

### 11.3 技术债务
| 债务 | 产生原因 | 影响（利息） | 计划偿还时间 |
|------|----------|--------------|--------------|
|      |          |              |              |

### 11.4 团队分工
| 模块 | 负责人/团队 | 上下游依赖 |
|------|-------------|------------|
|      |             |            |

---

## 附录

### 术语表
| 术语 | 定义 |
|------|------|
|      |      |

### 参考文档
&lt;!-- 相关 PRD、技术预研报告、竞品分析等 --&gt;

### 变更记录
| 版本 | 日期 | 变更人 | 变更内容 |
|------|------|--------|----------|
| v1.0 |      |        | 初稿     |
</code></pre>
18:T5c64,<blockquote>
<p>微服务架构已经成为互联网后端系统的主流架构范式。然而，从单体架构迁移到微服务，绝不仅仅是把代码拆成几个服务那么简单——它涉及服务如何注册与发现、如何通信与容错、如何部署与监控等一系列基础设施问题。本文从架构设计的核心关注点出发，结合业界最佳实践，系统性地梳理微服务架构落地所需的技术体系。</p>
</blockquote>
<h2>微服务架构概览</h2>
<h3>什么是微服务架构？</h3>
<p>与单体（Monolithic）架构不同，微服务架构是由一系列<strong>职责单一的细粒度服务</strong>构成的分布式网状结构，服务之间通过轻量级机制进行通信。这种架构带来了独立部署、技术异构、弹性伸缩等优势，但同时也引入了一系列新的技术挑战。</p>
<h3>核心技术关注点</h3>
<p>一个完整的微服务架构需要关注以下层面：</p>
<table>
<thead>
<tr>
<th>层面</th>
<th>关注点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>通信</strong></td>
<td>服务注册与发现、负载均衡、RPC 框架、API 网关</td>
</tr>
<tr>
<td><strong>可靠性</strong></td>
<td>服务容错（熔断、隔离、限流、降级）</td>
</tr>
<tr>
<td><strong>基础设施</strong></td>
<td>配置中心、缓存、消息队列、数据库</td>
</tr>
<tr>
<td><strong>交付</strong></td>
<td>CI/CD 流水线、自动化测试、灰度发布</td>
</tr>
<tr>
<td><strong>可观测性</strong></td>
<td>日志系统、监控告警、链路追踪</td>
</tr>
<tr>
<td><strong>部署</strong></td>
<td>负载均衡、DNS、CDN</td>
</tr>
</tbody></table>
<p>接下来，我们逐一展开讨论。</p>
<h2>服务注册、发现与负载均衡</h2>
<p>微服务架构下，服务提供方需要注册通告服务地址，服务调用方需要发现目标服务，同时服务提供方一般以集群方式提供服务，这就引入了负载均衡和健康检查问题。</p>
<p>根据负载均衡器（LB）所在位置的不同，目前主要有三种方案：</p>
<h3>方案一：集中式 LB</h3>
<p>在服务消费者和服务提供者之间设置独立的 LB（如 F5 硬件或 LVS/HAProxy 软件），LB 上有所有服务的地址映射表，由运维配置注册。服务消费方通过 DNS 域名指向 LB。</p>
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>实现简单，当前业界主流</td>
<td>单点问题，LB 容易成为瓶颈</td>
</tr>
<tr>
<td>易于做集中式访问控制</td>
<td>增加一跳（hop），有性能开销</td>
</tr>
<tr>
<td></td>
<td>一旦 LB 故障，影响是灾难性的</td>
</tr>
</tbody></table>
<h3>方案二：进程内 LB（客户端负载）</h3>
<p>将 LB 功能以库的形式集成到服务消费方进程内，也称为<strong>软负载（Soft Load Balancing）</strong>。需要配合服务注册表（Service Registry）支持服务自注册和自发现。</p>
<p>工作原理：</p>
<ol>
<li>服务提供方启动时，将地址注册到服务注册表，并定期发送心跳</li>
<li>服务消费方通过内置 LB 组件查询注册表，缓存并定期刷新目标地址列表</li>
<li>以某种负载均衡策略选择目标地址，直接发起请求</li>
</ol>
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>分布式方案，无单点问题</td>
<td>多语言栈需开发多种客户端库</td>
</tr>
<tr>
<td>服务间直接调用，性能好</td>
<td>客户端库升级需服务方重新发布</td>
</tr>
</tbody></table>
<p>典型案例：Netflix OSS（Eureka + Ribbon + Karyon）、阿里 Dubbo。</p>
<h3>方案三：主机独立 LB 进程（Sidecar 模式）</h3>
<p>将 LB 和服务发现功能从进程内移出，变成主机上的独立进程。同一主机上的多个服务共享该 LB 进程完成服务发现和负载均衡。</p>
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>无单点，一个 LB 挂只影响该主机</td>
<td>部署较复杂，环节多</td>
</tr>
<tr>
<td>不需要为不同语言开发客户端库</td>
<td>出错调试排查不方便</td>
</tr>
<tr>
<td>LB 升级不需要服务方改代码</td>
<td></td>
</tr>
</tbody></table>
<p>典型案例：Airbnb SmartStack（Zookeeper + Nerve + Synapse/HAProxy）、Kubernetes 内部服务发现。</p>
<blockquote>
<p>三种方案各有取舍，选择时需要综合考虑团队技术栈的多样性、运维能力和性能要求。当前趋势是方案三（Sidecar 模式）逐渐演化为 Service Mesh（服务网格），如 Istio + Envoy。</p>
</blockquote>
<h2>API 网关（Service Gateway）</h2>
<p>微服务最终需要以某种方式暴露给外部系统访问，这就需要<strong>服务网关</strong>。网关是连接企业内部和外部系统的一道门，承担以下关键职责：</p>
<table>
<thead>
<tr>
<th>职责</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>反向路由</strong></td>
<td>将外部请求路由到内部具体的微服务，对外呈现统一入口</td>
</tr>
<tr>
<td><strong>安全认证</strong></td>
<td>集中处理用户认证、授权和防爬虫</td>
</tr>
<tr>
<td><strong>限流容错</strong></td>
<td>流量高峰期限流保护后台，内部故障时集中容错</td>
</tr>
<tr>
<td><strong>监控</strong></td>
<td>集中监控访问量、调用延迟、错误计数</td>
</tr>
<tr>
<td><strong>日志</strong></td>
<td>收集所有访问日志，为后续分析提供数据</td>
</tr>
</tbody></table>
<p>除此之外，网关还可以实现<strong>线上引流、线上压测、金丝雀发布（Canary Testing）、数据中心双活</strong>等高级功能。</p>
<h3>微服务的分层架构</h3>
<p>引入网关和服务注册表之后，微服务可以简化为两层结构：</p>
<ul>
<li><strong>后端通用服务（Middle Tier Service）</strong>：启动时注册地址到注册表</li>
<li><strong>前端边缘服务（Edge Service）</strong>：查询注册表发现后端服务，对后端服务做聚合和裁剪后暴露给外部设备</li>
</ul>
<p>网关通过查询注册表将外部请求路由到前端服务，整个微服务体系的自注册、自发现和软路由就此串联起来。如果用设计模式的视角看——<strong>网关类似 Proxy/Facade 模式，服务注册表类似 IoC 依赖注入模式</strong>。</p>
<p>常见的网关组件：Netflix Zuul、Kong、APISIX、Spring Cloud Gateway。</p>
<h2>服务容错</h2>
<p>当企业微服务化后，服务之间存在错综复杂的依赖关系。一个前端请求一般依赖多个后端服务（1→N 扇出）。在生产环境中，如果一个应用不能对其依赖的故障进行容错和隔离，就面临被拖垮的风险。在高流量场景下，某个单一后端一旦发生延迟，可能在数秒内导致所有应用资源（线程、队列等）被耗尽，造成<strong>雪崩效应（Cascading Failure）</strong>。</p>
<p>业界总结出以下核心容错模式：</p>
<h3>熔断器模式（Circuit Breaker）</h3>
<p>原理类似家用电路熔断器。当目标服务慢或大量超时时，调用方主动熔断，防止服务被进一步拖垮。</p>
<p>熔断器有三种状态：</p>
<pre><code>Closed（正常）→ Open（熔断）→ Half-Open（半熔断）→ Closed/Open
</code></pre>
<ul>
<li><strong>Closed</strong>：正常状态，请求正常通过</li>
<li><strong>Open</strong>：调用持续出错或超时，进入熔断状态，后续请求直接拒绝（Fail Fast）</li>
<li><strong>Half-Open</strong>：一段时间后允许少量请求尝试，成功则恢复，失败则继续熔断</li>
</ul>
<h3>舱壁隔离模式（Bulkhead Isolation）</h3>
<p>像船舱一样对资源进行隔离。典型实现是<strong>线程隔离</strong>：假定应用 A 调用 Svc1/Svc2/Svc3 三个服务，容器共有 120 个工作线程，可以给每个服务各分配 40 个线程。当 Svc2 变慢时，只有分配给 Svc2 的 40 个线程被耗尽，Svc1 和 Svc3 的 80 个线程不受影响。</p>
<h3>限流（Rate Limiting）</h3>
<p>对服务限定并发访问量，比如单位时间只允许 100 个并发调用，超过限制的请求拒绝并回退。没有限流机制的服务在突发流量（秒杀、大促）时极易被冲垮。</p>
<h3>降级回退（Fallback）</h3>
<p>当熔断或限流发生时的后续处理策略：</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Fail Fast</td>
<td>直接抛出异常</td>
</tr>
<tr>
<td>返回缺省值</td>
<td>返回空值或默认数据</td>
</tr>
<tr>
<td>备份服务</td>
<td>从备份数据源获取数据</td>
</tr>
</tbody></table>
<blockquote>
<p>Netflix 将上述容错模式集成到 Hystrix 开源组件中（现已进入维护模式，社区推荐 Resilience4j 或 Sentinel 作为替代）。Spring Cloud Circuit Breaker 提供了统一的抽象层。</p>
</blockquote>
<h2>服务框架的核心能力</h2>
<p>微服务化后，为了让业务开发人员专注于业务逻辑，避免冗余和重复劳动，需要将公共关注点推到框架层面。一个成熟的服务框架应当封装以下能力：</p>
<table>
<thead>
<tr>
<th>能力</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>服务注册发现</td>
<td>服务端自注册，客户端自发现和负载均衡</td>
</tr>
<tr>
<td>监控日志</td>
<td>框架层日志、Metrics、调用链数据的记录和暴露</td>
</tr>
<tr>
<td>REST/RPC 与序列化</td>
<td>支持 HTTP/REST 和 Binary/RPC，可定制序列化（JSON/Protobuf 等）</td>
</tr>
<tr>
<td>动态配置</td>
<td>运行时动态调整参数和配置</td>
</tr>
<tr>
<td>限流容错</td>
<td>集成限流和熔断组件，结合动态配置实现动态限流</td>
</tr>
<tr>
<td>管理接口</td>
<td>在线查看和动态调整框架及服务内部状态（如 Spring Boot Actuator）</td>
</tr>
<tr>
<td>统一错误处理</td>
<td>框架层统一处理异常并记录日志</td>
</tr>
<tr>
<td>安全</td>
<td>访问控制逻辑的插件化封装</td>
</tr>
<tr>
<td>文档自动生成</td>
<td>如 Swagger/OpenAPI 的自动化文档方案</td>
</tr>
</tbody></table>
<p>当前业界成熟的微服务框架有：Spring Cloud/Spring Boot、Apache Dubbo、Go-Micro、gRPC 等。</p>
<h2>基础设施选型</h2>
<h3>RPC 框架选型</h3>
<p>RPC（Remote Procedure Call）框架大致分为两大流派：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>代表框架</th>
<th>特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>跨语言调用型</strong></td>
<td>gRPC、Thrift、Hprose</td>
<td>支持多语言调用，无服务治理机制</td>
<td>多语言调用场景</td>
</tr>
<tr>
<td><strong>服务治理型</strong></td>
<td>Dubbo、Motan、rpcx</td>
<td>功能丰富，含服务发现和治理能力</td>
<td>大型服务的解耦和治理</td>
</tr>
</tbody></table>
<p><strong>选型建议</strong>：如果是 Java 为主的团队，推荐 <strong>Dubbo</strong>（高性能，性能测试中比 Feign 强约 10 倍）。如果需要跨语言支持，Dubbo 也支持通过 Dubbo-Go 实现 Java + Go 双语言微服务架构。如果是纯粹的跨语言场景，<strong>gRPC</strong> 基于 HTTP/2 + Protobuf，是业界标准选择。</p>
<h3>注册中心选型</h3>
<p>所有的服务发现都依赖于一个高可用的服务注册表。主流选择：</p>
<table>
<thead>
<tr>
<th>注册中心</th>
<th>特点</th>
<th>一致性模型</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Nacos</strong></td>
<td>同时支持注册中心和配置中心，功能全面</td>
<td>AP/CP 可切换</td>
</tr>
<tr>
<td><strong>ZooKeeper</strong></td>
<td>最早的分布式协调服务，生态成熟</td>
<td>CP</td>
</tr>
<tr>
<td><strong>Etcd</strong></td>
<td>Kubernetes 默认存储，高可用和一致性</td>
<td>CP</td>
</tr>
<tr>
<td><strong>Consul</strong></td>
<td>支持多数据中心，内置健康检查</td>
<td>CP</td>
</tr>
<tr>
<td><strong>Eureka</strong></td>
<td>Netflix 开源，AP 模型，已停止维护</td>
<td>AP</td>
</tr>
</tbody></table>
<p><strong>选型建议</strong>：推荐 <strong>Nacos</strong>（nacos + MySQL 高可用部署），一站式解决注册中心和配置中心的需求。</p>
<h3>配置中心选型</h3>
<p>随着系统复杂度增长，配置管理面临越来越高的要求：配置修改实时生效、灰度发布、分环境/分集群管理、完善的权限审核机制。传统的配置文件方式已经无法满足需求。</p>
<p>配置中心的核心架构组件：</p>
<ul>
<li><strong>配置服务端</strong>：集中存储和管理所有配置信息</li>
<li><strong>配置客户端</strong>：通过<strong>定期拉取（Pull）</strong> 或 <strong>服务端推送（Push）</strong> 方式获取配置更新</li>
<li><strong>管理界面</strong>：配置的增删改查和审计</li>
</ul>
<table>
<thead>
<tr>
<th>配置中心</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Nacos</strong></td>
<td>阿里开源，同时支持注册和配置，生态活跃</td>
</tr>
<tr>
<td><strong>Apollo</strong></td>
<td>携程开源，功能完善，支持灰度发布和权限管理</td>
</tr>
<tr>
<td><strong>Spring Cloud Config</strong></td>
<td>Spring 生态原生支持，基于 Git 存储</td>
</tr>
</tbody></table>
<h3>缓存中间件选型</h3>
<table>
<thead>
<tr>
<th>缓存</th>
<th>特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Redis</strong></td>
<td>多数据结构，支持持久化和集群</td>
<td>通用缓存、分布式锁、排行榜等</td>
</tr>
<tr>
<td><strong>Memcached</strong></td>
<td>纯内存 KV，简单高效</td>
<td>简单的对象缓存</td>
</tr>
</tbody></table>
<p><strong>选型建议</strong>：推荐 <strong>Redis Cluster</strong> 高可用集群部署。</p>
<blockquote>
<p>需要特别关注 Redis 的 Big Key 问题。在高并发场景下，Big Key 会导致单个节点内存和网络带宽瓶颈，严重时可造成系统瘫痪。建议制定 Key 规范并定期扫描。</p>
</blockquote>
<h3>消息中间件选型</h3>
<p>消息中间件的三大核心场景：</p>
<table>
<thead>
<tr>
<th>场景</th>
<th>说明</th>
<th>典型案例</th>
</tr>
</thead>
<tbody><tr>
<td><strong>异步处理</strong></td>
<td>减少主流程等待时间，非核心逻辑异步执行</td>
<td>注册后发送邮件、异步更新缓存</td>
</tr>
<tr>
<td><strong>系统解耦</strong></td>
<td>上下游系统通过消息通信，不需要强一致</td>
<td>支付成功后通知 ERP/WMS/推荐等系统</td>
</tr>
<tr>
<td><strong>削峰填谷</strong></td>
<td>大流量请求放入队列，消费者按能力消化</td>
<td>秒杀系统的下单排队</td>
</tr>
</tbody></table>
<p>主流消息中间件对比：</p>
<table>
<thead>
<tr>
<th>中间件</th>
<th>吞吐量</th>
<th>延迟</th>
<th>可靠性</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Kafka</strong></td>
<td>极高</td>
<td>毫秒级</td>
<td>高（可配置）</td>
<td>日志收集、大数据流处理、事件溯源</td>
</tr>
<tr>
<td><strong>RocketMQ</strong></td>
<td>高</td>
<td>毫秒级</td>
<td>极高（事务消息）</td>
<td>电商交易、金融场景</td>
</tr>
<tr>
<td><strong>RabbitMQ</strong></td>
<td>中等</td>
<td>微秒级</td>
<td>高</td>
<td>实时性要求高、路由复杂的场景</td>
</tr>
</tbody></table>
<p><strong>选型建议</strong>：<strong>Kafka</strong> 用于日志采集和大数据场景，<strong>RocketMQ</strong> 用于业务消息和交易场景，二者搭配使用。</p>
<h3>数据库选型</h3>
<h4>关系型数据库</h4>
<table>
<thead>
<tr>
<th>类别</th>
<th>代表</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>传统 RDBMS</strong></td>
<td>MySQL、PostgreSQL</td>
<td>成熟稳定，生态丰富，百万级 PV 搭配主从 + 缓存可满足</td>
</tr>
<tr>
<td><strong>NewSQL</strong></td>
<td>TiDB、CockroachDB</td>
<td>完整 SQL 支持 + ACID 事务 + 弹性伸缩 + 高可用 + 大数据分析能力</td>
</tr>
</tbody></table>
<p>当 MySQL 需要分库分表且逻辑复杂度高、扩展性不足时，可以考虑 TiDB。</p>
<h4>NoSQL 数据库</h4>
<table>
<thead>
<tr>
<th>类型</th>
<th>代表</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>键值型</strong></td>
<td>Redis、Memcache</td>
<td>缓存、会话管理</td>
</tr>
<tr>
<td><strong>列式</strong></td>
<td>HBase、Cassandra</td>
<td>写多读少、时序数据</td>
</tr>
<tr>
<td><strong>文档型</strong></td>
<td>MongoDB、CouchDB</td>
<td>非结构化数据、灵活 Schema</td>
</tr>
<tr>
<td><strong>图数据库</strong></td>
<td>Neo4J</td>
<td>社交网络、推荐系统</td>
</tr>
</tbody></table>
<h2>CI/CD 流水线</h2>
<p>从代码到最终服务用户，可以分为三个阶段：</p>
<pre><code>Code → Artifact（制品库）→ Running Service → Production
</code></pre>
<ol>
<li><strong>代码到制品</strong>：持续构建，制品集中管理</li>
<li><strong>制品到服务</strong>：部署到指定环境</li>
<li><strong>开发到生产</strong>：变更在不同环境间的迁移和灰度发布</li>
</ol>
<h3>工具链推荐</h3>
<table>
<thead>
<tr>
<th>环节</th>
<th>推荐工具</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>代码管理</strong></td>
<td>GitLab</td>
<td>社区版功能丰富，结合 Gerrit 做 Code Review</td>
</tr>
<tr>
<td><strong>持续集成</strong></td>
<td>Jenkins / GitLab CI</td>
<td>Jenkins 插件生态强大；GitLab CI 与 GitLab 深度集成</td>
</tr>
<tr>
<td><strong>制品仓库</strong></td>
<td>Harbor</td>
<td>开源的 Docker 镜像仓库，支持镜像签名和漏洞扫描</td>
</tr>
<tr>
<td><strong>部署编排</strong></td>
<td>Kubernetes</td>
<td>容器编排的事实标准，支持声明式部署和自动伸缩</td>
</tr>
<tr>
<td><strong>项目管理</strong></td>
<td>Jira + Confluence</td>
<td>项目管理、任务跟踪和知识管理的行业标配</td>
</tr>
</tbody></table>
<p><strong>初期建议</strong>：Jenkins + GitLab + Harbor 的组合，可以覆盖制品管理、发布流程、权限控制、版本变更和服务回滚。</p>
<h3>自动化测试</h3>
<p>自动化测试平台是 CI/CD 流水线的重要一环：</p>
<ul>
<li><strong>单元测试</strong>：JUnit / TestNG，覆盖核心业务逻辑</li>
<li><strong>接口测试</strong>：可基于开源框架（如 SpringBoot + TestNG）搭建</li>
<li><strong>性能测试</strong>：JMeter / Gatling</li>
<li><strong>端到端测试</strong>：Selenium / Cypress</li>
</ul>
<h2>可观测性体系</h2>
<h3>日志系统</h3>
<p>日志系统涵盖日志打印、采集、中转、存储、分析、搜索和分发。日志系统的建设不仅是工具建设，还包括规范和组件建设——基本的日志（如全链路追踪 ID）应在框架和组件层面统一注入。</p>
<p><strong>常规方案：ELK Stack</strong></p>
<table>
<thead>
<tr>
<th>组件</th>
<th>职责</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Filebeat</strong></td>
<td>轻量级日志采集器，替代 Logstash-Forwarder</td>
</tr>
<tr>
<td><strong>Logstash</strong></td>
<td>日志收集、过滤和转换</td>
</tr>
<tr>
<td><strong>Elasticsearch</strong></td>
<td>分布式搜索引擎，存储和索引日志</td>
</tr>
<tr>
<td><strong>Kibana</strong></td>
<td>可视化界面，日志搜索和分析</td>
</tr>
</tbody></table>
<blockquote>
<p>免费版 ELK 没有安全机制，建议前置 Nginx 做反向代理和简单用户认证。</p>
</blockquote>
<p><strong>实时计算方案</strong>：对于需要实时分析的场景，可以采用 Flume + Kafka + Flink（或 Storm）的架构。Kafka 负责高吞吐的消息缓冲，Flume 负责多样化的数据采集，Flink 负责实时流计算。</p>
<h3>监控系统</h3>
<p>监控系统主要覆盖两个层面：</p>
<table>
<thead>
<tr>
<th>层面</th>
<th>监控指标</th>
</tr>
</thead>
<tbody><tr>
<td><strong>基础设施</strong></td>
<td>机器负载、IO、网络流量、CPU、内存</td>
</tr>
<tr>
<td><strong>服务质量</strong></td>
<td>可用性、成功率、失败率、QPS、延迟</td>
</tr>
</tbody></table>
<p><strong>推荐方案：Prometheus + Grafana</strong></p>
<p>Prometheus 是 Google BorgMon 的开源版本，使用 Go 开发，采用 <strong>Pull</strong> 模式主动拉取指标数据。其核心组件：</p>
<table>
<thead>
<tr>
<th>组件</th>
<th>职责</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Prometheus Server</strong></td>
<td>数据采集和存储，提供 PromQL 查询</td>
</tr>
<tr>
<td><strong>Exporter</strong></td>
<td>各类数据采集组件（数据库、硬件、MQ、HTTP 服务器等）</td>
</tr>
<tr>
<td><strong>Push Gateway</strong></td>
<td>支持短生命周期 Job 主动推送指标</td>
</tr>
<tr>
<td><strong>Alertmanager</strong></td>
<td>灵活的报警规则和通知管理</td>
</tr>
<tr>
<td><strong>Grafana</strong></td>
<td>高度定制化的可视化监控面板</td>
</tr>
</tbody></table>
<p>Prometheus + Grafana 搭配统一的服务框架，可以满足绝大部分中小团队的监控需求。</p>
<h2>生产环境部署架构</h2>
<h3>DNS</h3>
<p>DNS 是基础服务，一般直接选择云厂商：</p>
<ul>
<li><strong>国内</strong>：阿里云 DNS 或腾讯 DNSPod，线上产品建议使用付费版</li>
<li><strong>海外</strong>：优先选择 AWS Route 53</li>
<li><strong>国内外互通</strong>：建议在 APP 层实现容灾逻辑或智能调度，因为没有单一 DNS 服务能同时很好地覆盖国内外</li>
</ul>
<h3>负载均衡（LB）</h3>
<table>
<thead>
<tr>
<th>场景</th>
<th>方案</th>
</tr>
</thead>
<tbody><tr>
<td>云服务环境</td>
<td>直接使用云厂商 LB（阿里云 SLB / 腾讯云 CLB / AWS ELB）</td>
</tr>
<tr>
<td>自建机房</td>
<td>LVS（四层）+ Nginx（七层）</td>
</tr>
</tbody></table>
<p>云厂商 LB 通常支持四层（TCP/UDP）和七层（HTTP/HTTPS）协议、集中化证书管理和健康检查。</p>
<h3>CDN</h3>
<p>CDN 的选型主要看业务覆盖区域：</p>
<table>
<thead>
<tr>
<th>区域</th>
<th>推荐</th>
</tr>
</thead>
<tbody><tr>
<td>国内</td>
<td>阿里云 CDN、腾讯云 CDN</td>
</tr>
<tr>
<td>海外</td>
<td>AWS CloudFront、Akamai</td>
</tr>
</tbody></table>
<h2>总结</h2>
<p>微服务架构的落地是一个系统工程，核心技术关注点可以归纳为以下几个层面：</p>
<ol>
<li><strong>服务通信</strong>：通过注册中心 + 负载均衡 + API 网关，构建服务间和内外部的通信体系</li>
<li><strong>服务可靠性</strong>：通过熔断、隔离、限流和降级四大模式，保障系统在故障和高峰期的稳定性</li>
<li><strong>服务框架</strong>：将公共关注点下沉到框架层，让业务开发专注于业务逻辑</li>
<li><strong>基础设施</strong>：根据业务需求和团队技术栈，选择合适的 RPC、注册中心、缓存、消息队列和数据库</li>
<li><strong>持续交付</strong>：通过 CI/CD 流水线实现代码到生产环境的自动化、可重复的发布流程</li>
<li><strong>可观测性</strong>：通过日志、监控和链路追踪构建系统的透明度，为问题排查和性能优化提供数据支撑</li>
</ol>
<p>好的架构不是设计出来的，而是演进出来的。架构师需要在不同阶段做出合适的判断——既不过度设计，也不欠缺考虑。关键是保持对技术的敏锐度，在实践中不断验证和调整。</p>
<blockquote>
<p>路漫漫其修远兮，架构求索无止尽也。</p>
</blockquote>
19:T824e,<h3>整体思考 <a href="#item-0-2" id="item-0-2"></a></h3>
<h4>1 秒杀存在的问题 <a href="#item-0-3" id="item-0-3"></a></h4>
<p>对于一个日常平稳的业务系统，如果直接开通秒杀功能的话，往往会出现很多问题——</p>
<table>
<thead>
<tr>
<th>干系人</th>
<th>问题分类</th>
<th>业务出现的问题</th>
<th>设计要求</th>
</tr>
</thead>
<tbody><tr>
<td>用户</td>
<td>体验较差</td>
<td>秒杀开始，系统瞬间承受平时数十倍甚至上百倍的流量，直接宕掉</td>
<td>高性能</td>
</tr>
<tr>
<td></td>
<td></td>
<td>用户下单后却付不了款，显示商品已经被其他人买走了</td>
<td>一致性</td>
</tr>
<tr>
<td>商家</td>
<td>商品超卖</td>
<td>100 件商品，却出现 200 人下单成功，成功下单买到商品的人数远远超过活动商品数量的上限</td>
<td>一致性</td>
</tr>
<tr>
<td></td>
<td>资金受损</td>
<td>竞争对手通过恶意下单的方式将活动商品全部下单，导致库存清零，商家无法正常售卖</td>
<td>高可用</td>
</tr>
<tr>
<td></td>
<td></td>
<td>秒杀器猖獗，黄牛通过秒杀器扫货，商家无法达到营销目的</td>
<td>高可用</td>
</tr>
<tr>
<td>平台</td>
<td>风险不可控</td>
<td>系统的其它与秒杀活动不相关的模块变得异常缓慢，业务影响面扩散</td>
<td>高可用</td>
</tr>
<tr>
<td></td>
<td>拖垮网站</td>
<td>在线人数创新高，核心链路涉及的上下游服务从前到后都在告警</td>
<td>高性能</td>
</tr>
<tr>
<td></td>
<td></td>
<td>库存只有一份，所有请求集中读写同一个数据，DB 出现单点</td>
<td>高性能</td>
</tr>
</tbody></table>
<h4>2 设计方向的思考 <a href="#item-0-4" id="item-0-4"></a></h4>
<p>秒杀本质是要求一个瞬时高发下的承压系统，这也是其区别于其他业务的核心场景。对日常系统秒杀产生的问题逐一进行拆解分类，秒杀对应到架构设计，其实就是高可用、一致性和高性能的要求。关于秒杀系统的设计思考，本文即基于此 3 层依次推进，简述如下——</p>
<ul>
<li>高性能。 秒杀涉及高读和高写的支持，如何支撑高并发，如何抵抗高IOPS？核心优化理念其实是类似的：高读就尽量&quot;少读&quot;或&quot;读少&quot;，高写就数据拆分。本文将从动静分离、热点优化以及服务端性能优化 3 个方面展开</li>
<li>一致性。 秒杀的核心关注是商品库存，有限的商品在同一时间被多个请求同时扣减，而且要保证准确性，显而易见是一个难题。如何做到既不多又不少？本文将从业界通用的几种减库存方案切入，讨论一致性设计的核心逻辑</li>
<li>高可用。 大型分布式系统在实际运行过程中面对的工况是非常复杂的，业务流量的突增、依赖服务的不稳定、应用自身的瓶颈、物理资源的损坏等方方面面都会对系统的运行带来大大小小的的冲击。如何保障应用在复杂工况环境下还能高效稳定运行，如何预防和面对突发问题，系统设计时应该从哪些方面着手？本文将从架构落地的全景视角进行关注思考</li>
</ul>
<h3>高性能 <a href="#item-0-5" id="item-0-5"></a></h3>
<h4>1 动静分离 <a href="#item-0-6" id="item-0-6"></a></h4>
<p>大家可能会注意到，秒杀过程中你是不需要刷新整个页面的，只有时间在不停跳动。这是因为一般都会对大流量的秒杀系统做系统的静态化改造，即数据意义上的动静分离。动静分离三步走：1、数据拆分；2、静态缓存；3、数据整合。</p>
<p><strong>1.1 数据拆分</strong></p>
<p>动静分离的首要目的是将动态页面改造成适合缓存的静态页面。因此第一步就是分离出动态数据，主要从以下 2 个方面进行：</p>
<ol>
<li>用户。用户身份信息包括登录状态以及登录画像等，相关要素可以单独拆分出来，通过动态请求进行获取；与之相关的广平推荐，如用户偏好、地域偏好等，同样可以通过异步方式进行加载</li>
<li>时间。秒杀时间是由服务端统一管控的，可以通过动态请求进行获取</li>
</ol>
<p>这里你可以打开电商平台的一个秒杀页面，看看这个页面里都有哪些动静数据。</p>
<p><strong>1.2 静态缓存</strong></p>
<p>分离出动静态数据之后，第二步就是将静态数据进行合理的缓存，由此衍生出两个问题：1、怎么缓存；2、哪里缓存</p>
<p><strong>1.2.1 怎么缓存</strong></p>
<p>静态化改造的一个特点是直接缓存整个 HTTP 连接而不是仅仅缓存静态数据，如此一来，Web 代理服务器根据请求 URL，可以直接取出对应的响应体然后直接返回，响应过程无需重组 HTTP 协议，也无需解析 HTTP 请求头。而作为缓存键，URL唯一化是必不可少的，只是对于商品系统，URL 天然是可以基于商品 ID 来进行唯一标识的，比如淘宝的 <a href="https://link.segmentfault.com/?enc=MYTx%2B1O98e0H2OhCbw2yrg%3D%3D.eRmrBGs8O3Hv0%2BVZ4yO3J8RpgwhKne1xP0L6oRT4KFKWY4nTZucRo5o6QTC3xhtN">https://item.taobao.com/item....</a>。</p>
<p><strong>1.2.2 哪里缓存</strong></p>
<p>静态数据缓存到哪里呢？可以有三种方式：1、浏览器；2、CDN ；3、服务端。</p>
<p>浏览器当然是第一选择，但用户的浏览器是不可控的，主要体现在如果用户不主动刷新，系统很难主动地把消息推送给用户（注意，当讨论静态数据时，潜台词是 “相对不变”，言外之意是 “可能会变”），如此可能会导致用户端在很长一段时间内看到的信息都是错误的。对于秒杀系统，保证缓存可以在秒级时间内失效是不可或缺的。</p>
<p>服务端主要进行动态逻辑计算及加载，本身并不擅长处理大量连接，每个连接消耗内存较多，同时 Servlet 容器解析 HTTP 较慢，容易侵占逻辑计算资源；另外，静态数据下沉至此也会拉长请求路径。</p>
<p>因此通常将静态数据缓存在 CDN，其本身更擅长处理大并发的静态文件请求，既可以做到主动失效，又离用户尽可能近，同时规避 Java 语言层面的弱点。需要注意的是，上 CDN 有以下几个问题需要解决：</p>
<ol>
<li>失效问题。任何一个缓存都应该是有时效的，尤其对于一个秒杀场景。所以，系统需要保证全国各地的 CDN 在秒级时间内失效掉缓存信息，这实际对 CDN 的失效系统要求是很高的</li>
<li>命中率问题。高命中是缓存系统最为核心的性能要求，不然缓存就失去了意义。如果将数据放到全国各地的 CDN ，势必会导致请求命中同一个缓存的可能性降低，那么命中率就成为一个问题</li>
</ol>
<p>因此，将数据放到全国所有的 CDN 节点是不太现实的，失效问题、命中率问题都会面临比较大的挑战。更为可行的做法是选择若干 CDN 节点进行静态化改造，节点的选取通常需要满足以下几个条件：</p>
<ol>
<li>临近访问量集中的地区</li>
<li>距离主站较远的地区</li>
<li>节点与主站间网络质量良好的地区</li>
</ol>
<p>基于以上因素，选择 CDN 的二级缓存比较合适，因为二级缓存数量偏少，容量也更大，访问量相对集中，这样就可以较好解决缓存的失效问题以及命中率问题，是当前比较理想的一种 CDN 化方案。部署方式如下图所示：</p>
<p><img src="/images/blog/engineering/system-image_1_2.png" alt="image_1_2.png"></p>
<p><strong>1.3 数据整合</strong></p>
<p>分离出动静态数据之后，前端如何组织数据页就是一个新的问题，主要在于动态数据的加载处理，通常有两种方案：ESI（Edge Side Includes）方案和 CSI（Client Side Include）方案。</p>
<ol>
<li>ESI 方案：Web 代理服务器上请求动态数据，并将动态数据插入到静态页面中，用户看到页面时已经是一个完整的页面。这种方式对服务端性能要求高，但用户体验较好</li>
<li>CSI 方案：Web 代理服务器上只返回静态页面，前端单独发起一个异步 JS 请求动态数据。这种方式对服务端性能友好，但用户体验稍差</li>
</ol>
<p><strong>1.4 小结</strong></p>
<p>动静分离对于性能的提升，抽象起来只有两点，一是数据要尽量少，以便减少没必要的请求，二是路径要尽量短，以便提高单次请求的效率。具体方法其实就是基于这个大方向进行的。</p>
<h4>2 热点优化 <a href="#item-0-7" id="item-0-7"></a></h4>
<p>热点分为热点操作和热点数据，以下分开进行讨论。</p>
<p><strong>2.1 热点操作</strong></p>
<p>零点刷新、零点下单、零点添加购物车等都属于热点操作。热点操作是用户的行为，不好改变，但可以做一些限制保护，比如用户频繁刷新页面时进行提示阻断。</p>
<p><strong>2.2 热点数据</strong></p>
<p>热点数据的处理三步走，一是热点识别，二是热点隔离，三是热点优化。</p>
<p><strong>2.2.1 热点识别</strong></p>
<p>热点数据分为静态热点和动态热点，具体如下：</p>
<ol>
<li>静态热点：能够提前预测的热点数据。大促前夕，可以根据大促的行业特点、活动商家等纬度信息分析出热点商品，或者通过卖家报名的方式提前筛选；另外，还可以通过技术手段提前预测，例如对买家每天访问的商品进行大数据计算，然后统计出 TOP N 的商品，即可视为热点商品</li>
<li>动态热点：无法提前预测的热点数据。冷热数据往往是随实际业务场景发生交替变化的，尤其是如今直播卖货模式的兴起——带货商临时做一个广告，就有可能导致一件商品在短时间内被大量购买。由于此类商品日常访问较少，即使在缓存系统中一段时间后也会被逐出或过期掉，甚至在db中也是冷数据。瞬时流量的涌入，往往导致缓存被击穿，请求直接到达DB，引发DB压力过大</li>
</ol>
<p>因此秒杀系统需要实现热点数据的动态发现能力，一个常见的实现思路是：</p>
<ol>
<li>异步采集交易链路各个环节的热点 Key 信息，如 Nginx采集访问URL或 Agent 采集热点日志（一些中间件本身已具备热点发现能力），提前识别潜在的热点数据</li>
<li>聚合分析热点数据，达到一定规则的热点数据，通过订阅分发推送到链路系统，各系统根据自身需求决定如何处理热点数据，或限流或缓存，从而实现热点保护</li>
</ol>
<p>需要注意的是：</p>
<ol>
<li>热点数据采集最好采用异步方式，一方面不会影响业务的核心交易链路，一方面可以保证采集方式的通用性</li>
<li>热点发现最好做到秒级实时，这样动态发现才有意义，实际上也是对核心节点的数据采集和分析能力提出了较高的要求</li>
</ol>
<p><strong>2.2.2 热点隔离</strong></p>
<p>热点数据识别出来之后，第一原则就是将热点数据隔离出来，不要让 1% 影响到另外的 99%，可以基于以下几个层次实现热点隔离：</p>
<ol>
<li>业务隔离。秒杀作为一种营销活动，卖家需要单独报名，从技术上来说，系统可以提前对已知热点做缓存预热</li>
<li>系统隔离。系统隔离是运行时隔离，通过分组部署和另外 99% 进行分离，另外秒杀也可以申请单独的域名，入口层就让请求落到不同的集群中</li>
<li>数据隔离。秒杀数据作为热点数据，可以启用单独的缓存集群或者DB服务组，从而更好的实现横向或纵向能力扩展</li>
</ol>
<p>当然，实现隔离还有很多种办法。比如，可以按照用户来区分，为不同的用户分配不同的 Cookie，入口层路由到不同的服务接口中；再比如，域名保持一致，但后端调用不同的服务接口；又或者在数据层给数据打标进行区分等等，这些措施的目的都是把已经识别的热点请求和普通请求区分开来。</p>
<p><strong>2.2.3 热点优化</strong></p>
<p>热点数据隔离之后，也就方便对这 1% 的请求做针对性的优化，方式无外乎两种：</p>
<ol>
<li>缓存：热点缓存是最为有效的办法。如果热点数据做了动静分离，那么可以长期缓存静态数据</li>
<li>限流：流量限制更多是一种保护机制。需要注意的是，各服务要时刻关注请求是否触发限流并及时进行review</li>
</ol>
<p><strong>2.2.4 小结</strong></p>
<p>数据的热点优化与动静分离是不一样的，热点优化是基于二八原则对数据进行了纵向拆分，以便进行针对性地处理。热点识别和隔离不仅对“秒杀”这个场景有意义，对其他的高性能分布式系统也非常有参考价值。</p>
<h4>3 系统优化 <a href="#item-0-8" id="item-0-8"></a></h4>
<p>对于一个软件系统，提高性能可以有很多种手段，如提升硬件水平、调优JVM 性能，这里主要关注代码层面的性能优化——</p>
<ol>
<li>减少序列化：减少 Java 中的序列化操作可以很好的提升系统性能。序列化大部分是在 RPC 阶段发生，因此应该尽量减少 RPC 调用，一种可行的方案是将多个关联性较强的应用进行 “合并部署”，从而减少不同应用之间的 RPC 调用（微服务设计规范）</li>
<li>直接输出流数据：只要涉及字符串的I/O操作，无论是磁盘 I/O 还是网络 I/O，都比较耗费 CPU 资源，因为字符需要转换成字节，而这个转换又必须查表编码。所以对于常用数据，比如静态字符串，推荐提前编码成字节并缓存，具体到代码层面就是通过 OutputStream() 类函数从而减少数据的编码转换；另外，热点方法toString()不要直接调用ReflectionToString实现，推荐直接硬编码，并且只打印DO的基础要素和核心要素</li>
<li>裁剪日志异常堆栈：无论是外部系统异常还是应用本身异常，都会有堆栈打出，超大流量下，频繁的输出完整堆栈，只会加剧系统当前负载。可以通过日志配置文件控制异常堆栈输出的深度</li>
<li>去组件框架：极致优化要求下，可以去掉一些组件框架，比如去掉传统的 MVC 框架，直接使用 Servlet 处理请求。这样可以绕过一大堆复杂且用处不大的处理逻辑，节省毫秒级的时间，当然，需要合理评估你对框架的依赖程度</li>
</ol>
<h4>4 总结一下 <a href="#item-0-9" id="item-0-9"></a></h4>
<p>性能优化需要一个基准值，所以系统还需要做好应用基线，比如性能基线（何时性能突然下降）、成本基线（去年大促用了多少机器）、链路基线（核心流程发生了哪些变化），通过基线持续关注系统性能，促使系统在代码层面持续提升编码质量、业务层面及时下掉不合理调用、架构层面不断优化改进。</p>
<h3>一致性 <a href="#item-0-10" id="item-0-10"></a></h3>
<p>秒杀系统中，库存是个关键数据，卖不出去是个问题，超卖更是个问题。秒杀场景下的一致性问题，主要就是库存扣减的准确性问题。</p>
<h4>1 减库存的方式 <a href="#item-0-11" id="item-0-11"></a></h4>
<p>电商场景下的购买过程一般分为两步：下单和付款。“提交订单”即为下单，“支付订单”即为付款。基于此设定，减库存一般有以下几个方式：</p>
<ol>
<li>下单减库存。买家下单后，扣减商品库存。下单减库存是最简单的减库存方式，也是控制最为精确的一种</li>
<li>付款减库存。买家下单后，并不立即扣减库存，而是等到付款后才真正扣减库存。但因为付款时才减库存，如果并发比较高，可能出现买家下单后付不了款的情况，因为商品已经被其他人买走了</li>
<li>预扣库存。这种方式相对复杂一些，买家下单后，库存为其保留一定的时间（如 15 分钟），超过这段时间，库存自动释放，释放后其他买家可以购买</li>
</ol>
<p>能够看到，减库存方式是基于购物过程的多阶段进行划分的，但无论是在下单阶段还是付款阶段，都会存在一些问题，下面进行具体分析。</p>
<h4>2 减库存的问题 <a href="#item-0-12" id="item-0-12"></a></h4>
<p><strong>2.1 下单减库存</strong></p>
<p>优势：用户体验最好。下单减库存是最简单的减库存方式，也是控制最精确的一种。下单时可以直接通过数据库事务机制控制商品库存，所以一定不会出现已下单却付不了款的情况。</p>
<p>劣势：可能卖不出去。正常情况下，买家下单后付款概率很高，所以不会有太大问题。但有一种场景例外，就是当卖家参加某个促销活动时，竞争对手通过恶意下单的方式将该商品全部下单，导致库存清零，那么这就不能正常售卖了——要知道，恶意下单的人是不会真正付款的，这正是 “下单减库存” 的不足之处。</p>
<p><strong>2.2 付款减库存</strong></p>
<p>优势：一定实际售卖。“下单减库存” 可能导致恶意下单，从而影响卖家的商品销售， “付款减库存” 由于需要付出真金白银，可以有效避免。</p>
<p>劣势：用户体验较差。用户下单后，不一定会实际付款，假设有 100 件商品，就可能出现 200 人下单成功的情况，因为下单时不会减库存，所以也就可能出现下单成功数远远超过真正库存数的情况，这尤其会发生在大促的热门商品上。如此一来就会导致很多买家下单成功后却付不了款，购物体验自然是比较差的。</p>
<p><strong>2.3 预扣库存</strong></p>
<p>优势：缓解了以上两种方式的问题。预扣库存实际就是“下单减库存”和 “付款减库存”两种方式的结合，将两次操作进行了前后关联，下单时预扣库存，付款时释放库存。</p>
<p>劣势：并没有彻底解决以上问题。比如针对恶意下单的场景，虽然可以把有效付款时间设置为 10 分钟，但恶意买家完全可以在 10 分钟之后再次下单。</p>
<p><strong>2.4 小结</strong></p>
<p>减库存的问题主要体现在用户体验和商业诉求两方面，其本质原因在于购物过程存在两步甚至多步操作，在不同阶段减库存，容易存在被恶意利用的漏洞。</p>
<h4>3 实际如何减库存 <a href="#item-0-13" id="item-0-13"></a></h4>
<p>业界最为常见的是预扣库存。无论是外卖点餐还是电商购物，下单后一般都有个 “有效付款时间”，超过该时间订单自动释放，这就是典型的预扣库存方案。但如上所述，预扣库存还需要解决恶意下单的问题，保证商品卖的出去；另一方面，如何避免超卖，也是一个痛点。</p>
<ol>
<li>卖的出去：恶意下单的解决方案主要还是结合安全和反作弊措施来制止。比如，识别频繁下单不付款的买家并进行打标，这样可以在打标买家下单时不减库存；再比如为大促商品设置单人最大购买件数，一人最多只能买 N 件商品；又或者对重复下单不付款的行为进行次数限制阻断等</li>
<li>避免超卖：库存超卖的情况实际分为两种。对于普通商品，秒杀只是一种大促手段，即使库存超卖，商家也可以通过补货来解决；而对于一些商品，秒杀作为一种营销手段，完全不允许库存为负，也就是在数据一致性上，需要保证大并发请求时数据库中的库存字段值不能为负，一般有多种方案：一是在通过事务来判断，即保证减后库存不能为负，否则就回滚；二是直接设置数据库字段类型为无符号整数，这样一旦库存为负就会在执行 SQL 时报错；三是使用 CASE WHEN 判断语句——</li>
</ol>
<pre><code class="language-sql">UPDATE item SET inventory = CASE WHEN inventory &gt;= xxx THEN inventory-xxx ELSE inventory END
</code></pre>
<p>业务手段保证商品卖的出去，技术手段保证商品不会超卖，库存问题从来就不是简单的技术难题，解决问题的视角是多种多样的。</p>
<h4>4 一致性性能的优化 <a href="#item-0-14" id="item-0-14"></a></h4>
<p>库存是个关键数据，更是个热点数据。对系统来说，热点的实际影响就是 “高读” 和 “高写”，也是秒杀场景下最为核心的一个技术难题。</p>
<p><strong>4.1 高并发读</strong></p>
<p>秒杀场景解决高并发读问题，关键词是“分层校验”。即在读链路时，只进行不影响性能的检查操作，如用户是否具有秒杀资格、商品状态是否正常、用户答题是否正确、秒杀是否已经结束、是否非法请求等，而不做一致性校验等容易引发瓶颈的检查操作；直到写链路时，才对库存做一致性检查，在数据层保证最终准确性。</p>
<p>因此，在分层校验设定下，系统可以采用分布式缓存甚至LocalCache来抵抗高并发读。即允许读场景下一定的脏数据，这样只会导致少量原本无库存的下单请求被误认为是有库存的，等到真正写数据时再保证最终一致性，由此做到高可用和一致性之间的平衡。</p>
<p>实际上，分层校验的核心思想是：不同层次尽可能过滤掉无效请求，只在“漏斗” 最末端进行有效处理，从而缩短系统瓶颈的影响路径。</p>
<p><strong>4.2 高并发写</strong></p>
<p>高并发写的优化方式，一种是更换DB选型，一种是优化DB性能，以下分别进行讨论。</p>
<p>4.2.1 更换DB选型</p>
<p>秒杀商品和普通商品的减库存是有差异的，核心区别在数据量级小、交易时间短，因此能否把秒杀减库存直接放到缓存系统中实现呢，也就是直接在一个带有持久化功能的缓存中进行减库存操作，比如 Redis？</p>
<p>如果减库存逻辑非常单一的话，比如没有复杂的 SKU 库存和总库存这种联动关系的话，个人认为是完全可以的。但如果有比较复杂的减库存逻辑，或者需要使用到事务，那就必须在数据库中完成减库存操作。</p>
<p>4.2.2 优化DB性能</p>
<p>库存数据落地到数据库实现其实是一行存储（MySQL），因此会有大量线程来竞争 InnoDB 行锁。但并发越高，等待线程就会越多，TPS 下降，RT 上升，吞吐量会受到严重影响——注意，这里假设数据库已基于上文【性能优化】完成数据隔离，以便于讨论聚焦 。</p>
<p>解决并发锁的问题，有两种办法：</p>
<ol>
<li>应用层排队。通过缓存加入集群分布式锁，从而控制集群对数据库同一行记录进行操作的并发度，同时也能控制单个商品占用数据库连接的数量，防止热点商品占用过多的数据库连接</li>
<li>数据层排队。应用层排队是有损性能的，数据层排队是最为理想的。业界中，阿里的数据库团队开发了针对InnoDB 层上的补丁程序（patch），可以基于DB层对单行记录做并发排队，从而实现秒杀场景下的定制优化——注意，排队和锁竞争是有区别的，如果熟悉 MySQL 的话，就会知道 InnoDB 内部的死锁检测，以及 MySQL Server 和 InnoDB 的切换都是比较消耗性能的。另外阿里的数据库团队还做了很多其他方面的优化，如 COMMIT_ON_SUCCESS 和 ROLLBACK_ON_FAIL 的补丁程序，通过在 SQL 里加入提示（hint），实现事务不需要等待实时提交，而是在数据执行完最后一条 SQL 后，直接根据 TARGET_AFFECT_ROW 的结果进行提交或回滚，减少网络等待的时间（毫秒级）。目前阿里已将包含这些补丁程序的 MySQL 开源：<a href="https://link.segmentfault.com/?enc=I%2FCSvHmwhqWWaIcvx%2BoGuw%3D%3D.9twostwyDIweyMcUrly3Zp6KPhQuFFX7eS%2FNY5bGVH3W2EN%2FLLcCmxl3cSe4rhiflb5yVJtZxmcjdRE5e2CqiLn7gsvN1ymwz5bAEmNgIr0%3D">AliSQL</a></li>
</ol>
<p><strong>4.3 小结</strong></p>
<p>高读和高写的两种处理方式大相径庭。读请求的优化空间要大一些，而写请求的瓶颈一般都在存储层，优化思路的本质还是基于 CAP 理论做平衡。</p>
<h4>5 总结一下 <a href="#item-0-15" id="item-0-15"></a></h4>
<p>当然，减库存还有很多细节问题，例如预扣的库存超时后如何进行回补，再比如第三方支付如何保证减库存和付款时的状态一致性，这些也是很大的挑战。</p>
<h3>高可用 <a href="#item-0-16" id="item-0-16"></a></h3>
<p>盯过秒杀流量监控的话，会发现它不是一条蜿蜒而起的曲线，而是一条挺拔的直线，这是因为秒杀请求高度集中于某一特定的时间点。这样一来就会造成一个特别高的零点峰值，而对资源的消耗也几乎是瞬时的。所以秒杀系统的可用性保护是不可或缺的。</p>
<h4>1 流量削峰 <a href="#item-0-17" id="item-0-17"></a></h4>
<p>对于秒杀的目标场景，最终能够抢到商品的人数是固定的，无论 100 人和 10000 人参加结果都是一样的，即有效请求额度是有限的。并发度越高，无效请求也就越多。但秒杀作为一种商业营销手段，活动开始之前是希望有更多的人来刷页面，只是真正开始后，秒杀请求不是越多越好。因此系统可以设计一些规则，人为的延缓秒杀请求，甚至可以过滤掉一些无效请求。</p>
<p><strong>1.1 答题</strong></p>
<p>早期秒杀只是简单的点击秒杀按钮，后来才增加了答题。为什么要增加答题呢？主要是通过提升购买的复杂度，达到两个目的：</p>
<ol>
<li>防止作弊。早期秒杀器比较猖獗，存在恶意买家或竞争对手使用秒杀器扫货的情况，商家没有达到营销的目的，所以增加答题来进行限制</li>
<li>延缓请求。零点流量的起效时间是毫秒级的，答题可以人为拉长峰值下单的时长，由之前的 &lt;1s 延长到 &lt;10s。这个时间对于服务端非常重要，会大大减轻高峰期并发压力；另外，由于请求具有先后顺序，答题后置的请求到来时可能已经没有库存了，因此根本无法下单，此阶段落到数据层真正的写也就非常有限了</li>
</ol>
<p>需要注意的是，答题除了做正确性验证，还需要对提交时间做验证，比如&lt;1s 人为操作的可能性就很小，可以进一步防止机器答题的情况。</p>
<p>答题目前已经使用的非常普遍了，本质是通过在入口层削减流量，从而让系统更好地支撑瞬时峰值。</p>
<p><strong>1.2 排队</strong></p>
<p>最为常见的削峰方案是使用消息队列，通过把同步的直接调用转换成异步的间接推送缓冲瞬时流量。除了消息队列，类似的排队方案还有很多，例如：</p>
<ol>
<li>线程池加锁等待</li>
<li>本地内存蓄洪等待</li>
<li>本地文件序列化写，再顺序读</li>
</ol>
<p>排队方式的弊端也是显而易见的，主要有两点：</p>
<ol>
<li>请求积压。流量高峰如果长时间持续，达到了队列的水位上限，队列同样会被压垮，这样虽然保护了下游系统，但是和请求直接丢弃也没多大区别</li>
<li>用户体验。异步推送的实时性和有序性自然是比不上同步调用的，由此可能出现请求先发后至的情况，影响部分敏感用户的购物体验</li>
</ol>
<p>排队本质是在业务层将一步操作转变成两步操作，从而起到缓冲的作用，但鉴于此种方式的弊端，最终还是要基于业务量级和秒杀场景做出妥协和平衡。</p>
<p><strong>1.3 过滤</strong></p>
<p>过滤的核心结构在于分层，通过在不同层次过滤掉无效请求，达到数据读写的精准触发。常见的过滤主要有以下几层：</p>
<p>1、读限流：对读请求做限流保护，将超出系统承载能力的请求过滤掉 2、读缓存：对读请求做数据缓存，将重复的请求过滤掉 3、写限流：对写请求做限流保护，将超出系统承载能力的请求过滤掉 4、写校验：对写请求做一致性校验，只保留最终的有效数据</p>
<p>过滤的核心目的是通过减少无效请求的数据IO保障有效请求的IO性能。</p>
<p><strong>1.4 小结</strong></p>
<p>系统可以通过入口层的答题、业务层的排队、数据层的过滤达到流量削峰的目的，本质是在寻求商业诉求与架构性能之间的平衡。另外，新的削峰手段也层出不穷，以业务切入居多，比如零点大促时同步发放优惠券或发起抽奖活动，将一部分流量分散到其他系统，这样也能起到削峰的作用。</p>
<h4>2 Plan B <a href="#item-0-18" id="item-0-18"></a></h4>
<p>当一个系统面临持续的高峰流量时，其实是很难单靠自身调整来恢复状态的，日常运维没有人能够预估所有情况，意外总是无法避免。尤其在秒杀这一场景下，为了保证系统的高可用，必须设计一个 Plan B 方案来进行兜底。</p>
<p>高可用建设，其实是一个系统工程，贯穿在系统建设的整个生命周期。</p>
<p><img src="/images/blog/engineering/system-image_1_1.png" alt="image_1_1.png"></p>
<p>具体来说，系统的高可用建设涉及架构阶段、编码阶段、测试阶段、发布阶段、运行阶段，以及故障发生时，逐一进行分析：</p>
<ol>
<li>架构阶段：考虑系统的可扩展性和容错性，避免出现单点问题。例如多地单元化部署，即使某个IDC甚至地市出现故障，仍不会影响系统运转</li>
<li>编码阶段：保证代码的健壮性，例如RPC调用时，设置合理的超时退出机制，防止被其他系统拖垮，同时也要对无法预料的返回错误进行默认的处理</li>
<li>测试阶段：保证CI的覆盖度以及Sonar的容错率，对基础质量进行二次校验，并定期产出整体质量的趋势报告</li>
<li>发布阶段：系统部署最容易暴露错误，因此要有前置的checklist模版、中置的上下游周知机制以及后置的回滚机制</li>
<li>运行阶段：系统多数时间处于运行态，最重要的是运行时的实时监控，及时发现问题、准确报警并能提供详细数据，以便排查问题</li>
<li>故障发生：首要目标是及时止损，防止影响面扩大，然后定位原因、解决问题，最后恢复服务</li>
</ol>
<p>对于日常运维而言，高可用更多是针对运行阶段而言的，此阶段需要额外进行加强建设，主要有以下几种手段：</p>
<ol>
<li>预防：建立常态压测体系，定期对服务进行单点压测以及全链路压测，摸排水位</li>
<li>管控：做好线上运行的降级、限流和熔断保护。需要注意的是，无论是限流、降级还是熔断，对业务都是有损的，所以在进行操作前，一定要和上下游业务确认好再进行。就拿限流来说，哪些业务可以限、什么情况下限、限流时间多长、什么情况下进行恢复，都要和业务方反复确认</li>
<li>监控：建立性能基线，记录性能的变化趋势；建立报警体系，发现问题及时预警</li>
<li>恢复：遇到故障能够及时止损，并提供快速的数据订正工具，不一定要好，但一定要有</li>
</ol>
<p>在系统建设的整个生命周期中，每个环节中都可能犯错，甚至有些环节犯的错，后面是无法弥补的或者成本极高的。所以高可用是一个系统工程，必须放到整个生命周期中进行全面考虑。同时，考虑到服务的增长性，高可用更需要长期规划并进行体系化建设。</p>
<h4>3 总结一下 <a href="#item-0-19" id="item-0-19"></a></h4>
<p>高可用其实是在说 “稳定性”，稳定性是一个平时不重要，但出了问题就要命的事情，然而它的落地又是一个问题——平时业务发展良好，稳定性建设就会降级给业务让路。解决这个问题必须在组织上有所保障，比如让业务负责人背上稳定性绩效指标，同时在部门中建立稳定性建设小组，小组成员由每条线的核心力量兼任，绩效由稳定性负责人来打分，这样就可以把体系化的建设任务落实到具体的业务系统中了。</p>
<h3>个人总结 <a href="#item-0-20" id="item-0-20"></a></h3>
<p>一个秒杀系统的设计，可以根据不同级别的流量，由简单到复杂打造出不同的架构，本质是各方面的取舍和权衡。当然，你可能注意到，本文并没有涉及具体的选型方案，因为这些对于架构来说并不重要，作为架构师，应该时刻提醒自己主线是什么。</p>
<p>同时也在这里抽象、提炼一下，主要是个人对于秒杀设计的提纲式整理，方便各位同学进行参考——!</p>
<p><img src="/images/blog/engineering/system-image_1_3.png" alt="image_1_3.png"></p>
1a:T7179,<h2>为什么你的系统需要限流</h2>
<p>先看两个真实事故。</p>
<p><strong>事故一：短信轰炸。</strong> 电商大促，运营要向 200 万用户推送促销短信。开发对接了短信服务商 API，写了批量发送任务就上线。活动当天，200 万条请求几乎同时涌向服务商。服务商 API 上限是 400 QPS。没有任何限流措施，前几秒就把接口打崩，后续请求全部超时或静默丢弃。几个小时后才发现，超过一半的短信根本没送达。</p>
<p><strong>事故二：风控反噬。</strong> 某大型互联网公司风控系统，平时运行稳定。双十一流量瞬间飙到日常 10 倍，风控依赖的下游评分服务没做流量保护，直接崩溃。连锁反应：所有经过风控的交易请求因调用超时被拦截——包括完全正常的用户交易。最终损失不是来自欺诈，而是自己的系统把正常用户挡在了门外。</p>
<p>两个事故揭示同一个本质：<strong>限流不是为了&quot;限制&quot;，而是为了&quot;保护&quot;。</strong></p>
<p>在高并发系统设计中，缓存、降级和限流被称为&quot;三大利器&quot;：</p>
<table>
<thead>
<tr>
<th>手段</th>
<th>解决的问题</th>
<th>核心机制</th>
<th>局限性</th>
</tr>
</thead>
<tbody><tr>
<td><strong>缓存</strong></td>
<td>提速</td>
<td>将高频数据放入更快的存储层</td>
<td>对写操作无能为力</td>
</tr>
<tr>
<td><strong>降级</strong></td>
<td>止损</td>
<td>放弃非核心功能保核心链路</td>
<td>前提是有东西可降，秒杀场景无法降级</td>
</tr>
<tr>
<td><strong>限流</strong></td>
<td>控流</td>
<td>主动丢弃/延迟超量请求</td>
<td>需要准确的容量评估，否则误杀或漏放</td>
</tr>
</tbody></table>
<p>三者各有分工，但限流的不可替代性在于：当稀缺资源被争抢、写操作高并发、昂贵查询集中调用时，缓存和降级都帮不了你。</p>
<hr>
<h2>四种限流算法：原理、适用场景与工程取舍</h2>
<h3>漏桶算法（Leaky Bucket）</h3>
<p><strong>核心原理</strong></p>
<p>漏桶的逻辑可以用一句话概括：<strong>无论流入多快，流出永远恒定。</strong></p>
<pre><code>请求流入 → [  桶（有容量上限）  ] → 恒定速率流出 → 下游处理
                    ↓
              桶满则丢弃
</code></pre>
<ul>
<li>请求以任意速率流入桶中</li>
<li>桶底以固定速率流出（处理请求）</li>
<li>桶有容量上限，溢出的请求被直接丢弃</li>
</ul>
<p><strong>核心参数</strong></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
<th>设计考量</th>
</tr>
</thead>
<tbody><tr>
<td>流出速率</td>
<td>下游能承受的恒定处理能力</td>
<td>取决于下游系统的稳态吞吐上限</td>
</tr>
<tr>
<td>桶容量</td>
<td>允许暂存的最大请求数</td>
<td>过大导致延迟积累，过小导致突发流量全被丢弃</td>
</tr>
</tbody></table>
<p><strong>适用场景</strong></p>
<ul>
<li>对接物理设备或硬件接口（严格不允许任何突发）</li>
<li>需要绝对平滑的输出流量（如音视频流的恒定码率传输）</li>
<li>流量整形（traffic shaping）场景</li>
</ul>
<p><strong>不适用场景</strong></p>
<ul>
<li>互联网业务的 API 限流（真实流量天然是突发的，漏桶的死板会浪费系统空闲容量）</li>
<li>需要快速响应突发请求的场景</li>
</ul>
<p><strong>工程实践：Nginx 的 <code>limit_req</code> 就是漏桶实现</strong></p>
<pre><code class="language-nginx"># 定义限流区域：10MB 共享内存，每个 IP 每秒 10 个请求
limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;

server {
    location /api/ {
        # burst=20：桶容量为 20，超出的排队
        # nodelay：排队请求不延迟，立即处理（占用 burst 配额）
        limit_req zone=api burst=20 nodelay;

        # 超限返回 429 而非默认的 503
        limit_req_status 429;
    }
}
</code></pre>
<p>这里有个常见误区：<code>burst=20 nodelay</code> 不是&quot;允许突发 20 个请求&quot;那么简单。<code>nodelay</code> 的含义是突发请求立即转发（不排队等待），但每个突发请求会&quot;占用&quot;一个 burst 槽位，槽位按 <code>rate</code> 的速率恢复。实际效果是：瞬间可以通过 30 个请求（rate + burst），但之后必须等槽位恢复。</p>
<hr>
<h3>令牌桶算法（Token Bucket）</h3>
<p><strong>核心原理</strong></p>
<p>令牌桶的理念与漏桶相反：<strong>在空闲时积蓄能力，在繁忙时释放能力。</strong></p>
<pre><code>令牌生成器 ──恒定速率──→ [  令牌桶（有容量上限）  ]
                                    ↓
                         请求到达 → 取令牌 → 有令牌则通过
                                           → 无令牌则拒绝/等待
</code></pre>
<ul>
<li>系统以恒定速率向桶中放入令牌</li>
<li>每个请求消耗一个（或多个）令牌</li>
<li>令牌充足时请求立即通过</li>
<li>令牌耗尽时请求被拒绝或阻塞等待</li>
<li>桶有容量上限，多余令牌溢出</li>
</ul>
<p><strong>核心参数</strong></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
<th>设计考量</th>
</tr>
</thead>
<tbody><tr>
<td>令牌生成速率</td>
<td>系统的持续处理能力</td>
<td>对应系统稳态吞吐上限</td>
</tr>
<tr>
<td>桶容量</td>
<td>允许的最大突发量</td>
<td>编码了对突发流量的容忍度</td>
</tr>
</tbody></table>
<p><strong>适用场景</strong></p>
<ul>
<li>互联网 API 限流（绝大多数场景的首选）</li>
<li>允许合理突发的业务场景（秒杀、热点事件引发的流量脉冲）</li>
<li>需要区分长期速率和瞬时峰值的场景</li>
</ul>
<p><strong>工程实践：Guava RateLimiter 的两种模式</strong></p>
<p>Guava 提供了两种令牌桶实现，对应两种不同的业务需求：</p>
<pre><code class="language-java">// 模式一：SmoothBursty —— 允许突发
// 以每秒 100 个令牌的速率生成，桶容量等于 1 秒的产量（100）
RateLimiter limiter = RateLimiter.create(100.0);

// 场景：API 网关限流
// 特点：空闲期积累的令牌可以一次性消费，应对突发
if (limiter.tryAcquire()) {
    processRequest();
} else {
    return Response.status(429).build();
}
</code></pre>
<pre><code class="language-java">// 模式二：SmoothWarmingUp —— 冷启动预热
// 速率 100/s，预热期 3 秒
RateLimiter limiter = RateLimiter.create(100.0, 3, TimeUnit.SECONDS);

// 场景：数据库连接池、缓存冷启动
// 特点：系统刚启动时不会全速放量，给下游一个&quot;热身&quot;时间
// 预热期内速率从低到高线性增长，避免冷系统被瞬时流量打垮
</code></pre>
<p><strong>SmoothBursty vs SmoothWarmingUp 的选择</strong></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>SmoothBursty</th>
<th>SmoothWarmingUp</th>
</tr>
</thead>
<tbody><tr>
<td>突发处理</td>
<td>允许消费积累的令牌，支持突发</td>
<td>冷启动期间限制突发</td>
</tr>
<tr>
<td>典型场景</td>
<td>API 限流、消息推送</td>
<td>数据库预热、缓存预热</td>
</tr>
<tr>
<td>核心关注</td>
<td>流量的峰谷平衡</td>
<td>系统的冷热状态转换</td>
</tr>
</tbody></table>
<p><strong>关键注意</strong>：Guava RateLimiter 是<strong>单机限流</strong>。它只能控制当前 JVM 进程的流量，在分布式环境下需要配合 Redis 方案使用。</p>
<hr>
<h3>固定窗口计数器（Fixed Window Counter）</h3>
<p><strong>核心原理</strong></p>
<p>在一个固定时间窗口内维护计数器，超过阈值就拒绝，窗口结束时归零。</p>
<pre><code>|← 窗口1 (0-1s) →|← 窗口2 (1-2s) →|
    count=0→100        count=0→...
    阈值=100           阈值=100
</code></pre>
<p><strong>经典问题：窗口边界的 2 倍峰值</strong></p>
<pre><code>|← 窗口1 →|← 窗口2 →|
      ↑
   最后100ms涌入100个  最前100ms涌入100个

   → 200ms 内实际通过了 200 个请求（2 倍于阈值）
</code></pre>
<p><strong>适用场景</strong></p>
<ul>
<li>精度要求不高的简单限流（大部分业务场景）</li>
<li>需要快速实现的场景</li>
<li>阈值本身留有足够余量（2 倍偶发峰值可承受）</li>
</ul>
<p><strong>工程判断</strong>：在很多场景中，固定窗口的精度已经足够。边界处偶尔的 2 倍峰值，对于留有余量的系统来说不是问题。不要为理论上的完美过度工程化。</p>
<hr>
<h3>滑动窗口计数器（Sliding Window）</h3>
<p><strong>核心原理</strong></p>
<p>将时间窗口划分为更细的子窗口（slot），统计时基于当前时间点向前滑动统计。</p>
<pre><code>子窗口:  |s1|s2|s3|s4|s5|s6|s7|s8|s9|s10|
当前统计范围:          |←————————————→|
</code></pre>
<p><strong>与固定窗口的对比</strong></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>固定窗口</th>
<th>滑动窗口</th>
</tr>
</thead>
<tbody><tr>
<td>精度</td>
<td>存在边界 2 倍峰值</td>
<td>消除边界效应</td>
</tr>
<tr>
<td>实现复杂度</td>
<td>一个计数器</td>
<td>N 个子窗口计数器</td>
</tr>
<tr>
<td>存储开销</td>
<td>O(1)</td>
<td>O(N)，N 为子窗口数</td>
</tr>
<tr>
<td>适用场景</td>
<td>精度要求低、快速实现</td>
<td>精度要求高、阈值接近系统极限</td>
</tr>
</tbody></table>
<p><strong>工程实践：Sentinel 的滑动窗口实现</strong></p>
<p>阿里巴巴的 Sentinel 框架使用 <code>LeapArray</code> 数据结构实现滑动窗口：</p>
<ul>
<li>将 1 秒划分为若干个 <code>WindowWrap</code>（默认 2 个，即 500ms 一个子窗口）</li>
<li>每个子窗口维护独立的 pass/block/exception 等计数器</li>
<li>通过环形数组 + 时间戳判断实现窗口滑动，避免频繁创建销毁对象</li>
</ul>
<hr>
<h3>四种算法对比总结</h3>
<table>
<thead>
<tr>
<th>算法</th>
<th>核心特征</th>
<th>突发处理</th>
<th>实现复杂度</th>
<th>推荐场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>漏桶</strong></td>
<td>恒定输出</td>
<td>不允许突发</td>
<td>低</td>
<td>流量整形、硬件接口</td>
</tr>
<tr>
<td><strong>令牌桶</strong></td>
<td>弹性输出</td>
<td>允许有限突发</td>
<td>中</td>
<td>API 限流（首选）</td>
</tr>
<tr>
<td><strong>固定窗口</strong></td>
<td>简单计数</td>
<td>边界可能 2 倍峰值</td>
<td>最低</td>
<td>快速实现、精度要求低</td>
</tr>
<tr>
<td><strong>滑动窗口</strong></td>
<td>精确计数</td>
<td>平滑</td>
<td>高</td>
<td>精度要求高、阈值紧</td>
</tr>
</tbody></table>
<p><strong>选择策略</strong>：如果没有特殊需求，令牌桶是互联网业务的默认选择。如果需要极致简单，用固定窗口。如果下游绝对不能承受波动，用漏桶。如果阈值非常接近系统极限，用滑动窗口。</p>
<hr>
<h2>从单机到分布式：最关键的认知跃迁</h2>
<h3>单机限流为什么在集群中失效</h3>
<p>一个团队用 Guava RateLimiter 限制短信 API 调用为 400 QPS，本地测试完美。代码部署到 4 个节点后，4 个节点各自以 400 QPS 发送，服务商实际承受 1600 QPS，接口再次崩溃。</p>
<p><strong>根因：单机限流只能控制单个进程的流量，对其他节点一无所知。</strong></p>
<p>直觉的修复是均分配额：4 个节点各分 100 QPS。但这引入新问题：</p>
<pre><code>理想中：
  节点A: 100 QPS → 25%
  节点B: 100 QPS → 25%
  节点C: 100 QPS → 25%
  节点D: 100 QPS → 25%

现实中（负载不均）：
  节点A: 240 QPS → 只放行 100，拒绝 140 ✗
  节点B: 120 QPS → 只放行 100，拒绝  20 ✗
  节点C:  30 QPS → 只用了 30，浪费  70
  节点D:  10 QPS → 只用了 10，浪费  90

  总放行：240 QPS（理论可放 400，实际只放了 240）
  → 系统实际吞吐远低于理论上限
</code></pre>
<p>动态调整配额（根据节点负载实时重新分配）？复杂度爆炸——你需要协调机制感知节点上下线、收集实时负载、计算下发配额，这本身就是一个分布式系统问题。</p>
<p><strong>标准答案：将限流状态提升到共享的集中存储中。</strong></p>
<h3>分布式限流的核心原则</h3>
<blockquote>
<p><strong>限流的粒度决定了它的准确性。</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th>保护对象</th>
<th>限流粒度</th>
<th>方案</th>
</tr>
</thead>
<tbody><tr>
<td>本机 CPU/内存</td>
<td>进程级</td>
<td>Guava RateLimiter、Sentinel</td>
</tr>
<tr>
<td>外部 API 配额</td>
<td>系统级（全集群）</td>
<td>Redis 分布式计数器</td>
</tr>
<tr>
<td>业务规则（如用户发送频率）</td>
<td>用户级</td>
<td>Redis + 用户维度 key</td>
</tr>
</tbody></table>
<hr>
<h2>Redis 分布式限流：为什么是标准答案</h2>
<p>Redis 之所以成为分布式限流的事实标准，是因为它的特性精确匹配了限流的每一个核心需求：</p>
<table>
<thead>
<tr>
<th>限流需求</th>
<th>Redis 特性</th>
<th>为什么匹配</th>
</tr>
</thead>
<tbody><tr>
<td>原子性：&quot;读取-判断-递增&quot;必须原子</td>
<td>INCR 原子命令 + Lua 脚本</td>
<td>单线程模型，天然无并发冲突</td>
</tr>
<tr>
<td>极致性能：每个请求都要过限流</td>
<td>内存操作，亚毫秒级延迟</td>
<td>不成为业务瓶颈</td>
</tr>
<tr>
<td>共享状态：所有节点看到同一个计数器</td>
<td>独立服务，集群可访问</td>
<td>分布式协调问题消失</td>
</tr>
<tr>
<td>自动过期：时间窗口结束后计数器清零</td>
<td>Key 级别 TTL</td>
<td>无需额外清理逻辑</td>
</tr>
</tbody></table>
<h3>工程实践：基于 Redis + Lua 的固定窗口限流</h3>
<p><strong>为什么必须用 Lua 脚本？</strong></p>
<p>不用 Lua 的伪代码：</p>
<pre><code>count = redis.GET(key)          -- 步骤1：读取
if count &lt; threshold:           -- 步骤2：判断
    redis.INCR(key)             -- 步骤3：递增
    return ALLOW
else:
    return REJECT
</code></pre>
<p>并发问题：两个节点同时读到 count=399（阈值 400），都判断&quot;未超限&quot;，都执行 INCR。最终 count=401，但两个请求都通过了。高并发下，这种竞态条件被急剧放大，限流形同虚设。</p>
<p><strong>Lua 脚本实现（原子操作）</strong></p>
<pre><code class="language-lua">-- KEYS[1]: 限流 key，如 &quot;rate_limit:sms_api:1609459200&quot;
-- ARGV[1]: 阈值
-- ARGV[2]: 窗口过期时间（秒）

local key = KEYS[1]
local threshold = tonumber(ARGV[1])
local expire_time = tonumber(ARGV[2])

local current = tonumber(redis.call(&#39;GET&#39;, key) or &quot;0&quot;)

if current + 1 &gt; threshold then
    return 0  -- 拒绝
else
    redis.call(&#39;INCR&#39;, key)
    if current == 0 then
        redis.call(&#39;EXPIRE&#39;, key, expire_time)
    end
    return 1  -- 放行
end
</code></pre>
<p><strong>Key 设计规范</strong></p>
<pre><code>格式：rate_limit:{业务标识}:{维度}:{时间窗口}
示例：
  rate_limit:sms_api:global:1609459200       -- 全局短信 API 限流
  rate_limit:login:user:12345:1609459200     -- 用户维度登录限流
  rate_limit:order:tenant:abc:1609459200     -- 租户维度下单限流
</code></pre>
<h3>工程实践：基于 Redis 的滑动窗口限流</h3>
<p>当固定窗口的边界问题不可接受时，可以用 Redis Sorted Set 实现滑动窗口：</p>
<pre><code class="language-lua">-- KEYS[1]: 限流 key
-- ARGV[1]: 阈值
-- ARGV[2]: 窗口大小（毫秒）
-- ARGV[3]: 当前时间戳（毫秒）
-- ARGV[4]: 唯一请求ID

local key = KEYS[1]
local threshold = tonumber(ARGV[1])
local window = tonumber(ARGV[2])
local now = tonumber(ARGV[3])
local request_id = ARGV[4]

-- 移除窗口外的过期记录
redis.call(&#39;ZREMRANGEBYSCORE&#39;, key, 0, now - window)

-- 统计当前窗口内的请求数
local count = redis.call(&#39;ZCARD&#39;, key)

if count &lt; threshold then
    -- 添加当前请求，score 为时间戳
    redis.call(&#39;ZADD&#39;, key, now, request_id)
    redis.call(&#39;PEXPIRE&#39;, key, window)
    return 1  -- 放行
else
    return 0  -- 拒绝
end
</code></pre>
<p><strong>两种 Redis 方案的对比</strong></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>固定窗口（String + INCR）</th>
<th>滑动窗口（Sorted Set）</th>
</tr>
</thead>
<tbody><tr>
<td>存储开销</td>
<td>O(1)，一个 key 一个计数器</td>
<td>O(N)，N 为窗口内请求数</td>
</tr>
<tr>
<td>时间复杂度</td>
<td>O(1)</td>
<td>O(log N)</td>
</tr>
<tr>
<td>精度</td>
<td>边界可能 2 倍峰值</td>
<td>精确</td>
</tr>
<tr>
<td>适用</td>
<td>大部分场景</td>
<td>阈值紧、精度要求高</td>
</tr>
</tbody></table>
<p><strong>工程建议</strong>：优先用固定窗口方案。只有当阈值非常接近系统极限（余量 &lt; 20%）时，才需要滑动窗口的精度。</p>
<h3>关于时钟同步</h3>
<p>分布式系统中，各节点用本地时间计算 Redis key 中的时间窗口标识，时钟偏移可能导致不同节点在不同窗口中计数。严格做法是用 Redis 服务端时间 <code>redis.call(&#39;TIME&#39;)</code>。但现代服务器通过 NTP 同步后的时钟偏差通常在毫秒级，对秒级窗口几乎无影响。</p>
<p><strong>工程判断</strong>：对于秒级窗口，使用本地时间戳即可。对于百毫秒级窗口或对精度有极端要求的场景，使用 Redis 服务端时间。</p>
<hr>
<h2>多层限流：纵深防御架构</h2>
<p>一个常见误区是试图在某一层解决所有限流问题。良好的限流架构应该是分层的——每一层保护不同的东西，承担不同的职责。</p>
<pre><code>                     请求流入
                        ↓
┌──────────────────────────────────────────┐
│  第一层：接入层（Nginx / CDN）            │  ← 挡住恶意流量和 DDoS
│  基于 IP 的连接数和请求速率限制            │
└──────────────────────────────────────────┘
                        ↓
┌──────────────────────────────────────────┐
│  第二层：API 网关（Gateway）              │  ← 业务感知型限流
│  基于用户/租户/API 维度的差异化限流        │
└──────────────────────────────────────────┘
                        ↓
┌──────────────────────────────────────────┐
│  第三层：业务层                           │  ← 业务规则型限流
│  业务语义的频率控制（发帖/下单/发短信）     │
└──────────────────────────────────────────┘
                        ↓
┌──────────────────────────────────────────┐
│  第四层：数据层                           │  ← 最后一道防线
│  连接池 / 线程池隔离 / 熔断器             │
└──────────────────────────────────────────┘
</code></pre>
<h3>各层详细对比</h3>
<table>
<thead>
<tr>
<th>层级</th>
<th>保护对象</th>
<th>限流维度</th>
<th>典型工具</th>
<th>算法</th>
</tr>
</thead>
<tbody><tr>
<td>接入层</td>
<td>基础设施</td>
<td>IP、连接数</td>
<td>Nginx <code>limit_req</code>/<code>limit_conn</code></td>
<td>漏桶</td>
</tr>
<tr>
<td>API 网关</td>
<td>服务处理能力</td>
<td>用户 ID、API Key、租户</td>
<td>Redis + Lua、Sentinel</td>
<td>令牌桶/滑动窗口</td>
</tr>
<tr>
<td>业务层</td>
<td>业务规则</td>
<td>业务实体（用户行为频率）</td>
<td>Redis + 业务代码</td>
<td>固定窗口</td>
</tr>
<tr>
<td>数据层</td>
<td>存储和依赖</td>
<td>并发连接数</td>
<td>连接池、Hystrix、Resilience4j</td>
<td>信号量/熔断</td>
</tr>
</tbody></table>
<h3>各层工程实践</h3>
<p><strong>接入层：Nginx 配置示例</strong></p>
<pre><code class="language-nginx">http {
    # IP 维度的请求速率限制
    limit_req_zone $binary_remote_addr zone=ip_rate:10m rate=100r/s;

    # IP 维度的并发连接数限制
    limit_conn_zone $binary_remote_addr zone=ip_conn:10m;

    server {
        # API 接口：每 IP 100r/s，突发 50
        location /api/ {
            limit_req zone=ip_rate burst=50 nodelay;
            limit_conn ip_conn 50;
            limit_req_status 429;
        }

        # 登录接口：更严格的限制
        location /api/login {
            limit_req zone=ip_rate burst=5;
            limit_req_status 429;
        }
    }
}
</code></pre>
<p><strong>API 网关层：差异化限流</strong></p>
<pre><code class="language-java">// 不同级别用户的限流配置
public class RateLimitConfig {
    // 免费用户：60 次/分钟
    // 付费用户：600 次/分钟
    // 企业用户：6000 次/分钟

    public int getThreshold(User user) {
        return switch (user.getTier()) {
            case FREE       -&gt; 60;
            case PREMIUM    -&gt; 600;
            case ENTERPRISE -&gt; 6000;
        };
    }

    // 不同 API 端点的限流配置
    // 重查询接口：50 QPS
    // 轻量读接口：5000 QPS
    // 写操作接口：200 QPS

    public int getThreshold(String endpoint) {
        return switch (endpoint) {
            case &quot;/api/report/generate&quot; -&gt; 50;    // 计算密集
            case &quot;/api/user/info&quot;       -&gt; 5000;  // 轻量读
            case &quot;/api/order/create&quot;    -&gt; 200;   // 写操作
            default                     -&gt; 1000;
        };
    }
}
</code></pre>
<p><strong>业务层：业务规则型限流</strong></p>
<pre><code class="language-java">// 业务限流的阈值来自产品需求，不是压测
public class BusinessRateLimiter {

    // 防骚扰：每用户每分钟最多 5 条短信
    public boolean allowSendSms(long userId) {
        String key = &quot;biz:sms:&quot; + userId + &quot;:&quot; + currentMinute();
        return redisRateLimiter.tryAcquire(key, 5, 60);
    }

    // 反垃圾：新账号 24 小时内最多发 10 条帖子
    public boolean allowPost(long userId, boolean isNewAccount) {
        if (!isNewAccount) return true;
        String key = &quot;biz:post:new:&quot; + userId + &quot;:&quot; + today();
        return redisRateLimiter.tryAcquire(key, 10, 86400);
    }

    // 运营策略：商家每天最多创建 100 个促销活动
    public boolean allowCreatePromotion(long merchantId) {
        String key = &quot;biz:promo:&quot; + merchantId + &quot;:&quot; + today();
        return redisRateLimiter.tryAcquire(key, 100, 86400);
    }
}
</code></pre>
<p><strong>数据层：隐式限流</strong></p>
<p>数据层的&quot;限流&quot;通常不以限流的名义出现，但本质上发挥着同样的作用：</p>
<ul>
<li><strong>连接池</strong>：连接池满时新请求排队等待 → 并发度上限</li>
<li><strong>线程池隔离</strong>：为每个下游依赖分配独立线程池 → 故障隔离</li>
<li><strong>熔断器</strong>：错误率超阈值时直接停止调用 → 自适应限流</li>
</ul>
<p><strong>每一层保护不同的东西。</strong> 接入层保护基础设施不被滥用流量冲垮；API 网关保护服务处理能力不被超载；业务层保护业务规则不被绕过；数据层保护最脆弱的存储和依赖。</p>
<hr>
<h2>限流之后：被拒绝的请求去哪了</h2>
<p>大多数限流讨论都集中在&quot;如何拒绝&quot;，很少有人思考&quot;拒绝之后怎么办&quot;。而在真实业务中，后者往往更重要。</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>做法</th>
<th>适用场景</th>
<th>风险</th>
</tr>
</thead>
<tbody><tr>
<td><strong>直接拒绝</strong></td>
<td>返回 429 + Retry-After</td>
<td>开放 API、程序化调用方</td>
<td>用户体验差</td>
</tr>
<tr>
<td><strong>排队等待</strong></td>
<td>写入 MQ，消费者限速消费</td>
<td>异步操作（短信、邮件、报表）</td>
<td>队列积压导致延迟不可控</td>
</tr>
<tr>
<td><strong>降级响应</strong></td>
<td>返回缓存/兜底数据</td>
<td>推荐、搜索、详情页非核心模块</td>
<td>数据时效性降低</td>
</tr>
<tr>
<td><strong>引流分担</strong></td>
<td>导向备用路径（CDN/只读副本）</td>
<td>读多写少的场景</td>
<td>需要备用链路的维护成本</td>
</tr>
</tbody></table>
<p><strong>关键原则：限流策略和拒绝策略必须配套设计。</strong></p>
<p>回到短信发送事故：被限流的短信不能直接丢弃，必须进入重试队列。秒杀请求被限流？直接告知&quot;已售罄&quot;比让用户苦等体验更好。商品详情页被限流？返回缓存数据即可，用户感知的是&quot;数据没那么新&quot;而不是&quot;服务挂了&quot;。</p>
<p>只设计了限流而没考虑拒绝后的处理，就像只安装了闸门却没修泄洪渠——水是拦住了，但迟早会溃坝。</p>
<hr>
<h2>阈值从哪来：限流的度量方法论</h2>
<p>所有限流工程中最难的问题不是技术实现，而是：<strong>阈值应该设多少？</strong></p>
<h3>四步确定阈值</h3>
<table>
<thead>
<tr>
<th>步骤</th>
<th>方法</th>
<th>产出</th>
</tr>
</thead>
<tbody><tr>
<td><strong>1. 压测基线</strong></td>
<td>逐步加压，观察 P99 延迟和错误率的拐点</td>
<td>系统实际容量边界</td>
</tr>
<tr>
<td><strong>2. 安全系数</strong></td>
<td>阈值 = 容量边界 × 70%~80%</td>
<td>留出余量应对突发波动</td>
</tr>
<tr>
<td><strong>3. 持续监控</strong></td>
<td>监控 P99、错误率、CPU、内存</td>
<td>发现容量变化及时调整</td>
</tr>
<tr>
<td><strong>4. 渐进调整</strong></td>
<td>从保守值开始，观察线上表现后逐步放宽</td>
<td>避免上线即翻车</td>
</tr>
</tbody></table>
<h3>自适应限流</h3>
<p>更高级的形态是基于实时指标的自动限流。以 Sentinel 为例：</p>
<pre><code class="language-java">// 基于系统负载的自适应限流
SystemRule rule = new SystemRule();
rule.setHighestCpuUsage(0.8);    // CPU &gt; 80% 时触发限流
rule.setHighestSystemLoad(2.5);   // System Load &gt; 2.5 时触发限流
rule.setAvgRt(200);               // 平均 RT &gt; 200ms 时触发限流

// 优点：省去人为猜测阈值
// 风险：正常流量波动可能触发误限，需仔细调试灵敏度
</code></pre>
<h3>阈值是业务决策</h3>
<blockquote>
<p><strong>限流阈值不是纯技术参数，而是一个业务决策。</strong></p>
</blockquote>
<p>它编码的是&quot;我们愿意承受多大负载，以及拒绝超额流量的业务成本是什么&quot;。</p>
<ul>
<li>面向消费者的核心交易链路：拒绝一个请求 = 损失一笔订单 → 阈值宜宽</li>
<li>内部数据分析任务：晚执行几分钟无损失 → 阈值可严</li>
<li>计算密集的报表接口：单个请求消耗大量资源 → 阈值必须严</li>
</ul>
<p>阈值设定必须综合技术容量和业务容忍度，需要工程团队和产品团队协同决策。</p>
<hr>
<h2>总结：限流是一种系统思维</h2>
<p>限流从表面看是算法选择题，但真正落地到生产环境时，它是一个系统设计问题：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>核心问题</th>
</tr>
</thead>
<tbody><tr>
<td><strong>容量</strong></td>
<td>系统到底能承受多少？需要压测和监控，不是拍脑袋</td>
</tr>
<tr>
<td><strong>优先级</strong></td>
<td>必须拒绝时，拒绝谁？VIP vs 普通、核心 vs 边缘、写 vs 读</td>
</tr>
<tr>
<td><strong>失败模式</strong></td>
<td>限流触发后怎么办？报错、排队、降级还是引流</td>
</tr>
<tr>
<td><strong>权衡</strong></td>
<td>平滑性 vs 响应性、精确性 vs 性能、简单性 vs 灵活性</td>
</tr>
</tbody></table>
<p>最好的限流系统是你感觉不到它存在的系统。流量平稳时安静旁观，突增时默默吸收合理突发，真正超限时优雅拒绝——确保已接受的请求仍能正常处理。它不是一堵墙，而是一个阀门：精确控制流量进出，让系统在极端压力下保持可控、可预测、可依赖。</p>
<p><strong>限流的本质，是对系统能力边界的敬畏，以及在边界之内追求最大价值的工程智慧。</strong></p>
5:["$","article",null,{"className":"min-h-screen","children":["$","div",null,{"className":"mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12","children":[["$","header",null,{"className":"mb-8","children":[["$","nav",null,{"className":"flex items-center gap-1 text-sm mb-4","children":[["$","$L13",null,{"href":"/blog/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"博客"}],["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"Engineering"}],[["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/architecture/page/1","className":"text-blue-600 hover:text-blue-700 transition-colors","children":"架构设计"}]]]}],["$","div",null,{"className":"flex items-center mb-6","children":["$","div",null,{"className":"inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal","children":[["$","svg",null,{"className":"w-4 h-4 mr-2 text-gray-400","fill":"none","stroke":"currentColor","viewBox":"0 0 24 24","children":["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","strokeWidth":2,"d":"M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z"}]}],["$","time",null,{"dateTime":"2024-03-17","children":"2024年03月17日"}]]}]}],["$","h1",null,{"className":"text-4xl font-bold text-gray-900 mb-6 text-center","children":"高并发系统设计的15个锦囊"}],["$","div",null,{"className":"flex flex-wrap gap-2 mb-6 justify-center","children":[["$","$L13","高并发",{"href":"/blog/tag/%E9%AB%98%E5%B9%B6%E5%8F%91/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"高并发"}],["$","$L13","系统架构",{"href":"/blog/tag/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"系统架构"}],["$","$L13","性能优化",{"href":"/blog/tag/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"性能优化"}]]}]]}],["$","div",null,{"className":"max-w-5xl mx-auto","children":["$","$L14",null,{"content":"$15"}]}],["$","$10",null,{"fallback":["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":"加载导航中..."}],"children":["$","$L16",null,{"globalNav":{"prev":{"slug":"engineering/architecture/架构设计模板","title":"架构设计模板","description":"一套可落地的架构设计文档模板，涵盖需求分析、架构总览、核心流程、详细设计等 11 个关键维度，附可直接复用的 Markdown 模板。","pubDate":"2024-03-16","tags":["架构设计","设计模板","方法论"],"heroImage":"$undefined","content":"$17"},"next":{"slug":"engineering/architecture/微服务架构落地指南：从核心模式到技术选型","title":"微服务架构落地指南：从核心模式到技术选型","description":"系统性地探讨微服务架构设计的核心关注点，包括服务注册发现、API 网关、服务容错、基础设施选型、CI/CD 流水线和可观测性体系，帮助你从 0 到 1 构建一套完整的微服务技术栈。","pubDate":"2024-03-18","tags":["架构设计","微服务","分布式系统","技术选型"],"heroImage":"$undefined","content":"$18"}},"tagNav":{"高并发":{"prev":{"slug":"engineering/architecture/一个秒杀系统的设计思考","title":"一个秒杀系统的设计思考","description":"前言 秒杀大家都不陌生。自2011年首次出现以来，无论是双十一购物还是 12306抢票，秒杀场景已随处可见。简单来说，秒杀就是在同一时刻大量请求争抢购买同一商品并完成交易的过程。从架构视角来看，秒杀系统本质是一个高性能、高一致、高可用的三高系统。","pubDate":"2024-03-14","tags":["秒杀系统","高并发","架构设计"],"heroImage":"$undefined","content":"$19"},"next":null},"系统架构":{"prev":null,"next":{"slug":"engineering/architecture/限流的本质：从令牌桶到分布式流控的架构思考","title":"限流的本质：从令牌桶到分布式流控的架构思考","description":"限流不是一个算法问题，而是一个系统设计问题。从单机令牌桶到分布式 Redis 计数器，从 Nginx 接入层到业务层精细化流控——每一层的限流策略背后，都是对系统容量、业务优先级和降级策略的深度思考。","pubDate":"2025-11-25","tags":["限流","分布式系统","系统架构","高可用"],"heroImage":"$undefined","content":"$1a"}},"性能优化":{"prev":null,"next":null}}}]}],["$","$L1b",null,{}]]}]}]}]
8:null
c:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
7:null
a:{"metadata":[["$","title","0",{"children":"高并发系统设计的15个锦囊 - Skyfalling Blog"}],["$","meta","1",{"name":"description","content":"记得很久之前，去面试过**字节跳动**。被三面的面试官问了一道场景设计题目：**如何设计一个高并发系统**。当时我回答得比较粗糙，最近回想起来，所以整理了设计高并发系统的15个锦囊，相信大家看完会有帮助的。 所谓设计**高并发**系统，就是设计一个系统，保证它**整体可用**的同时，能够**处理很高..."}],["$","meta","2",{"property":"og:title","content":"高并发系统设计的15个锦囊"}],["$","meta","3",{"property":"og:description","content":"记得很久之前，去面试过**字节跳动**。被三面的面试官问了一道场景设计题目：**如何设计一个高并发系统**。当时我回答得比较粗糙，最近回想起来，所以整理了设计高并发系统的15个锦囊，相信大家看完会有帮助的。 所谓设计**高并发**系统，就是设计一个系统，保证它**整体可用**的同时，能够**处理很高..."}],["$","meta","4",{"property":"og:type","content":"article"}],["$","meta","5",{"property":"article:published_time","content":"2024-03-17"}],["$","meta","6",{"property":"article:author","content":"Skyfalling"}],["$","meta","7",{"name":"twitter:card","content":"summary"}],["$","meta","8",{"name":"twitter:title","content":"高并发系统设计的15个锦囊"}],["$","meta","9",{"name":"twitter:description","content":"记得很久之前，去面试过**字节跳动**。被三面的面试官问了一道场景设计题目：**如何设计一个高并发系统**。当时我回答得比较粗糙，最近回想起来，所以整理了设计高并发系统的15个锦囊，相信大家看完会有帮助的。 所谓设计**高并发**系统，就是设计一个系统，保证它**整体可用**的同时，能够**处理很高..."}],["$","link","10",{"rel":"shortcut icon","href":"/favicon.png"}],["$","link","11",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","link","12",{"rel":"icon","href":"/favicon.png"}],["$","link","13",{"rel":"apple-touch-icon","href":"/favicon.png"}]],"error":null,"digest":"$undefined"}
12:{"metadata":"$a:metadata","error":null,"digest":"$undefined"}
