1:"$Sreact.fragment"
2:I[10616,["6874","static/chunks/6874-7791217feaf05c17.js","7177","static/chunks/app/layout-142e67ac4336647c.js"],"default"]
3:I[87555,[],""]
4:I[31295,[],""]
6:I[59665,[],"OutletBoundary"]
9:I[74911,[],"AsyncMetadataOutlet"]
b:I[59665,[],"ViewportBoundary"]
d:I[59665,[],"MetadataBoundary"]
f:I[26614,[],""]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/66b421ed9771e9de.css","style"]
0:{"P":null,"b":"C33gYo3klV3feVWcJcf5W","p":"","c":["","blog","engineering","middleware","Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%EF%BC%9A%E4%BB%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E9%80%89%E5%9E%8B%E5%88%B0%E6%8C%81%E4%B9%85%E5%8C%96%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%9D%83%E8%A1%A1",""],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","engineering/middleware/Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%EF%BC%9A%E4%BB%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E9%80%89%E5%9E%8B%E5%88%B0%E6%8C%81%E4%B9%85%E5%8C%96%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%9D%83%E8%A1%A1","c"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/66b421ed9771e9de.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"zh-CN","children":["$","body",null,{"className":"__className_f367f3","children":["$","div",null,{"className":"min-h-screen flex flex-col","children":[["$","$L2",null,{}],["$","main",null,{"className":"flex-1","children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"bg-[var(--background)]","children":["$","div",null,{"className":"mx-auto max-w-7xl px-6 py-12 lg:px-8","children":["$","p",null,{"className":"text-center text-xs leading-5 text-gray-400","children":["© ",2026," Skyfalling"]}]}]}]]}]}]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","engineering/middleware/Redis%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%EF%BC%9A%E4%BB%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E9%80%89%E5%9E%8B%E5%88%B0%E6%8C%81%E4%B9%85%E5%8C%96%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%9D%83%E8%A1%A1","c"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L5",null,["$","$L6",null,{"children":["$L7","$L8",["$","$L9",null,{"promise":"$@a"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","gwpeSdjc0XGtk-K6LgPjSv",{"children":[["$","$Lb",null,{"children":"$Lc"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$Ld",null,{"children":"$Le"}]]}],false]],"m":"$undefined","G":["$f","$undefined"],"s":false,"S":true}
10:"$Sreact.suspense"
11:I[74911,[],"AsyncMetadata"]
13:I[6874,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],""]
14:I[32923,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
16:I[40780,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
1a:I[85300,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
e:["$","div",null,{"hidden":true,"children":["$","$10",null,{"fallback":null,"children":["$","$L11",null,{"promise":"$@12"}]}]}]
15:T6119,<h2>一、Redis 为什么快：不只是&quot;内存&quot;</h2>
<p>&quot;Redis 快是因为数据在内存里。&quot;——这句话对但不够。如果只是内存操作快，那任何 HashMap 都够快了。Redis 的性能来自多个设计决策的叠加效应。</p>
<h3>单线程模型</h3>
<p>Redis 用<strong>单线程</strong>处理所有客户端请求。这不是技术限制，而是刻意的设计选择：</p>
<ul>
<li><strong>无锁竞争</strong>：所有操作天然串行化，不需要任何锁机制</li>
<li><strong>无上下文切换</strong>：没有线程调度开销</li>
<li><strong>数据结构可以更简单</strong>：不用考虑并发安全，实现更紧凑高效</li>
</ul>
<p>Redis 的瓶颈从来不是 CPU——单线程下 CPU 利用率很难跑满。真正的瓶颈在<strong>内存带宽</strong>和<strong>网络 I/O</strong>。</p>
<h3>I/O 多路复用</h3>
<p>单线程不意味着一次只能处理一个连接。Redis 使用 <code>epoll</code>/<code>kqueue</code> 等 I/O 多路复用技术，单线程也能同时监听成千上万个连接。当有数据可读时才去处理，避免空等。</p>
<h3>基准性能</h3>
<table>
<thead>
<tr>
<th>场景</th>
<th>吞吐量 / 延迟</th>
</tr>
</thead>
<tbody><tr>
<td>本地 Unix Socket，INCR 命令</td>
<td>100K+ TPS</td>
</tr>
<tr>
<td>LAN 网络，简单 GET/SET</td>
<td>~1ms 延迟</td>
</tr>
<tr>
<td>Pipeline 批量操作</td>
<td>吞吐提升 5~10 倍</td>
</tr>
</tbody></table>
<h3>SLOWLOG：发现真正的慢操作</h3>
<pre><code>SLOWLOG GET 10    -- 获取最近 10 条慢查询
</code></pre>
<p>Redis 默认记录执行时间超过 <strong>10ms</strong> 的命令（可配置 <code>slowlog-log-slower-than</code>）。注意：SLOWLOG 不包含网络 I/O 时间，只记录命令本身的执行耗时。如果 SLOWLOG 里出现了简单命令（如 GET），通常说明内存不足导致了 swap。</p>
<hr>
<h2>二、五种数据类型与内部编码</h2>
<p>Redis 不是一个简单的 Key-Value 存储，它的五种数据类型各有针对性的内部实现，选对类型是性能优化的起点。</p>
<h3>2.1 String：不只是字符串</h3>
<p><strong>内部结构</strong>：SDS（Simple Dynamic String）</p>
<pre><code class="language-c">struct sdshdr {
    long len;       // 已使用长度
    long free;      // 剩余可用空间
    char buf[];     // 实际数据（二进制安全）
};
</code></pre>
<p>与 C 字符串相比，SDS 的优势：</p>
<ul>
<li><strong>二进制安全</strong>：可以存储任意二进制数据（图片、序列化对象），不受 <code>\0</code> 截断</li>
<li><strong>O(1) 获取长度</strong>：直接读 <code>len</code> 字段</li>
<li><strong>空间预分配</strong>：<code>free</code> 字段减少内存重分配次数</li>
</ul>
<p><strong>核心操作与适用场景</strong>：</p>
<table>
<thead>
<tr>
<th>操作</th>
<th>命令</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>原子计数</td>
<td>INCR / INCRBY</td>
<td>计数器、限流、ID 生成</td>
</tr>
<tr>
<td>设置过期</td>
<td>SET key value EX seconds</td>
<td>缓存、分布式锁</td>
</tr>
<tr>
<td>批量读写</td>
<td>MGET / MSET</td>
<td>减少网络往返</td>
</tr>
</tbody></table>
<p><strong>内存开销</strong>：一个 String 类型的 Key-Value 约有 <strong>90 字节</strong> 的元数据开销。如果你存储大量短小的值（如用户 ID → 用户名映射），考虑改用 Hash 类型来降低开销。</p>
<p>最大值大小：<strong>1GB</strong>。</p>
<h3>2.2 List：双向链表</h3>
<p>底层实现为双向链表，O(1) 的头尾操作，O(N) 的随机访问。最大长度 2³²-1。</p>
<p><strong>核心操作</strong>：</p>
<pre><code>LPUSH / RPUSH     -- 头部/尾部插入
LPOP / RPOP       -- 头部/尾部弹出
LRANGE 0 -1       -- 获取全部元素
LINDEX 3          -- 按下标访问（O(N)，慎用）
LTRIM 0 99        -- 只保留前 100 个元素
</code></pre>
<p><strong>阻塞操作</strong>（消息队列语义）：</p>
<pre><code>BLPOP key 30      -- 阻塞弹出，最多等 30 秒
BRPOP key 0       -- 阻塞弹出，永久等待
RPOPLPUSH src dst -- 原子地从 src 尾部弹出，推入 dst 头部
</code></pre>
<p><code>RPOPLPUSH</code> 可以实现可靠队列：消费者从工作队列弹出任务的同时推入备份队列，处理完成后再从备份队列删除。如果消费者崩溃，备份队列中的任务不会丢失。</p>
<p><strong>适用场景</strong>：消息队列、最新动态（Timeline）、任务队列。</p>
<h3>2.3 Hash：对象存储的正确方式</h3>
<p><strong>内部编码演变</strong>：</p>
<table>
<thead>
<tr>
<th>条件</th>
<th>编码</th>
<th>性能</th>
<th>内存</th>
</tr>
</thead>
<tbody><tr>
<td>字段数 ≤ 64 且每个值 ≤ 512B</td>
<td>zipmap（紧凑编码）</td>
<td>O(N) 但对少量字段很快</td>
<td>极省</td>
</tr>
<tr>
<td>超过阈值</td>
<td>hashtable</td>
<td>O(1)</td>
<td>正常</td>
</tr>
</tbody></table>
<p>阈值可通过 <code>hash-max-zipmap-entries</code>（默认 64）和 <code>hash-max-zipmap-value</code>（默认 512）配置。</p>
<p><strong>为什么用 Hash 而不是多个 String？</strong></p>
<p>假设存储 100 万个用户的 3 个属性（name, age, email）：</p>
<table>
<thead>
<tr>
<th>方案</th>
<th>Key 数量</th>
<th>内存开销</th>
</tr>
</thead>
<tbody><tr>
<td>3 个 String：<code>user:1:name</code>, <code>user:1:age</code>, <code>user:1:email</code></td>
<td>300 万</td>
<td>每个 Key 90 字节开销 × 300 万</td>
</tr>
<tr>
<td>1 个 Hash：<code>user:1</code> → {name, age, email}</td>
<td>100 万</td>
<td>共享一份 Key 元数据</td>
</tr>
</tbody></table>
<p>Hash 方案的内存节省通常在 <strong>50%~70%</strong>。</p>
<p><strong>核心命令</strong>：</p>
<pre><code>HSET user:1 name &quot;张三&quot; age 28    -- 设置字段
HGET user:1 name                   -- 获取单个字段
HMGET user:1 name age email        -- 批量获取
HINCRBY user:1 age 1               -- 原子递增
HGETALL user:1                     -- 获取所有字段（大 Hash 慎用）
</code></pre>
<p><strong>适用场景</strong>：用户信息、配置项、购物车、任何&quot;对象&quot;型数据。</p>
<h3>2.4 Set：集合运算</h3>
<p>Hash Table 实现，O(1) 的增删查。最大元素数 2³²-1。</p>
<p><strong>独特能力：集合运算</strong>：</p>
<pre><code>SINTER set1 set2        -- 交集：共同好友
SUNION set1 set2        -- 并集：合并标签
SDIFF set1 set2         -- 差集：可能认识的人
SRANDMEMBER set 3       -- 随机取 3 个元素（抽奖）
SPOP set                -- 随机弹出 1 个元素
</code></pre>
<p><strong>适用场景</strong>：标签系统、共同好友、去重、抽奖。</p>
<h3>2.5 Sorted Set：有序集合</h3>
<p><strong>内部实现</strong>：Skip List + Hash Table 混合结构。</p>
<pre><code>┌──────────────────────────────────────────┐
│ Hash Table: element → score    O(1) 查分 │
│ Skip List:  按 score 排序     O(log N) 范围 │
└──────────────────────────────────────────┘
</code></pre>
<p>两种数据结构各取所长：Hash Table 提供 O(1) 的分数查询，Skip List 提供 O(log N) 的排序和范围查询。Skip List 是双向链表式的，支持正向和反向遍历。</p>
<p><strong>核心命令</strong>：</p>
<pre><code>ZADD board 1000 &quot;user:1&quot;              -- 添加/更新分数
ZINCRBY board 10 &quot;user:1&quot;             -- 分数原子递增
ZREVRANGE board 0 9 WITHSCORES        -- 排行榜前 10（降序）
ZRANGEBYSCORE board 90 100            -- 分数在 90~100 的元素
ZRANK board &quot;user:1&quot;                  -- 排名（升序）
ZREVRANK board &quot;user:1&quot;               -- 排名（降序）
</code></pre>
<p><strong>适用场景</strong>：排行榜、延迟队列（score 存时间戳）、带权重的优先级队列。</p>
<h3>选型决策表</h3>
<table>
<thead>
<tr>
<th>场景</th>
<th>推荐类型</th>
<th>关键命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>缓存、计数器、分布式锁</td>
<td>String</td>
<td>GET/SET/INCR</td>
<td>最简单，最常用</td>
</tr>
<tr>
<td>消息队列、最新列表</td>
<td>List</td>
<td>LPUSH/BRPOP</td>
<td>阻塞弹出实现可靠消费</td>
</tr>
<tr>
<td>对象属性存储</td>
<td>Hash</td>
<td>HSET/HGET/HMGET</td>
<td>比多个 String 省内存 50%+</td>
</tr>
<tr>
<td>标签、去重、集合运算</td>
<td>Set</td>
<td>SADD/SINTER/SDIFF</td>
<td>交并差运算</td>
</tr>
<tr>
<td>排行榜、延迟队列</td>
<td>Sorted Set</td>
<td>ZADD/ZREVRANGE</td>
<td>O(log N) 范围查询</td>
</tr>
</tbody></table>
<hr>
<h2>三、过期策略：惰性删除 + 主动采样</h2>
<p>Redis 不会为每个设置了过期时间的 Key 启动一个定时器——那样百万个 Key 就需要百万个定时器。它采用两种策略配合。</p>
<h3>被动过期（Lazy Expiration）</h3>
<p>访问一个 Key 时，Redis 先检查它是否过期。如果过期了，删除并返回空。</p>
<p><strong>问题</strong>：如果一个 Key 设了过期时间但再也没有被访问，它就永远不会被删除，一直占着内存。</p>
<h3>主动过期（Active Expiration）</h3>
<p>Redis 每秒执行 <strong>10 次</strong>以下流程：</p>
<ol>
<li>从设置了过期时间的 Key 中随机采样 100 个</li>
<li>删除其中已过期的</li>
<li>如果过期 Key 超过 <strong>25%</strong>，回到步骤 1 继续</li>
</ol>
<p>这是一个概率性的清理策略——不保证所有过期 Key 都被及时清理，但保证过期 Key 不会大量累积。</p>
<h3>主从一致性</h3>
<p>过期删除<strong>只在 Master 上执行</strong>。Master 删除一个过期 Key 后，向 Slave 发送 DEL 命令。Slave 自己不会主动删除过期 Key——在收到 Master 的 DEL 之前，Slave 上的过期 Key 仍然可读。</p>
<h3>内存淘汰策略</h3>
<p>当内存使用达到 <code>maxmemory</code> 上限时，Redis 需要决定淘汰哪些 Key：</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>淘汰范围</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>volatile-lru</td>
<td>设了过期时间的 Key</td>
<td>淘汰最近最少使用的（默认）</td>
</tr>
<tr>
<td>volatile-ttl</td>
<td>设了过期时间的 Key</td>
<td>淘汰剩余 TTL 最短的</td>
</tr>
<tr>
<td>allkeys-lru</td>
<td>所有 Key</td>
<td>淘汰最近最少使用的</td>
</tr>
<tr>
<td>noeviction</td>
<td>不淘汰</td>
<td>写入报错（OOM）</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>工程建议</strong>：生产环境必须设置 <code>maxmemory</code>。如果不设置，Redis 内存持续增长，最终触发 OS swap，性能断崖式下降——从微秒级变成毫秒级甚至秒级。</p>
</blockquote>
<hr>
<h2>四、持久化：RDB 与 AOF 的设计权衡</h2>
<p>Redis 是内存数据库，但它不是&quot;重启就丢数据&quot;的玩具。持久化机制让 Redis 在进程崩溃甚至断电后仍能恢复数据。</p>
<h3>4.1 写操作的五步管线</h3>
<p>要理解持久化的数据安全性，需要先理解一次写操作在操作系统层面经历的五个阶段：</p>
<pre><code>Client                 Redis Server              OS Kernel              Disk
  │                        │                        │                    │
  │── write request ──→    │                        │                    │
  │                    ①存入内存                     │                    │
  │                        │── write() ──→          │                    │
  │                        │                    ②写入内核缓冲区          │
  │                        │                        │── transfer ──→    │
  │                        │                        │                ③到达磁盘控制器缓存
  │                        │                        │                    │
  │                        │                        │                ④写入物理介质
</code></pre>
<p><strong>数据安全性分析</strong>：</p>
<table>
<thead>
<tr>
<th>故障类型</th>
<th>数据安全时机</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Redis 进程崩溃</td>
<td>第 ② 步之后</td>
<td>内核缓冲区还在，OS 会最终刷盘</td>
</tr>
<tr>
<td>操作系统崩溃</td>
<td>第 ③ 步之后</td>
<td>磁盘控制器缓存有电容保护（企业级）</td>
</tr>
<tr>
<td>断电</td>
<td>第 ④ 步之后</td>
<td>只有写入物理介质才绝对安全</td>
</tr>
</tbody></table>
<p><code>fsync</code> 的作用：强制将内核缓冲区的数据推到磁盘控制器（乃至物理介质），消除第 ②→④ 之间的不确定性。</p>
<h3>4.2 RDB 快照</h3>
<p>RDB 在指定的时间间隔内生成内存数据的全量快照。</p>
<p><strong>触发条件</strong>（可配置）：</p>
<pre><code>save 900 1        # 900 秒内至少 1 次写入
save 300 10       # 300 秒内至少 10 次写入
save 60 10000     # 60 秒内至少 10000 次写入
</code></pre>
<p><strong>实现机制</strong>：</p>
<pre><code>1. Redis Fork 子进程
2. 子进程遍历内存，将数据写入临时文件（RDB 格式）
3. 写完后，原子 rename 替换旧 RDB 文件
4. 父进程继续处理请求（通过 COW 机制共享内存页）
</code></pre>
<p><strong>COW（Copy-On-Write）</strong>：Fork 之后父子进程共享内存页。只有当父进程修改某个内存页时，OS 才会复制这个页。如果写入量不大，Fork 的额外内存开销很小。</p>
<p><strong>性能数据</strong>：</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>数据</th>
</tr>
</thead>
<tbody><tr>
<td>Fork 耗时</td>
<td>~10ms / GB 内存</td>
</tr>
<tr>
<td>1.5GB 快照 → 200MB 文件</td>
<td>~8 秒（PC 级机器）</td>
</tr>
<tr>
<td>RDB 加载速度</td>
<td>极快（直接映射到内存结构）</td>
</tr>
</tbody></table>
<p><strong>原子性保证</strong>：先写临时文件，再 rename 替换。如果写入过程中进程崩溃，旧 RDB 文件不受影响——数据永远不会损坏，最多丢失最后一次快照之后的写入。</p>
<p><strong>局限</strong>：两次快照之间的数据可能丢失。按默认配置，最坏情况下可能丢失 <strong>最近 5 分钟</strong> 的数据。</p>
<h3>4.3 AOF 日志</h3>
<p>AOF（Append Only File）记录每一条写命令，格式是 Redis 协议文本（人类可读）。</p>
<p><strong>三种 fsync 策略</strong>：</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>行为</th>
<th>最大数据丢失</th>
<th>性能影响</th>
</tr>
</thead>
<tbody><tr>
<td><code>appendfsync no</code></td>
<td>不主动 fsync，交给 OS</td>
<td>~30 秒（Linux 默认）</td>
<td>最小</td>
</tr>
<tr>
<td><code>appendfsync everysec</code></td>
<td>每秒 fsync 一次</td>
<td><strong>最多 2 秒</strong></td>
<td>小（推荐）</td>
</tr>
<tr>
<td><code>appendfsync always</code></td>
<td>每条命令都 fsync</td>
<td>不丢数据</td>
<td>大</td>
</tr>
</tbody></table>
<p>为什么 <code>everysec</code> 最多丢 2 秒而不是 1 秒？因为如果上一次 fsync 超过 1 秒还没完成，Redis 会<strong>延迟</strong>当前的 fsync（避免阻塞主线程），最终可能累积两秒的数据。</p>
<p><strong>AOF 重写</strong>：</p>
<p>AOF 文件随写入不断增长。Redis 通过重写来压缩文件：Fork 子进程遍历内存，生成等价的最小命令集。比如一个 Key 被 SET 了 100 次，重写后只保留最后一次的 SET 命令。</p>
<p><strong>性能数据</strong>：1.4GB AOF 文件加载约 <strong>13 秒</strong>。</p>
<h3>4.4 RDB vs AOF 决策</h3>
<table>
<thead>
<tr>
<th>维度</th>
<th>RDB</th>
<th>AOF</th>
</tr>
</thead>
<tbody><tr>
<td>数据安全性</td>
<td>可能丢数分钟</td>
<td>最多丢 1~2 秒（everysec）</td>
</tr>
<tr>
<td>文件大小</td>
<td>紧凑（二进制）</td>
<td>较大（文本命令，需定期重写）</td>
</tr>
<tr>
<td>启动恢复速度</td>
<td>极快（直接加载）</td>
<td>较慢（逐条重放命令）</td>
</tr>
<tr>
<td>写性能影响</td>
<td>Fork 时有短暂阻塞</td>
<td>每秒 fsync 影响小</td>
</tr>
<tr>
<td>文件可读性</td>
<td>不可读</td>
<td>可读（Redis 协议文本）</td>
</tr>
<tr>
<td>适合做备份</td>
<td>是（完整快照，可远程传输）</td>
<td>不适合（文件持续增长）</td>
</tr>
</tbody></table>
<p><strong>工程建议</strong>：</p>
<ul>
<li><strong>生产环境两者都开</strong>：RDB 做定期冷备（便于传输和恢复），AOF 做热恢复（丢数据少）</li>
<li>如果只能选一个：选 AOF（数据安全性更高）</li>
<li><strong>内存安全边界</strong>：预留 <strong>2 倍</strong> 已用内存给 Fork 的 COW 开销。1GB 数据 → 至少准备 2GB 可用内存</li>
</ul>
<hr>
<h2>五、复制与高可用</h2>
<h3>5.1 主从复制</h3>
<p>Redis 的复制是<strong>异步</strong>的，Slave 最终一致。</p>
<p><strong>全量同步</strong>（首次连接或数据差异过大时）：</p>
<pre><code>Slave 发送 SLAVEOF master_ip port
    ↓
Master 执行 BGSAVE，生成 RDB 文件
    ↓
Master 将 RDB 传输给 Slave
    ↓
Slave 丢弃旧数据，加载 RDB
    ↓
Master 将 BGSAVE 期间的写命令发送给 Slave
    ↓
进入增量同步状态
</code></pre>
<p><strong>增量同步</strong>：Master 将每条写命令实时发送给 Slave 重放。</p>
<p><strong>PSYNC（Redis 2.8+）</strong>：部分重同步。Master 维护一个<strong>复制积压缓冲区（replication backlog）</strong>，如果 Slave 断连后再连上，且断连期间的数据还在缓冲区内，就只发送缺失的命令，避免全量复制。</p>
<h3>5.2 Sentinel 哨兵</h3>
<p>主从复制解决了数据冗余，但 Master 挂了需要人工切换。Sentinel 实现自动故障转移。</p>
<p><strong>架构</strong>：</p>
<pre><code>┌─────────┐     ┌─────────┐     ┌─────────┐
│Sentinel 1│     │Sentinel 2│     │Sentinel 3│   ← 独立进程，至少 3 个
└────┬────┘     └────┬────┘     └────┬────┘
     │               │               │
     └───── 监控 ─────┼───── 监控 ────┘
                      │
              ┌───────┴───────┐
              │    Master     │
              └───┬───────┬──┘
                  │       │
           ┌──────┘       └──────┐
           │                     │
      ┌────┴────┐          ┌────┴────┐
      │ Slave 1 │          │ Slave 2 │
      └─────────┘          └─────────┘
</code></pre>
<p><strong>故障检测过程</strong>：</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>行为</th>
<th>时间</th>
</tr>
</thead>
<tbody><tr>
<td>心跳</td>
<td>每个 Sentinel 每秒向 Master/Slave 发 PING</td>
<td>持续</td>
</tr>
<tr>
<td>主观下线（sdown）</td>
<td>某个 Sentinel 连续无响应</td>
<td>30 秒（可配置）</td>
</tr>
<tr>
<td>客观下线（odown）</td>
<td>多数 Sentinel（quorum，默认 2）在 5 秒内一致认为 Master 不可用</td>
<td>5 秒</td>
</tr>
<tr>
<td>故障转移</td>
<td>选举 Leader Sentinel → 选择最优 Slave → 提升为 Master → 重定向其他 Slave</td>
<td>秒级</td>
</tr>
</tbody></table>
<p><strong>Sentinel 之间的发现</strong>：通过 Pub/Sub 通道，每 5 秒广播自己的信息。新加入的 Sentinel 自动被发现。</p>
<h3>5.3 Cluster（简述）</h3>
<p>Redis Cluster 提供数据自动分片和高可用：</p>
<ul>
<li><strong>16384 个 slot</strong>：每个 Key 通过 CRC16 哈希映射到一个 slot，每个节点负责一部分 slot</li>
<li><strong>客户端重定向</strong>：请求的 Key 不在当前节点时，返回 <code>MOVED</code> 或 <code>ASK</code> 指引客户端到正确节点</li>
<li><strong>自动故障转移</strong>：每个 Master 有一个或多个 Slave，Master 挂掉后 Slave 自动提升</li>
</ul>
<p>vs 客户端分片（如 Jedis ShardedJedis）：Cluster 支持在线迁移 slot（动态扩缩容），客户端分片不支持。</p>
<hr>
<h2>六、性能调优实践</h2>
<h3>Pipeline：减少网络往返</h3>
<p>Redis 命令的延迟大部分花在网络往返（RTT）上。Pipeline 将多个命令打包成一次网络请求：</p>
<pre><code>-- 不用 Pipeline：100 个 SET → 100 次 RTT
-- 用 Pipeline：100 个 SET → 1 次 RTT
</code></pre>
<p>Pipeline 不是原子操作（中间可能插入其他客户端的命令），但吞吐可提升 <strong>5~10 倍</strong>。</p>
<h3>Lua 脚本：服务端执行</h3>
<pre><code>EVAL &quot;redis.call(&#39;SET&#39;, KEYS[1], ARGV[1]); redis.call(&#39;EXPIRE&#39;, KEYS[1], ARGV[2])&quot; 1 mykey myvalue 60
</code></pre>
<p>Lua 脚本在 Redis 中天然是<strong>原子</strong>的——脚本执行期间不会被其他命令打断。适合需要&quot;读-判断-写&quot;原子性的场景（如分布式锁、限流计数器）。</p>
<h3>大 Key 问题</h3>
<table>
<thead>
<tr>
<th>问题</th>
<th>原因</th>
<th>解决方案</th>
</tr>
</thead>
<tbody><tr>
<td>DEL 大 Key 阻塞</td>
<td>释放大块内存需要时间</td>
<td>用 UNLINK（异步删除，Redis 4.0+）</td>
</tr>
<tr>
<td>Set 扩容阻塞</td>
<td>Hash Table resize 需要写锁</td>
<td>控制 Set 大小，拆分到多个 Key</td>
</tr>
<tr>
<td>HGETALL 大 Hash 阻塞</td>
<td>遍历大量字段</td>
<td>用 HSCAN 分批获取</td>
</tr>
<tr>
<td>网络阻塞</td>
<td>大 Value 传输占用带宽</td>
<td>Value 大小控制在 10KB 以内</td>
</tr>
</tbody></table>
<h3>生产禁用命令</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>风险</th>
<th>替代方案</th>
</tr>
</thead>
<tbody><tr>
<td><code>KEYS *</code></td>
<td>O(N) 遍历所有 Key，阻塞主线程</td>
<td><code>SCAN</code>（游标分批遍历）</td>
</tr>
<tr>
<td><code>FLUSHDB</code> / <code>FLUSHALL</code></td>
<td>清空数据库</td>
<td>线上应禁用</td>
</tr>
<tr>
<td><code>SORT</code></td>
<td>CPU 密集型排序</td>
<td>在 Slave 上执行，或应用层排序</td>
</tr>
</tbody></table>
<h3>连接池配置</h3>
<p>以 Jedis 为例，默认最大连接数为 <strong>8</strong>。高并发场景下远远不够：</p>
<pre><code class="language-java">JedisPoolConfig config = new JedisPoolConfig();
config.setMaxTotal(200);        // 最大连接数
config.setMaxIdle(50);          // 最大空闲连接
config.setMinIdle(10);          // 最小空闲连接
config.setMaxWaitMillis(3000);  // 获取连接超时
</code></pre>
<p><strong>配置原则</strong>：连接数 ≥ 峰值并发线程数，但不要过多（Redis 单线程，连接再多也是排队）。</p>
<hr>
<h2>七、Redis vs Memcached：选型依据</h2>
<p>两者都是内存缓存，但定位不同。</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>Redis</th>
<th>Memcached</th>
</tr>
</thead>
<tbody><tr>
<td>数据结构</td>
<td>String/List/Hash/Set/Sorted Set</td>
<td>纯 Key-Value</td>
</tr>
<tr>
<td>持久化</td>
<td>RDB + AOF</td>
<td>无</td>
</tr>
<tr>
<td>高可用</td>
<td>主从复制 + Sentinel</td>
<td>无内置方案</td>
</tr>
<tr>
<td>内存效率</td>
<td>有对象开销（~90 字节/Key）</td>
<td>更高效的 slab 分配</td>
</tr>
<tr>
<td>单 Value 上限</td>
<td>1GB</td>
<td>1MB</td>
</tr>
<tr>
<td>多线程</td>
<td>单线程（6.0 起 I/O 多线程）</td>
<td>多线程</td>
</tr>
<tr>
<td>集群</td>
<td>Redis Cluster</td>
<td>客户端一致性 Hash</td>
</tr>
</tbody></table>
<p><strong>选择 Memcached 的场景</strong>：</p>
<ul>
<li>纯缓存，不需要持久化</li>
<li>大 Value（&gt; 100KB），Memcached 的 slab 内存分配更高效</li>
<li>简单 KV，不需要复杂数据结构</li>
<li>已有成熟的 Memcached 集群</li>
</ul>
<p><strong>选择 Redis 的场景</strong>：</p>
<ul>
<li>需要持久化（缓存 + 存储双角色）</li>
<li>需要复杂数据结构（排行榜、队列、集合运算）</li>
<li>需要内置高可用（Sentinel、Cluster）</li>
<li>需要 Pub/Sub、Lua 脚本等高级特性</li>
</ul>
<blockquote>
<p>大多数新项目选 Redis——功能超集，生态更活跃。选 Memcached 的理由通常是&quot;已经在用&quot;或&quot;纯缓存场景下更高的内存效率&quot;。</p>
</blockquote>
17:Tda6e,<h2>一、为什么你的系统需要限流</h2>
<p>每个系统都有容量边界。缓存解决读的问题，降级解决非核心链路的问题，但当写操作高并发、稀缺资源被争抢、昂贵查询集中涌入时——<strong>只有限流能保护你。</strong></p>
<table>
<thead>
<tr>
<th>手段</th>
<th>解决的问题</th>
<th>核心机制</th>
<th>局限性</th>
</tr>
</thead>
<tbody><tr>
<td><strong>缓存</strong></td>
<td>提速</td>
<td>将高频数据放入更快的存储层</td>
<td>对写操作无能为力</td>
</tr>
<tr>
<td><strong>降级</strong></td>
<td>止损</td>
<td>放弃非核心功能保核心链路</td>
<td>前提是有东西可降，秒杀场景无法降级</td>
</tr>
<tr>
<td><strong>限流</strong></td>
<td>控流</td>
<td>主动丢弃/延迟超量请求</td>
<td>需要准确的容量评估，否则误杀或漏放</td>
</tr>
</tbody></table>
<p>但&quot;限流&quot;这两个字过于笼统。请求涌入太快是限流问题，同时处理的请求太多也是限流问题，同样叫限流，控制的东西完全不同。<strong>限流不是一个算法，而是一套控制体系。</strong> 要选对方案，首先要搞清楚：你到底在控制什么？</p>
<hr>
<h2>二、你到底在控制什么——四种限流模型</h2>
<p>大多数人一提&quot;限流&quot;就想到令牌桶、漏桶。但算法只是手段，在选算法之前要先回答一个更根本的问题：<strong>你要控制的是什么物理量？</strong></p>
<p>现实中的限流需求可以归纳为四种控制模型，每种控制着不同的&quot;物理量&quot;，适用不同的场景，也对应不同的算法家族：</p>
<h3>到达速率控制——控制&quot;多快进来&quot;</h3>
<blockquote>
<p>本质：单位时间内允许通过的请求数量。</p>
</blockquote>
<p>典型场景：API 接口限制每秒 1000 次调用、用户登录接口限制每分钟 5 次尝试、短信验证码 60 秒内只能发一次。</p>
<p>这是最常见的限流需求。它的核心关注点是&quot;单位时间的请求数&quot;——不管每个请求要跑多久、占多少资源，只要单位时间内的数量不超标就放行。</p>
<h3>并发占用控制——控制&quot;同时多少个&quot;</h3>
<blockquote>
<p>本质：任意时刻正在处理的请求数量。</p>
</blockquote>
<p>典型场景：数据库连接池最多 50 个连接、报表导出接口最多同时执行 3 个、文件上传同时只允许 10 个。</p>
<p>与速率控制的区别：速率控制不关心每个请求&quot;待多久&quot;，并发控制则相反——一个跑 10 分钟的报表任务，速率限制器根本管不住它。如果你有 10 个这样的任务同时运行，速率限制器显示&quot;每秒只进来 1 个&quot;一切正常，但系统已经被压垮了。</p>
<h3>长期配额控制——控制&quot;总共多少次&quot;</h3>
<blockquote>
<p>本质：一个较长周期内允许消耗的总量。</p>
</blockquote>
<p>典型场景：免费用户每天 100 次 API 调用、每月 10GB 流量配额、每个租户每月 100 万次查询。</p>
<p>配额控制关注的是&quot;累计消耗&quot;，时间尺度从小时到月不等。它与速率控制看似相似（都是&quot;一段时间内的请求数&quot;），但有本质区别：速率控制关注的是&quot;瞬时压力&quot;——保护系统不被打垮；配额控制关注的是&quot;商业资源&quot;——控制成本或实现产品差异化。一个配额为每天 1000 次的用户，完全可以在第一秒就用完所有配额，速率限制器不会拦他。</p>
<h3>执行节奏控制——控制&quot;多快出去&quot;</h3>
<blockquote>
<p>本质：请求被处理和释放的速率，确保输出均匀平稳。</p>
</blockquote>
<p>典型场景：消息队列消费速率控制、音视频流恒定码率传输、对接物理设备接口（打印机、传感器）。</p>
<p>前面三种都是在&quot;入口&quot;做控制：请求来了，判断能不能进。节奏控制不同，它控制的是&quot;出口&quot;——请求已经被接受，但要排队按固定节奏释放。即使系统空闲、令牌充裕，也不会加速处理。</p>
<h3>为什么不能互相替代</h3>
<table>
<thead>
<tr>
<th>控制模型</th>
<th>控制的物理量</th>
<th>如果只用速率限制…</th>
</tr>
</thead>
<tbody><tr>
<td>到达速率</td>
<td>单位时间请求数</td>
<td>✅ 这正是它干的事</td>
</tr>
<tr>
<td>并发占用</td>
<td>同时在处理的请求数</td>
<td>❌ 10 个慢请求各跑 10 分钟，速率上看只有&quot;1 个/分钟&quot;，但并发已爆</td>
</tr>
<tr>
<td>长期配额</td>
<td>累计消耗总量</td>
<td>❌ 速率 100/s 的限制管不住&quot;每天只许用 1000 次&quot;的商业规则</td>
</tr>
<tr>
<td>执行节奏</td>
<td>输出的均匀程度</td>
<td>❌ 令牌桶允许突发消费，下游设备收到脉冲流量就炸了</td>
</tr>
</tbody></table>
<h3>需求 → 算法决策总表</h3>
<p>在进入具体算法之前，先给出一张导航图。后续章节会逐一展开每种算法的原理和实现：</p>
<table>
<thead>
<tr>
<th>你的需求</th>
<th>控制模型</th>
<th>推荐算法</th>
<th>章节</th>
</tr>
</thead>
<tbody><tr>
<td>API 限制每秒 N 次调用</td>
<td>到达速率</td>
<td>固定窗口 / 滑动窗口计数器</td>
<td>3.1</td>
</tr>
<tr>
<td>精确统计每个请求的时间分布</td>
<td>到达速率</td>
<td>滑动窗口日志</td>
<td>3.1</td>
</tr>
<tr>
<td>允许突发但限制平均速率</td>
<td>突发 + 速率</td>
<td>令牌桶 / GCRA</td>
<td>3.2</td>
</tr>
<tr>
<td>下游绝对不能承受波动</td>
<td>执行节奏</td>
<td>漏桶</td>
<td>3.3</td>
</tr>
<tr>
<td>限制同时处理的请求数</td>
<td>并发占用</td>
<td>信号量 / Bulkhead</td>
<td>3.4</td>
</tr>
<tr>
<td>每天/每月 N 次调用额度</td>
<td>长期配额</td>
<td>固定窗口长周期 / 滚动配额</td>
<td>3.5</td>
</tr>
</tbody></table>
<hr>
<h2>三、限流算法详解与工程实现</h2>
<p>下面给出五种经典限流算法的 Java 实现——纯 JDK、per-key、线程安全，不依赖任何第三方库。所有实现遵循统一接口：</p>
<pre><code class="language-java">interface RateLimiter {
    boolean allow(String key);
}
</code></pre>
<h3>3.1 速率控制家族——控制&quot;多快进来&quot;</h3>
<p>这一族算法的共同目标：在一个时间窗口内，限制请求的通过数量。区别在于如何定义和计算&quot;窗口&quot;。</p>
<h4>固定窗口计数器（Fixed Window Counter）</h4>
<p><strong>核心原理</strong></p>
<p>在一个固定时间窗口内维护计数器，超过阈值就拒绝，窗口结束时归零。</p>
<pre><code>|← 窗口1 (0-1s) →|← 窗口2 (1-2s) →|
    count=0→100        count=0→...
    阈值=100           阈值=100
</code></pre>
<p><strong>Java 实现</strong></p>
<p>固定窗口和滑动窗口共用一个 <code>Window</code> 状态类：</p>
<pre><code class="language-java">class Window {
    long windowStart;
    int count;
    int preCount;  // 滑动窗口用：上一个窗口的计数

    Window(long windowStart) {
        this.windowStart = windowStart;
    }
}
</code></pre>
<pre><code class="language-java">class FixedWindowRateLimiter implements RateLimiter {

    private final int limit;
    private final long windowNanos;
    private final ConcurrentHashMap&lt;String, Window&gt; map = new ConcurrentHashMap&lt;&gt;();

    // limit: 窗口内最大请求数, windowMillis: 窗口大小（毫秒）
    FixedWindowRateLimiter(int limit, long windowMillis) {
        this.limit = limit;
        this.windowNanos = windowMillis * 1_000_000L;
    }

    @Override
    public boolean allow(String key) {
        long now = System.nanoTime();
        Window w = map.computeIfAbsent(key, _ -&gt; new Window(now));
        synchronized (w) {
            long elapsed = now - w.windowStart;
            // 窗口过期：对齐到窗口边界（而不是 windowStart = now）
            if (elapsed &gt;= windowNanos) {
                long periods = elapsed / windowNanos;
                w.windowStart += periods * windowNanos;
                w.count = 0;
            }
            if (w.count &lt; limit) {
                w.count++;
                return true;
            }
            return false;
        }
    }
}
</code></pre>
<p>注意窗口过期时用 <code>windowStart += periods * windowNanos</code> 对齐到窗口边界，而不是直接 <code>windowStart = now</code>。后者会导致窗口漂移——每次重置都把窗口起点推到当前时间，使得窗口大小不再固定。</p>
<p><strong>经典问题：窗口边界的 2 倍峰值</strong></p>
<pre><code>|← 窗口1 →|← 窗口2 →|
      ↑
   最后100ms涌入100个  最前100ms涌入100个

   → 200ms 内实际通过了 200 个请求（2 倍于阈值）
</code></pre>
<p><strong>适用场景</strong></p>
<ul>
<li>精度要求不高的简单限流（大部分业务场景）</li>
<li>需要快速实现的场景</li>
<li>阈值本身留有足够余量（2 倍偶发峰值可承受）</li>
</ul>
<p><strong>工程判断</strong>：在很多场景中，固定窗口的精度已经足够。边界处偶尔的 2 倍峰值，对于留有余量的系统来说不是问题。不要为理论上的完美过度工程化。</p>
<hr>
<h4>滑动窗口计数器（Sliding Window Counter）</h4>
<p><strong>核心原理</strong></p>
<p>固定窗口的边界问题源于&quot;硬切割&quot;——窗口一旦翻页，上个窗口的计数彻底清零。滑动窗口计数器的思路是：保留上一个窗口的计数，用加权平均来近似&quot;滑动&quot;效果。</p>
<pre><code>|← 上一个窗口 →|← 当前窗口 →|
   preCount=80     count=20
                     ↑ now（已过 30% 窗口）

估算值 = count + preCount × (1 - 30%) = 20 + 80 × 0.7 = 76
</code></pre>
<p>窗口刚翻页时（elapsed ≈ 0），上个窗口的权重接近 100%，相当于还在&quot;旧窗口&quot;里计数；窗口快结束时（elapsed ≈ windowSize），上个窗口的权重接近 0%，退化为固定窗口。这种线性插值在大多数场景下精度足够，且存储开销和固定窗口一样是 O(1)。</p>
<p><strong>与固定窗口的对比</strong></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>固定窗口</th>
<th>滑动窗口计数器</th>
</tr>
</thead>
<tbody><tr>
<td>精度</td>
<td>存在边界 2 倍峰值</td>
<td>加权平滑，消除边界效应</td>
</tr>
<tr>
<td>实现复杂度</td>
<td>一个计数器</td>
<td>两个计数器 + 加权计算</td>
</tr>
<tr>
<td>存储开销</td>
<td>O(1)</td>
<td>O(1)</td>
</tr>
<tr>
<td>适用场景</td>
<td>精度要求低、快速实现</td>
<td>精度要求高、阈值接近系统极限</td>
</tr>
</tbody></table>
<p><strong>Java 实现</strong></p>
<pre><code class="language-java">class SlidingWindowRateLimiter implements RateLimiter {

    private final int limit;
    private final long windowNanos;
    private final ConcurrentHashMap&lt;String, Window&gt; map = new ConcurrentHashMap&lt;&gt;();

    SlidingWindowRateLimiter(int limit, long windowMillis) {
        this.limit = limit;
        this.windowNanos = windowMillis * 1_000_000L;
    }

    @Override
    public boolean allow(String key) {
        long now = System.nanoTime();
        Window w = map.computeIfAbsent(key, _ -&gt; new Window(now));
        synchronized (w) {
            long elapsed = now - w.windowStart;
            if (elapsed &gt;= windowNanos) {
                long periods = elapsed / windowNanos;
                // 跨了 2 个以上窗口，上个窗口数据已无意义
                w.preCount = (periods &gt;= 2) ? 0 : w.count;
                w.count = 0;
                w.windowStart += periods * windowNanos;
                elapsed = now - w.windowStart;
            }
            // 加权估算：当前计数 + 上一窗口计数 × 未过期比例
            double weight = (double) elapsed / windowNanos;
            double estimated = w.count + w.preCount * (1.0 - weight);
            if (estimated &lt; limit) {
                w.count++;
                return true;
            }
            return false;
        }
    }
}
</code></pre>
<p><strong>工程实践：更精细的子窗口方案</strong></p>
<p>上面的两窗口加权方案简洁高效，在大多数场景下已足够。如果需要更精细的滑动效果，可以将窗口划分为多个子窗口（slot），例如阿里巴巴的 Sentinel 框架使用 <code>LeapArray</code> 数据结构：</p>
<ul>
<li>将 1 秒划分为若干个 <code>WindowWrap</code>（默认 2 个，即 500ms 一个子窗口）</li>
<li>每个子窗口维护独立的 pass/block/exception 等计数器</li>
<li>通过环形数组 + 时间戳判断实现窗口滑动，避免频繁创建销毁对象</li>
</ul>
<p>存储开销从 O(1) 变为 O(N)（N 为子窗口数），精度更高，但实现复杂度也相应增加。</p>
<hr>
<h4>滑动窗口日志（Sliding Window Log）</h4>
<p><strong>核心原理</strong></p>
<p>记录每一个请求的精确时间戳，判断时移除窗口外的过期记录，然后统计剩余记录数。这是精度最高的速率控制算法——没有子窗口的近似，每个请求的时间位置都被精确记录。</p>
<pre><code>窗口大小 = 1s，阈值 = 5

请求日志: [0.1, 0.3, 0.5, 0.8, 0.9]
                                    ↑ 当前时间 = 1.2s

移除 &lt; 0.2 的记录 → [0.3, 0.5, 0.8, 0.9]
当前窗口内 4 个请求 &lt; 阈值 5 → 放行，记录 1.2
</code></pre>
<p><strong>伪代码</strong></p>
<pre><code class="language-python">def sliding_log_allow(key, threshold, window_size):
    now = current_time()

    # 移除窗口外的过期记录
    store.remove_before(key, now - window_size)

    # 统计当前窗口内的请求数
    count = store.count(key)

    if count &lt; threshold:
        store.add(key, now)  # 记录本次请求时间戳
        return True
    return False
</code></pre>
<p><strong>优势与代价</strong></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>精度</td>
<td>完美——没有任何窗口边界问题</td>
</tr>
<tr>
<td>存储开销</td>
<td>O(N)，N 为窗口内的请求数。QPS 1000 + 1s 窗口 = 1000 条记录</td>
</tr>
<tr>
<td>适用场景</td>
<td>低 QPS + 高精度要求（如 API 计费、安全审计）</td>
</tr>
<tr>
<td>不适用</td>
<td>高 QPS 场景——存储和清理开销过大</td>
</tr>
</tbody></table>
<p><strong>工程判断</strong>：滑动窗口日志的精度是三种窗口算法中最高的，但存储成本也最高。在 Redis 中通常用 Sorted Set 实现（第四章会详细展示）。对于大多数业务场景，滑动窗口计数器是更好的平衡点。</p>
<hr>
<h3>3.2 突发与平均速率——控制&quot;允许多大的脉冲&quot;</h3>
<p>窗口类算法有一个共同的局限：它们只看&quot;窗口内的总量&quot;，不区分&quot;均匀到达&quot;和&quot;一瞬间全来&quot;。令牌桶和 GCRA 解决的正是这个问题——允许一定程度的突发，但限制长期平均速率。</p>
<h4>令牌桶算法（Token Bucket）</h4>
<p><strong>核心原理</strong></p>
<p>令牌桶的理念：<strong>在空闲时积蓄能力，在繁忙时释放能力。</strong></p>
<pre><code>令牌生成器 ──恒定速率──→ [  令牌桶（有容量上限）  ]
                                    ↓
                         请求到达 → 取令牌 → 有令牌则通过
                                           → 无令牌则拒绝/等待
</code></pre>
<ul>
<li>系统以恒定速率向桶中放入令牌</li>
<li>每个请求消耗一个（或多个）令牌</li>
<li>令牌充足时请求立即通过</li>
<li>令牌耗尽时请求被拒绝或阻塞等待</li>
<li>桶有容量上限，多余令牌溢出</li>
</ul>
<p><strong>核心参数</strong></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
<th>设计考量</th>
</tr>
</thead>
<tbody><tr>
<td>令牌生成速率</td>
<td>系统的持续处理能力</td>
<td>对应系统稳态吞吐上限</td>
</tr>
<tr>
<td>桶容量</td>
<td>允许的最大突发量</td>
<td>编码了对突发流量的容忍度</td>
</tr>
</tbody></table>
<p><strong>Java 实现</strong></p>
<pre><code class="language-java">class TokenBucketRateLimiter implements RateLimiter {

    private final double capacity;       // 桶容量（最大突发量）
    private final double refillPerNano;  // 每纳秒补充的令牌数
    private final boolean warmUp;        // true = 冷启动从 0 开始

    private static class Bucket {
        double tokens;
        long lastRefillTime;
    }

    private final ConcurrentHashMap&lt;String, Bucket&gt; map = new ConcurrentHashMap&lt;&gt;();

    /**
     * @param permitsPerSecond 每秒放入的令牌数
     * @param burst            桶容量（最大突发量）
     * @param warmUp           true = 新 key 从 0 令牌开始（冷启动）
     */
    TokenBucketRateLimiter(double permitsPerSecond, int burst, boolean warmUp) {
        this.capacity = burst;
        this.refillPerNano = permitsPerSecond / 1_000_000_000.0;
        this.warmUp = warmUp;
    }

    @Override
    public boolean allow(String key) {
        long now = System.nanoTime();
        Bucket b = map.computeIfAbsent(key, _ -&gt; {
            Bucket bucket = new Bucket();
            bucket.tokens = warmUp ? 0 : capacity;  // 冷启动 vs 满桶启动
            bucket.lastRefillTime = now;
            return bucket;
        });
        synchronized (b) {
            long elapsed = now - b.lastRefillTime;
            if (elapsed &gt; 0) {
                // 懒填充：按经过的时间补充令牌
                b.tokens = Math.min(capacity, b.tokens + elapsed * refillPerNano);
                b.lastRefillTime = now;
            }
            if (b.tokens &gt;= 1.0) {
                b.tokens -= 1.0;
                return true;
            }
            return false;
        }
    }
}
</code></pre>
<p>关键实现细节：&quot;懒填充&quot;（lazy refill）。不需要一个后台线程不断往桶里放令牌，只需在每次请求到来时，根据距上次填充的时间差计算应补充的令牌数。这让实现变得高效。<code>warmUp</code> 参数控制新 key 是从满桶开始（适合 API 限流）还是从空桶开始（适合缓存预热）。</p>
<p><strong>适用场景</strong></p>
<ul>
<li>互联网 API 限流（绝大多数场景的首选）</li>
<li>允许合理突发的业务场景（秒杀、热点事件引发的流量脉冲）</li>
<li>需要区分长期速率和瞬时峰值的场景</li>
</ul>
<p><strong>工程实践：Guava RateLimiter 的两种模式</strong></p>
<p>Guava 提供了两种令牌桶实现，对应两种不同的业务需求：</p>
<pre><code class="language-java">// 模式一：SmoothBursty —— 允许突发
// 以每秒 100 个令牌的速率生成，桶容量等于 1 秒的产量（100）
RateLimiter limiter = RateLimiter.create(100.0);

// 场景：API 网关限流
// 特点：空闲期积累的令牌可以一次性消费，应对突发
if (limiter.tryAcquire()) {
    processRequest();
} else {
    return Response.status(429).build();
}
</code></pre>
<pre><code class="language-java">// 模式二：SmoothWarmingUp —— 冷启动预热
// 速率 100/s，预热期 3 秒
RateLimiter limiter = RateLimiter.create(100.0, 3, TimeUnit.SECONDS);

// 场景：数据库连接池、缓存冷启动
// 特点：系统刚启动时不会全速放量，给下游一个&quot;热身&quot;时间
// 预热期内速率从低到高线性增长，避免冷系统被瞬时流量打垮
</code></pre>
<p><strong>SmoothBursty vs SmoothWarmingUp 的选择</strong></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>SmoothBursty</th>
<th>SmoothWarmingUp</th>
</tr>
</thead>
<tbody><tr>
<td>突发处理</td>
<td>允许消费积累的令牌，支持突发</td>
<td>冷启动期间限制突发</td>
</tr>
<tr>
<td>典型场景</td>
<td>API 限流、消息推送</td>
<td>数据库预热、缓存预热</td>
</tr>
<tr>
<td>核心关注</td>
<td>流量的峰谷平衡</td>
<td>系统的冷热状态转换</td>
</tr>
</tbody></table>
<p><strong>关键注意</strong>：Guava RateLimiter 是<strong>单机限流</strong>。它只能控制当前 JVM 进程的流量，在分布式环境下需要配合 Redis 方案使用。</p>
<hr>
<h4>GCRA（Generic Cell Rate Algorithm）</h4>
<p><strong>核心原理</strong></p>
<p>GCRA 是令牌桶的数学等价形式，但实现更精简。它不维护&quot;当前令牌数&quot;，而是维护一个&quot;理论到达时间&quot;（TAT，Theoretical Arrival Time）——下一个请求最早应该在什么时候到达。</p>
<p>核心思想：如果请求到达得比预期频率更快，TAT 会不断后推；如果请求到达得比预期慢，TAT 会被拉回到当前时间附近（但不会超前太多，受突发容量限制）。</p>
<pre><code>参数：
  emission_interval = 1/rate     -- 每个请求的理论间隔
  burst_tolerance   = burst * emission_interval  -- 允许的最大提前量

判断逻辑：
  TAT = max(TAT, now) + emission_interval
  如果 TAT - now &gt; burst_tolerance → 拒绝（超前太多，突发已耗尽）
  否则 → 放行，更新 TAT
</code></pre>
<p><strong>Java 实现</strong></p>
<p>与前面的算法不同，GCRA 使用 <code>AtomicLong</code> + CAS 实现<strong>无锁</strong>设计，天然适合高并发场景：</p>
<pre><code class="language-java">class GcraRateLimiter implements RateLimiter {

    private final long T;    // 请求间隔 (ns)
    private final long tau;  // 突发容忍窗口 (ns) = burstPermits × T

    // 无锁设计：CAS 自旋
    private final ConcurrentHashMap&lt;String, AtomicLong&gt; tatByKey = new ConcurrentHashMap&lt;&gt;();

    GcraRateLimiter(double permitsPerSecond, int burstPermits) {
        this.T = (long) (TimeUnit.SECONDS.toNanos(1) / permitsPerSecond);
        this.tau = burstPermits * T;
    }

    @Override
    public boolean allow(String key) {
        long now = System.nanoTime();
        AtomicLong tatRef = tatByKey.computeIfAbsent(key, _ -&gt; new AtomicLong(now));
        while (true) {
            long tat = tatRef.get();
            if (now &lt; tat - tau) {
                return false;  // 请求来得太早，拒绝
            }
            long newTat = Math.max(now, tat) + T;
            if (tatRef.compareAndSet(tat, newTat)) {
                return true;
            }
            Thread.onSpinWait();  // CAS 失败，降低自旋 CPU 开销
        }
    }
}
</code></pre>
<p>判断逻辑先于更新执行：<code>now &lt; tat - tau</code> 直接拒绝，避免了&quot;先更新 TAT 再判断是否超限&quot;的回滚问题。<code>Thread.onSpinWait()</code>（Java 9+）在 CAS 失败时降低 CPU 空转开销。</p>
<p><strong>为什么 GCRA 值得关注</strong></p>
<table>
<thead>
<tr>
<th>优势</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>状态极简</td>
<td>只需存储一个值（TAT），对比令牌桶需要存 tokens + last_refill</td>
</tr>
<tr>
<td>天然适合分布式</td>
<td>一个 Redis key 存一个时间戳，原子 CAS 即可更新</td>
</tr>
<tr>
<td>数学精确</td>
<td>与令牌桶行为完全等价，但无浮点累积误差</td>
</tr>
</tbody></table>
<p><strong>工程实践</strong>：GCRA 广泛用于网络设备的 ATM 流量控制（这也是它名字中&quot;Cell Rate&quot;的来源），在互联网领域被 Cloudflare、Stripe 等公司采用作为 API 限流的核心算法。</p>
<hr>
<h3>3.3 流量整形——控制&quot;多快出去&quot;</h3>
<h4>漏桶算法（Leaky Bucket）</h4>
<p><strong>核心原理</strong></p>
<p>漏桶的逻辑可以用一句话概括：<strong>无论流入多快，流出永远恒定。</strong></p>
<pre><code>请求流入 → [  桶（有容量上限）  ] → 恒定速率流出 → 下游处理
                    ↓
              桶满则丢弃
</code></pre>
<ul>
<li>请求以任意速率流入桶中</li>
<li>桶底以固定速率流出（处理请求）</li>
<li>桶有容量上限，溢出的请求被直接丢弃</li>
</ul>
<p><strong>核心参数</strong></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
<th>设计考量</th>
</tr>
</thead>
<tbody><tr>
<td>流出速率</td>
<td>下游能承受的恒定处理能力</td>
<td>取决于下游系统的稳态吞吐上限</td>
</tr>
<tr>
<td>桶容量</td>
<td>允许暂存的最大请求数</td>
<td>过大导致延迟积累，过小导致突发流量全被丢弃</td>
</tr>
</tbody></table>
<p><strong>Java 实现</strong></p>
<p>下面的实现用&quot;下一次允许通过的时间&quot;来建模漏桶的恒定流出：每放行一个请求，<code>nextAllowedTime</code> 就往后推一个 <code>intervalNanos</code>。如果请求到达时已经超前太多（超出 burst 容忍量），直接拒绝。</p>
<pre><code class="language-java">class LeakyBucketRateLimiter implements RateLimiter {

    private final long intervalNanos;  // 每个请求的理论间隔
    private final long burstNanos;     // 突发容忍量（纳秒）

    private static class Bucket {
        long nextAllowedTime;
        Bucket(long t) { this.nextAllowedTime = t; }
    }

    private final ConcurrentHashMap&lt;String, Bucket&gt; map = new ConcurrentHashMap&lt;&gt;();

    LeakyBucketRateLimiter(double permitsPerSecond, int burstPermits) {
        this.intervalNanos = (long) (TimeUnit.SECONDS.toNanos(1) / permitsPerSecond);
        this.burstNanos = burstPermits * intervalNanos;
    }

    @Override
    public boolean allow(String key) {
        long now = System.nanoTime();
        Bucket b = map.computeIfAbsent(key, _ -&gt; new Bucket(now));
        synchronized (b) {
            long allowAt = b.nextAllowedTime - burstNanos;
            if (now &lt; allowAt) {
                return false;  // 桶满了，拒绝
            }
            b.nextAllowedTime = Math.max(now, b.nextAllowedTime) + intervalNanos;
            return true;
        }
    }
}
</code></pre>
<p>当 <code>burstPermits = 0</code> 时，漏桶不允许任何突发，请求严格按 <code>intervalNanos</code> 的间隔逐个放行——这正是&quot;恒定速率流出&quot;的语义。</p>
<p><strong>与令牌桶的本质区别</strong></p>
<p>令牌桶控制的是&quot;允许进入的速率&quot;（入口），漏桶控制的是&quot;实际处理的速率&quot;（出口）。令牌桶在空闲后可以突发放行一批请求，漏桶永远不会——即使桶里积攒了很多请求，也只能按固定速率一个个出去。</p>
<p><strong>适用场景</strong></p>
<ul>
<li>对接物理设备或硬件接口（严格不允许任何突发）</li>
<li>需要绝对平滑的输出流量（如音视频流的恒定码率传输）</li>
<li>流量整形（traffic shaping）场景</li>
</ul>
<p><strong>不适用场景</strong></p>
<ul>
<li>互联网业务的 API 限流（真实流量天然是突发的，漏桶的死板会浪费系统空闲容量）</li>
<li>需要快速响应突发请求的场景</li>
</ul>
<p><strong>工程实践：Nginx 的 <code>limit_req</code> 就是漏桶实现</strong></p>
<pre><code class="language-nginx"># 定义限流区域：10MB 共享内存，每个 IP 每秒 10 个请求
limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;

server {
    location /api/ {
        # burst=20：桶容量为 20，超出的排队
        # nodelay：排队请求不延迟，立即处理（占用 burst 配额）
        limit_req zone=api burst=20 nodelay;

        # 超限返回 429 而非默认的 503
        limit_req_status 429;
    }
}
</code></pre>
<p>这里有个常见误区：<code>burst=20 nodelay</code> 不是&quot;允许突发 20 个请求&quot;那么简单。<code>nodelay</code> 的含义是突发请求立即转发（不排队等待），但每个突发请求会&quot;占用&quot;一个 burst 槽位，槽位按 <code>rate</code> 的速率恢复。实际效果是：瞬间可以通过 30 个请求（rate + burst），但之后必须等槽位恢复。</p>
<hr>
<h3>3.4 并发控制——控制&quot;同时多少个&quot;</h3>
<p>前面所有算法都在控制&quot;速率&quot;——单位时间通过多少个请求。但有些场景下，速率不是瓶颈，并发才是。</p>
<p><strong>问题场景</strong>：一个报表导出接口，每次调用需要 30 秒完成，消耗大量 CPU 和内存。即使限制为每秒 1 个请求，如果 30 秒内每秒都来一个，就有 30 个同时在执行——足以打垮服务。</p>
<h4>信号量 / Bulkhead</h4>
<p><strong>核心原理</strong></p>
<p>维护一个并发计数器。请求进入时 +1，请求完成时 -1。计数器达到上限时，新请求被拒绝或排队。</p>
<pre><code>请求进入 → 计数器 +1 → [正在处理：当前 3/5] → 完成 → 计数器 -1
                ↓
          计数器 = 5 → 拒绝/排队
</code></pre>
<p><strong>伪代码</strong></p>
<pre><code class="language-python">class ConcurrencyLimiter:
    def __init__(self, max_concurrent):
        self.max_concurrent = max_concurrent
        self.current = 0         # 当前并发数
        self.lock = Lock()

    def acquire(self):
        with self.lock:
            if self.current &gt;= self.max_concurrent:
                return False
            self.current += 1
            return True

    def release(self):
        with self.lock:
            self.current -= 1
</code></pre>
<p><strong>关键区别：为什么速率限制替代不了并发控制？</strong></p>
<table>
<thead>
<tr>
<th>场景</th>
<th>速率限制（10 req/s）</th>
<th>并发控制（max=5）</th>
</tr>
</thead>
<tbody><tr>
<td>快请求（10ms）</td>
<td>正常工作</td>
<td>不会触发（并发始终很低）</td>
</tr>
<tr>
<td>慢请求（30s）</td>
<td>30s 内放入 300 个请求，全部同时在跑</td>
<td>只允许 5 个同时执行，第 6 个等待</td>
</tr>
<tr>
<td>资源保护效果</td>
<td>慢请求场景下完全失效</td>
<td>精确保护下游并发能力</td>
</tr>
</tbody></table>
<p><strong>工程实践</strong></p>
<ul>
<li>Java：<code>Semaphore</code>、Resilience4j 的 <code>Bulkhead</code></li>
<li>数据库：连接池本质就是并发控制</li>
<li>Nginx：<code>limit_conn</code> 限制并发连接数</li>
<li>Hystrix/Sentinel：线程池隔离（每个下游依赖独立的并发上限）</li>
</ul>
<p><strong>Bulkhead（舱壁隔离）模式</strong>：把不同依赖的并发限制隔离开，A 服务的慢查询把自己的并发额度用完，不会影响 B 服务的调用。</p>
<hr>
<h3>3.5 配额控制——控制&quot;总共多少次&quot;</h3>
<h4>固定窗口长周期 / 滚动配额</h4>
<p><strong>核心原理</strong></p>
<p>配额控制在技术实现上往往就是一个大窗口的固定窗口计数器——窗口大小从分钟级变成天/月级。但它的设计意图完全不同：速率控制保护系统不被打垮，配额控制实现商业规则。</p>
<p><strong>典型实现</strong></p>
<pre><code class="language-python">def check_quota(user_id, tier):
    quotas = {
        &quot;free&quot;: {&quot;daily&quot;: 100, &quot;monthly&quot;: 1000},
        &quot;pro&quot;:  {&quot;daily&quot;: 10000, &quot;monthly&quot;: 100000},
    }

    daily_key = f&quot;quota:daily:{user_id}:{today()}&quot;
    monthly_key = f&quot;quota:monthly:{user_id}:{this_month()}&quot;

    daily_count = store.get(daily_key, 0)
    monthly_count = store.get(monthly_key, 0)

    limits = quotas[tier]
    if daily_count &gt;= limits[&quot;daily&quot;] or monthly_count &gt;= limits[&quot;monthly&quot;]:
        return False, remaining(limits, daily_count, monthly_count)

    store.increment(daily_key)
    store.increment(monthly_key)
    return True, remaining(limits, daily_count + 1, monthly_count + 1)
</code></pre>
<p><strong>工程要点</strong></p>
<ul>
<li>配额通常需要返回剩余量（<code>X-RateLimit-Remaining</code> header），方便调用方规划使用</li>
<li>长周期配额的窗口边界（如月初重置）是产品决策，不是技术决策</li>
<li>配额超限的拒绝策略通常比速率限制更&quot;温和&quot;——返回明确的额度信息和升级引导，而非简单的 429</li>
</ul>
<hr>
<h3>算法对比总表</h3>
<table>
<thead>
<tr>
<th>算法</th>
<th>控制模型</th>
<th>核心特征</th>
<th>突发处理</th>
<th>存储开销</th>
<th>推荐场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>固定窗口</strong></td>
<td>到达速率</td>
<td>简单计数</td>
<td>边界 2 倍峰值</td>
<td>O(1)</td>
<td>快速实现、精度要求低</td>
</tr>
<tr>
<td><strong>滑动窗口计数器</strong></td>
<td>到达速率</td>
<td>双窗口加权</td>
<td>平滑</td>
<td>O(1)</td>
<td>精度要求高、阈值紧</td>
</tr>
<tr>
<td><strong>滑动窗口日志</strong></td>
<td>到达速率</td>
<td>精确时间戳</td>
<td>完美</td>
<td>O(N) 请求数</td>
<td>低 QPS + 高精度</td>
</tr>
<tr>
<td><strong>令牌桶</strong></td>
<td>突发 + 速率</td>
<td>积蓄+释放</td>
<td>允许有限突发</td>
<td>O(1)</td>
<td>API 限流（首选）</td>
</tr>
<tr>
<td><strong>GCRA</strong></td>
<td>突发 + 速率</td>
<td>单时间戳</td>
<td>允许有限突发</td>
<td>O(1)</td>
<td>分布式 API 限流</td>
</tr>
<tr>
<td><strong>漏桶</strong></td>
<td>执行节奏</td>
<td>恒定输出</td>
<td>不允许突发</td>
<td>O(1)</td>
<td>流量整形、硬件接口</td>
</tr>
<tr>
<td><strong>信号量</strong></td>
<td>并发占用</td>
<td>进出计数</td>
<td>不涉及</td>
<td>O(1)</td>
<td>慢请求、连接池</td>
</tr>
<tr>
<td><strong>固定窗口长周期</strong></td>
<td>长期配额</td>
<td>累计统计</td>
<td>不涉及</td>
<td>O(1)</td>
<td>商业配额、计费</td>
</tr>
</tbody></table>
<p><strong>选择策略</strong>：先确定你的控制模型（速率/并发/配额/节奏），再在对应的算法家族中选择。如果没有特殊需求，令牌桶是互联网业务的默认选择。</p>
<hr>
<h2>四、从单机到分布式：最关键的认知跃迁</h2>
<h3>单机限流为什么在集群中失效</h3>
<p>一个团队用 Guava RateLimiter 限制短信 API 调用为 400 QPS，本地测试完美。代码部署到 4 个节点后，4 个节点各自以 400 QPS 发送，服务商实际承受 1600 QPS，接口再次崩溃。</p>
<p><strong>根因：单机限流只能控制单个进程的流量，对其他节点一无所知。</strong></p>
<p>直觉的修复是均分配额：4 个节点各分 100 QPS。但这引入新问题：</p>
<pre><code>理想中：
  节点A: 100 QPS → 25%
  节点B: 100 QPS → 25%
  节点C: 100 QPS → 25%
  节点D: 100 QPS → 25%

现实中（负载不均）：
  节点A: 240 QPS → 只放行 100，拒绝 140 ✗
  节点B: 120 QPS → 只放行 100，拒绝  20 ✗
  节点C:  30 QPS → 只用了 30，浪费  70
  节点D:  10 QPS → 只用了 10，浪费  90

  总放行：240 QPS（理论可放 400，实际只放了 240）
  → 系统实际吞吐远低于理论上限
</code></pre>
<p>动态调整配额（根据节点负载实时重新分配）？复杂度爆炸——你需要协调机制感知节点上下线、收集实时负载、计算下发配额，这本身就是一个分布式系统问题。</p>
<p><strong>标准答案：将限流状态提升到共享的集中存储中。</strong></p>
<h3>分布式限流的核心原则</h3>
<blockquote>
<p><strong>限流的粒度决定了它的准确性。</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th>保护对象</th>
<th>限流粒度</th>
<th>方案</th>
</tr>
</thead>
<tbody><tr>
<td>本机 CPU/内存</td>
<td>进程级</td>
<td>Guava RateLimiter、Sentinel</td>
</tr>
<tr>
<td>外部 API 配额</td>
<td>系统级（全集群）</td>
<td>Redis 分布式计数器</td>
</tr>
<tr>
<td>业务规则（如用户发送频率）</td>
<td>用户级</td>
<td>Redis + 用户维度 key</td>
</tr>
</tbody></table>
<h3>Redis 分布式限流：为什么是标准答案</h3>
<p>Redis 之所以成为分布式限流的事实标准，是因为它的特性精确匹配了限流的每一个核心需求：</p>
<table>
<thead>
<tr>
<th>限流需求</th>
<th>Redis 特性</th>
<th>为什么匹配</th>
</tr>
</thead>
<tbody><tr>
<td>原子性：&quot;读取-判断-递增&quot;必须原子</td>
<td>INCR 原子命令 + Lua 脚本</td>
<td>单线程模型，天然无并发冲突</td>
</tr>
<tr>
<td>极致性能：每个请求都要过限流</td>
<td>内存操作，亚毫秒级延迟</td>
<td>不成为业务瓶颈</td>
</tr>
<tr>
<td>共享状态：所有节点看到同一个计数器</td>
<td>独立服务，集群可访问</td>
<td>分布式协调问题消失</td>
</tr>
<tr>
<td>自动过期：时间窗口结束后计数器清零</td>
<td>Key 级别 TTL</td>
<td>无需额外清理逻辑</td>
</tr>
</tbody></table>
<h3>工程实践：基于 Redis + Lua 的固定窗口限流</h3>
<p><strong>为什么必须用 Lua 脚本？</strong></p>
<p>不用 Lua 的伪代码：</p>
<pre><code>count = redis.GET(key)          -- 步骤1：读取
if count &lt; threshold:           -- 步骤2：判断
    redis.INCR(key)             -- 步骤3：递增
    return ALLOW
else:
    return REJECT
</code></pre>
<p>并发问题：两个节点同时读到 count=399（阈值 400），都判断&quot;未超限&quot;，都执行 INCR。最终 count=401，但两个请求都通过了。高并发下，这种竞态条件被急剧放大，限流形同虚设。</p>
<p><strong>Lua 脚本实现（原子操作）</strong></p>
<pre><code class="language-lua">-- KEYS[1]: 限流 key，如 &quot;rate_limit:sms_api:1609459200&quot;
-- ARGV[1]: 阈值
-- ARGV[2]: 窗口过期时间（秒）

local key = KEYS[1]
local threshold = tonumber(ARGV[1])
local expire_time = tonumber(ARGV[2])

local current = tonumber(redis.call(&#39;GET&#39;, key) or &quot;0&quot;)

if current + 1 &gt; threshold then
    return 0  -- 拒绝
else
    redis.call(&#39;INCR&#39;, key)
    if current == 0 then
        redis.call(&#39;EXPIRE&#39;, key, expire_time)
    end
    return 1  -- 放行
end
</code></pre>
<p><strong>Key 设计规范</strong></p>
<pre><code>格式：rate_limit:{业务标识}:{维度}:{时间窗口}
示例：
  rate_limit:sms_api:global:1609459200       -- 全局短信 API 限流
  rate_limit:login:user:12345:1609459200     -- 用户维度登录限流
  rate_limit:order:tenant:abc:1609459200     -- 租户维度下单限流
</code></pre>
<h3>工程实践：基于 Redis 的滑动窗口限流</h3>
<p>当固定窗口的边界问题不可接受时，可以用 Redis Sorted Set 实现滑动窗口：</p>
<pre><code class="language-lua">-- KEYS[1]: 限流 key
-- ARGV[1]: 阈值
-- ARGV[2]: 窗口大小（毫秒）
-- ARGV[3]: 当前时间戳（毫秒）
-- ARGV[4]: 唯一请求ID

local key = KEYS[1]
local threshold = tonumber(ARGV[1])
local window = tonumber(ARGV[2])
local now = tonumber(ARGV[3])
local request_id = ARGV[4]

-- 移除窗口外的过期记录
redis.call(&#39;ZREMRANGEBYSCORE&#39;, key, 0, now - window)

-- 统计当前窗口内的请求数
local count = redis.call(&#39;ZCARD&#39;, key)

if count &lt; threshold then
    -- 添加当前请求，score 为时间戳
    redis.call(&#39;ZADD&#39;, key, now, request_id)
    redis.call(&#39;PEXPIRE&#39;, key, window)
    return 1  -- 放行
else
    return 0  -- 拒绝
end
</code></pre>
<p><strong>两种 Redis 方案的对比</strong></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>固定窗口（String + INCR）</th>
<th>滑动窗口（Sorted Set）</th>
</tr>
</thead>
<tbody><tr>
<td>存储开销</td>
<td>O(1)，一个 key 一个计数器</td>
<td>O(N)，N 为窗口内请求数</td>
</tr>
<tr>
<td>时间复杂度</td>
<td>O(1)</td>
<td>O(log N)</td>
</tr>
<tr>
<td>精度</td>
<td>边界可能 2 倍峰值</td>
<td>精确</td>
</tr>
<tr>
<td>适用</td>
<td>大部分场景</td>
<td>阈值紧、精度要求高</td>
</tr>
</tbody></table>
<p><strong>工程建议</strong>：优先用固定窗口方案。只有当阈值非常接近系统极限（余量 &lt; 20%）时，才需要滑动窗口的精度。</p>
<h3>关于时钟同步</h3>
<p>分布式系统中，各节点用本地时间计算 Redis key 中的时间窗口标识，时钟偏移可能导致不同节点在不同窗口中计数。严格做法是用 Redis 服务端时间 <code>redis.call(&#39;TIME&#39;)</code>。但现代服务器通过 NTP 同步后的时钟偏差通常在毫秒级，对秒级窗口几乎无影响。</p>
<p><strong>工程判断</strong>：对于秒级窗口，使用本地时间戳即可。对于百毫秒级窗口或对精度有极端要求的场景，使用 Redis 服务端时间。</p>
<hr>
<h2>五、多层限流：纵深防御架构</h2>
<p>一个常见误区是试图在某一层解决所有限流问题。良好的限流架构应该是分层的——每一层保护不同的东西，承担不同的职责。</p>
<pre><code>                     请求流入
                        ↓
┌──────────────────────────────────────────┐
│  第一层：接入层（Nginx / CDN）            │  ← 挡住恶意流量和 DDoS
│  基于 IP 的连接数和请求速率限制            │
└──────────────────────────────────────────┘
                        ↓
┌──────────────────────────────────────────┐
│  第二层：API 网关（Gateway）              │  ← 业务感知型限流
│  基于用户/租户/API 维度的差异化限流        │
└──────────────────────────────────────────┘
                        ↓
┌──────────────────────────────────────────┐
│  第三层：业务层                           │  ← 业务规则型限流
│  业务语义的频率控制（发帖/下单/发短信）     │
└──────────────────────────────────────────┘
                        ↓
┌──────────────────────────────────────────┐
│  第四层：数据层                           │  ← 最后一道防线
│  连接池 / 线程池隔离 / 熔断器             │
└──────────────────────────────────────────┘
</code></pre>
<h3>各层详细对比</h3>
<table>
<thead>
<tr>
<th>层级</th>
<th>保护对象</th>
<th>限流维度</th>
<th>典型工具</th>
<th>算法</th>
</tr>
</thead>
<tbody><tr>
<td>接入层</td>
<td>基础设施</td>
<td>IP、连接数</td>
<td>Nginx <code>limit_req</code>/<code>limit_conn</code></td>
<td>漏桶</td>
</tr>
<tr>
<td>API 网关</td>
<td>服务处理能力</td>
<td>用户 ID、API Key、租户</td>
<td>Redis + Lua、Sentinel</td>
<td>令牌桶/滑动窗口</td>
</tr>
<tr>
<td>业务层</td>
<td>业务规则</td>
<td>业务实体（用户行为频率）</td>
<td>Redis + 业务代码</td>
<td>固定窗口</td>
</tr>
<tr>
<td>数据层</td>
<td>存储和依赖</td>
<td>并发连接数</td>
<td>连接池、Hystrix、Resilience4j</td>
<td>信号量/熔断</td>
</tr>
</tbody></table>
<h3>各层工程实践</h3>
<p><strong>接入层：Nginx 配置示例</strong></p>
<pre><code class="language-nginx">http {
    # IP 维度的请求速率限制
    limit_req_zone $binary_remote_addr zone=ip_rate:10m rate=100r/s;

    # IP 维度的并发连接数限制
    limit_conn_zone $binary_remote_addr zone=ip_conn:10m;

    server {
        # API 接口：每 IP 100r/s，突发 50
        location /api/ {
            limit_req zone=ip_rate burst=50 nodelay;
            limit_conn ip_conn 50;
            limit_req_status 429;
        }

        # 登录接口：更严格的限制
        location /api/login {
            limit_req zone=ip_rate burst=5;
            limit_req_status 429;
        }
    }
}
</code></pre>
<p><strong>API 网关层：差异化限流</strong></p>
<pre><code class="language-java">// 不同级别用户的限流配置
public class RateLimitConfig {
    // 免费用户：60 次/分钟
    // 付费用户：600 次/分钟
    // 企业用户：6000 次/分钟

    public int getThreshold(User user) {
        return switch (user.getTier()) {
            case FREE       -&gt; 60;
            case PREMIUM    -&gt; 600;
            case ENTERPRISE -&gt; 6000;
        };
    }

    // 不同 API 端点的限流配置
    // 重查询接口：50 QPS
    // 轻量读接口：5000 QPS
    // 写操作接口：200 QPS

    public int getThreshold(String endpoint) {
        return switch (endpoint) {
            case &quot;/api/report/generate&quot; -&gt; 50;    // 计算密集
            case &quot;/api/user/info&quot;       -&gt; 5000;  // 轻量读
            case &quot;/api/order/create&quot;    -&gt; 200;   // 写操作
            default                     -&gt; 1000;
        };
    }
}
</code></pre>
<p><strong>业务层：业务规则型限流</strong></p>
<pre><code class="language-java">// 业务限流的阈值来自产品需求，不是压测
public class BusinessRateLimiter {

    // 防骚扰：每用户每分钟最多 5 条短信
    public boolean allowSendSms(long userId) {
        String key = &quot;biz:sms:&quot; + userId + &quot;:&quot; + currentMinute();
        return redisRateLimiter.tryAcquire(key, 5, 60);
    }

    // 反垃圾：新账号 24 小时内最多发 10 条帖子
    public boolean allowPost(long userId, boolean isNewAccount) {
        if (!isNewAccount) return true;
        String key = &quot;biz:post:new:&quot; + userId + &quot;:&quot; + today();
        return redisRateLimiter.tryAcquire(key, 10, 86400);
    }

    // 运营策略：商家每天最多创建 100 个促销活动
    public boolean allowCreatePromotion(long merchantId) {
        String key = &quot;biz:promo:&quot; + merchantId + &quot;:&quot; + today();
        return redisRateLimiter.tryAcquire(key, 100, 86400);
    }
}
</code></pre>
<p><strong>数据层：隐式限流</strong></p>
<p>数据层的&quot;限流&quot;通常不以限流的名义出现，但本质上发挥着同样的作用：</p>
<ul>
<li><strong>连接池</strong>：连接池满时新请求排队等待 → 并发度上限</li>
<li><strong>线程池隔离</strong>：为每个下游依赖分配独立线程池 → 故障隔离</li>
<li><strong>熔断器</strong>：错误率超阈值时直接停止调用 → 自适应限流</li>
</ul>
<p><strong>每一层保护不同的东西。</strong> 接入层保护基础设施不被滥用流量冲垮；API 网关保护服务处理能力不被超载；业务层保护业务规则不被绕过；数据层保护最脆弱的存储和依赖。</p>
<hr>
<h2>六、限流的工程闭环</h2>
<p>限流架构设计完了，还差两个关键环节：阈值从哪来？被拒绝的请求去哪了？这两个问题不解决，限流就是半成品。</p>
<h3>6.1 阈值从哪来</h3>
<p>所有限流工程中最难的问题不是技术实现，而是：<strong>阈值应该设多少？</strong></p>
<p><strong>四步确定阈值</strong></p>
<table>
<thead>
<tr>
<th>步骤</th>
<th>方法</th>
<th>产出</th>
</tr>
</thead>
<tbody><tr>
<td><strong>1. 压测基线</strong></td>
<td>逐步加压，观察 P99 延迟和错误率的拐点</td>
<td>系统实际容量边界</td>
</tr>
<tr>
<td><strong>2. 安全系数</strong></td>
<td>阈值 = 容量边界 × 70%~80%</td>
<td>留出余量应对突发波动</td>
</tr>
<tr>
<td><strong>3. 持续监控</strong></td>
<td>监控 P99、错误率、CPU、内存</td>
<td>发现容量变化及时调整</td>
</tr>
<tr>
<td><strong>4. 渐进调整</strong></td>
<td>从保守值开始，观察线上表现后逐步放宽</td>
<td>避免上线即翻车</td>
</tr>
</tbody></table>
<p><strong>自适应限流</strong></p>
<p>更高级的形态是基于实时指标的自动限流。以 Sentinel 为例：</p>
<pre><code class="language-java">// 基于系统负载的自适应限流
SystemRule rule = new SystemRule();
rule.setHighestCpuUsage(0.8);    // CPU &gt; 80% 时触发限流
rule.setHighestSystemLoad(2.5);   // System Load &gt; 2.5 时触发限流
rule.setAvgRt(200);               // 平均 RT &gt; 200ms 时触发限流

// 优点：省去人为猜测阈值
// 风险：正常流量波动可能触发误限，需仔细调试灵敏度
</code></pre>
<p><strong>阈值是业务决策</strong></p>
<blockquote>
<p><strong>限流阈值不是纯技术参数，而是一个业务决策。</strong></p>
</blockquote>
<p>它编码的是&quot;我们愿意承受多大负载，以及拒绝超额流量的业务成本是什么&quot;。</p>
<ul>
<li>面向消费者的核心交易链路：拒绝一个请求 = 损失一笔订单 → 阈值宜宽</li>
<li>内部数据分析任务：晚执行几分钟无损失 → 阈值可严</li>
<li>计算密集的报表接口：单个请求消耗大量资源 → 阈值必须严</li>
</ul>
<p>阈值设定必须综合技术容量和业务容忍度，需要工程团队和产品团队协同决策。</p>
<h3>6.2 被拒绝的请求去哪了</h3>
<p>大多数限流讨论都集中在&quot;如何拒绝&quot;，很少有人思考&quot;拒绝之后怎么办&quot;。而在真实业务中，后者往往更重要。</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>做法</th>
<th>适用场景</th>
<th>风险</th>
</tr>
</thead>
<tbody><tr>
<td><strong>直接拒绝</strong></td>
<td>返回 429 + Retry-After</td>
<td>开放 API、程序化调用方</td>
<td>用户体验差</td>
</tr>
<tr>
<td><strong>排队等待</strong></td>
<td>写入 MQ，消费者限速消费</td>
<td>异步操作（短信、邮件、报表）</td>
<td>队列积压导致延迟不可控</td>
</tr>
<tr>
<td><strong>降级响应</strong></td>
<td>返回缓存/兜底数据</td>
<td>推荐、搜索、详情页非核心模块</td>
<td>数据时效性降低</td>
</tr>
<tr>
<td><strong>引流分担</strong></td>
<td>导向备用路径（CDN/只读副本）</td>
<td>读多写少的场景</td>
<td>需要备用链路的维护成本</td>
</tr>
</tbody></table>
<p><strong>关键原则：限流策略和拒绝策略必须配套设计。</strong></p>
<p>回到短信发送事故：被限流的短信不能直接丢弃，必须进入重试队列。秒杀请求被限流？直接告知&quot;已售罄&quot;比让用户苦等体验更好。商品详情页被限流？返回缓存数据即可，用户感知的是&quot;数据没那么新&quot;而不是&quot;服务挂了&quot;。</p>
<p>只设计了限流而没考虑拒绝后的处理，就像只安装了闸门却没修泄洪渠——水是拦住了，但迟早会溃坝。</p>
<hr>
<h2>七、金融场景：限流 ≠ 正确性</h2>
<p>在金融、支付等对正确性有极高要求的领域，限流只是防御体系的一环。很多团队犯的错误是：觉得&quot;加了限流就安全了&quot;。现实是，限流解决的是<strong>流量问题</strong>，不是<strong>正确性问题</strong>。</p>
<h3>三层防护：限流 + 并发控制 + 幂等</h3>
<p>考虑一个支付场景：用户点了两次&quot;付款&quot;按钮。</p>
<table>
<thead>
<tr>
<th>防护层</th>
<th>解决的问题</th>
<th>如果只有这一层</th>
</tr>
</thead>
<tbody><tr>
<td><strong>限流</strong></td>
<td>防止支付接口被高频调用打垮</td>
<td>两次点击间隔 100ms，速率限制 10/s → 都放行，扣两次款</td>
</tr>
<tr>
<td><strong>并发控制</strong></td>
<td>同一笔订单同一时刻只允许一个支付请求在处理</td>
<td>第二次被排队/拒绝，但如果第一次失败后重试呢？</td>
</tr>
<tr>
<td><strong>幂等</strong></td>
<td>同一笔支付操作无论执行几次，结果只生效一次</td>
<td>无论重试多少次、并发多少个，最终只扣一次款</td>
</tr>
</tbody></table>
<p><strong>三层必须配合使用：</strong></p>
<ul>
<li>限流是<strong>外围护栏</strong>——挡住异常流量，保护系统不被打垮</li>
<li>并发控制是<strong>执行调度</strong>——同一资源同一时刻只有一个操作在执行</li>
<li>幂等是<strong>正确性保障</strong>——即使前两层被突破，最终结果仍然正确</li>
</ul>
<h3>伪代码：PaymentProtection 组合示例</h3>
<pre><code class="language-python">class PaymentProtection:
    def __init__(self):
        self.rate_limiter = TokenBucket(rate=100, capacity=200)    # 限流
        self.locks = DistributedLockManager()                       # 并发控制
        self.idempotency = IdempotencyStore()                       # 幂等

    def process_payment(self, order_id, idempotency_key, amount):
        # 第一层：限流 —— 保护系统不被打垮
        if not self.rate_limiter.allow():
            return Error(&quot;RATE_LIMITED&quot;, &quot;系统繁忙，请稍后重试&quot;)

        # 第二层：幂等检查 —— 如果这个操作已经成功过，直接返回之前的结果
        existing = self.idempotency.get(idempotency_key)
        if existing:
            return existing  # 重复请求，返回之前的结果

        # 第三层：并发控制 —— 同一订单同一时刻只处理一个支付请求
        lock = self.locks.acquire(f&quot;payment:{order_id}&quot;, timeout=10)
        if not lock:
            return Error(&quot;CONCURRENT&quot;, &quot;订单正在处理中&quot;)

        try:
            # 再次检查幂等（拿到锁之后的 double-check）
            existing = self.idempotency.get(idempotency_key)
            if existing:
                return existing

            # 执行实际支付
            result = do_payment(order_id, amount)

            # 记录幂等结果
            self.idempotency.store(idempotency_key, result)
            return result
        finally:
            lock.release()
</code></pre>
<p><strong>核心认知</strong>：限流是流量层面的保护，幂等才是业务正确性的最后防线。在金融场景中，这三层缺一不可。</p>
<hr>
<h2>八、总结：限流是一种系统思维</h2>
<p>限流从表面看是算法选择题，但真正落地到生产环境时，它是一个系统设计问题：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>核心问题</th>
</tr>
</thead>
<tbody><tr>
<td><strong>控制对象</strong></td>
<td>你在控制什么？速率、并发、配额还是节奏？控制模型选错，算法再精妙也解决不了问题</td>
</tr>
<tr>
<td><strong>容量</strong></td>
<td>系统到底能承受多少？需要压测和监控，不是拍脑袋</td>
</tr>
<tr>
<td><strong>优先级</strong></td>
<td>必须拒绝时，拒绝谁？VIP vs 普通、核心 vs 边缘、写 vs 读</td>
</tr>
<tr>
<td><strong>失败模式</strong></td>
<td>限流触发后怎么办？报错、排队、降级还是引流</td>
</tr>
<tr>
<td><strong>权衡</strong></td>
<td>平滑性 vs 响应性、精确性 vs 性能、简单性 vs 灵活性</td>
</tr>
</tbody></table>
<p>最好的限流系统是你感觉不到它存在的系统。流量平稳时安静旁观，突增时默默吸收合理突发，真正超限时优雅拒绝——确保已接受的请求仍能正常处理。它不是一堵墙，而是一个阀门：精确控制流量进出，让系统在极端压力下保持可控、可预测、可依赖。</p>
<p><strong>限流的本质，是对系统能力边界的敬畏，以及在边界之内追求最大价值的工程智慧。</strong></p>
18:T6f06,<h2>一、为什么需要索引：从磁盘 I/O 说起</h2>
<p>&quot;给这个查询加个索引就好了。&quot;——这句话说起来简单，但如果不理解索引为什么有效，就无法判断什么时候该加、怎么加、以及加了为什么还是慢。</p>
<p>答案藏在磁盘里。</p>
<h3>内存与磁盘：10 万倍的速度鸿沟</h3>
<p>数据库的数据最终存储在磁盘上。一次磁盘 I/O 的真实耗时：</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>耗时</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>寻道（Seek）</td>
<td>~5ms</td>
<td>磁头移动到目标磁道</td>
</tr>
<tr>
<td>旋转延迟（Rotation）</td>
<td>~4.17ms</td>
<td>7200 RPM 磁盘，平均半圈</td>
</tr>
<tr>
<td>数据传输（Transfer）</td>
<td>~0.1ms</td>
<td>读取数据到内存</td>
</tr>
<tr>
<td><strong>总计</strong></td>
<td><strong>~9ms</strong></td>
<td>一次随机 I/O 的代价</td>
</tr>
</tbody></table>
<p>9 毫秒看起来不多，但换算到 CPU 视角：一台 500-MIPS 的机器每秒执行 5 亿条指令，9ms 就是 <strong>450 万条指令</strong> 的时间。在 CPU 看来，等一次磁盘 I/O 就像等了一个世纪。</p>
<p>如果一张百万行的表没有索引，查一条记录需要全表扫描——假设每行读一次磁盘，那就是百万次 I/O。这就是为什么没有索引的查询会慢得不可接受。</p>
<h3>操作系统的预读优化</h3>
<p>操作系统做了一个关键优化：<strong>页（Page）预读</strong>。当你从磁盘读取一个字节时，OS 会把这个字节所在的整个页（通常 4KB 或 8KB）一次性加载到内存。读 1 字节和读 4KB 的 I/O 成本是一样的——都是 1 次磁盘 I/O。</p>
<p>这意味着：<strong>如果一种数据结构能保证每次查询只需要少量的 I/O，并且每次 I/O 都能充分利用页的空间，那它就是高效的索引结构。</strong></p>
<p>B+Tree 正是为此而设计的。</p>
<hr>
<h2>二、B+Tree：为磁盘而生的数据结构</h2>
<p>为什么不用二叉树、红黑树这些内存中高效的数据结构？</p>
<p>关键在于<strong>树的高度</strong>。二叉搜索树的高度是 log₂N，100 万条数据需要 20 层。每一层都意味着一次磁盘 I/O——20 次随机 I/O，每次 9ms，一个简单查询就要 180ms。</p>
<p>B+Tree 的解决思路：<strong>增大每个节点的扇出（fanout），压低树的高度。</strong></p>
<h3>B+Tree 的三个关键设计决策</h3>
<pre><code>             [17 | 35]              ← 非叶子节点：只存键值，不存数据
            /    |    \
     [8|12]   [26|30]   [60|75]     ← 非叶子节点
      / | \    / | \     / | \
  [3,5][9,10][13,15][28,29][36][60][75,79][90,99]  ← 叶子节点：存储实际数据
   ↔     ↔      ↔      ↔     ↔    ↔      ↔       ← 叶子节点横向链表
</code></pre>
<p><strong>决策一：非叶子节点只存键值不存数据。</strong> 这样一个磁盘页（16KB，InnoDB 默认页大小）能放下更多的键值，单节点的扇出可以达到 <strong>1200+</strong>（每个键值 8 字节 + 指针 6 字节，16KB / 14B ≈ 1170）。</p>
<p><strong>决策二：数据全部下沉到叶子节点。</strong> 不管查什么数据，走过的路径长度是一样的。查询性能稳定可预测。</p>
<p><strong>决策三：叶子节点之间用双向链表连接。</strong> 范围查询（如 <code>WHERE id BETWEEN 100 AND 200</code>）只需定位到起点，然后顺着链表遍历，不用回到树根。</p>
<h3>真实数据：22.1GB 表的 B+Tree 长什么样</h3>
<p>以一张 22.1GB 的 InnoDB 表为例：</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>数据</th>
</tr>
</thead>
<tbody><tr>
<td>叶子节点容纳量</td>
<td>~468 行/页</td>
</tr>
<tr>
<td>非叶子节点扇出</td>
<td>~1200 路</td>
</tr>
<tr>
<td>B+Tree 高度</td>
<td><strong>3 层</strong></td>
</tr>
<tr>
<td>非叶子节点总内存</td>
<td><strong>&lt; 18.8MB</strong></td>
</tr>
<tr>
<td>高度 4 层时可容纳</td>
<td>25.9TB</td>
</tr>
</tbody></table>
<p>3 层 B+Tree 意味着：查找任意一条记录只需 <strong>3 次磁盘 I/O</strong>。而非叶子节点只占 18.8MB，完全可以常驻内存——实际上只有最后一次叶子节点的读取是真正的磁盘 I/O。</p>
<p>这就是索引高效的根本原因：<strong>将百万次随机 I/O 压缩为 1~3 次。</strong></p>
<p>高度公式：<code>h = log(m+1)N</code>，其中 m 是每个节点的扇出数，N 是总记录数。扇出越大，高度越低。这也解释了为什么主键用 int（4 字节）比 uuid（36 字节）好——键越短，一个页能放下越多键，扇出越大，树越矮。</p>
<blockquote>
<p>关于 B+Tree、B-Tree、LSM-Tree 等存储引擎数据结构的理论细节，参见<a href="/blog/engineering/data-structure/%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E6%A0%B8%E5%BF%83%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%EF%BC%9Ab-tree%E5%AE%B6%E6%97%8F%E4%B8%8Elsm-tree%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%9D%83%E8%A1%A1">《存储引擎核心数据结构：B-Tree 家族与 LSM-Tree 的设计权衡》</a>。本文聚焦 MySQL 层面的索引使用和优化。</p>
</blockquote>
<hr>
<h2>三、InnoDB 索引的存储结构</h2>
<p>理解了 B+Tree 的通用原理后，还需要理解 InnoDB 对 B+Tree 的具体实现方式——它直接决定了&quot;回表&quot;的代价和覆盖索引的价值。</p>
<h3>聚簇索引 vs 非聚簇索引</h3>
<p>InnoDB 和 MyISAM 在索引的组织方式上有根本区别：</p>
<pre><code>MyISAM（非聚簇）：
  索引文件(.MYI)          数据文件(.MYD)
  ┌──────────┐           ┌──────────┐
  │ key → 地址 │  ──→     │  行数据    │
  └──────────┘           └──────────┘
  索引和数据分离，索引叶子节点存储数据文件中的物理地址

InnoDB（聚簇）：
  主键索引(.ibd)
  ┌─────────────────┐
  │ primary key → 行数据 │    ← 主键索引的叶子节点就是数据本身
  └─────────────────┘

  二级索引(.ibd)
  ┌──────────────────────┐
  │ index key → primary key │  ← 二级索引的叶子节点存主键值
  └──────────────────────┘
</code></pre>
<p><strong>InnoDB 的聚簇索引</strong>：主键和数据存在一起。主键索引的叶子节点就是完整的行数据。这意味着按主键查找只需一棵 B+Tree。</p>
<p><strong>InnoDB 的二级索引</strong>：叶子节点存储的不是数据的物理地址，而是主键值。通过二级索引查找时，先在二级索引树中找到主键值，再到主键索引树中找到完整数据——这个过程叫<strong>回表</strong>。</p>
<h3>回表的代价</h3>
<p>一次二级索引查询 = <strong>两棵 B+Tree 的查找</strong>。</p>
<pre><code>SELECT * FROM users WHERE name = &#39;张三&#39;;
-- 假设 name 上有索引

步骤 1：在 name 索引树中查找 &#39;张三&#39; → 得到主键 id = 42
步骤 2：在主键索引树中查找 id = 42 → 得到完整行数据（回表）
</code></pre>
<p>如果查询返回大量行，每一行都要回表一次，性能会急剧下降。</p>
<h3>覆盖索引：避免回表</h3>
<p>如果索引中已经包含了查询需要的所有列，就不需要回表。这就是<strong>覆盖索引（Covering Index）</strong>。</p>
<pre><code class="language-sql">-- 索引：INDEX idx_name_age (name, age)

-- 需要回表：SELECT * FROM users WHERE name = &#39;张三&#39;
-- 索引里没有 email、address 等列，必须回表取完整数据

-- 覆盖索引：SELECT name, age FROM users WHERE name = &#39;张三&#39;
-- 索引里就有 name 和 age，直接返回，不用回表
-- EXPLAIN 的 Extra 列会显示 &quot;Using index&quot;
</code></pre>
<h3>为什么 InnoDB 必须有主键</h3>
<p>InnoDB 的数据组织方式决定了它必须依赖一个聚簇索引。如果你没有显式定义主键：</p>
<ol>
<li>InnoDB 会选择第一个<strong>非空唯一索引</strong>作为聚簇索引</li>
<li>如果也没有，InnoDB 会自动生成一个 6 字节的隐藏 RowID</li>
</ol>
<p>自动生成的 RowID 用户不可见、不可查询，浪费了聚簇索引的优势。<strong>建议总是显式定义自增整型主键</strong>——它既短（扇出大）又有序（插入不会导致页分裂）。</p>
<hr>
<h2>四、索引的使用规则</h2>
<p>建了索引不代表查询一定会用。MySQL 优化器在决定是否使用索引时有一套严格的规则。搞清楚这些规则，才能建出真正有效的索引。</p>
<h3>4.1 最左前缀匹配（最重要的规则）</h3>
<p>复合索引 <code>(a, b, c, d)</code> 的匹配遵循<strong>从左到右</strong>的顺序，遇到范围查询（<code>&gt;</code>, <code>&lt;</code>, <code>BETWEEN</code>, <code>LIKE</code>）就停止匹配。</p>
<pre><code class="language-sql">-- 索引：INDEX idx (a, b, c, d)

WHERE a = 1 AND b = 2 AND c &gt; 3 AND d = 4
-- 命中：a ✓, b ✓, c ✓（范围）, d ✗（c 之后停止匹配）
-- 实际使用了 a, b, c 三列

WHERE a = 1 AND b = 2 AND d = 4
-- 命中：a ✓, b ✓, c 跳过（不在 WHERE 中）, d ✗
-- 实际使用了 a, b 两列

WHERE b = 2 AND c = 3
-- 命中：a 缺失 → 整个索引不可用 ✗
-- 没有最左列 a，无法使用这个索引
</code></pre>
<p><strong>优化技巧</strong>：如果某列在 WHERE 中是范围条件，把它放到复合索引的最后面。</p>
<pre><code class="language-sql">-- 查询：WHERE a = 1 AND b = 2 AND c &gt; 3 AND d = 4

-- 差索引：INDEX (a, b, c, d) → 只用 a, b, c
-- 好索引：INDEX (a, b, d, c) → 用到 a, b, d, c 四列全命中
</code></pre>
<h3>4.2 选择性（Selectivity）</h3>
<p>选择性衡量一个列能过滤掉多少数据：</p>
<pre><code>选择性 = COUNT(DISTINCT col) / COUNT(*)
</code></pre>
<table>
<thead>
<tr>
<th>选择性</th>
<th>含义</th>
<th>建索引价值</th>
</tr>
</thead>
<tbody><tr>
<td>&gt; 0.1</td>
<td>每 10 行中有 1 个不同值</td>
<td>高，适合建索引</td>
</tr>
<tr>
<td>0.01 ~ 0.1</td>
<td>重复较多</td>
<td>取决于实际数据分布</td>
</tr>
<tr>
<td>&lt; 0.01</td>
<td>高度重复（如 status: 0/1/2）</td>
<td>通常不适合，但有例外</td>
</tr>
</tbody></table>
<p><strong>反直觉案例：低选择性也可能有效。</strong> 如果一个 status 字段只有 3 个值（-1, 0, 1），但业务上 99.9% 的记录是 status=1，你要查的恰好是 status=0 的那一小批——索引的效果取决于你要查的值的分布，而不是列整体的选择性。</p>
<blockquote>
<p><strong>关键洞察</strong>：选择性公式给出的是统计平均，但实际查询命中的是具体值的分布。对于数据分布极度不均匀的列，需要结合业务场景判断。</p>
</blockquote>
<h3>4.3 五条工程戒律</h3>
<p><strong>① 不要在索引列上做计算或函数调用</strong></p>
<pre><code class="language-sql">-- ✗ 不走索引：函数包裹了索引列
WHERE FROM_UNIXTIME(create_time) = &#39;2024-05-29&#39;
WHERE YEAR(created_date) = 2024

-- ✓ 走索引：把计算移到值一侧
WHERE create_time = UNIX_TIMESTAMP(&#39;2024-05-29&#39;)
WHERE created_date &gt;= &#39;2024-01-01&#39; AND created_date &lt; &#39;2025-01-01&#39;
</code></pre>
<p>原因：对索引列施加函数后，B+Tree 无法利用键值的有序性。</p>
<p><strong>② = 和 IN 的顺序不影响索引使用</strong></p>
<pre><code class="language-sql">-- 以下两种写法等价，优化器会自动重排
WHERE a = 1 AND b = 2 AND c = 3
WHERE c = 3 AND a = 1 AND b = 2
</code></pre>
<p><strong>③ 扩展已有索引，而非新建</strong></p>
<pre><code class="language-sql">-- 已有索引：INDEX idx_a (a)
-- 现在需要查 WHERE a = ? AND b = ?

-- ✗ 新建：INDEX idx_ab (a, b)  → 现在有两个索引，浪费空间且写入变慢
-- ✓ 扩展：把 idx_a 改为 INDEX idx_ab (a, b)  → idx_ab 同时覆盖单列查询
</code></pre>
<p><strong>④ 尽量用覆盖索引减少回表</strong></p>
<p>如果查询只需要少量列，把这些列都放进索引里，避免回表。</p>
<p><strong>⑤ 复合索引中选择性高的列放前面</strong></p>
<p>选择性高的列放前面，能更快地缩小候选集。但这条规则要让位于最左前缀匹配——如果查询条件固定，优先保证查询能命中索引。</p>
<hr>
<h2>五、EXPLAIN：读懂优化器的决策</h2>
<p>索引建好了，查询到底用没用、怎么用？EXPLAIN 是唯一的答案。</p>
<pre><code class="language-sql">EXPLAIN SELECT * FROM users WHERE name = &#39;张三&#39; AND age &gt; 20;
</code></pre>
<h3>核心字段解读</h3>
<table>
<thead>
<tr>
<th>字段</th>
<th>含义</th>
<th>关注点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>type</strong></td>
<td>访问类型</td>
<td>从好到差：system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL</td>
</tr>
<tr>
<td><strong>key</strong></td>
<td>实际使用的索引</td>
<td>NULL 表示没走索引</td>
</tr>
<tr>
<td><strong>rows</strong></td>
<td>预估扫描行数</td>
<td><strong>最关键指标</strong>，越接近结果行数越好</td>
</tr>
<tr>
<td><strong>Extra</strong></td>
<td>附加信息</td>
<td>关注 Using index / Using filesort / Using temporary</td>
</tr>
</tbody></table>
<h3>type 等级详解</h3>
<table>
<thead>
<tr>
<th>type</th>
<th>含义</th>
<th>触发条件</th>
<th>性能</th>
</tr>
</thead>
<tbody><tr>
<td>const</td>
<td>通过主键或唯一索引定位一行</td>
<td><code>WHERE id = 1</code></td>
<td>极快</td>
</tr>
<tr>
<td>eq_ref</td>
<td>JOIN 时被驱动表主键等值匹配</td>
<td><code>JOIN ON a.id = b.id</code></td>
<td>极快</td>
</tr>
<tr>
<td>ref</td>
<td>非唯一索引等值匹配</td>
<td><code>WHERE name = &#39;张三&#39;</code></td>
<td>快</td>
</tr>
<tr>
<td>range</td>
<td>索引范围扫描</td>
<td><code>WHERE id &gt; 100</code> / <code>WHERE id IN (1,2,3)</code></td>
<td>较快</td>
</tr>
<tr>
<td>index</td>
<td>全索引扫描</td>
<td>覆盖索引但无 WHERE 条件</td>
<td>一般</td>
</tr>
<tr>
<td>ALL</td>
<td>全表扫描</td>
<td>无可用索引</td>
<td>最慢</td>
</tr>
</tbody></table>
<h3>Extra 中的关键信号</h3>
<table>
<thead>
<tr>
<th>Extra</th>
<th>含义</th>
<th>是否需要优化</th>
</tr>
</thead>
<tbody><tr>
<td>Using index</td>
<td>覆盖索引，无需回表</td>
<td>好，不用动</td>
</tr>
<tr>
<td>Using where</td>
<td>在存储引擎返回数据后由 Server 层过滤</td>
<td>看情况</td>
</tr>
<tr>
<td>Using filesort</td>
<td>无法利用索引排序，需额外排序</td>
<td>通常需要优化</td>
</tr>
<tr>
<td>Using temporary</td>
<td>需要创建临时表（常见于 GROUP BY）</td>
<td>需要优化</td>
</tr>
</tbody></table>
<p><strong>实战口诀</strong>：</p>
<ul>
<li>看到 <code>ALL</code> → 考虑加索引</li>
<li>看到 <code>Using filesort</code> → 检查 ORDER BY 是否能走索引</li>
<li>看到 <code>Using temporary</code> → 检查 GROUP BY 是否能走索引</li>
<li><code>rows</code> 远大于实际结果行数 → 索引选择性不够或索引列不对</li>
</ul>
<hr>
<h2>六、ORDER BY 与 GROUP BY 的索引优化</h2>
<p>排序和分组是慢查询的常见元凶。MySQL 能利用索引的有序性避免额外排序（filesort），但条件很严格。</p>
<h3>6.1 ORDER BY 能走索引的条件</h3>
<pre><code class="language-sql">-- 场景一：纯 ORDER BY
-- 索引 (sort_col)
SELECT * FROM t ORDER BY sort_col;       -- ✓ 走索引

-- 场景二：WHERE + ORDER BY
-- 索引 (col_a, sort_col)
SELECT * FROM t WHERE col_a = 1 ORDER BY sort_col;  -- ✓ 走索引

-- 场景三：多列排序
-- 索引 (uid, x, y)
SELECT * FROM t WHERE uid = 1 ORDER BY x, y LIMIT 10;  -- ✓ 走索引
</code></pre>
<p>关键原则：<strong>WHERE 条件列和 ORDER BY 列必须在同一个复合索引中，且满足最左前缀。</strong></p>
<h3>6.2 ORDER BY 不能走索引的五种情况</h3>
<pre><code class="language-sql">-- ① 排序列来自不同索引
-- 有 INDEX(key1) 和 INDEX(key2)
ORDER BY key1, key2          -- ✗ 两个索引无法合并排序

-- ② 跳过了复合索引的中间列
-- INDEX(key_part1, key_part2)
WHERE key_part1 = 1
ORDER BY key_part2            -- ✓ 连续的

WHERE other_col = 1
ORDER BY key_part2            -- ✗ key_part1 缺失

-- ③ ASC 和 DESC 混用
-- INDEX(a, b)
ORDER BY a ASC, b DESC       -- ✗ 方向不一致（MySQL 8.0 之前）

-- ④ WHERE 和 ORDER BY 用了不同索引的列
-- INDEX(key1), INDEX(key2)
WHERE key1 = 1 ORDER BY key2 -- ✗ 走 key1 索引做过滤，但无法用它排序 key2

-- ⑤ 排序列上有函数
ORDER BY YEAR(login_date)     -- ✗ 函数破坏了索引有序性
</code></pre>
<h3>6.3 GROUP BY + Top-N 查询模式</h3>
<p>&quot;每个分组取前 N 条&quot;是常见的业务需求。几种实现方式的对比：</p>
<p><strong>方案一：子查询 + MAX（推荐）</strong></p>
<pre><code class="language-sql">-- 每组取最大值
SELECT a.* FROM tb a
WHERE val = (SELECT MAX(val) FROM tb WHERE name = a.name)
ORDER BY a.name;
</code></pre>
<p><strong>方案二：INNER JOIN + GROUP BY（推荐）</strong></p>
<pre><code class="language-sql">SELECT a.* FROM tb a
INNER JOIN (SELECT name, MAX(val) val FROM tb GROUP BY name) b
ON a.name = b.name AND a.val = b.val
ORDER BY a.name;
</code></pre>
<p><strong>方案三：窗口函数（MySQL 8.0+，最简洁）</strong></p>
<pre><code class="language-sql">-- ROW_NUMBER：严格排名，不并列
SELECT * FROM (
    SELECT *, ROW_NUMBER() OVER (PARTITION BY subject ORDER BY score DESC) rn
    FROM tb_score
) t WHERE rn &lt;= 3;

-- DENSE_RANK：允许并列，无间隔
SELECT * FROM (
    SELECT *, DENSE_RANK() OVER (PARTITION BY subject ORDER BY score DESC) rk
    FROM tb_score
) t WHERE rk &lt;= 3;
</code></pre>
<table>
<thead>
<tr>
<th>函数</th>
<th>处理并列</th>
<th>示例：分数 92, 92, 88</th>
</tr>
</thead>
<tbody><tr>
<td>ROW_NUMBER</td>
<td>不并列</td>
<td>1, 2, 3</td>
</tr>
<tr>
<td>RANK</td>
<td>并列，有间隔</td>
<td>1, 1, 3</td>
</tr>
<tr>
<td>DENSE_RANK</td>
<td>并列，无间隔</td>
<td>1, 1, 2</td>
</tr>
</tbody></table>
<hr>
<h2>七、慢查询优化实战</h2>
<p>理论讲完了，以下三个真实案例覆盖了慢查询优化中最常见的思路和最重要的教训。</p>
<h3>优化前的必要步骤</h3>
<pre><code class="language-sql">-- 排除查询缓存的干扰
SELECT SQL_NO_CACHE * FROM ...;
</code></pre>
<h3>7.1 案例一：JOIN 重构——从 1.87s 到 10ms</h3>
<p><strong>原始查询</strong>：查找最近一段时间内有更新的员工。</p>
<pre><code class="language-sql">SELECT DISTINCT cert.emp_id
FROM cm_log cl
INNER JOIN (
    SELECT emp.id emp_id, emp_cert.id cert_id
    FROM employee emp
    LEFT JOIN emp_certificate emp_cert ON emp.id = emp_cert.emp_id
    WHERE emp.is_deleted = 0
) cert
ON (cl.ref_table = &#39;Employee&#39; AND cl.ref_oid = cert.emp_id)
   OR (cl.ref_table = &#39;EmpCertificate&#39; AND cl.ref_oid = cert.cert_id)
WHERE cl.last_upd_date &gt;= &#39;2013-11-07 15:03:00&#39;
  AND cl.last_upd_date &lt;= &#39;2013-11-08 16:00:00&#39;;
</code></pre>
<p><strong>问题诊断</strong>：</p>
<ul>
<li>结果：53 条记录，耗时 <strong>1.87 秒</strong></li>
<li>EXPLAIN 显示：cm_log 用 <code>idx_last_upd_date</code> 过滤后只有 <strong>379 行</strong></li>
<li>但 JOIN 的派生表（cert）返回 <strong>63,727 行</strong></li>
<li>379 × 63,727 ≈ 2,400 万次比较，绝大多数是无用功</li>
</ul>
<p><strong>根因</strong>：OR 连接两种关联条件导致无法走索引 JOIN，退化为笛卡尔积。</p>
<p><strong>优化方案</strong>：拆成两条查询 + UNION，让小表 cm_log 先过滤。</p>
<pre><code class="language-sql">SELECT emp.id FROM cm_log cl
INNER JOIN employee emp
    ON cl.ref_table = &#39;Employee&#39; AND cl.ref_oid = emp.id
WHERE cl.last_upd_date &gt;= &#39;2013-11-07 15:03:00&#39;
  AND cl.last_upd_date &lt;= &#39;2013-11-08 16:00:00&#39;
  AND emp.is_deleted = 0

UNION

SELECT emp.id FROM cm_log cl
INNER JOIN emp_certificate ec
    ON cl.ref_table = &#39;EmpCertificate&#39; AND cl.ref_oid = ec.id
INNER JOIN employee emp ON emp.id = ec.emp_id
WHERE cl.last_upd_date &gt;= &#39;2013-11-07 15:03:00&#39;
  AND cl.last_upd_date &lt;= &#39;2013-11-08 16:00:00&#39;
  AND emp.is_deleted = 0;
</code></pre>
<p><strong>结果</strong>：<strong>10ms</strong>，提升 <strong>187 倍</strong>。</p>
<p><strong>教训</strong>：JOIN 中的 OR 条件几乎总是性能杀手。拆成 UNION 让每个分支都能走索引。</p>
<hr>
<h3>7.2 案例二：低选择性索引——从 6.22s 到 200ms</h3>
<p><strong>原始查询</strong>：查找待同步的 POI 数据。</p>
<pre><code class="language-sql">SELECT * FROM stage_poi sp
WHERE sp.accurate_result = 1
  AND sp.sync_status IN (0, 2, 4);
</code></pre>
<p><strong>问题诊断</strong>：</p>
<ul>
<li>结果：951 条记录，耗时 <strong>6.22 秒</strong></li>
<li>EXPLAIN：type = ALL，全表扫描 <strong>361 万行</strong></li>
<li>两个字段的选择性都极低：<ul>
<li><code>accurate_result</code>：只有 -1, 0, 1 三个值</li>
<li><code>sync_status</code>：只有 0, 1, 2, 3, 4 五个值</li>
</ul>
</li>
</ul>
<p>按常规判断，这两列的选择性太差，不适合建索引。</p>
<p><strong>转折点：理解业务上下文。</strong></p>
<p>这是一个数据同步任务，每 5 分钟执行一次：</p>
<ul>
<li>处理状态为 0/2/4 的记录（待同步）</li>
<li>处理完毕后将状态改为 1（已同步）</li>
<li><strong>在任意时刻，待同步的数据不超过 1000 条</strong>，其余 360 万条都是 status=1</li>
</ul>
<p>也就是说，虽然 sync_status 只有 5 个值，但 <strong>你要查的值的数据量只占 0.03%</strong>。</p>
<p><strong>优化方案</strong>：</p>
<pre><code class="language-sql">ALTER TABLE stage_poi ADD INDEX idx_acc_status(accurate_result, sync_status);
</code></pre>
<p><strong>结果</strong>：<strong>200ms</strong>，提升 <strong>31 倍</strong>。</p>
<p><strong>教训</strong>：数据分布比选择性统计更重要。在数据严重倾斜的场景下，低选择性的列也能从索引中获益。</p>
<hr>
<h3>7.3 案例三：不可优化的查询——13s 且无解</h3>
<p><strong>原始查询</strong>：分页查询联系人。</p>
<pre><code class="language-sql">SELECT c.id, c.name, c.position, c.sex, c.phone, ...
FROM contact c
INNER JOIN contact_branch cb ON c.id = cb.contact_id
INNER JOIN branch_user bu ON cb.branch_id = bu.branch_id
INNER JOIN org_emp_info oei ON oei.data_id = bu.user_id
WHERE bu.status IN (&#39;0&#39;, &#39;1&#39;)
  AND oei.node_left = 2875 AND oei.node_right = 10802
  AND oei.org_category = -1
ORDER BY c.created_time
LIMIT 0, 10;
</code></pre>
<p><strong>问题诊断</strong>：</p>
<ul>
<li>结果：10 条记录，耗时 <strong>13.06 秒</strong></li>
<li>单表索引都没问题，JOIN 行数也合理</li>
<li>但 JOIN 结果有 <strong>77.8 万行</strong>，然后对这 77.8 万行排序取前 10 条</li>
</ul>
<p><strong>尝试优化</strong>：改写为 EXISTS 子查询。</p>
<pre><code class="language-sql">SELECT c.id, c.name, ...
FROM contact c
WHERE EXISTS (
    SELECT 1 FROM contact_branch cb
    INNER JOIN branch_user bu ON cb.branch_id = bu.branch_id
    INNER JOIN org_emp_info oei ON oei.data_id = bu.user_id
    WHERE c.id = cb.contact_id
      AND bu.status IN (&#39;0&#39;, &#39;1&#39;)
      AND oei.node_left = 2875 AND oei.node_right = 10802
      AND oei.org_category = -1
)
ORDER BY c.created_time LIMIT 0, 10;
</code></pre>
<p><strong>结果</strong>：在当前参数下 <strong>0ms</strong>。但换一组参数（匹配 0 行的情况），查询耗时 <strong>218 秒</strong>。</p>
<p><strong>根因</strong>：MySQL 的嵌套循环 + LIMIT 策略在匹配率极低时退化——每次从 contact 表取 10 行，去子查询里匹配，没匹配到就取下一批 10 行，直到遍历整张表。</p>
<p><strong>最终结论</strong>：<strong>不是所有慢查询都能在 SQL 层面解决。</strong> 当 JOIN 结果集巨大且排序字段不在过滤条件中时，需要在应用层寻找出路——比如预计算排序、异步分页、或改变产品交互方式。</p>
<hr>
<h2>八、分页查询优化</h2>
<p>深度分页是一个高频性能问题。<code>LIMIT 100000, 20</code> 看起来只取 20 条，实际上 MySQL 需要扫描前 100,020 行，丢弃前 100,000 行。</p>
<h3>四种优化方案</h3>
<p><strong>方案一：基于主键翻页（最推荐）</strong></p>
<pre><code class="language-sql">-- 前端传入上一页最后一条记录的 id
SELECT * FROM users WHERE id &gt; 456891 ORDER BY id LIMIT 20;
-- 无论&quot;第几页&quot;，永远只扫描 20 行
</code></pre>
<p>限制：只能&quot;下一页&quot;，不能跳页。适合瀑布流、无限滚动。</p>
<p><strong>方案二：子查询定位起点</strong></p>
<pre><code class="language-sql">SELECT * FROM users
WHERE id &gt;= (SELECT id FROM users ORDER BY id LIMIT 100000, 1)
ORDER BY id LIMIT 20;
-- 子查询走覆盖索引（只查 id），速度快
-- 外层查询从定位点开始，只扫描 20 行
</code></pre>
<p><strong>方案三：反向查询</strong></p>
<pre><code class="language-sql">-- 如果总共 160 万行，要取 LIMIT 1200000, 20（偏移 75%）
-- 反向查询：ORDER BY id DESC LIMIT 400000, 20（偏移 25%）
-- 扫描量从 120 万降到 40 万
</code></pre>
<p>适用：偏移量超过总量 50% 时。</p>
<p><strong>方案四：延迟关联</strong></p>
<pre><code class="language-sql">-- 先查主键列表（走覆盖索引，无回表）
SELECT a.* FROM users a
INNER JOIN (SELECT id FROM users ORDER BY id LIMIT 100000, 20) b
ON a.id = b.id;
</code></pre>
<p>子查询只在索引上操作，外层 JOIN 只回表 20 行。</p>
<table>
<thead>
<tr>
<th>方案</th>
<th>扫描行数</th>
<th>可跳页</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>基于主键翻页</td>
<td>约等于 pageSize</td>
<td>不可以</td>
<td>瀑布流、列表翻页</td>
</tr>
<tr>
<td>子查询定位</td>
<td>索引扫描 + pageSize</td>
<td>可以</td>
<td>通用分页</td>
</tr>
<tr>
<td>反向查询</td>
<td>减半</td>
<td>可以</td>
<td>偏移超过 50%</td>
</tr>
<tr>
<td>延迟关联</td>
<td>索引扫描 + pageSize</td>
<td>可以</td>
<td>需回表的分页</td>
</tr>
</tbody></table>
<hr>
<h2>九、索引设计决策指南</h2>
<h3>该不该建索引</h3>
<table>
<thead>
<tr>
<th>场景</th>
<th>建议</th>
<th>原因</th>
</tr>
</thead>
<tbody><tr>
<td>WHERE 条件中的等值查询列</td>
<td>建</td>
<td>直接命中</td>
</tr>
<tr>
<td>WHERE 条件中的范围查询列</td>
<td>建（放复合索引最后）</td>
<td>范围查询后的列不会被使用</td>
</tr>
<tr>
<td>JOIN 关联字段</td>
<td>必须建</td>
<td>否则每次 JOIN 都全表扫描</td>
</tr>
<tr>
<td>ORDER BY 字段</td>
<td>考虑和 WHERE 列组成复合索引</td>
<td>避免 filesort</td>
</tr>
<tr>
<td>高频查询但选择性低的列</td>
<td>看数据分布</td>
<td>统计选择性不等于实际过滤效果</td>
</tr>
<tr>
<td>很少出现在 WHERE 中的列</td>
<td>不建</td>
<td>索引的写入代价大于查询收益</td>
</tr>
</tbody></table>
<h3>索引过多的代价</h3>
<p>索引不是免费的。每多一个索引：</p>
<ul>
<li>每次 INSERT 需要额外维护一棵 B+Tree（写入变慢）</li>
<li>每次 UPDATE 涉及索引列时需要更新索引</li>
<li>每个索引都占磁盘空间</li>
<li>索引太多时优化器可能选错索引（需要 <code>FORCE INDEX</code> 纠正）</li>
</ul>
<p><strong>经验法则</strong>：单表索引数量建议不超过 5~6 个。优先使用复合索引覆盖多种查询，而非为每个查询建单独的索引。</p>
<h3>核心原则</h3>
<blockquote>
<p><strong>索引优化的本质，是让 EXPLAIN 中的 <code>rows</code> 尽可能接近查询的实际结果行数。</strong> 扫描的行数和返回的行数之间的差距，就是浪费的 I/O。</p>
</blockquote>
19:T72c6,<h1>SET化架构：从单元化原理到大规模落地实践</h1>
<blockquote>
<p>当系统规模突破单机房、单集群的承载极限，当一次机房故障就可能导致全站不可用时，SET 化架构就成为了必然选择。它不是一种特定的技术方案，而是一种<strong>将系统划分为独立自治单元，实现水平扩展和故障隔离</strong>的架构思想。</p>
</blockquote>
<p>互联网业务的高速增长给架构带来了两个根本性挑战：<strong>容量的天花板</strong>和<strong>可用性的脆弱性</strong>。传统的垂直扩展（Scale-up）终有极限，而简单的水平扩展（Scale-out）在数据一致性、服务依赖、运维复杂度等方面又面临诸多困难。</p>
<p>SET 化架构（也称为单元化架构、Cell-based Architecture）正是为了系统性地解决这些问题而诞生的。本文将从原理到实践，全面解析 SET 化架构的设计与落地。</p>
<h2>什么是 SET 化架构？</h2>
<h3>概念定义</h3>
<p>SET（Scalable Elastic Topology，可扩展弹性拓扑）化架构是一种<strong>将系统按照某个维度（通常是用户 ID）划分为多个独立、自包含的部署单元</strong>的架构模式。每个 SET 都是一个&quot;小型完整系统&quot;，拥有独立的应用服务、缓存、数据库等全套基础设施，能够独立处理分配给它的流量。</p>
<pre><code>SET 化的核心思想：

传统架构：         所有用户 → 一套系统
                    （纵向扩展，存在单点瓶颈）

SET 化架构：       用户按规则分组 → 每组对应一个 SET
                    SET-1: 用户 0~999W    → 独立的一套完整系统
                    SET-2: 用户 1000W~1999W → 独立的一套完整系统
                    SET-3: 用户 2000W~2999W → 独立的一套完整系统
                    （水平扩展，理论上无上限）
</code></pre>
<h3>SET 的核心特征</h3>
<table>
<thead>
<tr>
<th>特征</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>自包含</strong></td>
<td>每个 SET 拥有完整的服务栈（应用、缓存、DB），能独立处理请求</td>
</tr>
<tr>
<td><strong>对等部署</strong></td>
<td>所有 SET 的架构相同，只是处理的数据分片不同</td>
</tr>
<tr>
<td><strong>故障隔离</strong></td>
<td>单个 SET 的故障不会影响其他 SET</td>
</tr>
<tr>
<td><strong>水平扩展</strong></td>
<td>通过增加 SET 数量实现容量扩展</td>
</tr>
<tr>
<td><strong>流量可调度</strong></td>
<td>通过路由规则灵活调度流量在 SET 间的分配</td>
</tr>
</tbody></table>
<h3>SET 化与传统分布式的区别</h3>
<table>
<thead>
<tr>
<th>维度</th>
<th>传统分布式架构</th>
<th>SET 化架构</th>
</tr>
</thead>
<tbody><tr>
<td>扩展方式</td>
<td>各层独立扩展（加应用节点、加 DB 从库）</td>
<td>整体作为一个单元扩展</td>
</tr>
<tr>
<td>故障影响</td>
<td>某一层故障影响全局</td>
<td>故障隔离在单个 SET 内</td>
</tr>
<tr>
<td>数据分片</td>
<td>数据库层分片，应用层无感知</td>
<td>从入口到数据库全链路分片</td>
</tr>
<tr>
<td>部署单元</td>
<td>按服务部署</td>
<td>按 SET（单元）部署</td>
</tr>
<tr>
<td>容量规划</td>
<td>各组件独立评估</td>
<td>按 SET 整体评估</td>
</tr>
</tbody></table>
<h2>SET 化架构演进历程</h2>
<p>SET 化不是一步到位的设计，而是随着业务规模增长逐步演化的结果。</p>
<h3>阶段一：单体架构</h3>
<pre><code>用户 → 应用服务器 → 数据库
</code></pre>
<p>所有功能在一个应用中，单库单表。适用于初创期，简单高效。</p>
<p><strong>瓶颈</strong>：单机容量有限，数据库成为瓶颈。</p>
<h3>阶段二：读写分离 + 缓存</h3>
<pre><code>用户 → 应用集群 → 缓存 → 主库（写）/ 从库（读）
</code></pre>
<p>通过读写分离缓解数据库压力，引入缓存降低 DB 负载。</p>
<p><strong>瓶颈</strong>：写入瓶颈无法解决，主库仍是单点。</p>
<h3>阶段三：分库分表</h3>
<pre><code>用户 → 应用集群 → 数据库中间件 → DB 分片 1 / DB 分片 2 / DB 分片 N
</code></pre>
<p>数据库水平拆分，解决写入瓶颈。但分片逻辑散落在各处，跨分片查询复杂。</p>
<p><strong>瓶颈</strong>：应用层无分片感知，缓存与 DB 分片不对齐，运维复杂。</p>
<h3>阶段四：服务化（微服务）</h3>
<pre><code>用户 → API 网关 → 微服务 A / 微服务 B / ... → 各自的 DB
</code></pre>
<p>按业务域拆分为独立服务，各服务独立部署和扩展。</p>
<p><strong>瓶颈</strong>：服务间调用复杂，全链路缺乏统一的分片和隔离机制。</p>
<h3>阶段五：SET 化（单元化）</h3>
<pre><code>用户 → 统一路由层 → SET-1（完整服务栈）/ SET-2 / SET-N
                       ↕ 数据同步
</code></pre>
<p>全链路按统一维度分片，每个 SET 自包含完整服务栈，实现真正的水平扩展和故障隔离。</p>
<p><strong>这就是 SET 化架构的终态。</strong> 下面详细介绍每个核心组件的设计。</p>
<h2>核心设计一：流量路由</h2>
<p>流量路由是 SET 化架构的&quot;大脑&quot;，它决定了每个请求应该被路由到哪个 SET。</p>
<h3>路由键的选择</h3>
<p>路由键（Sharding Key）是 SET 化的核心决策之一，选择不当会导致严重的跨 SET 调用问题。</p>
<table>
<thead>
<tr>
<th>路由键</th>
<th>优点</th>
<th>缺点</th>
<th>适用业务</th>
</tr>
</thead>
<tbody><tr>
<td><strong>用户 ID</strong></td>
<td>用户维度天然隔离，覆盖面广</td>
<td>用户间交互需跨 SET</td>
<td>电商、社交、O2O</td>
</tr>
<tr>
<td><strong>商户 ID</strong></td>
<td>商户维度隔离</td>
<td>用户下单需跨 SET</td>
<td>B 端平台</td>
</tr>
<tr>
<td><strong>地理区域</strong></td>
<td>天然的流量隔离</td>
<td>跨区域业务需特殊处理</td>
<td>本地生活、物流</td>
</tr>
<tr>
<td><strong>订单 ID</strong></td>
<td>订单维度隔离</td>
<td>需要提前生成带路由信息的 ID</td>
<td>交易系统</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>实践经验</strong>：绝大多数 C 端业务选择<strong>用户 ID</strong> 作为路由键，因为用户是最核心的业务实体，以用户为维度分片可以最大程度地减少跨 SET 调用。</p>
</blockquote>
<h3>路由架构设计</h3>
<p>SET 化的路由通常分为三层：</p>
<p><strong>第一层：接入路由（DNS / LB 层）</strong></p>
<p>在最外层通过 DNS 或负载均衡器将流量分配到对应的 SET。</p>
<pre><code>用户请求 → DNS 解析 → 全局负载均衡（GSLB）
                            ↓
                    根据用户 ID 哈希路由
                    ↓           ↓           ↓
                 SET-1 LB    SET-2 LB    SET-3 LB
</code></pre>
<p><strong>第二层：网关路由（API Gateway 层）</strong></p>
<p>API 网关根据请求中的路由键（如 Header、Cookie、Token 中的用户 ID）将请求路由到正确的 SET。</p>
<pre><code>请求 → API Gateway → 提取路由键 → 查询路由表 → 转发到目标 SET
</code></pre>
<p><strong>第三层：服务路由（RPC 层）</strong></p>
<p>服务间调用时，RPC 框架自动根据上下文中的路由键将请求路由到同 SET 的服务实例。</p>
<pre><code>Service A (SET-1) → RPC Framework → 自动路由到 → Service B (SET-1)
                    （通过上下文传递 SET 标识）
</code></pre>
<h3>路由表设计</h3>
<p>路由表是映射用户到 SET 的核心数据结构：</p>
<pre><code>路由表结构：
┌──────────────┬──────────┬──────────┐
│  分片范围      │  SET ID  │  状态     │
├──────────────┼──────────┼──────────┤
│  0 ~ 999      │  SET-1   │  Active  │
│  1000 ~ 1999  │  SET-2   │  Active  │
│  2000 ~ 2999  │  SET-3   │  Active  │
│  3000 ~ 3999  │  SET-1   │  Active  │  ← 同一个 SET 可承载多个分片
└──────────────┴──────────┴──────────┘
</code></pre>
<p>路由策略的关键设计要点：</p>
<ol>
<li><strong>虚拟分片</strong>：不直接将用户映射到物理 SET，而是先映射到虚拟分片（如 1024 个），再将虚拟分片映射到物理 SET。这样扩容时只需调整虚拟分片的映射关系</li>
<li><strong>路由缓存</strong>：路由表在网关和服务端本地缓存，避免每次请求都查询路由服务</li>
<li><strong>路由一致性</strong>：路由表变更时需要保证全链路一致性，避免请求被路由到错误的 SET</li>
</ol>
<h2>核心设计二：数据分片与同步</h2>
<p>数据层是 SET 化最复杂的部分，需要解决数据分片、跨 SET 数据访问、数据同步等问题。</p>
<h3>数据分类</h3>
<p>SET 化架构中的数据按照与路由键的关系分为三类：</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>定义</th>
<th>存储方式</th>
<th>举例</th>
</tr>
</thead>
<tbody><tr>
<td><strong>SET 内数据</strong></td>
<td>与路由键强绑定的数据</td>
<td>仅存储在对应 SET</td>
<td>用户订单、用户资产、购物车</td>
</tr>
<tr>
<td><strong>全局数据</strong></td>
<td>所有 SET 共享的数据</td>
<td>全局存储 + 各 SET 只读副本</td>
<td>商品信息、配置数据、类目</td>
</tr>
<tr>
<td><strong>跨 SET 数据</strong></td>
<td>涉及多个路由键的数据</td>
<td>全局存储或冗余存储</td>
<td>商户维度的聚合数据、排行榜</td>
</tr>
</tbody></table>
<h3>SET 内数据</h3>
<p>SET 内数据遵循&quot;谁的数据谁存储&quot;原则，每个 SET 只处理和存储自己分片内的数据：</p>
<pre><code>SET-1 数据库：只存储 UserID 0~999 的数据
SET-2 数据库：只存储 UserID 1000~1999 的数据

用户 A (ID=500) 下单 → 请求路由到 SET-1 → 订单写入 SET-1 DB
用户 B (ID=1500) 下单 → 请求路由到 SET-2 → 订单写入 SET-2 DB
</code></pre>
<h3>全局数据</h3>
<p>全局数据（如商品信息）需要所有 SET 都能访问，通常采用以下方案：</p>
<table>
<thead>
<tr>
<th>方案</th>
<th>原理</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>全局服务</strong></td>
<td>独立部署的全局服务 + 数据库</td>
<td>数据一致性好</td>
<td>全局服务成为依赖瓶颈</td>
</tr>
<tr>
<td><strong>数据广播</strong></td>
<td>写入全局库后异步同步到各 SET</td>
<td>本地读取性能好</td>
<td>数据有延迟，存储冗余</td>
</tr>
<tr>
<td><strong>缓存分发</strong></td>
<td>全局数据写入后推送到各 SET 缓存</td>
<td>读取极快</td>
<td>缓存一致性需要保障</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>实践建议</strong>：高频读取的全局数据（如商品详情）采用&quot;数据广播 + 本地缓存&quot;方案；低频但要求强一致的全局数据（如配置变更）采用&quot;全局服务&quot;方案。</p>
</blockquote>
<h3>数据同步机制</h3>
<p>SET 间的数据同步是保证业务连续性的关键，特别是在故障切换场景下：</p>
<pre><code>                     主 SET                          备 SET
                 ┌──────────┐                    ┌──────────┐
                 │  应用层    │                    │  应用层    │
                 │  缓存层    │                    │  缓存层    │
                 │  数据库    │ ── Binlog 同步 ──→ │  数据库    │
                 └──────────┘                    └──────────┘

        同步方式：MySQL Binlog → Canal/DTS → 目标 SET 数据库
        同步延迟：通常 &lt; 1s，需要监控告警
</code></pre>
<p>数据同步的关键指标：</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>目标值</th>
<th>监控方式</th>
</tr>
</thead>
<tbody><tr>
<td>同步延迟</td>
<td>&lt; 1 秒</td>
<td>Binlog 位点差监控</td>
</tr>
<tr>
<td>数据一致性</td>
<td>99.99%</td>
<td>定期全量对账</td>
</tr>
<tr>
<td>同步可用性</td>
<td>99.99%</td>
<td>同步链路健康检查</td>
</tr>
</tbody></table>
<h2>核心设计三：全局服务</h2>
<p>有些服务天然不能被 SET 化，它们需要作为全局服务为所有 SET 提供能力。</p>
<h3>全局 ID 生成</h3>
<p>在 SET 化架构中，ID 生成必须保证全局唯一且带有路由信息：</p>
<pre><code>ID 结构设计：
┌────────────┬──────────┬───────────┬──────────┐
│  时间戳      │  SET ID  │  机器 ID   │  序列号   │
│  41 bits    │  5 bits  │  5 bits   │  12 bits │
└────────────┴──────────┴───────────┴──────────┘

总长度：63 bits（Long 类型）
</code></pre>
<table>
<thead>
<tr>
<th>生成方案</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>全局 ID 服务</strong></td>
<td>全局唯一性保证最强</td>
<td>依赖外部服务，存在可用性风险</td>
<td>核心业务（订单、支付）</td>
</tr>
<tr>
<td><strong>本地 Snowflake</strong></td>
<td>无外部依赖，性能最高</td>
<td>需要解决时钟回拨问题</td>
<td>非核心业务</td>
</tr>
<tr>
<td><strong>号段模式</strong></td>
<td>批量获取减少调用</td>
<td>号段用尽时有短暂延迟</td>
<td>通用场景</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>兜底策略</strong>：本地 ID 生成作为兜底方案，当全局 ID 服务不可用时自动降级为本地生成，确保业务不中断。</p>
</blockquote>
<h3>全局配置中心</h3>
<p>配置中心负责管理所有 SET 的路由规则、业务配置和开关：</p>
<pre><code>配置中心架构：
                  ┌─────────────────┐
                  │   配置中心集群     │
                  │  (ZK/Nacos/etcd) │
                  └────────┬────────┘
                     ↙     ↓     ↘
            SET-1 Agent  SET-2 Agent  SET-3 Agent
               ↓            ↓            ↓
            本地缓存      本地缓存      本地缓存

推送机制：配置变更 → 配置中心 → 推送给各 SET Agent → 更新本地缓存
</code></pre>
<h3>全局调度中心</h3>
<p>负责 SET 的健康监控、故障检测和流量调度：</p>
<table>
<thead>
<tr>
<th>功能</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>健康检查</td>
<td>定期探测各 SET 的健康状态</td>
</tr>
<tr>
<td>故障检测</td>
<td>发现 SET 异常时触发告警</td>
</tr>
<tr>
<td>流量切换</td>
<td>故障 SET 的流量自动切换到备用 SET</td>
</tr>
<tr>
<td>容量管理</td>
<td>监控各 SET 的容量使用率</td>
</tr>
<tr>
<td>扩缩容编排</td>
<td>新增或下线 SET 时的流量编排</td>
</tr>
</tbody></table>
<h2>核心设计四：故障隔离与切换</h2>
<p>故障隔离是 SET 化架构最核心的价值之一。</p>
<h3>故障域划分</h3>
<p>SET 化架构将故障影响范围从&quot;全站&quot;缩小到&quot;单个 SET&quot;：</p>
<pre><code>传统架构故障：
  DB 主库宕机 → 全站不可用 → 影响 100% 用户

SET 化架构故障：
  SET-2 DB 宕机 → 仅 SET-2 不可用 → 影响约 33% 用户（假设 3 个 SET）
                    ↓ 自动切换
                 SET-2 流量切换到备用 → 影响时间 &lt; 分钟级
</code></pre>
<h3>故障切换策略</h3>
<table>
<thead>
<tr>
<th>策略</th>
<th>切换速度</th>
<th>数据风险</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>主备切换</strong></td>
<td>秒级~分钟级</td>
<td>可能丢失未同步数据</td>
<td>SET 内部 DB 主备切换</td>
</tr>
<tr>
<td><strong>SET 间切换</strong></td>
<td>分钟级</td>
<td>依赖数据同步延迟</td>
<td>整个 SET 故障</td>
</tr>
<tr>
<td><strong>跨机房切换</strong></td>
<td>分钟级~小时级</td>
<td>需要全量数据同步</td>
<td>机房级故障</td>
</tr>
</tbody></table>
<h3>故障切换流程</h3>
<pre><code>正常状态：
  用户流量 → 路由层 → SET-2（主）

故障检测：
  健康检查失败 → 确认 SET-2 不可用 → 触发切换流程

切换执行：
  1. 停止 SET-2 的流量接入（路由层摘除）
  2. 等待 SET-2 → SET-2-备 的数据同步完成（或接受部分数据丢失）
  3. 更新路由表：SET-2 的分片 → SET-2-备
  4. 开放 SET-2-备 的流量接入
  5. 验证切换后的业务正确性

恢复状态：
  用户流量 → 路由层 → SET-2-备（新主）
</code></pre>
<h3>容灾等级</h3>
<table>
<thead>
<tr>
<th>等级</th>
<th>容灾范围</th>
<th>实现方式</th>
<th>RTO</th>
</tr>
</thead>
<tbody><tr>
<td><strong>L1</strong></td>
<td>单机故障</td>
<td>应用集群 + DB 主备</td>
<td>秒级</td>
</tr>
<tr>
<td><strong>L2</strong></td>
<td>机架故障</td>
<td>跨机架部署</td>
<td>秒级</td>
</tr>
<tr>
<td><strong>L3</strong></td>
<td>机房故障</td>
<td>同城双机房 SET 互备</td>
<td>分钟级</td>
</tr>
<tr>
<td><strong>L4</strong></td>
<td>城市故障</td>
<td>异地 SET 互备</td>
<td>分钟级~小时级</td>
</tr>
</tbody></table>
<h2>核心设计五：SET 扩缩容</h2>
<p>SET 化架构的一个重要优势是可以通过增减 SET 数量来调整系统容量。</p>
<h3>扩容流程</h3>
<pre><code>扩容场景：当前 3 个 SET 容量不足，需要扩容到 4 个 SET

Step 1: 部署新 SET（SET-4）
  - 部署完整的应用服务、缓存、数据库
  - 从现有 SET 同步全局数据

Step 2: 数据迁移
  - 将 SET-1 的部分虚拟分片的数据迁移到 SET-4
  - 采用双写方案保证迁移过程不中断服务

Step 3: 路由切换
  - 更新路由表：迁移的虚拟分片指向 SET-4
  - 灰度切换流量，逐步验证

Step 4: 清理
  - 验证完成后，清理 SET-1 中已迁移的数据
  - 回收空闲资源
</code></pre>
<h3>虚拟分片的价值</h3>
<p>虚拟分片是实现平滑扩缩容的关键：</p>
<pre><code>初始状态（3 个 SET，1024 个虚拟分片）：
  SET-1: 虚拟分片 0~341
  SET-2: 虚拟分片 342~682
  SET-3: 虚拟分片 683~1023

扩容到 4 个 SET（只需调整虚拟分片映射）：
  SET-1: 虚拟分片 0~255
  SET-2: 虚拟分片 256~511
  SET-3: 虚拟分片 512~767
  SET-4: 虚拟分片 768~1023

优势：用户 → 虚拟分片的映射不变，只调整虚拟分片 → 物理 SET 的映射
</code></pre>
<h2>实践案例：电商交易系统 SET 化</h2>
<p>以一个典型的电商交易系统为例，展示 SET 化的具体落地方案。</p>
<h3>业务分析</h3>
<table>
<thead>
<tr>
<th>服务</th>
<th>路由键关系</th>
<th>SET 化策略</th>
</tr>
</thead>
<tbody><tr>
<td>用户服务</td>
<td>用户 ID（强绑定）</td>
<td>SET 内部署</td>
</tr>
<tr>
<td>订单服务</td>
<td>用户 ID（强绑定）</td>
<td>SET 内部署</td>
</tr>
<tr>
<td>支付服务</td>
<td>用户 ID（强绑定）</td>
<td>SET 内部署</td>
</tr>
<tr>
<td>商品服务</td>
<td>无关（全局数据）</td>
<td>全局部署 + 数据广播</td>
</tr>
<tr>
<td>库存服务</td>
<td>商品维度（跨 SET）</td>
<td>全局部署</td>
</tr>
<tr>
<td>搜索服务</td>
<td>无关（全局数据）</td>
<td>全局部署</td>
</tr>
<tr>
<td>营销服务</td>
<td>活动维度（跨 SET）</td>
<td>全局部署</td>
</tr>
</tbody></table>
<h3>整体架构</h3>
<pre><code>                        ┌──────────────────────────────────┐
                        │          统一接入层（GSLB）         │
                        └───────────────┬──────────────────┘
                                        ↓
                        ┌──────────────────────────────────┐
                        │         API Gateway（路由层）       │
                        │    提取 UserID → 查询路由表 → 转发   │
                        └──┬──────────────┬────────────┬───┘
                           ↓              ↓            ↓
                    ┌─────────────┐ ┌─────────────┐ ┌─────────────┐
                    │   SET-1     │ │   SET-2     │ │   SET-3     │
                    │ ┌─────────┐ │ │ ┌─────────┐ │ │ ┌─────────┐ │
                    │ │用户服务  │ │ │ │用户服务  │ │ │ │用户服务  │ │
                    │ │订单服务  │ │ │ │订单服务  │ │ │ │订单服务  │ │
                    │ │支付服务  │ │ │ │支付服务  │ │ │ │支付服务  │ │
                    │ │Redis    │ │ │ │Redis    │ │ │ │Redis    │ │
                    │ │MySQL    │ │ │ │MySQL    │ │ │ │MySQL    │ │
                    │ └─────────┘ │ │ └─────────┘ │ │ └─────────┘ │
                    └─────────────┘ └─────────────┘ └─────────────┘
                           ↕              ↕            ↕
                    ┌──────────────────────────────────────────┐
                    │              全局服务层                     │
                    │  商品服务 │ 库存服务 │ 搜索服务 │ 营销服务    │
                    │         全局 ID 服务 │ 配置中心              │
                    └──────────────────────────────────────────┘
</code></pre>
<h3>下单流程的 SET 化处理</h3>
<pre><code>用户 A（ID=500）下单购买商品 X：

1. 请求到达 API Gateway
2. Gateway 提取 UserID=500，查路由表 → SET-1
3. 请求转发到 SET-1 的订单服务
4. 订单服务调用全局商品服务查询商品信息
5. 订单服务调用全局库存服务扣减库存
6. 订单服务在 SET-1 本地 DB 创建订单
7. 订单服务调用 SET-1 本地的支付服务发起支付
8. 支付完成后，SET-1 的订单服务更新本地订单状态
</code></pre>
<p>关键点：</p>
<ul>
<li>用户维度的数据操作（创建订单、支付）在 SET 内完成，无跨 SET 调用</li>
<li>商品、库存等全局数据通过全局服务访问</li>
<li>RPC 框架自动将 SET 标识通过上下文传递，保证 SET 内调用的正确性</li>
</ul>
<h2>SET 化实施路线</h2>
<p>SET 化是一个渐进式的过程，不应该一步到位。</p>
<h3>阶段规划</h3>
<table>
<thead>
<tr>
<th>阶段</th>
<th>目标</th>
<th>关键动作</th>
<th>周期</th>
</tr>
</thead>
<tbody><tr>
<td><strong>P0：基础设施准备</strong></td>
<td>具备 SET 化的基础能力</td>
<td>统一 RPC 框架、引入路由组件、改造 ID 生成</td>
<td>1~2 月</td>
</tr>
<tr>
<td><strong>P1：核心链路 SET 化</strong></td>
<td>交易核心链路实现 SET 化</td>
<td>订单、支付、用户服务 SET 化部署</td>
<td>2~3 月</td>
</tr>
<tr>
<td><strong>P2：全链路 SET 化</strong></td>
<td>所有服务完成 SET 化改造</td>
<td>非核心服务 SET 化、全局服务治理</td>
<td>3~6 月</td>
</tr>
<tr>
<td><strong>P3：异地 SET</strong></td>
<td>实现异地多活能力</td>
<td>跨机房 SET 部署、数据同步、故障切换</td>
<td>3~6 月</td>
</tr>
</tbody></table>
<h3>改造清单</h3>
<p><strong>应用层改造</strong>：</p>
<ul>
<li>所有服务支持从请求上下文中提取和传递路由键</li>
<li>RPC 框架支持基于路由键的服务路由</li>
<li>消息队列的生产和消费支持 SET 路由</li>
<li>定时任务支持按 SET 分片执行</li>
</ul>
<p><strong>数据层改造</strong>：</p>
<ul>
<li>数据库按 SET 进行物理隔离</li>
<li>缓存按 SET 进行 namespace 隔离</li>
<li>全局数据的同步机制建设</li>
<li>数据对账和修复工具</li>
</ul>
<p><strong>基础设施改造</strong>：</p>
<ul>
<li>统一路由服务建设</li>
<li>全局 ID 生成服务建设</li>
<li>监控体系支持 SET 维度</li>
<li>发布系统支持按 SET 灰度</li>
</ul>
<h2>SET 化与异地多活的关系</h2>
<p>SET 化架构是异地多活的基础。两者的关系可以这样理解：</p>
<pre><code>SET 化 = 单元化部署 + 流量路由 + 数据分片
异地多活 = SET 化 + 跨地域部署 + 数据同步 + 故障切换
</code></pre>
<table>
<thead>
<tr>
<th>维度</th>
<th>同城 SET 化</th>
<th>异地多活 SET 化</th>
</tr>
</thead>
<tbody><tr>
<td>部署范围</td>
<td>同城多机房</td>
<td>跨城市多机房</td>
</tr>
<tr>
<td>网络延迟</td>
<td>&lt; 1ms</td>
<td>10~50ms</td>
</tr>
<tr>
<td>数据同步</td>
<td>同步/半同步复制</td>
<td>异步复制（最终一致性）</td>
</tr>
<tr>
<td>故障切换</td>
<td>自动秒级切换</td>
<td>手动/半自动分钟级切换</td>
</tr>
<tr>
<td>核心挑战</td>
<td>路由准确性</td>
<td>数据一致性 + 切换决策</td>
</tr>
</tbody></table>
<blockquote>
<p>SET 化架构天然具备&quot;每个 SET 独立自治&quot;的特性，这为异地多活提供了完美的基础。只需将不同的 SET 部署到不同的地域，配合数据同步和流量调度，就能实现异地多活。</p>
</blockquote>
<h2>常见问题与解决方案</h2>
<h3>跨 SET 调用问题</h3>
<p><strong>问题</strong>：部分业务场景不可避免需要跨 SET 访问数据。</p>
<p><strong>解决方案</strong>：</p>
<table>
<thead>
<tr>
<th>场景</th>
<th>解决方案</th>
</tr>
</thead>
<tbody><tr>
<td>用户查看商户信息</td>
<td>商户数据作为全局数据广播</td>
</tr>
<tr>
<td>商户查看所有订单</td>
<td>聚合服务从各 SET 并行查询后合并</td>
</tr>
<tr>
<td>全站排行榜</td>
<td>各 SET 本地计算后汇总到全局服务</td>
</tr>
<tr>
<td>跨用户转账</td>
<td>通过消息队列异步通知目标 SET</td>
</tr>
</tbody></table>
<h3>数据迁移问题</h3>
<p><strong>问题</strong>：扩容时需要在 SET 间迁移数据。</p>
<p><strong>解决方案</strong>：双写方案</p>
<pre><code>Phase 1: 新 SET 开始从旧 SET 同步增量数据（Binlog 订阅）
Phase 2: 同步追上后，开启双写模式（新请求同时写入新旧 SET）
Phase 3: 路由切换，新请求全部路由到新 SET
Phase 4: 验证无误后，停止双写，清理旧数据
</code></pre>
<h3>全局服务瓶颈</h3>
<p><strong>问题</strong>：全局服务成为所有 SET 的共同依赖，可能成为瓶颈。</p>
<p><strong>解决方案</strong>：</p>
<ol>
<li><strong>数据本地化</strong>：全局数据尽可能广播到各 SET 本地，减少全局服务调用</li>
<li><strong>缓存优先</strong>：全局数据走多级缓存，降低对全局 DB 的访问</li>
<li><strong>异步化</strong>：非实时性要求的全局操作通过消息队列异步处理</li>
<li><strong>弹性扩展</strong>：全局服务本身也需要集群化部署和弹性扩展</li>
</ol>
<h2>总结</h2>
<p>SET 化架构是应对互联网业务规模化增长的系统性解决方案。它的核心思想并不复杂——<strong>把一个大系统拆分成多个独立自治的小系统</strong>——但真正的挑战在于落地过程中的每一个细节。</p>
<p>回顾 SET 化的关键设计决策：</p>
<ol>
<li><strong>路由键选择决定了架构的天花板</strong>。选错路由键会导致大量跨 SET 调用，抵消 SET 化的优势</li>
<li><strong>数据分类是 SET 化的基础</strong>。明确哪些是 SET 内数据、哪些是全局数据，才能设计合理的数据架构</li>
<li><strong>虚拟分片是弹性扩展的关键</strong>。不要将用户直接映射到物理 SET，虚拟分片层带来的灵活性至关重要</li>
<li><strong>全局服务的治理不能忽视</strong>。全局服务是所有 SET 的共同依赖，必须做到高可用和高性能</li>
<li><strong>渐进式实施是务实的选择</strong>。从核心链路开始，逐步扩展，而不是试图一步到位</li>
</ol>
<blockquote>
<p><strong>SET 化不是目的，而是手段。</strong> 它服务于两个根本目标：让系统能够水平扩展以承载业务增长，让故障影响可控以保障用户体验。在实施 SET 化之前，先问自己：当前的业务规模真的需要 SET 化吗？</p>
</blockquote>
5:["$","article",null,{"className":"min-h-screen","children":["$","div",null,{"className":"mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12","children":[["$","header",null,{"className":"mb-8","children":[["$","nav",null,{"className":"flex items-center gap-1 text-sm mb-4","children":[["$","$L13",null,{"href":"/blog/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"博客"}],["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"Engineering"}],[["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/middleware/page/1","className":"text-blue-600 hover:text-blue-700 transition-colors","children":"中间件"}]]]}],["$","div",null,{"className":"flex items-center mb-6","children":["$","div",null,{"className":"inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal","children":[["$","svg",null,{"className":"w-4 h-4 mr-2 text-gray-400","fill":"none","stroke":"currentColor","viewBox":"0 0 24 24","children":["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","strokeWidth":2,"d":"M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z"}]}],["$","time",null,{"dateTime":"2025-11-25","children":"2025年11月25日"}]]}]}],["$","h1",null,{"className":"text-4xl font-bold text-gray-900 mb-6 text-center","children":"Redis 核心机制与工程实践：从数据结构选型到持久化的设计权衡"}],["$","div",null,{"className":"flex flex-wrap gap-2 mb-6 justify-center","children":[["$","$L13","Redis",{"href":"/blog/tag/Redis/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"Redis"}],["$","$L13","缓存",{"href":"/blog/tag/%E7%BC%93%E5%AD%98/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"缓存"}],["$","$L13","持久化",{"href":"/blog/tag/%E6%8C%81%E4%B9%85%E5%8C%96/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"持久化"}],["$","$L13","高可用",{"href":"/blog/tag/%E9%AB%98%E5%8F%AF%E7%94%A8/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"高可用"}]]}]]}],["$","div",null,{"className":"max-w-5xl mx-auto","children":["$","$L14",null,{"content":"$15"}]}],["$","$10",null,{"fallback":["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":"加载导航中..."}],"children":["$","$L16",null,{"globalNav":{"prev":{"slug":"engineering/architecture/限流的本质：从限流算法到分布式流控的架构思考","title":"限流的本质：从限流算法到分布式流控的架构思考","description":"限流不是一个算法问题，而是一个系统设计问题。从速率控制到并发保护，从单机令牌桶到分布式 Redis 计数器，从 Nginx 接入层到业务层精细化流控——每一层的限流策略背后，都是对系统容量、业务优先级和降级策略的深度思考。","pubDate":"2025-11-25","tags":["限流","分布式系统","系统架构","高可用"],"heroImage":"$undefined","content":"$17"},"next":{"slug":"engineering/middleware/MySQL索引原理与查询优化：从B+Tree到EXPLAIN的工程实践","title":"MySQL 索引原理与查询优化：从 B+Tree 到 EXPLAIN 的工程实践","description":"索引不是加了就快的魔法，而是一套需要理解底层数据结构、遵循匹配规则、结合业务场景做判断的工程实践。从磁盘 I/O 的物理约束理解 B+Tree 的设计动机，从最左前缀匹配理解复合索引的使用规则，从 EXPLAIN 的输出理解优化器的真实决策——每一步都是在缩小扫描行数与实际需要行数之间的差距。","pubDate":"2025-11-25","tags":["MySQL","索引优化","慢查询","数据库"],"heroImage":"$undefined","content":"$18"}},"tagNav":{"Redis":{"prev":null,"next":null},"缓存":{"prev":null,"next":null},"持久化":{"prev":null,"next":null},"高可用":{"prev":"$5:props:children:props:children:props:children:2:props:children:props:globalNav:prev","next":{"slug":"engineering/architecture/SET化架构：从单元化原理到大规模落地实践","title":"SET化架构：从单元化原理到大规模落地实践","description":"深入剖析SET化（单元化）架构的核心原理与设计实践，涵盖流量路由、数据分片、全局服务、故障隔离等关键环节，结合美团、阿里等大厂实践经验，构建可水平扩展的弹性架构体系。","pubDate":"2025-12-05","tags":["架构设计","SET化架构","单元化","异地多活","高可用"],"heroImage":"$undefined","content":"$19"}}}}]}],["$","$L1a",null,{}]]}]}]}]
8:null
c:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
7:null
a:{"metadata":[["$","title","0",{"children":"Redis 核心机制与工程实践：从数据结构选型到持久化的设计权衡 - Skyfalling Blog"}],["$","meta","1",{"name":"description","content":"Redis 的快不是因为内存数据库四个字就能解释的，而是单线程模型、精心设计的数据结构、惰性过期策略和高效持久化机制共同作用的结果。从五种数据类型的内部编码理解选型依据，从 RDB 和 AOF 的写入管线理解持久化保障，从 Sentinel 的故障检测理解高可用设计——每一个工程决策都在性能、安全和复杂度之间寻找平衡点。"}],["$","meta","2",{"property":"og:title","content":"Redis 核心机制与工程实践：从数据结构选型到持久化的设计权衡"}],["$","meta","3",{"property":"og:description","content":"Redis 的快不是因为内存数据库四个字就能解释的，而是单线程模型、精心设计的数据结构、惰性过期策略和高效持久化机制共同作用的结果。从五种数据类型的内部编码理解选型依据，从 RDB 和 AOF 的写入管线理解持久化保障，从 Sentinel 的故障检测理解高可用设计——每一个工程决策都在性能、安全和复杂度之间寻找平衡点。"}],["$","meta","4",{"property":"og:type","content":"article"}],["$","meta","5",{"property":"article:published_time","content":"2025-11-25"}],["$","meta","6",{"property":"article:author","content":"Skyfalling"}],["$","meta","7",{"name":"twitter:card","content":"summary"}],["$","meta","8",{"name":"twitter:title","content":"Redis 核心机制与工程实践：从数据结构选型到持久化的设计权衡"}],["$","meta","9",{"name":"twitter:description","content":"Redis 的快不是因为内存数据库四个字就能解释的，而是单线程模型、精心设计的数据结构、惰性过期策略和高效持久化机制共同作用的结果。从五种数据类型的内部编码理解选型依据，从 RDB 和 AOF 的写入管线理解持久化保障，从 Sentinel 的故障检测理解高可用设计——每一个工程决策都在性能、安全和复杂度之间寻找平衡点。"}],["$","link","10",{"rel":"shortcut icon","href":"/favicon.png"}],["$","link","11",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","link","12",{"rel":"icon","href":"/favicon.png"}],["$","link","13",{"rel":"apple-touch-icon","href":"/favicon.png"}]],"error":null,"digest":"$undefined"}
12:{"metadata":"$a:metadata","error":null,"digest":"$undefined"}
