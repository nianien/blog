1:"$Sreact.fragment"
2:I[10616,["6874","static/chunks/6874-7791217feaf05c17.js","7177","static/chunks/app/layout-142e67ac4336647c.js"],"default"]
3:I[87555,[],""]
4:I[31295,[],""]
6:I[59665,[],"OutletBoundary"]
9:I[74911,[],"AsyncMetadataOutlet"]
b:I[59665,[],"ViewportBoundary"]
d:I[59665,[],"MetadataBoundary"]
f:I[26614,[],""]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/7dd6b3ec14b0b1d8.css","style"]
0:{"P":null,"b":"2rrmzfsoknNGuymzsZdxz","p":"","c":["","blog","engineering","agentic","04-The%20Agent%20Control%20Loop",""],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","engineering/agentic/04-The%20Agent%20Control%20Loop","c"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/7dd6b3ec14b0b1d8.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"zh-CN","children":["$","body",null,{"className":"__className_f367f3","children":["$","div",null,{"className":"min-h-screen flex flex-col","children":[["$","$L2",null,{}],["$","main",null,{"className":"flex-1","children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"bg-[var(--background)]","children":["$","div",null,{"className":"mx-auto max-w-7xl px-6 py-12 lg:px-8","children":["$","p",null,{"className":"text-center text-xs leading-5 text-gray-400","children":["© ",2026," Skyfalling"]}]}]}]]}]}]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","engineering/agentic/04-The%20Agent%20Control%20Loop","c"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L5",null,["$","$L6",null,{"children":["$L7","$L8",["$","$L9",null,{"promise":"$@a"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","xbTQov4iLN1qUgF96cudLv",{"children":[["$","$Lb",null,{"children":"$Lc"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$Ld",null,{"children":"$Le"}]]}],false]],"m":"$undefined","G":["$f","$undefined"],"s":false,"S":true}
10:"$Sreact.suspense"
11:I[74911,[],"AsyncMetadata"]
13:I[6874,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],""]
14:I[32923,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
16:I[40780,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
1c:I[85300,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
e:["$","div",null,{"hidden":true,"children":["$","$10",null,{"fallback":null,"children":["$","$L11",null,{"promise":"$@12"}]}]}]
15:Tc7d3,<h1>The Agent Control Loop: Agent 运行时的核心抽象</h1>
<blockquote>
<p>如果说 LLM 是 Agent 的大脑，那么 Control Loop 就是 Agent 的心跳。</p>
<p>大多数教程在讲 Agent 时，上来就接框架、调 API、跑 demo。但如果你不理解 Agent 运行时的核心抽象——控制循环——你永远只是在用别人的黑盒。</p>
<p>本文是 Agentic 系列第 04 篇，整个系列的技术基石。我们会从状态机模型出发，逐层拆解 Agent Control Loop 的每一个阶段，给出完整的 Python 实现，并深入分析实际工程中的 trade-off。</p>
</blockquote>
<hr>
<h2>1. Agent 的本质：可中断的控制循环</h2>
<p>一个常见的误解是把 Agent 等同于&quot;一次 LLM 调用&quot;。实际上，Agent 和 LLM 的关系，类似于操作系统和 CPU 的关系——LLM 是执行推理的计算单元，而 Agent 是管理整个执行生命周期的运行时系统。</p>
<p><strong>LLM 是一个函数：</strong> <code>f(prompt) -&gt; completion</code>，输入文本，输出文本，调用一次就结束。</p>
<p><strong>Agent 是一个循环：</strong> 它持续运行，在每一轮中观察环境、调用 LLM 进行推理、执行动作、评估结果，然后决定是否继续。</p>
<pre><code>LLM:    Input ──→ Output            (一次调用)

Agent:  Input ──→ [Observe → Think → Act → Reflect] ──→ ... ──→ Output
                  └──────── 循环 N 次 ────────────┘     (多轮控制)
</code></pre>
<p>这个循环有几个关键特性：</p>
<ul>
<li><strong>可中断</strong>：循环可以在任何阶段暂停，等待外部输入（用户确认、异步工具返回）后恢复</li>
<li><strong>有状态</strong>：循环维护上下文信息，每一轮的输出影响下一轮的输入</li>
<li><strong>有终止条件</strong>：循环不会无限运行，它在满足特定条件时停止</li>
<li><strong>可观测</strong>：循环的每一步都应该是可追踪、可回溯的</li>
</ul>
<p>理解了这一点，Agent 编程的核心问题就变成了：<strong>如何设计和实现这个控制循环？</strong></p>
<hr>
<h2>2. 状态机模型：形式化定义</h2>
<p>要严谨地描述 Control Loop，最自然的方式是用<strong>有限状态机（FSM）</strong>。</p>
<h3>2.1 状态定义</h3>
<p>一个 Agent Control Loop 可以用以下状态集合描述：</p>
<pre><code class="language-python">from enum import Enum

class AgentState(Enum):
    OBSERVE  = &quot;observe&quot;   # 接收并归一化输入
    THINK    = &quot;think&quot;     # LLM 推理，决定下一步行动
    ACT      = &quot;act&quot;       # 执行工具调用或产出结果
    REFLECT  = &quot;reflect&quot;   # 评估执行结果，决定是否继续
    DONE     = &quot;done&quot;      # 终止：任务完成
    ERROR    = &quot;error&quot;     # 终止：不可恢复错误
</code></pre>
<h3>2.2 状态转移图</h3>
<pre><code>                    ┌─────────────────────────────────────────┐
                    │                                         │
                    ▼                                         │
              ┌──────────┐                                    │
   Input ───→│ OBSERVE  │                                    │
              └────┬─────┘                                    │
                   │                                         │
                   ▼                                         │
              ┌──────────┐    need_action    ┌──────────┐    │
              │  THINK   │ ───────────────→ │   ACT    │    │
              └────┬─────┘                   └────┬─────┘    │
                   │                              │          │
                   │ has_answer                   │          │
                   │                              ▼          │
                   │                        ┌──────────┐     │
                   │                        │ REFLECT  │ ────┘
                   │                        └────┬─────┘  continue
                   │                             │
                   ▼                             ▼
              ┌──────────┐                  ┌──────────┐
              │   DONE   │                  │  ERROR   │
              └──────────┘                  └──────────┘
                                       (max_retries exceeded
                                        / unrecoverable)
</code></pre>
<p>状态转移规则：</p>
<table>
<thead>
<tr>
<th>当前状态</th>
<th>条件</th>
<th>下一状态</th>
</tr>
</thead>
<tbody><tr>
<td>OBSERVE</td>
<td>输入就绪</td>
<td>THINK</td>
</tr>
<tr>
<td>THINK</td>
<td>LLM 返回 tool_call</td>
<td>ACT</td>
</tr>
<tr>
<td>THINK</td>
<td>LLM 返回最终回答</td>
<td>DONE</td>
</tr>
<tr>
<td>THINK</td>
<td>LLM 调用异常</td>
<td>ERROR</td>
</tr>
<tr>
<td>ACT</td>
<td>工具执行完成</td>
<td>REFLECT</td>
</tr>
<tr>
<td>ACT</td>
<td>工具执行失败</td>
<td>REFLECT (带错误信息)</td>
</tr>
<tr>
<td>REFLECT</td>
<td>需要继续</td>
<td>OBSERVE (将结果作为新输入)</td>
</tr>
<tr>
<td>REFLECT</td>
<td>任务完成</td>
<td>DONE</td>
</tr>
<tr>
<td>REFLECT</td>
<td>超过重试上限</td>
<td>ERROR</td>
</tr>
</tbody></table>
<h3>2.3 与 OODA Loop 的对比</h3>
<p>Agent Control Loop 并不是凭空发明的，它和军事决策理论中的 <strong>OODA Loop（Observe-Orient-Decide-Act）</strong> 有深层的结构对应：</p>
<pre><code>OODA Loop:          Agent Control Loop:
┌─────────┐         ┌─────────┐
│ Observe │ ──────→ │ OBSERVE │  感知环境
├─────────┤         ├─────────┤
│ Orient  │ ──────→ │ THINK   │  理解上下文，形成判断
├─────────┤         │         │
│ Decide  │ ──────→ │         │  (LLM 在 THINK 中同时完成 Orient+Decide)
├─────────┤         ├─────────┤
│  Act    │ ──────→ │  ACT    │  执行行动
└─────────┘         ├─────────┤
                    │ REFLECT │  OODA 中没有显式的反思阶段
                    └─────────┘
</code></pre>
<p>关键区别在于 <strong>REFLECT 阶段</strong>。传统 OODA Loop 假设决策者能实时感知行动效果并自然融入下一轮 Observe。但 LLM Agent 不具备这种连续感知能力——它需要一个显式的反思步骤来评估工具返回值、判断是否需要修正。这是 Agent Control Loop 相对于经典决策循环的重要改进。</p>
<hr>
<h2>3. 循环中每个阶段的深入分析</h2>
<h3>3.1 OBSERVE：输入归一化</h3>
<p>OBSERVE 阶段的职责是<strong>收集并归一化各种来源的输入</strong>，将它们统一为 LLM 可理解的格式。</p>
<p>输入来源远不止&quot;用户消息&quot;一种：</p>
<pre><code>输入来源                    归一化后
┌─────────────────┐       ┌──────────────────────┐
│ 用户消息         │ ────→ │ {&quot;role&quot;: &quot;user&quot;,     │
│ 工具返回值       │ ────→ │  &quot;content&quot;: &quot;...&quot;}   │
│ 系统事件         │ ────→ │                      │
│ 定时触发         │ ────→ │ {&quot;role&quot;: &quot;system&quot;,   │
│ 外部 Webhook    │ ────→ │  &quot;content&quot;: &quot;...&quot;}   │
│ 上一轮反思结果   │ ────→ │                      │
└─────────────────┘       └──────────────────────┘
</code></pre>
<p><strong>输入归一化的核心原则：</strong></p>
<ol>
<li><p><strong>所有输入都必须序列化为 message 格式</strong>。不管来源是什么，最终都要变成 <code>{&quot;role&quot;: ..., &quot;content&quot;: ...}</code> 的形式，因为 LLM 只理解 message 序列。</p>
</li>
<li><p><strong>工具返回值需要结构化包装</strong>。不要直接把原始 JSON 甩给 LLM，要附上工具名称、执行状态和必要的摘要信息。</p>
</li>
<li><p><strong>输入需要截断和优先级排序</strong>。当多个输入同时到达时，需要决定哪些放进当前轮次的 Context Window，哪些缓存到下一轮。</p>
</li>
</ol>
<pre><code class="language-python">def observe(self, raw_inputs: list[dict]) -&gt; list[dict]:
    &quot;&quot;&quot;将原始输入归一化为 LLM message 格式&quot;&quot;&quot;
    messages = []
    for inp in raw_inputs:
        match inp[&quot;type&quot;]:
            case &quot;user_message&quot;:
                messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: inp[&quot;text&quot;]})
            case &quot;tool_result&quot;:
                messages.append({
                    &quot;role&quot;: &quot;tool&quot;,
                    &quot;tool_call_id&quot;: inp[&quot;call_id&quot;],
                    &quot;content&quot;: self._format_tool_result(inp),
                })
            case &quot;system_event&quot;:
                messages.append({
                    &quot;role&quot;: &quot;system&quot;,
                    &quot;content&quot;: f&quot;[System Event] {inp[&#39;event&#39;]}&quot;,
                })
    return messages
</code></pre>
<h3>3.2 THINK：LLM 推理</h3>
<p>THINK 阶段是控制循环中最核心的一环——调用 LLM，让它基于当前上下文做出决策。</p>
<p>这个阶段要解决三个问题：</p>
<p><strong>问题一：Context Window 构建</strong></p>
<p>LLM 的输入不是当前轮次的消息，而是<strong>从任务开始到现在的完整上下文</strong>。构建 Context Window 的典型结构：</p>
<pre><code>┌─────────────────────────────────────────────┐
│ System Prompt                               │  固定不变
│ (角色定义 + 能力边界 + 输出格式要求)           │
├─────────────────────────────────────────────┤
│ Tool Definitions                            │  固定不变
│ (可用工具的 JSON Schema 定义)                │
├─────────────────────────────────────────────┤
│ Message History                             │  随轮次增长
│ (user → assistant → tool → assistant → ...) │
├─────────────────────────────────────────────┤
│ Current Turn Input                          │  当前轮次
│ (本轮 OBSERVE 阶段归一化的输入)              │
└─────────────────────────────────────────────┘
</code></pre>
<p><strong>问题二：Token 预算控制</strong></p>
<p>Context Window 有上限（4K / 8K / 128K / 200K），而每一轮循环都会增加 message history。如果不加控制，几轮之后就会超限。</p>
<p>常见的预算控制策略：</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>实现方式</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>硬截断</td>
<td>只保留最近 N 条消息</td>
<td>简单场景</td>
</tr>
<tr>
<td>滑动窗口</td>
<td>System Prompt 固定 + 最近 K 轮对话</td>
<td>工具调用场景</td>
</tr>
<tr>
<td>摘要压缩</td>
<td>将早期对话用 LLM 生成摘要后替换</td>
<td>长对话场景</td>
</tr>
<tr>
<td>优先级保留</td>
<td>按消息重要性排序，低优先级先丢弃</td>
<td>复杂多步任务</td>
</tr>
</tbody></table>
<pre><code class="language-python">def _build_context(self, new_messages: list[dict]) -&gt; list[dict]:
    &quot;&quot;&quot;构建符合 Token 预算的 Context Window&quot;&quot;&quot;
    self.message_history.extend(new_messages)

    context = [self.system_prompt] + self.tool_definitions
    remaining_budget = self.max_tokens - self._count_tokens(context)

    # 从最新消息开始向前填充，直到预算耗尽
    selected = []
    for msg in reversed(self.message_history):
        msg_tokens = self._count_tokens([msg])
        if msg_tokens &gt; remaining_budget:
            break
        selected.insert(0, msg)
        remaining_budget -= msg_tokens

    return context + selected
</code></pre>
<p><strong>问题三：LLM 输出解析</strong></p>
<p>LLM 的返回可能是纯文本回答（任务完成），也可能是工具调用请求。需要根据返回类型决定下一步状态转移：</p>
<pre><code class="language-python">def think(self, context: list[dict]) -&gt; ThinkResult:
    &quot;&quot;&quot;调用 LLM 进行推理&quot;&quot;&quot;
    response = self.client.chat.completions.create(
        model=self.model,
        messages=context,
        tools=self.tool_schemas,
    )
    choice = response.choices[0]

    if choice.finish_reason == &quot;tool_calls&quot;:
        return ThinkResult(
            action=&quot;tool_call&quot;,
            tool_calls=choice.message.tool_calls,
            raw_message=choice.message,
        )
    else:
        return ThinkResult(
            action=&quot;answer&quot;,
            content=choice.message.content,
            raw_message=choice.message,
        )
</code></pre>
<h3>3.3 ACT：执行层</h3>
<p>ACT 阶段负责<strong>执行 THINK 阶段决定的动作</strong>——通常是调用工具（Tool Calling）。</p>
<p>执行层的核心挑战不是&quot;调用工具&quot;本身，而是以下几个工程问题：</p>
<p><strong>同步 vs 异步执行</strong></p>
<pre><code>同步执行（Simple）：
  think → call_tool_1 → wait → call_tool_2 → wait → reflect
  延迟 = T1 + T2

异步 / 并行执行（Optimized）：
  think → call_tool_1 ─┬─→ reflect
        → call_tool_2 ─┘
  延迟 = max(T1, T2)
</code></pre>
<p>当 LLM 在一次返回中请求多个工具调用（parallel tool calling）时，应该并行执行以降低延迟：</p>
<pre><code class="language-python">import asyncio

async def act(self, tool_calls: list[ToolCall]) -&gt; list[dict]:
    &quot;&quot;&quot;并行执行多个工具调用&quot;&quot;&quot;
    tasks = [self._execute_tool(tc) for tc in tool_calls]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    tool_results = []
    for tc, result in zip(tool_calls, results):
        if isinstance(result, Exception):
            tool_results.append({
                &quot;type&quot;: &quot;tool_result&quot;,
                &quot;call_id&quot;: tc.id,
                &quot;status&quot;: &quot;error&quot;,
                &quot;content&quot;: f&quot;Tool &#39;{tc.function.name}&#39; failed: {result}&quot;,
            })
        else:
            tool_results.append({
                &quot;type&quot;: &quot;tool_result&quot;,
                &quot;call_id&quot;: tc.id,
                &quot;status&quot;: &quot;success&quot;,
                &quot;content&quot;: str(result),
            })
    return tool_results
</code></pre>
<p><strong>执行安全</strong></p>
<p>工具执行不是无条件信任的。需要考虑：</p>
<ul>
<li><strong>超时控制</strong>：每个工具调用必须有 timeout，防止阻塞整个循环</li>
<li><strong>结果大小限制</strong>：工具返回值可能非常大（比如查数据库返回 10 万行），需要截断</li>
<li><strong>权限校验</strong>：某些工具（文件写入、网络请求、代码执行）需要额外的权限检查</li>
<li><strong>沙箱执行</strong>：代码执行类工具应该在沙箱中运行</li>
</ul>
<h3>3.4 REFLECT：输出质量评估</h3>
<p>REFLECT 阶段回答一个关键问题：<strong>上一步的执行结果是否满意？是继续、重试还是停止？</strong></p>
<p>这个阶段有两种实现方式：</p>
<p><strong>方式一：隐式反思——让 LLM 在下一轮 THINK 中自行判断</strong></p>
<p>这是最简单的方式。把工具返回值直接送进下一轮 THINK，让 LLM 自己决定是否需要修正。大多数框架（如 OpenAI Assistants API）默认采用这种方式。</p>
<p>优点：实现简单，不增加额外的 LLM 调用。</p>
<p>缺点：LLM 可能&quot;自信地&quot;忽略错误，特别是在返回值看起来合理但语义错误的情况下。</p>
<p><strong>方式二：显式反思——用独立的 LLM 调用进行自我评估</strong></p>
<pre><code class="language-python">def reflect(self, action_result: dict, task_goal: str) -&gt; ReflectResult:
    &quot;&quot;&quot;显式反思：评估执行结果&quot;&quot;&quot;
    prompt = f&quot;&quot;&quot;评估以下工具执行结果是否达成了任务目标。

任务目标: {task_goal}
执行结果: {json.dumps(action_result, ensure_ascii=False)}

请回答：
1. 结果是否正确？(yes/no)
2. 是否需要进一步行动？(yes/no)
3. 如果需要，下一步应该做什么？
&quot;&quot;&quot;
    response = self.client.chat.completions.create(
        model=self.model,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
    )
    # 解析反思结果...
    return ReflectResult(
        is_correct=...,
        needs_more_action=...,
        next_step_hint=...,
    )
</code></pre>
<p><strong>Trade-off 分析：</strong></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>隐式反思</th>
<th>显式反思</th>
</tr>
</thead>
<tbody><tr>
<td>Token 消耗</td>
<td>低</td>
<td>高（额外一次 LLM 调用）</td>
</tr>
<tr>
<td>质量把控</td>
<td>依赖 LLM 自觉</td>
<td>有独立的质量评估</td>
</tr>
<tr>
<td>延迟</td>
<td>低</td>
<td>增加一轮 LLM 延迟</td>
</tr>
<tr>
<td>适用场景</td>
<td>简单工具调用</td>
<td>复杂推理链、高准确性要求</td>
</tr>
</tbody></table>
<p>实际工程中，常用的折中方案是：<strong>对关键步骤用显式反思，对常规步骤用隐式反思</strong>。</p>
<h3>3.5 终止条件：什么时候停下来？</h3>
<p>一个 Agent 如果不知道什么时候停，就是一个烧钱的死循环。终止条件的设计是 Control Loop 中最容易被忽视、但对生产环境最重要的部分。</p>
<pre><code class="language-python">def should_stop(self, state: LoopState) -&gt; tuple[bool, str]:
    &quot;&quot;&quot;判断是否应该终止循环&quot;&quot;&quot;
    # 1. LLM 认为任务完成
    if state.last_think_result.action == &quot;answer&quot;:
        return True, &quot;task_completed&quot;

    # 2. 达到最大轮次
    if state.turn_count &gt;= self.max_turns:
        return True, &quot;max_turns_exceeded&quot;

    # 3. Token 预算耗尽
    if state.total_tokens &gt;= self.token_budget:
        return True, &quot;token_budget_exceeded&quot;

    # 4. 连续错误过多
    if state.consecutive_errors &gt;= self.max_consecutive_errors:
        return True, &quot;too_many_errors&quot;

    # 5. 死循环检测（重复输出相同内容）
    if self._detect_loop(state.recent_outputs):
        return True, &quot;loop_detected&quot;

    return False, &quot;&quot;
</code></pre>
<p>各终止条件的设计考量：</p>
<ul>
<li><strong>max_turns</strong>：硬上限，防止失控。一般设 10-30 轮。过小会导致复杂任务被截断，过大会导致 Token 浪费</li>
<li><strong>token_budget</strong>：成本控制。根据业务场景设定每次交互的 Token 上限</li>
<li><strong>consecutive_errors</strong>：容错阈值。工具偶尔失败是正常的，但连续 3 次以上通常意味着系统性问题</li>
<li><strong>loop_detected</strong>：死循环检测。如果 Agent 连续 N 轮输出相同或高度相似的内容，说明它陷入了无效循环</li>
</ul>
<hr>
<h2>4. 两种主流 Loop 模式对比</h2>
<h3>4.1 ReAct 模式</h3>
<p><strong>ReAct（Reason + Act）</strong> 是目前最主流的 Agent Loop 模式，由 Yao et al. 2022 提出。其核心思想是让 LLM 交替进行推理和行动：</p>
<pre><code>┌──────────────────────────────────────────────────────┐
│                   ReAct Loop                         │
│                                                      │
│  ┌─────────┐    ┌─────────┐    ┌─────────────────┐  │
│  │ Thought │ →  │ Action  │ →  │  Observation    │  │
│  │(LLM推理)│    │(工具调用)│    │(工具返回值)      │  │
│  └─────────┘    └─────────┘    └────────┬────────┘  │
│       ▲                                  │          │
│       └──────────────────────────────────┘          │
│                  循环直到完成                         │
└──────────────────────────────────────────────────────┘
</code></pre>
<p>一个典型的 ReAct 执行轨迹（Trace）：</p>
<pre><code>Thought: 用户想知道北京今天的天气。我需要调用天气 API。
Action:  get_weather(city=&quot;北京&quot;)
Observation: {&quot;temp&quot;: 28, &quot;condition&quot;: &quot;晴&quot;, &quot;humidity&quot;: 45}

Thought: 已经获取到天气数据，我可以直接回答用户。
Answer:  北京今天晴天，气温 28°C，湿度 45%。
</code></pre>
<p><strong>ReAct 的优势：</strong></p>
<ul>
<li>每一步都基于最新的观察结果做决策，<strong>适应性强</strong></li>
<li>Thought 过程可见，<strong>可解释性好</strong></li>
<li>实现简单，与 Tool Calling API 天然契合</li>
</ul>
<p><strong>ReAct 的劣势：</strong></p>
<ul>
<li>逐步决策，无法全局优化执行顺序</li>
<li>每一步都需要一次 LLM 调用，<strong>延迟累积</strong></li>
<li>对于需要协调多个子任务的复杂场景，容易陷入局部最优</li>
</ul>
<h3>4.2 Plan-then-Execute 模式</h3>
<p>与 ReAct 的&quot;走一步看一步&quot;不同，Plan-then-Execute 先生成一个<strong>完整的执行计划</strong>，然后按计划依次执行：</p>
<pre><code>┌──────────────────────────────────────────────────────────┐
│              Plan-then-Execute Loop                       │
│                                                          │
│  ┌──────────────────────────────────────┐                │
│  │           Planning Phase             │                │
│  │  Input → LLM → [Step1, Step2, ...]   │                │
│  └───────────────┬──────────────────────┘                │
│                  │                                        │
│                  ▼                                        │
│  ┌──────────────────────────────────────┐                │
│  │         Execution Phase              │                │
│  │  Step1 → Execute → Result1           │                │
│  │  Step2 → Execute → Result2           │                │
│  │  ...                                 │                │
│  └───────────────┬──────────────────────┘                │
│                  │                                        │
│                  ▼                                        │
│  ┌──────────────────────────────────────┐                │
│  │    Replan (if needed)                │                │
│  │  检查是否需要调整计划                   │                │
│  └──────────────────────────────────────┘                │
└──────────────────────────────────────────────────────────┘
</code></pre>
<p>执行轨迹示例：</p>
<pre><code>Plan:
  1. 查询北京天气
  2. 查询上海天气
  3. 对比两地天气差异
  4. 生成出行建议

Execute Step 1: get_weather(city=&quot;北京&quot;) → {&quot;temp&quot;: 28, &quot;condition&quot;: &quot;晴&quot;}
Execute Step 2: get_weather(city=&quot;上海&quot;) → {&quot;temp&quot;: 32, &quot;condition&quot;: &quot;多云&quot;}
Execute Step 3: (LLM 对比分析)
Execute Step 4: (LLM 生成建议)

Answer: ...
</code></pre>
<h3>4.3 Trade-off 分析</h3>
<pre><code>                        灵活性
                          ▲
                          │
                 ReAct ●  │
                          │
                          │        ● Hybrid
                          │          (ReAct + Plan)
                          │
              Plan-then   │
              -Execute ●  │
                          │
                          └──────────────────→ 效率
                                          (LLM 调用次数)
</code></pre>
<table>
<thead>
<tr>
<th>维度</th>
<th>ReAct</th>
<th>Plan-then-Execute</th>
</tr>
</thead>
<tbody><tr>
<td>灵活性</td>
<td>高。每步实时调整</td>
<td>低。偏离计划时需要 Replan</td>
</tr>
<tr>
<td>LLM 调用次数</td>
<td>多（每步一次推理）</td>
<td>少（规划一次 + 执行时可能不需要 LLM）</td>
</tr>
<tr>
<td>可控性</td>
<td>低。难以预测执行路径</td>
<td>高。计划可审核、可修改</td>
</tr>
<tr>
<td>适合场景</td>
<td>工具调用为主、步骤不确定</td>
<td>多步骤、有依赖关系、需要全局协调</td>
</tr>
<tr>
<td>错误恢复</td>
<td>自然。下一步可以直接修正</td>
<td>需要 Replan 机制</td>
</tr>
<tr>
<td>人类干预</td>
<td>难以在中途插入</td>
<td>容易。可以审核和修改计划</td>
</tr>
</tbody></table>
<p><strong>实际工程建议：</strong> 大多数场景从 ReAct 开始。当你发现 Agent 频繁在多步任务中&quot;迷路&quot;或做出低效的工具调用序列时，再考虑引入 Plan-then-Execute 或混合模式。</p>
<hr>
<h2>5. 状态管理</h2>
<p>Control Loop 的状态管理决定了 Agent 的<strong>持久性</strong>和<strong>可恢复性</strong>。</p>
<h3>5.1 Stateless Agent</h3>
<p>Stateless Agent 不维护执行状态，所有上下文通过 <strong>message history</strong> 传递。</p>
<pre><code>Request 1:  [system, user_msg_1]                     → response_1
Request 2:  [system, user_msg_1, response_1, user_2] → response_2
Request 3:  [system, user_msg_1, response_1, user_2, response_2, user_3] → response_3
</code></pre>
<p><strong>特点：</strong></p>
<ul>
<li>实现最简单，无需持久化</li>
<li>每次请求都是自包含的</li>
<li>message history 不断膨胀，最终超过 Context Window</li>
<li>不支持暂停/恢复</li>
</ul>
<p>这是大多数 &quot;chat completion&quot; 应用的工作方式。适合单轮或短对话场景。</p>
<h3>5.2 Stateful Agent</h3>
<p>Stateful Agent 维护一个独立的 <strong>execution state</strong>，它不仅包含 message history，还包含任务进度、中间结果、工具状态等信息。</p>
<pre><code class="language-python">@dataclass
class ExecutionState:
    &quot;&quot;&quot;Agent 执行状态&quot;&quot;&quot;
    session_id: str
    status: AgentState
    turn_count: int
    message_history: list[dict]

    # 任务状态
    task_goal: str
    current_plan: list[str] | None
    completed_steps: list[str]

    # 资源消耗
    total_input_tokens: int
    total_output_tokens: int

    # 错误追踪
    consecutive_errors: int
    error_log: list[dict]

    # 时间戳
    created_at: float
    updated_at: float
</code></pre>
<h3>5.3 状态持久化方案</h3>
<p>当 Agent 需要支持暂停/恢复、跨进程执行、或长时间运行时，执行状态必须持久化。</p>
<pre><code>┌─────────────┐     ┌──────────────┐     ┌──────────────┐
│   In-Memory  │     │    Redis     │     │   Database   │
│  (dict/obj)  │     │  (KV Store)  │     │ (PostgreSQL) │
├─────────────┤     ├──────────────┤     ├──────────────┤
│ 最快         │     │ 快，支持 TTL  │     │ 持久可靠     │
│ 进程重启丢失  │     │ 跨进程共享    │     │ 支持查询分析  │
│ 单进程使用    │     │ 重启后可保留  │     │ 适合生产环境  │
│ 适合开发/测试 │     │ 适合 session  │     │ 适合审计追溯  │
└─────────────┘     └──────────────┘     └──────────────┘
</code></pre>
<p><strong>Checkpoint 与恢复</strong> 是 Stateful Agent 的核心能力。思路很直接：在每轮循环的关键节点保存一次快照，异常恢复时从最近的快照重新开始。</p>
<pre><code class="language-python">class CheckpointManager:
    def save(self, state: ExecutionState) -&gt; str:
        &quot;&quot;&quot;保存 checkpoint，返回 checkpoint_id&quot;&quot;&quot;
        snapshot = {
            &quot;state&quot;: asdict(state),
            &quot;timestamp&quot;: time.time(),
        }
        checkpoint_id = f&quot;{state.session_id}:{state.turn_count}&quot;
        self.store.set(checkpoint_id, json.dumps(snapshot))
        return checkpoint_id

    def restore(self, checkpoint_id: str) -&gt; ExecutionState:
        &quot;&quot;&quot;从 checkpoint 恢复执行状态&quot;&quot;&quot;
        snapshot = json.loads(self.store.get(checkpoint_id))
        return ExecutionState(**snapshot[&quot;state&quot;])
</code></pre>
<p>实际系统中，checkpoint 的保存频率需要权衡：</p>
<ul>
<li><strong>每轮都保存</strong>：恢复粒度最细，但写入开销大</li>
<li><strong>关键节点保存</strong>（如每次工具调用前后）：开销适中，覆盖最重要的故障场景</li>
<li><strong>定时保存</strong>：实现简单，但可能丢失最近几轮的状态</li>
</ul>
<hr>
<h2>6. 完整代码实现</h2>
<p>下面是一个最小但完整的 Agent Control Loop 实现。不依赖任何框架，仅使用 Python 标准库 + OpenAI SDK。</p>
<pre><code class="language-python">&quot;&quot;&quot;
Minimal Agent Control Loop
不依赖任何框架，纯 Python + OpenAI SDK
&quot;&quot;&quot;
import json
import time
from enum import Enum
from dataclasses import dataclass, field
from openai import OpenAI


class State(Enum):
    OBSERVE = &quot;observe&quot;
    THINK = &quot;think&quot;
    ACT = &quot;act&quot;
    REFLECT = &quot;reflect&quot;
    DONE = &quot;done&quot;
    ERROR = &quot;error&quot;


@dataclass
class LoopContext:
    messages: list[dict] = field(default_factory=list)
    turn: int = 0
    total_tokens: int = 0
    consecutive_errors: int = 0
    recent_outputs: list[str] = field(default_factory=list)


# ── Tool Registry ────────────────────────────────────

TOOL_FUNCTIONS = {}

def register_tool(name: str, description: str, parameters: dict):
    &quot;&quot;&quot;装饰器：注册工具函数及其 schema&quot;&quot;&quot;
    def decorator(fn):
        TOOL_FUNCTIONS[name] = {
            &quot;fn&quot;: fn,
            &quot;schema&quot;: {
                &quot;type&quot;: &quot;function&quot;,
                &quot;function&quot;: {
                    &quot;name&quot;: name,
                    &quot;description&quot;: description,
                    &quot;parameters&quot;: parameters,
                },
            },
        }
        return fn
    return decorator


@register_tool(
    name=&quot;get_weather&quot;,
    description=&quot;获取指定城市的当前天气&quot;,
    parameters={
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;city&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;城市名称&quot;},
        },
        &quot;required&quot;: [&quot;city&quot;],
    },
)
def get_weather(city: str) -&gt; str:
    # 示例实现，实际中调用真实 API
    return json.dumps({&quot;city&quot;: city, &quot;temp&quot;: 28, &quot;condition&quot;: &quot;晴&quot;})


# ── Agent Control Loop ───────────────────────────────

class Agent:
    def __init__(
        self,
        system_prompt: str,
        model: str = &quot;gpt-4o&quot;,
        max_turns: int = 15,
        token_budget: int = 50_000,
        max_consecutive_errors: int = 3,
    ):
        self.client = OpenAI()
        self.model = model
        self.system_prompt = system_prompt
        self.max_turns = max_turns
        self.token_budget = token_budget
        self.max_errors = max_consecutive_errors
        self.tool_schemas = [t[&quot;schema&quot;] for t in TOOL_FUNCTIONS.values()]

    def run(self, user_input: str) -&gt; str:
        ctx = LoopContext()
        ctx.messages = [
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: self.system_prompt},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input},
        ]
        state = State.THINK  # 首轮输入已就绪，直接进入 THINK

        while state not in (State.DONE, State.ERROR):
            match state:
                case State.THINK:
                    state, ctx = self._think(ctx)
                case State.ACT:
                    state, ctx = self._act(ctx)
                case State.REFLECT:
                    state, ctx = self._reflect(ctx)
            ctx.turn += 1

        # 提取最终回答
        for msg in reversed(ctx.messages):
            if msg[&quot;role&quot;] == &quot;assistant&quot; and msg.get(&quot;content&quot;):
                return msg[&quot;content&quot;]
        return &quot;[Agent finished without a final answer]&quot;

    def _think(self, ctx: LoopContext) -&gt; tuple[State, LoopContext]:
        &quot;&quot;&quot;调用 LLM 推理&quot;&quot;&quot;
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=ctx.messages,
                tools=self.tool_schemas or None,
            )
        except Exception as e:
            ctx.consecutive_errors += 1
            ctx.messages.append({
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;content&quot;: f&quot;[LLM Error] {e}&quot;,
            })
            if ctx.consecutive_errors &gt;= self.max_errors:
                return State.ERROR, ctx
            return State.THINK, ctx  # 重试

        # 记录 token 消耗
        usage = response.usage
        ctx.total_tokens += (usage.prompt_tokens + usage.completion_tokens)
        ctx.consecutive_errors = 0

        choice = response.choices[0]
        assistant_msg = choice.message.model_dump()
        ctx.messages.append(assistant_msg)

        # 决定下一状态
        if choice.message.tool_calls:
            return State.ACT, ctx
        else:
            return State.DONE, ctx

    def _act(self, ctx: LoopContext) -&gt; tuple[State, LoopContext]:
        &quot;&quot;&quot;执行工具调用&quot;&quot;&quot;
        assistant_msg = ctx.messages[-1]
        tool_calls = assistant_msg.get(&quot;tool_calls&quot;, [])

        for tc in tool_calls:
            fn_name = tc[&quot;function&quot;][&quot;name&quot;]
            fn_args = json.loads(tc[&quot;function&quot;][&quot;arguments&quot;])

            tool_entry = TOOL_FUNCTIONS.get(fn_name)
            if not tool_entry:
                result = f&quot;Error: unknown tool &#39;{fn_name}&#39;&quot;
            else:
                try:
                    result = tool_entry[&quot;fn&quot;](**fn_args)
                except Exception as e:
                    result = f&quot;Error: tool &#39;{fn_name}&#39; raised {type(e).__name__}: {e}&quot;
                    ctx.consecutive_errors += 1

            ctx.messages.append({
                &quot;role&quot;: &quot;tool&quot;,
                &quot;tool_call_id&quot;: tc[&quot;id&quot;],
                &quot;content&quot;: str(result),
            })

        return State.REFLECT, ctx

    def _reflect(self, ctx: LoopContext) -&gt; tuple[State, LoopContext]:
        &quot;&quot;&quot;反思：检查终止条件&quot;&quot;&quot;
        # 最大轮次
        if ctx.turn &gt;= self.max_turns:
            ctx.messages.append({
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;content&quot;: &quot;[Agent stopped: max turns exceeded]&quot;,
            })
            return State.ERROR, ctx

        # Token 预算
        if ctx.total_tokens &gt;= self.token_budget:
            ctx.messages.append({
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;content&quot;: &quot;[Agent stopped: token budget exceeded]&quot;,
            })
            return State.ERROR, ctx

        # 连续错误
        if ctx.consecutive_errors &gt;= self.max_errors:
            return State.ERROR, ctx

        # 死循环检测：最近 3 次输出相同
        tool_results = [
            m[&quot;content&quot;] for m in ctx.messages[-6:]
            if m.get(&quot;role&quot;) == &quot;tool&quot;
        ]
        if len(tool_results) &gt;= 3 and len(set(tool_results[-3:])) == 1:
            ctx.messages.append({
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;content&quot;: &quot;[Agent stopped: loop detected]&quot;,
            })
            return State.ERROR, ctx

        # 继续下一轮推理
        return State.THINK, ctx


# ── 使用示例 ─────────────────────────────────────────

if __name__ == &quot;__main__&quot;:
    agent = Agent(
        system_prompt=&quot;你是一个天气助手。使用 get_weather 工具回答天气问题。&quot;,
        max_turns=10,
    )
    answer = agent.run(&quot;北京今天天气怎么样？&quot;)
    print(answer)
</code></pre>
<p>这段代码约 130 行，涵盖了 Control Loop 的所有核心要素：</p>
<ul>
<li>状态机驱动的循环控制</li>
<li>工具注册与动态调用</li>
<li>LLM 异常重试</li>
<li>Token 消耗追踪</li>
<li>多种终止条件（max_turns / token_budget / consecutive_errors / loop_detected）</li>
<li>工具执行错误处理</li>
</ul>
<p>它不是生产级代码，但足以说明 Control Loop 的核心机制。在此基础上增加异步执行、状态持久化、日志追踪，就能逐步演进为生产级实现。</p>
<hr>
<h2>7. 错误处理策略</h2>
<p>生产环境中，Agent Control Loop 最常遇到的四类错误：</p>
<h3>7.1 Tool 调用失败</h3>
<p>工具调用失败是最高频的错误。正确的处理方式不是抛异常终止，而是<strong>将错误信息作为 Observation 返回给 LLM</strong>，让它决定如何应对。</p>
<pre><code class="language-python"># 错误的做法：直接终止
try:
    result = call_tool(name, args)
except Exception:
    raise  # Agent 直接崩溃

# 正确的做法：将错误反馈给 LLM
try:
    result = call_tool(name, args)
except TimeoutError:
    result = &quot;Tool timed out after 30s. Consider using different parameters.&quot;
except ValueError as e:
    result = f&quot;Invalid arguments: {e}. Please check parameter types.&quot;
except Exception as e:
    result = f&quot;Tool failed: {type(e).__name__}: {e}&quot;
</code></pre>
<p>LLM 在收到错误信息后，通常能自主修正——换一组参数重试、换一个工具、或者告知用户当前无法完成任务。</p>
<h3>7.2 LLM 返回格式异常</h3>
<p>LLM 偶尔会返回不符合预期的格式：JSON 不合法、tool_call 参数缺失、content 为空等。</p>
<pre><code class="language-python">def _parse_tool_call_safe(self, tool_call) -&gt; tuple[str, dict]:
    &quot;&quot;&quot;安全解析工具调用参数&quot;&quot;&quot;
    name = tool_call.function.name
    try:
        args = json.loads(tool_call.function.arguments)
    except json.JSONDecodeError:
        # LLM 返回了非法 JSON，尝试修复或跳过
        args = {}
        self.logger.warning(
            f&quot;Invalid JSON in tool_call arguments: &quot;
            f&quot;{tool_call.function.arguments}&quot;
        )
    return name, args
</code></pre>
<h3>7.3 超时处理</h3>
<p>整个 Agent 执行需要有全局超时，防止无限挂起：</p>
<pre><code class="language-python">import signal

class TimeoutError(Exception):
    pass

def run_with_timeout(fn, timeout_seconds: int, *args, **kwargs):
    &quot;&quot;&quot;为函数执行添加超时限制&quot;&quot;&quot;
    def handler(signum, frame):
        raise TimeoutError(f&quot;Execution timed out after {timeout_seconds}s&quot;)

    old_handler = signal.signal(signal.SIGALRM, handler)
    signal.alarm(timeout_seconds)
    try:
        return fn(*args, **kwargs)
    finally:
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)
</code></pre>
<h3>7.4 死循环检测</h3>
<p>当 Agent 陷入死循环时，它会反复执行相同的操作序列。检测策略：</p>
<pre><code class="language-python">def _detect_loop(self, messages: list[dict], window: int = 6) -&gt; bool:
    &quot;&quot;&quot;检测 Agent 是否陷入重复循环&quot;&quot;&quot;
    recent = messages[-window:]

    # 策略 1：完全重复检测
    contents = [m.get(&quot;content&quot;, &quot;&quot;) for m in recent if m[&quot;role&quot;] == &quot;assistant&quot;]
    if len(contents) &gt;= 3 and len(set(contents[-3:])) == 1:
        return True

    # 策略 2：工具调用序列重复检测
    tool_calls = []
    for m in recent:
        if m.get(&quot;tool_calls&quot;):
            for tc in m[&quot;tool_calls&quot;]:
                tool_calls.append(f&quot;{tc[&#39;function&#39;][&#39;name&#39;]}:{tc[&#39;function&#39;][&#39;arguments&#39;]}&quot;)

    if len(tool_calls) &gt;= 4:
        half = len(tool_calls) // 2
        if tool_calls[:half] == tool_calls[half:2*half]:
            return True

    return False
</code></pre>
<hr>
<h2>8. 性能考量</h2>
<h3>8.1 Token 消耗与循环次数的关系</h3>
<p>Agent Control Loop 的 Token 消耗不是线性增长，而是<strong>二次增长</strong>——因为每一轮都要携带之前所有轮次的 message history。</p>
<pre><code>轮次    新增消息 Token    累计 Context Token    本轮总消耗
1       T               S + T                S + T
2       T               S + 2T               S + 2T
3       T               S + 3T               S + 3T
...
N       T               S + NT               S + NT

总消耗 = N*S + T*(1+2+...+N) = N*S + T*N*(N+1)/2

其中 S = System Prompt Token 数，T = 平均每轮消息 Token 数
</code></pre>
<p>这意味着 <strong>10 轮的 Agent 消耗的 Token 不是 1 轮的 10 倍，而可能是 55 倍</strong>。这对成本控制至关重要。</p>
<h3>8.2 Context Window 膨胀问题</h3>
<p>随着轮次增加，Context Window 持续膨胀，导致：</p>
<ol>
<li><strong>延迟增加</strong>：LLM 推理时间与输入 Token 数正相关</li>
<li><strong>成本增加</strong>：按 Token 计费，输入越长越贵</li>
<li><strong>质量下降</strong>：过长的 Context 会导致 LLM &quot;注意力分散&quot;，关键信息被淹没（lost in the middle 问题）</li>
</ol>
<h3>8.3 消息压缩/摘要策略</h3>
<p>应对 Context Window 膨胀的核心策略：</p>
<p><strong>策略一：滑动窗口</strong></p>
<p>只保留最近 K 轮对话，丢弃更早的历史。简单粗暴但有效。</p>
<pre><code class="language-python">def _sliding_window(self, messages: list[dict], keep_last: int = 10) -&gt; list[dict]:
    system_msgs = [m for m in messages if m[&quot;role&quot;] == &quot;system&quot;]
    non_system = [m for m in messages if m[&quot;role&quot;] != &quot;system&quot;]
    return system_msgs + non_system[-keep_last:]
</code></pre>
<p><strong>策略二：摘要压缩</strong></p>
<p>当 message history 超过阈值时，用 LLM 对早期对话生成摘要，替换原始消息。</p>
<pre><code class="language-python">def _compress_history(self, messages: list[dict], threshold: int = 20) -&gt; list[dict]:
    if len(messages) &lt;= threshold:
        return messages

    # 将早期消息压缩为摘要
    early = messages[1:-threshold]  # 跳过 system prompt，保留最近的
    summary_prompt = (
        &quot;请用 3-5 句话总结以下对话的关键信息和已完成的操作：\n&quot;
        + &quot;\n&quot;.join(m.get(&quot;content&quot;, &quot;&quot;) for m in early if m.get(&quot;content&quot;))
    )

    summary = self.client.chat.completions.create(
        model=&quot;gpt-4o-mini&quot;,  # 用小模型做摘要，节省成本
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: summary_prompt}],
    ).choices[0].message.content

    return (
        [messages[0]]  # system prompt
        + [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: f&quot;[Earlier conversation summary] {summary}&quot;}]
        + messages[-threshold:]
    )
</code></pre>
<p><strong>策略三：选择性保留</strong></p>
<p>不是所有消息都同等重要。工具的原始返回值（可能非常长）通常可以只保留摘要：</p>
<pre><code class="language-python">def _trim_tool_results(self, messages: list[dict], max_len: int = 500) -&gt; list[dict]:
    &quot;&quot;&quot;截断过长的工具返回值&quot;&quot;&quot;
    trimmed = []
    for m in messages:
        if m[&quot;role&quot;] == &quot;tool&quot; and len(m.get(&quot;content&quot;, &quot;&quot;)) &gt; max_len:
            m = {**m, &quot;content&quot;: m[&quot;content&quot;][:max_len] + &quot;\n...[truncated]&quot;}
        trimmed.append(m)
    return trimmed
</code></pre>
<p><strong>三种策略的对比：</strong></p>
<table>
<thead>
<tr>
<th>策略</th>
<th>信息保留</th>
<th>实现成本</th>
<th>Token 节省</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>滑动窗口</td>
<td>低</td>
<td>极低</td>
<td>高</td>
<td>短对话、工具调用为主</td>
</tr>
<tr>
<td>摘要压缩</td>
<td>中</td>
<td>中（需要额外 LLM 调用）</td>
<td>高</td>
<td>长对话、需要历史上下文</td>
</tr>
<tr>
<td>选择性保留</td>
<td>高</td>
<td>低</td>
<td>中</td>
<td>工具返回值较大的场景</td>
</tr>
</tbody></table>
<p>实际工程中，通常<strong>组合使用</strong>：先用选择性保留截断大结果，再用滑动窗口控制总长度，在关键节点用摘要压缩保留全局上下文。</p>
<hr>
<h2>9. 小结与进一步思考</h2>
<p>本文从状态机模型出发，完整地拆解了 Agent Control Loop 的核心抽象：</p>
<ul>
<li><strong>OBSERVE</strong> 负责输入归一化——将各种来源的信息统一为 LLM 可理解的 message 格式</li>
<li><strong>THINK</strong> 是核心推理阶段——管理 Context Window、控制 Token 预算、解析 LLM 输出</li>
<li><strong>ACT</strong> 是执行层——处理工具调用的同步/异步执行、超时控制、安全隔离</li>
<li><strong>REFLECT</strong> 负责质量评估——决定是继续、重试还是终止</li>
<li><strong>终止条件</strong>是成本和安全的兜底——max_turns、token_budget、error_threshold、loop_detection</li>
</ul>
<p>我们对比了 ReAct 和 Plan-then-Execute 两种主流模式，分析了 Stateless 与 Stateful 两种状态管理策略，并实现了一个不依赖任何框架的完整 Control Loop。</p>
<p>但控制循环只是 Agent 运行时的骨架。它的灵魂在于 <strong>Tool Calling</strong>——正是工具让 Agent 从&quot;能说会道的语言模型&quot;变成&quot;能做事的智能体&quot;。</p>
<p>在下一篇 <strong>《Tool Calling Deep Dive: 让 LLM 成为可编程接口》</strong> 中，我们会深入工具调用的设计哲学：JSON Schema 作为契约、Tool Registry 的实现、参数校验、错误传播，以及 Structured Output 为什么优于自由文本。</p>
<p>留几个值得进一步思考的问题：</p>
<ol>
<li><strong>Control Loop 的嵌套</strong>：当一个 Agent 的工具是另一个 Agent 时，控制循环如何嵌套？外层循环和内层循环的终止条件如何协调？</li>
<li><strong>人机协作中的循环</strong>：如何在 Control Loop 中优雅地插入人类审批节点？这和 Stateful Agent 的 checkpoint 机制有什么关系？</li>
<li><strong>流式输出与控制循环</strong>：当 Agent 需要边思考边输出（streaming）时，状态机模型还适用吗？需要做哪些调整？</li>
<li><strong>多模态输入的归一化</strong>：当 OBSERVE 阶段接收的不只是文本，还有图片、音频、视频时，输入归一化策略如何演化？</li>
</ol>
<hr>
<blockquote>
<p><strong>系列导航</strong>：本文是 Agentic 系列的第 04 篇。</p>
<ul>
<li>上一篇：<a href="/blog/engineering/agentic/03-Agent%20vs%20Workflow%20vs%20Automation">03 | Agent vs Workflow vs Automation</a></li>
<li>下一篇：<a href="/blog/engineering/agentic/05-Tool%20Calling%20Deep%20Dive">05 | Tool Calling Deep Dive</a></li>
<li>完整目录：<a href="/blog/engineering/agentic/01-From%20LLM%20to%20Agent">01 | From LLM to Agent</a></li>
</ul>
</blockquote>
17:T5c64,<blockquote>
<p>微服务架构已经成为互联网后端系统的主流架构范式。然而，从单体架构迁移到微服务，绝不仅仅是把代码拆成几个服务那么简单——它涉及服务如何注册与发现、如何通信与容错、如何部署与监控等一系列基础设施问题。本文从架构设计的核心关注点出发，结合业界最佳实践，系统性地梳理微服务架构落地所需的技术体系。</p>
</blockquote>
<h2>微服务架构概览</h2>
<h3>什么是微服务架构？</h3>
<p>与单体（Monolithic）架构不同，微服务架构是由一系列<strong>职责单一的细粒度服务</strong>构成的分布式网状结构，服务之间通过轻量级机制进行通信。这种架构带来了独立部署、技术异构、弹性伸缩等优势，但同时也引入了一系列新的技术挑战。</p>
<h3>核心技术关注点</h3>
<p>一个完整的微服务架构需要关注以下层面：</p>
<table>
<thead>
<tr>
<th>层面</th>
<th>关注点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>通信</strong></td>
<td>服务注册与发现、负载均衡、RPC 框架、API 网关</td>
</tr>
<tr>
<td><strong>可靠性</strong></td>
<td>服务容错（熔断、隔离、限流、降级）</td>
</tr>
<tr>
<td><strong>基础设施</strong></td>
<td>配置中心、缓存、消息队列、数据库</td>
</tr>
<tr>
<td><strong>交付</strong></td>
<td>CI/CD 流水线、自动化测试、灰度发布</td>
</tr>
<tr>
<td><strong>可观测性</strong></td>
<td>日志系统、监控告警、链路追踪</td>
</tr>
<tr>
<td><strong>部署</strong></td>
<td>负载均衡、DNS、CDN</td>
</tr>
</tbody></table>
<p>接下来，我们逐一展开讨论。</p>
<h2>服务注册、发现与负载均衡</h2>
<p>微服务架构下，服务提供方需要注册通告服务地址，服务调用方需要发现目标服务，同时服务提供方一般以集群方式提供服务，这就引入了负载均衡和健康检查问题。</p>
<p>根据负载均衡器（LB）所在位置的不同，目前主要有三种方案：</p>
<h3>方案一：集中式 LB</h3>
<p>在服务消费者和服务提供者之间设置独立的 LB（如 F5 硬件或 LVS/HAProxy 软件），LB 上有所有服务的地址映射表，由运维配置注册。服务消费方通过 DNS 域名指向 LB。</p>
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>实现简单，当前业界主流</td>
<td>单点问题，LB 容易成为瓶颈</td>
</tr>
<tr>
<td>易于做集中式访问控制</td>
<td>增加一跳（hop），有性能开销</td>
</tr>
<tr>
<td></td>
<td>一旦 LB 故障，影响是灾难性的</td>
</tr>
</tbody></table>
<h3>方案二：进程内 LB（客户端负载）</h3>
<p>将 LB 功能以库的形式集成到服务消费方进程内，也称为<strong>软负载（Soft Load Balancing）</strong>。需要配合服务注册表（Service Registry）支持服务自注册和自发现。</p>
<p>工作原理：</p>
<ol>
<li>服务提供方启动时，将地址注册到服务注册表，并定期发送心跳</li>
<li>服务消费方通过内置 LB 组件查询注册表，缓存并定期刷新目标地址列表</li>
<li>以某种负载均衡策略选择目标地址，直接发起请求</li>
</ol>
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>分布式方案，无单点问题</td>
<td>多语言栈需开发多种客户端库</td>
</tr>
<tr>
<td>服务间直接调用，性能好</td>
<td>客户端库升级需服务方重新发布</td>
</tr>
</tbody></table>
<p>典型案例：Netflix OSS（Eureka + Ribbon + Karyon）、阿里 Dubbo。</p>
<h3>方案三：主机独立 LB 进程（Sidecar 模式）</h3>
<p>将 LB 和服务发现功能从进程内移出，变成主机上的独立进程。同一主机上的多个服务共享该 LB 进程完成服务发现和负载均衡。</p>
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>无单点，一个 LB 挂只影响该主机</td>
<td>部署较复杂，环节多</td>
</tr>
<tr>
<td>不需要为不同语言开发客户端库</td>
<td>出错调试排查不方便</td>
</tr>
<tr>
<td>LB 升级不需要服务方改代码</td>
<td></td>
</tr>
</tbody></table>
<p>典型案例：Airbnb SmartStack（Zookeeper + Nerve + Synapse/HAProxy）、Kubernetes 内部服务发现。</p>
<blockquote>
<p>三种方案各有取舍，选择时需要综合考虑团队技术栈的多样性、运维能力和性能要求。当前趋势是方案三（Sidecar 模式）逐渐演化为 Service Mesh（服务网格），如 Istio + Envoy。</p>
</blockquote>
<h2>API 网关（Service Gateway）</h2>
<p>微服务最终需要以某种方式暴露给外部系统访问，这就需要<strong>服务网关</strong>。网关是连接企业内部和外部系统的一道门，承担以下关键职责：</p>
<table>
<thead>
<tr>
<th>职责</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>反向路由</strong></td>
<td>将外部请求路由到内部具体的微服务，对外呈现统一入口</td>
</tr>
<tr>
<td><strong>安全认证</strong></td>
<td>集中处理用户认证、授权和防爬虫</td>
</tr>
<tr>
<td><strong>限流容错</strong></td>
<td>流量高峰期限流保护后台，内部故障时集中容错</td>
</tr>
<tr>
<td><strong>监控</strong></td>
<td>集中监控访问量、调用延迟、错误计数</td>
</tr>
<tr>
<td><strong>日志</strong></td>
<td>收集所有访问日志，为后续分析提供数据</td>
</tr>
</tbody></table>
<p>除此之外，网关还可以实现<strong>线上引流、线上压测、金丝雀发布（Canary Testing）、数据中心双活</strong>等高级功能。</p>
<h3>微服务的分层架构</h3>
<p>引入网关和服务注册表之后，微服务可以简化为两层结构：</p>
<ul>
<li><strong>后端通用服务（Middle Tier Service）</strong>：启动时注册地址到注册表</li>
<li><strong>前端边缘服务（Edge Service）</strong>：查询注册表发现后端服务，对后端服务做聚合和裁剪后暴露给外部设备</li>
</ul>
<p>网关通过查询注册表将外部请求路由到前端服务，整个微服务体系的自注册、自发现和软路由就此串联起来。如果用设计模式的视角看——<strong>网关类似 Proxy/Facade 模式，服务注册表类似 IoC 依赖注入模式</strong>。</p>
<p>常见的网关组件：Netflix Zuul、Kong、APISIX、Spring Cloud Gateway。</p>
<h2>服务容错</h2>
<p>当企业微服务化后，服务之间存在错综复杂的依赖关系。一个前端请求一般依赖多个后端服务（1→N 扇出）。在生产环境中，如果一个应用不能对其依赖的故障进行容错和隔离，就面临被拖垮的风险。在高流量场景下，某个单一后端一旦发生延迟，可能在数秒内导致所有应用资源（线程、队列等）被耗尽，造成<strong>雪崩效应（Cascading Failure）</strong>。</p>
<p>业界总结出以下核心容错模式：</p>
<h3>熔断器模式（Circuit Breaker）</h3>
<p>原理类似家用电路熔断器。当目标服务慢或大量超时时，调用方主动熔断，防止服务被进一步拖垮。</p>
<p>熔断器有三种状态：</p>
<pre><code>Closed（正常）→ Open（熔断）→ Half-Open（半熔断）→ Closed/Open
</code></pre>
<ul>
<li><strong>Closed</strong>：正常状态，请求正常通过</li>
<li><strong>Open</strong>：调用持续出错或超时，进入熔断状态，后续请求直接拒绝（Fail Fast）</li>
<li><strong>Half-Open</strong>：一段时间后允许少量请求尝试，成功则恢复，失败则继续熔断</li>
</ul>
<h3>舱壁隔离模式（Bulkhead Isolation）</h3>
<p>像船舱一样对资源进行隔离。典型实现是<strong>线程隔离</strong>：假定应用 A 调用 Svc1/Svc2/Svc3 三个服务，容器共有 120 个工作线程，可以给每个服务各分配 40 个线程。当 Svc2 变慢时，只有分配给 Svc2 的 40 个线程被耗尽，Svc1 和 Svc3 的 80 个线程不受影响。</p>
<h3>限流（Rate Limiting）</h3>
<p>对服务限定并发访问量，比如单位时间只允许 100 个并发调用，超过限制的请求拒绝并回退。没有限流机制的服务在突发流量（秒杀、大促）时极易被冲垮。</p>
<h3>降级回退（Fallback）</h3>
<p>当熔断或限流发生时的后续处理策略：</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Fail Fast</td>
<td>直接抛出异常</td>
</tr>
<tr>
<td>返回缺省值</td>
<td>返回空值或默认数据</td>
</tr>
<tr>
<td>备份服务</td>
<td>从备份数据源获取数据</td>
</tr>
</tbody></table>
<blockquote>
<p>Netflix 将上述容错模式集成到 Hystrix 开源组件中（现已进入维护模式，社区推荐 Resilience4j 或 Sentinel 作为替代）。Spring Cloud Circuit Breaker 提供了统一的抽象层。</p>
</blockquote>
<h2>服务框架的核心能力</h2>
<p>微服务化后，为了让业务开发人员专注于业务逻辑，避免冗余和重复劳动，需要将公共关注点推到框架层面。一个成熟的服务框架应当封装以下能力：</p>
<table>
<thead>
<tr>
<th>能力</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>服务注册发现</td>
<td>服务端自注册，客户端自发现和负载均衡</td>
</tr>
<tr>
<td>监控日志</td>
<td>框架层日志、Metrics、调用链数据的记录和暴露</td>
</tr>
<tr>
<td>REST/RPC 与序列化</td>
<td>支持 HTTP/REST 和 Binary/RPC，可定制序列化（JSON/Protobuf 等）</td>
</tr>
<tr>
<td>动态配置</td>
<td>运行时动态调整参数和配置</td>
</tr>
<tr>
<td>限流容错</td>
<td>集成限流和熔断组件，结合动态配置实现动态限流</td>
</tr>
<tr>
<td>管理接口</td>
<td>在线查看和动态调整框架及服务内部状态（如 Spring Boot Actuator）</td>
</tr>
<tr>
<td>统一错误处理</td>
<td>框架层统一处理异常并记录日志</td>
</tr>
<tr>
<td>安全</td>
<td>访问控制逻辑的插件化封装</td>
</tr>
<tr>
<td>文档自动生成</td>
<td>如 Swagger/OpenAPI 的自动化文档方案</td>
</tr>
</tbody></table>
<p>当前业界成熟的微服务框架有：Spring Cloud/Spring Boot、Apache Dubbo、Go-Micro、gRPC 等。</p>
<h2>基础设施选型</h2>
<h3>RPC 框架选型</h3>
<p>RPC（Remote Procedure Call）框架大致分为两大流派：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>代表框架</th>
<th>特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>跨语言调用型</strong></td>
<td>gRPC、Thrift、Hprose</td>
<td>支持多语言调用，无服务治理机制</td>
<td>多语言调用场景</td>
</tr>
<tr>
<td><strong>服务治理型</strong></td>
<td>Dubbo、Motan、rpcx</td>
<td>功能丰富，含服务发现和治理能力</td>
<td>大型服务的解耦和治理</td>
</tr>
</tbody></table>
<p><strong>选型建议</strong>：如果是 Java 为主的团队，推荐 <strong>Dubbo</strong>（高性能，性能测试中比 Feign 强约 10 倍）。如果需要跨语言支持，Dubbo 也支持通过 Dubbo-Go 实现 Java + Go 双语言微服务架构。如果是纯粹的跨语言场景，<strong>gRPC</strong> 基于 HTTP/2 + Protobuf，是业界标准选择。</p>
<h3>注册中心选型</h3>
<p>所有的服务发现都依赖于一个高可用的服务注册表。主流选择：</p>
<table>
<thead>
<tr>
<th>注册中心</th>
<th>特点</th>
<th>一致性模型</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Nacos</strong></td>
<td>同时支持注册中心和配置中心，功能全面</td>
<td>AP/CP 可切换</td>
</tr>
<tr>
<td><strong>ZooKeeper</strong></td>
<td>最早的分布式协调服务，生态成熟</td>
<td>CP</td>
</tr>
<tr>
<td><strong>Etcd</strong></td>
<td>Kubernetes 默认存储，高可用和一致性</td>
<td>CP</td>
</tr>
<tr>
<td><strong>Consul</strong></td>
<td>支持多数据中心，内置健康检查</td>
<td>CP</td>
</tr>
<tr>
<td><strong>Eureka</strong></td>
<td>Netflix 开源，AP 模型，已停止维护</td>
<td>AP</td>
</tr>
</tbody></table>
<p><strong>选型建议</strong>：推荐 <strong>Nacos</strong>（nacos + MySQL 高可用部署），一站式解决注册中心和配置中心的需求。</p>
<h3>配置中心选型</h3>
<p>随着系统复杂度增长，配置管理面临越来越高的要求：配置修改实时生效、灰度发布、分环境/分集群管理、完善的权限审核机制。传统的配置文件方式已经无法满足需求。</p>
<p>配置中心的核心架构组件：</p>
<ul>
<li><strong>配置服务端</strong>：集中存储和管理所有配置信息</li>
<li><strong>配置客户端</strong>：通过<strong>定期拉取（Pull）</strong> 或 <strong>服务端推送（Push）</strong> 方式获取配置更新</li>
<li><strong>管理界面</strong>：配置的增删改查和审计</li>
</ul>
<table>
<thead>
<tr>
<th>配置中心</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Nacos</strong></td>
<td>阿里开源，同时支持注册和配置，生态活跃</td>
</tr>
<tr>
<td><strong>Apollo</strong></td>
<td>携程开源，功能完善，支持灰度发布和权限管理</td>
</tr>
<tr>
<td><strong>Spring Cloud Config</strong></td>
<td>Spring 生态原生支持，基于 Git 存储</td>
</tr>
</tbody></table>
<h3>缓存中间件选型</h3>
<table>
<thead>
<tr>
<th>缓存</th>
<th>特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Redis</strong></td>
<td>多数据结构，支持持久化和集群</td>
<td>通用缓存、分布式锁、排行榜等</td>
</tr>
<tr>
<td><strong>Memcached</strong></td>
<td>纯内存 KV，简单高效</td>
<td>简单的对象缓存</td>
</tr>
</tbody></table>
<p><strong>选型建议</strong>：推荐 <strong>Redis Cluster</strong> 高可用集群部署。</p>
<blockquote>
<p>需要特别关注 Redis 的 Big Key 问题。在高并发场景下，Big Key 会导致单个节点内存和网络带宽瓶颈，严重时可造成系统瘫痪。建议制定 Key 规范并定期扫描。</p>
</blockquote>
<h3>消息中间件选型</h3>
<p>消息中间件的三大核心场景：</p>
<table>
<thead>
<tr>
<th>场景</th>
<th>说明</th>
<th>典型案例</th>
</tr>
</thead>
<tbody><tr>
<td><strong>异步处理</strong></td>
<td>减少主流程等待时间，非核心逻辑异步执行</td>
<td>注册后发送邮件、异步更新缓存</td>
</tr>
<tr>
<td><strong>系统解耦</strong></td>
<td>上下游系统通过消息通信，不需要强一致</td>
<td>支付成功后通知 ERP/WMS/推荐等系统</td>
</tr>
<tr>
<td><strong>削峰填谷</strong></td>
<td>大流量请求放入队列，消费者按能力消化</td>
<td>秒杀系统的下单排队</td>
</tr>
</tbody></table>
<p>主流消息中间件对比：</p>
<table>
<thead>
<tr>
<th>中间件</th>
<th>吞吐量</th>
<th>延迟</th>
<th>可靠性</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Kafka</strong></td>
<td>极高</td>
<td>毫秒级</td>
<td>高（可配置）</td>
<td>日志收集、大数据流处理、事件溯源</td>
</tr>
<tr>
<td><strong>RocketMQ</strong></td>
<td>高</td>
<td>毫秒级</td>
<td>极高（事务消息）</td>
<td>电商交易、金融场景</td>
</tr>
<tr>
<td><strong>RabbitMQ</strong></td>
<td>中等</td>
<td>微秒级</td>
<td>高</td>
<td>实时性要求高、路由复杂的场景</td>
</tr>
</tbody></table>
<p><strong>选型建议</strong>：<strong>Kafka</strong> 用于日志采集和大数据场景，<strong>RocketMQ</strong> 用于业务消息和交易场景，二者搭配使用。</p>
<h3>数据库选型</h3>
<h4>关系型数据库</h4>
<table>
<thead>
<tr>
<th>类别</th>
<th>代表</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>传统 RDBMS</strong></td>
<td>MySQL、PostgreSQL</td>
<td>成熟稳定，生态丰富，百万级 PV 搭配主从 + 缓存可满足</td>
</tr>
<tr>
<td><strong>NewSQL</strong></td>
<td>TiDB、CockroachDB</td>
<td>完整 SQL 支持 + ACID 事务 + 弹性伸缩 + 高可用 + 大数据分析能力</td>
</tr>
</tbody></table>
<p>当 MySQL 需要分库分表且逻辑复杂度高、扩展性不足时，可以考虑 TiDB。</p>
<h4>NoSQL 数据库</h4>
<table>
<thead>
<tr>
<th>类型</th>
<th>代表</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>键值型</strong></td>
<td>Redis、Memcache</td>
<td>缓存、会话管理</td>
</tr>
<tr>
<td><strong>列式</strong></td>
<td>HBase、Cassandra</td>
<td>写多读少、时序数据</td>
</tr>
<tr>
<td><strong>文档型</strong></td>
<td>MongoDB、CouchDB</td>
<td>非结构化数据、灵活 Schema</td>
</tr>
<tr>
<td><strong>图数据库</strong></td>
<td>Neo4J</td>
<td>社交网络、推荐系统</td>
</tr>
</tbody></table>
<h2>CI/CD 流水线</h2>
<p>从代码到最终服务用户，可以分为三个阶段：</p>
<pre><code>Code → Artifact（制品库）→ Running Service → Production
</code></pre>
<ol>
<li><strong>代码到制品</strong>：持续构建，制品集中管理</li>
<li><strong>制品到服务</strong>：部署到指定环境</li>
<li><strong>开发到生产</strong>：变更在不同环境间的迁移和灰度发布</li>
</ol>
<h3>工具链推荐</h3>
<table>
<thead>
<tr>
<th>环节</th>
<th>推荐工具</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>代码管理</strong></td>
<td>GitLab</td>
<td>社区版功能丰富，结合 Gerrit 做 Code Review</td>
</tr>
<tr>
<td><strong>持续集成</strong></td>
<td>Jenkins / GitLab CI</td>
<td>Jenkins 插件生态强大；GitLab CI 与 GitLab 深度集成</td>
</tr>
<tr>
<td><strong>制品仓库</strong></td>
<td>Harbor</td>
<td>开源的 Docker 镜像仓库，支持镜像签名和漏洞扫描</td>
</tr>
<tr>
<td><strong>部署编排</strong></td>
<td>Kubernetes</td>
<td>容器编排的事实标准，支持声明式部署和自动伸缩</td>
</tr>
<tr>
<td><strong>项目管理</strong></td>
<td>Jira + Confluence</td>
<td>项目管理、任务跟踪和知识管理的行业标配</td>
</tr>
</tbody></table>
<p><strong>初期建议</strong>：Jenkins + GitLab + Harbor 的组合，可以覆盖制品管理、发布流程、权限控制、版本变更和服务回滚。</p>
<h3>自动化测试</h3>
<p>自动化测试平台是 CI/CD 流水线的重要一环：</p>
<ul>
<li><strong>单元测试</strong>：JUnit / TestNG，覆盖核心业务逻辑</li>
<li><strong>接口测试</strong>：可基于开源框架（如 SpringBoot + TestNG）搭建</li>
<li><strong>性能测试</strong>：JMeter / Gatling</li>
<li><strong>端到端测试</strong>：Selenium / Cypress</li>
</ul>
<h2>可观测性体系</h2>
<h3>日志系统</h3>
<p>日志系统涵盖日志打印、采集、中转、存储、分析、搜索和分发。日志系统的建设不仅是工具建设，还包括规范和组件建设——基本的日志（如全链路追踪 ID）应在框架和组件层面统一注入。</p>
<p><strong>常规方案：ELK Stack</strong></p>
<table>
<thead>
<tr>
<th>组件</th>
<th>职责</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Filebeat</strong></td>
<td>轻量级日志采集器，替代 Logstash-Forwarder</td>
</tr>
<tr>
<td><strong>Logstash</strong></td>
<td>日志收集、过滤和转换</td>
</tr>
<tr>
<td><strong>Elasticsearch</strong></td>
<td>分布式搜索引擎，存储和索引日志</td>
</tr>
<tr>
<td><strong>Kibana</strong></td>
<td>可视化界面，日志搜索和分析</td>
</tr>
</tbody></table>
<blockquote>
<p>免费版 ELK 没有安全机制，建议前置 Nginx 做反向代理和简单用户认证。</p>
</blockquote>
<p><strong>实时计算方案</strong>：对于需要实时分析的场景，可以采用 Flume + Kafka + Flink（或 Storm）的架构。Kafka 负责高吞吐的消息缓冲，Flume 负责多样化的数据采集，Flink 负责实时流计算。</p>
<h3>监控系统</h3>
<p>监控系统主要覆盖两个层面：</p>
<table>
<thead>
<tr>
<th>层面</th>
<th>监控指标</th>
</tr>
</thead>
<tbody><tr>
<td><strong>基础设施</strong></td>
<td>机器负载、IO、网络流量、CPU、内存</td>
</tr>
<tr>
<td><strong>服务质量</strong></td>
<td>可用性、成功率、失败率、QPS、延迟</td>
</tr>
</tbody></table>
<p><strong>推荐方案：Prometheus + Grafana</strong></p>
<p>Prometheus 是 Google BorgMon 的开源版本，使用 Go 开发，采用 <strong>Pull</strong> 模式主动拉取指标数据。其核心组件：</p>
<table>
<thead>
<tr>
<th>组件</th>
<th>职责</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Prometheus Server</strong></td>
<td>数据采集和存储，提供 PromQL 查询</td>
</tr>
<tr>
<td><strong>Exporter</strong></td>
<td>各类数据采集组件（数据库、硬件、MQ、HTTP 服务器等）</td>
</tr>
<tr>
<td><strong>Push Gateway</strong></td>
<td>支持短生命周期 Job 主动推送指标</td>
</tr>
<tr>
<td><strong>Alertmanager</strong></td>
<td>灵活的报警规则和通知管理</td>
</tr>
<tr>
<td><strong>Grafana</strong></td>
<td>高度定制化的可视化监控面板</td>
</tr>
</tbody></table>
<p>Prometheus + Grafana 搭配统一的服务框架，可以满足绝大部分中小团队的监控需求。</p>
<h2>生产环境部署架构</h2>
<h3>DNS</h3>
<p>DNS 是基础服务，一般直接选择云厂商：</p>
<ul>
<li><strong>国内</strong>：阿里云 DNS 或腾讯 DNSPod，线上产品建议使用付费版</li>
<li><strong>海外</strong>：优先选择 AWS Route 53</li>
<li><strong>国内外互通</strong>：建议在 APP 层实现容灾逻辑或智能调度，因为没有单一 DNS 服务能同时很好地覆盖国内外</li>
</ul>
<h3>负载均衡（LB）</h3>
<table>
<thead>
<tr>
<th>场景</th>
<th>方案</th>
</tr>
</thead>
<tbody><tr>
<td>云服务环境</td>
<td>直接使用云厂商 LB（阿里云 SLB / 腾讯云 CLB / AWS ELB）</td>
</tr>
<tr>
<td>自建机房</td>
<td>LVS（四层）+ Nginx（七层）</td>
</tr>
</tbody></table>
<p>云厂商 LB 通常支持四层（TCP/UDP）和七层（HTTP/HTTPS）协议、集中化证书管理和健康检查。</p>
<h3>CDN</h3>
<p>CDN 的选型主要看业务覆盖区域：</p>
<table>
<thead>
<tr>
<th>区域</th>
<th>推荐</th>
</tr>
</thead>
<tbody><tr>
<td>国内</td>
<td>阿里云 CDN、腾讯云 CDN</td>
</tr>
<tr>
<td>海外</td>
<td>AWS CloudFront、Akamai</td>
</tr>
</tbody></table>
<h2>总结</h2>
<p>微服务架构的落地是一个系统工程，核心技术关注点可以归纳为以下几个层面：</p>
<ol>
<li><strong>服务通信</strong>：通过注册中心 + 负载均衡 + API 网关，构建服务间和内外部的通信体系</li>
<li><strong>服务可靠性</strong>：通过熔断、隔离、限流和降级四大模式，保障系统在故障和高峰期的稳定性</li>
<li><strong>服务框架</strong>：将公共关注点下沉到框架层，让业务开发专注于业务逻辑</li>
<li><strong>基础设施</strong>：根据业务需求和团队技术栈，选择合适的 RPC、注册中心、缓存、消息队列和数据库</li>
<li><strong>持续交付</strong>：通过 CI/CD 流水线实现代码到生产环境的自动化、可重复的发布流程</li>
<li><strong>可观测性</strong>：通过日志、监控和链路追踪构建系统的透明度，为问题排查和性能优化提供数据支撑</li>
</ol>
<p>好的架构不是设计出来的，而是演进出来的。架构师需要在不同阶段做出合适的判断——既不过度设计，也不欠缺考虑。关键是保持对技术的敏锐度，在实践中不断验证和调整。</p>
<blockquote>
<p>路漫漫其修远兮，架构求索无止尽也。</p>
</blockquote>
18:T602d,<h1>高并发系统设计：原理、策略与工程实践</h1>
<blockquote>
<p>高并发不是一个单点问题，而是一个系统性工程。它要求在计算、存储、网络、容错等多个维度协同设计，在吞吐量、延迟、一致性、可用性之间做出精确的权衡。</p>
</blockquote>
<p>高并发系统的本质目标是：<strong>在保证系统整体可用的前提下，最大化单位时间内的请求处理能力</strong>。这涉及两个核心指标——<strong>吞吐量</strong>（TPS/QPS）和<strong>响应延迟</strong>（Latency），以及一个隐含约束——<strong>资源成本</strong>。</p>
<p>本文将高并发设计策略按作用层次分为四大类，逐一分析每种策略的底层原理、适用场景与决策依据。</p>
<h2>一、计算层：提升处理能力</h2>
<p>计算层的核心矛盾是<strong>单节点处理能力有限</strong>。解决思路有两条：纵向压榨单机性能，横向扩展节点数量。</p>
<h3>1.1 水平扩展</h3>
<p><strong>原理</strong>：将请求分散到多个对等节点并行处理，系统吞吐量随节点数近线性增长。</p>
<p>水平扩展是高并发的第一性原理——当单机无法承载时，加机器是最直接的手段。但前提是系统必须具备<strong>无状态性</strong>，否则扩展只是增加复杂度。</p>
<table>
<thead>
<tr>
<th>条件</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>无状态服务</td>
<td>请求可被任意节点处理，不依赖本地状态</td>
</tr>
<tr>
<td>负载均衡</td>
<td>流量均匀分配到各节点（轮询、加权、一致性哈希）</td>
</tr>
<tr>
<td>服务发现</td>
<td>新增/下线节点时自动感知</td>
</tr>
</tbody></table>
<p><strong>决策要点</strong>：</p>
<ul>
<li>水平扩展的收益存在拐点。当瓶颈不在计算层（如数据库连接数耗尽），加应用节点无法提升吞吐</li>
<li>扩展前先确认瓶颈位置：CPU 密集型看计算节点数，I/O 密集型看下游容量</li>
</ul>
<h3>1.2 服务拆分</h3>
<p><strong>原理</strong>：将单体应用按业务域拆分为独立服务，每个服务独立部署、独立扩展，使资源投放更精准。</p>
<p>服务拆分的高并发价值不在于&quot;拆&quot;本身，而在于<strong>差异化扩展</strong>——热点服务可以单独扩容，而不必整体扩展。</p>
<pre><code>单体应用：所有模块共享资源池
  → 商品查询 QPS 暴涨时，订单、支付模块的资源也被占用

服务拆分后：
  → 商品服务独立扩容 10 倍，订单服务保持不变
  → 资源利用率提升，扩容成本下降
</code></pre>
<p><strong>决策要点</strong>：</p>
<ul>
<li>拆分粒度不是越细越好。过度拆分导致服务间调用链路变长，网络开销和故障概率增加</li>
<li>拆分的依据是<strong>业务边界</strong>和<strong>扩展需求</strong>，而非代码量</li>
</ul>
<h3>1.3 异步化</h3>
<p><strong>原理</strong>：将同步阻塞调用转为异步非阻塞，释放线程资源去处理更多请求，从而提升单位时间内的吞吐量。</p>
<p>同步模型下，线程在等待下游响应期间处于阻塞状态，无法处理新请求。异步化的本质是<strong>把等待时间转化为处理能力</strong>。</p>
<table>
<thead>
<tr>
<th>异步方式</th>
<th>机制</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>消息队列</td>
<td>请求写入 MQ 后立即返回，消费者异步处理</td>
<td>非实时性业务（通知、日志、数据同步）</td>
</tr>
<tr>
<td>异步 I/O</td>
<td>NIO / Reactor 模型</td>
<td>高并发网络通信（Netty、WebFlux）</td>
</tr>
<tr>
<td>并行调用</td>
<td>CompletableFuture / 协程</td>
<td>多个独立下游调用并行执行</td>
</tr>
<tr>
<td>事件驱动</td>
<td>发布-订阅模式</td>
<td>服务间解耦</td>
</tr>
</tbody></table>
<p><strong>决策要点</strong>：</p>
<ul>
<li>异步化的前提是业务允许<strong>延迟处理</strong>。对于实时性要求高的链路（如支付扣款），不宜异步</li>
<li>引入异步后需要处理<strong>结果通知</strong>（回调、轮询）和<strong>失败重试</strong>，系统复杂度会上升</li>
<li>消息队列的削峰价值：瞬时 5000 QPS 的流量冲击，系统处理能力 2000 QPS，MQ 作为缓冲区，将超出部分排队处理，避免系统过载</li>
</ul>
<h3>1.4 池化</h3>
<p><strong>原理</strong>：预先创建并复用昂贵资源（连接、线程、对象），避免频繁创建/销毁带来的开销。</p>
<p>每次创建数据库连接需要 TCP 三次握手 + 认证，耗时通常在毫秒级。在高并发场景下，这些开销会被放大数百倍。</p>
<table>
<thead>
<tr>
<th>池化类型</th>
<th>复用的资源</th>
<th>关键参数</th>
</tr>
</thead>
<tbody><tr>
<td>数据库连接池</td>
<td>TCP 连接 + 认证会话</td>
<td>最大连接数、最小空闲数、获取超时</td>
</tr>
<tr>
<td>HTTP 连接池</td>
<td>TCP 连接（Keep-Alive）</td>
<td>最大连接数、每路由最大连接数</td>
</tr>
<tr>
<td>线程池</td>
<td>线程</td>
<td>核心线程数、最大线程数、队列长度、拒绝策略</td>
</tr>
<tr>
<td>对象池</td>
<td>重量级对象（如序列化器）</td>
<td>池大小、借出超时</td>
</tr>
</tbody></table>
<p><strong>最佳实践</strong>：</p>
<ul>
<li>连接池大小不是越大越好。过多连接会导致数据库端线程竞争加剧，反而降低性能。PostgreSQL 官方建议的公式：<code>连接数 = ((核心数 * 2) + 有效磁盘数)</code></li>
<li>线程池的队列策略直接影响系统行为：无界队列可能导致 OOM，有界队列需要配合合理的拒绝策略</li>
</ul>
<h2>二、数据层：突破存储瓶颈</h2>
<p>高并发系统中，数据库通常是第一个到达瓶颈的组件。数据层优化的核心思路是<strong>减少对数据库的直接访问</strong>和<strong>提升数据库本身的承载能力</strong>。</p>
<h3>2.1 缓存</h3>
<p><strong>原理</strong>：将热点数据存储在访问速度更快的介质中（内存），减少对慢速存储（磁盘数据库）的访问。</p>
<p>缓存是高并发系统中 ROI 最高的优化手段。一次 Redis 查询耗时约 0.5ms，一次 MySQL 查询耗时约 5<del>50ms，性能差距在 10</del>100 倍。</p>
<p><strong>多级缓存架构</strong>：</p>
<pre><code>请求 → L1 本地缓存（Caffeine）    命中率 ~60%
     → L2 分布式缓存（Redis）      命中率 ~95%
     → L3 数据库（MySQL）          兜底查询
</code></pre>
<p>每一层拦截掉大部分请求，最终到达数据库的流量可能不到总量的 5%。</p>
<p><strong>缓存三大问题及应对</strong>：</p>
<table>
<thead>
<tr>
<th>问题</th>
<th>成因</th>
<th>解决方案</th>
</tr>
</thead>
<tbody><tr>
<td><strong>穿透</strong></td>
<td>查询不存在的 Key，每次都打到 DB</td>
<td>布隆过滤器拦截；空值缓存（TTL 设短）</td>
</tr>
<tr>
<td><strong>击穿</strong></td>
<td>热点 Key 过期瞬间，大量请求涌入 DB</td>
<td>互斥锁重建；逻辑过期 + 异步刷新</td>
</tr>
<tr>
<td><strong>雪崩</strong></td>
<td>大批 Key 同时过期</td>
<td>过期时间加随机偏移；多级缓存兜底</td>
</tr>
</tbody></table>
<p><strong>决策要点</strong>：</p>
<ul>
<li>缓存适用于<strong>读多写少</strong>的场景。写频繁的数据缓存命中率低，且一致性维护成本高</li>
<li>缓存与数据库的一致性没有完美方案。常用策略是<strong>Cache Aside（旁路缓存）</strong>：读时先查缓存，miss 则查 DB 并回填；写时先更新 DB，再删除缓存</li>
<li>本地缓存适合体积小、变化少、一致性要求低的数据（如配置信息）；分布式缓存适合体积大、需要跨节点共享的数据</li>
</ul>
<h3>2.2 读写分离</h3>
<p><strong>原理</strong>：将数据库的读写流量分离到不同实例，主库承担写操作，从库承担读操作，利用数据复制实现读能力的水平扩展。</p>
<p>大多数业务系统的读写比在 7:3 到 9:1 之间。读写分离的本质是<strong>用廉价的从库分担主库的读压力</strong>。</p>
<pre><code>写请求 → 主库（Master）
                ↓ Binlog 复制
读请求 → 从库 1 / 从库 2 / 从库 N
</code></pre>
<p><strong>需要处理的关键问题</strong>：</p>
<table>
<thead>
<tr>
<th>问题</th>
<th>说明</th>
<th>解决方案</th>
</tr>
</thead>
<tbody><tr>
<td><strong>主从延迟</strong></td>
<td>从库数据滞后于主库（通常 ms~s 级）</td>
<td>强一致读走主库；半同步复制减少延迟</td>
</tr>
<tr>
<td><strong>延迟感知</strong></td>
<td>刚写入的数据立即读取可能读到旧值</td>
<td>写后读强制路由到主库（Session 级别）</td>
</tr>
<tr>
<td><strong>从库故障</strong></td>
<td>某个从库不可用</td>
<td>负载均衡自动摘除；从库集群冗余</td>
</tr>
</tbody></table>
<p><strong>决策要点</strong>：</p>
<ul>
<li>读写分离能解决读瓶颈，但无法解决写瓶颈。如果写 QPS 过高，需要考虑分库</li>
<li>对于实时性要求高的读操作（如支付后查询订单状态），必须路由到主库</li>
</ul>
<h3>2.3 分库分表</h3>
<p><strong>原理</strong>：将数据分散到多个数据库实例（分库）或多张表（分表），突破单实例的存储容量和连接数限制。</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>解决的问题</th>
<th>拆分维度</th>
</tr>
</thead>
<tbody><tr>
<td><strong>垂直分库</strong></td>
<td>不同业务的数据隔离</td>
<td>按业务域拆分（用户库、订单库、商品库）</td>
</tr>
<tr>
<td><strong>水平分库</strong></td>
<td>单库连接数/写入能力不足</td>
<td>按路由键分片到多个库实例</td>
</tr>
<tr>
<td><strong>水平分表</strong></td>
<td>单表数据量过大导致查询变慢</td>
<td>按路由键分片到多张表</td>
</tr>
</tbody></table>
<p><strong>分片策略对比</strong>：</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>原理</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>Hash 取模</td>
<td><code>shardId = hash(key) % N</code></td>
<td>数据分布均匀</td>
<td>扩容需要数据迁移</td>
</tr>
<tr>
<td>范围分片</td>
<td>按 ID 或时间范围划分</td>
<td>扩容简单，支持范围查询</td>
<td>可能出现热点分片</td>
</tr>
<tr>
<td>一致性哈希</td>
<td>哈希环 + 虚拟节点</td>
<td>扩容仅迁移部分数据</td>
<td>实现复杂度较高</td>
</tr>
</tbody></table>
<p><strong>最佳实践</strong>：</p>
<ul>
<li>单表数据量超过 <strong>1000 万~2000 万行</strong>时，B+ 树索引层级增加，查询性能开始下降，应考虑分表</li>
<li>分库分表会引入<strong>分布式事务</strong>和<strong>跨分片查询</strong>两大难题，在决策前需评估这些成本是否可接受</li>
<li>路由键的选择至关重要：选择查询最频繁的字段（通常是用户 ID），避免绝大多数查询变成跨分片查询</li>
</ul>
<h3>2.4 搜索引擎分流</h3>
<p><strong>原理</strong>：将搜索、模糊查询、聚合统计等对关系型数据库不友好的查询，分流到专用搜索引擎（Elasticsearch），减轻数据库压力。</p>
<p>MySQL 的 <code>LIKE &#39;%keyword%&#39;</code> 无法走索引，在大数据量下性能急剧下降。Elasticsearch 基于倒排索引，天然支持全文检索和聚合查询，且具备水平扩展能力。</p>
<table>
<thead>
<tr>
<th>适合搜索引擎的场景</th>
<th>不适合的场景</th>
</tr>
</thead>
<tbody><tr>
<td>全文搜索、模糊匹配</td>
<td>强事务性写入</td>
</tr>
<tr>
<td>多维度组合筛选</td>
<td>实时一致性要求高的读取</td>
</tr>
<tr>
<td>聚合统计分析</td>
<td>频繁更新的热点数据</td>
</tr>
</tbody></table>
<p><strong>决策要点</strong>：</p>
<ul>
<li>ES 的数据来源于数据库同步（Binlog 订阅或双写），存在秒级延迟，不适合作为事务性读取的主存储</li>
<li>ES 集群的运维成本较高（分片管理、索引优化、GC 调优），引入前需评估团队的运维能力</li>
</ul>
<h2>三、流量层：控制入口压力</h2>
<p>当流量超过系统承载能力时，需要在入口层进行管控，避免系统被打垮。</p>
<h3>3.1 CDN 静态加速</h3>
<p><strong>原理</strong>：将静态资源（图片、CSS、JS）分发到离用户最近的边缘节点，用户就近访问，减少源站压力和网络延迟。</p>
<p>CDN 的价值不仅是加速，更是<strong>将静态请求从应用服务器完全卸载</strong>。一个电商页面中，静态资源请求可能占总请求量的 80% 以上。</p>
<pre><code>无 CDN：  用户（深圳） → 源站（北京）   RTT ~40ms
有 CDN：  用户（深圳） → CDN 节点（深圳）  RTT ~5ms
</code></pre>
<p><strong>最佳实践</strong>：</p>
<ul>
<li>静态资源使用独立域名，避免携带不必要的 Cookie</li>
<li>文件名带内容哈希（如 <code>app.a3b2c1.js</code>），配合长缓存策略，既保证缓存命中率又支持即时更新</li>
</ul>
<h3>3.2 限流</h3>
<p><strong>原理</strong>：当入口流量超过系统容量时，主动丢弃超出部分的请求，保证系统在承载范围内正常服务。</p>
<p>限流是<strong>保护系统不被打垮的最后一道防线</strong>。它的前提假设是：服务部分用户优于服务零用户。</p>
<p><strong>主流限流算法对比</strong>：</p>
<table>
<thead>
<tr>
<th>算法</th>
<th>原理</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>固定窗口</strong></td>
<td>固定时间窗口内计数</td>
<td>实现简单</td>
<td>存在窗口边界突发问题</td>
</tr>
<tr>
<td><strong>滑动窗口</strong></td>
<td>滑动时间窗口内计数</td>
<td>平滑度优于固定窗口</td>
<td>内存占用略高</td>
</tr>
<tr>
<td><strong>漏桶</strong></td>
<td>请求以固定速率流出</td>
<td>流量绝对平滑</td>
<td>无法应对合理的突发流量</td>
</tr>
<tr>
<td><strong>令牌桶</strong></td>
<td>令牌以固定速率生成，请求消耗令牌</td>
<td>允许一定突发流量</td>
<td>参数调优有一定复杂度</td>
</tr>
</tbody></table>
<p><strong>限流的层次</strong>：</p>
<pre><code>接入层限流（Nginx / API Gateway）   → 粗粒度，按 IP 或接口
应用层限流（Sentinel / Guava）      → 细粒度，按用户、业务维度
数据层限流（连接池 / 信号量）         → 保护下游资源
</code></pre>
<p><strong>决策要点</strong>：</p>
<ul>
<li>限流阈值必须基于<strong>压测数据</strong>设定，而非拍脑袋。先压测确定系统容量，再按容量的 70%~80% 设置限流阈值</li>
<li>被限流的请求应返回明确的状态码（如 HTTP 429）和友好的提示，而非超时或错误</li>
</ul>
<h3>3.3 负载均衡</h3>
<p><strong>原理</strong>：将入口流量按策略分配到多个后端节点，避免单节点过载，同时实现故障自动摘除。</p>
<table>
<thead>
<tr>
<th>层级</th>
<th>实现</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td>DNS 负载均衡</td>
<td>DNS 多 A 记录</td>
<td>粗粒度，无法感知后端状态</td>
</tr>
<tr>
<td>L4 负载均衡</td>
<td>LVS / F5</td>
<td>高性能（百万级），基于 IP + 端口</td>
</tr>
<tr>
<td>L7 负载均衡</td>
<td>Nginx / HAProxy</td>
<td>灵活（可按 URL、Header 路由），性能略低于 L4</td>
</tr>
</tbody></table>
<p><strong>常用调度算法</strong>：</p>
<table>
<thead>
<tr>
<th>算法</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>轮询 / 加权轮询</td>
<td>后端节点性能一致或差异已知</td>
</tr>
<tr>
<td>最少连接</td>
<td>请求处理时间差异大</td>
</tr>
<tr>
<td>一致性哈希</td>
<td>需要会话亲和或缓存亲和</td>
</tr>
<tr>
<td>随机</td>
<td>后端节点对等，实现最简单</td>
</tr>
</tbody></table>
<h2>四、容错层：保障系统韧性</h2>
<p>高并发场景下，系统组件出现故障的概率随节点数增长而增大。容错设计的目标是<strong>局部故障不扩散为全局雪崩</strong>。</p>
<h3>4.1 熔断</h3>
<p><strong>原理</strong>：当下游服务的错误率或响应时间超过阈值时，自动切断对该服务的调用，防止故障沿调用链向上蔓延。</p>
<p>熔断器借鉴了电路断路器的设计，有三个状态：</p>
<pre><code>Closed（关闭）→ 正常放行请求
    ↓ 错误率超过阈值
Open（打开）→ 直接拒绝请求，返回降级结果
    ↓ 超时后放行少量探测请求
Half-Open（半开）→ 探测成功则恢复，失败则重新打开
</code></pre>
<p><strong>决策要点</strong>：</p>
<ul>
<li>熔断阈值的设定需要区分<strong>瞬时抖动</strong>和<strong>持续故障</strong>。通常使用滑动窗口统计，避免单次超时就触发熔断</li>
<li>熔断后的降级策略需要提前设计：返回默认值、返回缓存数据、或返回友好提示</li>
</ul>
<h3>4.2 降级</h3>
<p><strong>原理</strong>：在系统压力过大时，主动关闭非核心功能，将资源集中保障核心链路。</p>
<p>降级是一种<strong>有策略的功能取舍</strong>，核心思想是：宁可部分功能不可用，也不能让整个系统崩溃。</p>
<table>
<thead>
<tr>
<th>降级层次</th>
<th>策略</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td><strong>接口降级</strong></td>
<td>关闭非核心接口</td>
<td>大促期间关闭商品评论、推荐功能</td>
</tr>
<tr>
<td><strong>数据降级</strong></td>
<td>返回简化/缓存数据</td>
<td>库存查询降级为返回&quot;有货&quot;</td>
</tr>
<tr>
<td><strong>体验降级</strong></td>
<td>降低功能质量</td>
<td>图片返回低清版本、关闭个性化推荐</td>
</tr>
<tr>
<td><strong>写降级</strong></td>
<td>异步化写入</td>
<td>日志、埋点异步落盘</td>
</tr>
</tbody></table>
<p><strong>最佳实践</strong>：</p>
<ul>
<li>降级开关应提前埋入代码，通过配置中心实时生效，而非临时发版</li>
<li>建立业务优先级分类（P0~P3），明确各级业务在压力场景下的降级策略</li>
</ul>
<h3>4.3 超时与重试</h3>
<p><strong>原理</strong>：通过超时避免线程无限等待，通过重试应对瞬时故障。两者配合使用，在可靠性和资源效率之间取得平衡。</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>关键参数</th>
<th>注意事项</th>
</tr>
</thead>
<tbody><tr>
<td><strong>超时</strong></td>
<td>连接超时、读取超时</td>
<td>超时时间应基于下游 P99 延迟设定，而非经验值</td>
</tr>
<tr>
<td><strong>重试</strong></td>
<td>最大重试次数、退避策略</td>
<td>仅对<strong>幂等</strong>操作重试；使用指数退避避免重试风暴</td>
</tr>
</tbody></table>
<p><strong>重试的风险——重试风暴</strong>：</p>
<pre><code>正常情况：A → B → C，每层 1 次调用 = 1 次
重试场景：A(重试3次) → B(重试3次) → C
  C 的实际请求量 = 3 × 3 = 9 倍放大
</code></pre>
<p><strong>最佳实践</strong>：</p>
<ul>
<li>在调用链的<strong>最外层</strong>设置重试，中间层尽量不重试，避免指数级放大</li>
<li>重试需配合<strong>熔断</strong>使用：当下游已经熔断时，不应继续重试</li>
</ul>
<h3>4.4 隔离</h3>
<p><strong>原理</strong>：将不同业务或不同调用方的资源隔离开，防止某一个慢请求或故障请求耗尽全局资源。</p>
<table>
<thead>
<tr>
<th>隔离方式</th>
<th>机制</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>线程池隔离</strong></td>
<td>每个下游调用使用独立线程池</td>
<td>调用外部服务，需要严格隔离</td>
</tr>
<tr>
<td><strong>信号量隔离</strong></td>
<td>限制某类请求的并发数</td>
<td>轻量级隔离，开销比线程池小</td>
</tr>
<tr>
<td><strong>进程隔离</strong></td>
<td>不同业务部署在独立进程/容器</td>
<td>核心业务与非核心业务隔离</td>
</tr>
<tr>
<td><strong>机房/泳道隔离</strong></td>
<td>流量按泳道划分到独立基础设施</td>
<td>SET 化架构、灰度发布</td>
</tr>
</tbody></table>
<h2>五、验证层：建立量化基准</h2>
<p>以上所有策略的效果，最终都需要通过压力测试来验证。</p>
<h3>5.1 压力测试</h3>
<p>压测的目的不是&quot;测试系统能抗多少&quot;，而是<strong>建立系统容量的量化认知</strong>：</p>
<table>
<thead>
<tr>
<th>压测指标</th>
<th>含义</th>
<th>目标</th>
</tr>
</thead>
<tbody><tr>
<td><strong>QPS/TPS</strong></td>
<td>每秒处理请求/事务数</td>
<td>确定系统吞吐上限</td>
</tr>
<tr>
<td><strong>P99 延迟</strong></td>
<td>99% 的请求响应时间</td>
<td>确定延迟是否可接受</td>
</tr>
<tr>
<td><strong>错误率</strong></td>
<td>失败请求占比</td>
<td>确定系统稳定性边界</td>
</tr>
<tr>
<td><strong>资源利用率</strong></td>
<td>CPU、内存、网络、磁盘</td>
<td>确定瓶颈所在</td>
</tr>
</tbody></table>
<p><strong>压测原则</strong>：</p>
<ul>
<li><strong>全链路压测</strong>：仅压测单个服务无法反映真实瓶颈，需要从入口到数据库全链路施压</li>
<li><strong>梯度加压</strong>：从低流量逐步增加，观察每个阶段的指标变化，而非直接打到目标流量</li>
<li><strong>压测环境隔离</strong>：避免压测流量影响线上数据，使用影子库/影子表隔离</li>
</ul>
<h3>5.2 容量规划</h3>
<p>基于压测数据建立容量模型：</p>
<pre><code>所需节点数 = 预估峰值 QPS / 单节点安全 QPS × 冗余系数

示例：
  预估峰值 QPS：10,000
  单节点压测 QPS：2,000（P99 &lt; 50ms 时）
  冗余系数：1.5（预留 50% 余量应对突发）

  所需节点数 = 10,000 / 2,000 × 1.5 = 7.5 → 8 个节点
</code></pre>
<p><strong>最佳实践</strong>：</p>
<ul>
<li>容量规划以 <strong>P99 延迟可接受时的 QPS</strong> 为基准，而非极限 QPS</li>
<li>预留 30%~50% 的余量应对突发流量和非预期场景</li>
<li>建立常态化的容量巡检机制，而非仅在大促前才做压测</li>
</ul>
<h2>六、策略选择决策框架</h2>
<p>面对高并发问题时，不同策略的优先级和适用条件不同。以下是一个决策参考框架：</p>
<h3>按瓶颈类型选择策略</h3>
<table>
<thead>
<tr>
<th>瓶颈类型</th>
<th>表现</th>
<th>优先策略</th>
</tr>
</thead>
<tbody><tr>
<td><strong>CPU 瓶颈</strong></td>
<td>CPU 利用率持续 &gt; 80%</td>
<td>水平扩展、异步化、算法优化</td>
</tr>
<tr>
<td><strong>数据库瓶颈（读）</strong></td>
<td>慢查询多、从库延迟高</td>
<td>缓存、读写分离、索引优化</td>
</tr>
<tr>
<td><strong>数据库瓶颈（写）</strong></td>
<td>主库 TPS 到顶、锁等待严重</td>
<td>分库分表、异步写入、批量合并</td>
</tr>
<tr>
<td><strong>网络瓶颈</strong></td>
<td>带宽打满、延迟升高</td>
<td>CDN、数据压缩、减少调用次数</td>
</tr>
<tr>
<td><strong>连接数瓶颈</strong></td>
<td>too many connections</td>
<td>池化、读写分离、分库</td>
</tr>
</tbody></table>
<h3>按投入产出比排序</h3>
<p>高并发优化应遵循<strong>先低成本高收益，再高成本高收益</strong>的顺序：</p>
<pre><code>第一梯队（低成本、高收益）：
  缓存 → 池化 → 索引优化 → CDN

第二梯队（中等成本）：
  读写分离 → 异步化 → 限流/熔断/降级

第三梯队（高成本）：
  分库分表 → 水平扩展 → 服务拆分 → SET 化
</code></pre>
<h2>总结</h2>
<p>高并发系统设计不是某个单一技巧的应用，而是多种策略在不同层次的协同配合。核心原则可以归纳为三点：</p>
<ol>
<li><strong>先定位瓶颈，再选择策略</strong>。不做盲目优化，压测数据是一切决策的基础</li>
<li><strong>优先选择低成本方案</strong>。缓存、池化、异步化往往能以最小代价解决 80% 的并发问题</li>
<li><strong>容错比性能更重要</strong>。系统在高并发下&quot;不崩&quot;比&quot;更快&quot;更关键——限流、熔断、降级是系统韧性的底线</li>
</ol>
<blockquote>
<p>一个成熟的高并发系统，不是在每个环节都做到极致，而是在每个环节都做出了正确的取舍。</p>
</blockquote>
19:T9547,<h1>Agent vs Workflow vs Automation: 选对抽象才是关键</h1>
<blockquote>
<p>系列第 03 篇。上一篇我们讲了&quot;LLM 本身不是 Agent&quot;，这一篇要回答一个更实际的问题：<strong>你的问题，真的需要 Agent 吗？</strong></p>
</blockquote>
<hr>
<h2>1. 开篇：Agent 万能论的陷阱</h2>
<p>2024 年以来，&quot;Agent&quot; 这个词已经被严重滥用。打开任何一篇技术文章，似乎所有系统都应该被重写为 Agent——客服要 Agent、ETL 要 Agent、运维要 Agent、审批要 Agent。</p>
<p>但现实是：<strong>大部分生产系统中，80% 的任务用 if/else 和 DAG 就能解决，且解决得更好。</strong></p>
<p>Agent 不是银弹。它是一种特定的执行范式，适用于特定的问题空间。盲目使用 Agent 的代价是：更高的 Token 成本、更长的延迟、更难的调试、更差的可预测性。</p>
<p>这篇文章的目标很简单：帮你建立一个清晰的选型框架。面对一个具体问题，你应该能在 30 秒内判断——<strong>用 Automation、用 Workflow、还是用 Agent。</strong></p>
<hr>
<h2>2. 三种执行范式</h2>
<h3>2.1 Rule-based Automation</h3>
<p><strong>定义</strong>：用预定义规则驱动的全自动执行。输入确定，规则确定，输出确定。</p>
<p>典型实现：if/else 逻辑、Rule Engine（Drools、Rete）、Cron Job、Event Trigger。</p>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                 Rule-based Automation                    │
│                                                         │
│   Input ──→ [Rule Match] ──→ Action A                   │
│                  │                                      │
│                  ├──→ Action B                           │
│                  │                                      │
│                  └──→ Action C                           │
│                                                         │
│   特征：路径在编写时完全确定，运行时无决策               │
│   类比：铁轨上的火车，轨道已铺好                         │
└─────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>核心特征</strong>：</p>
<ul>
<li>零运行时决策——所有分支在代码 / 规则编写时就已确定</li>
<li>确定性：相同输入永远产生相同输出</li>
<li>延迟极低（微秒到毫秒级）</li>
<li>可解释性最强——每一步都可以追溯到具体规则</li>
</ul>
<pre><code class="language-python"># 典型的 Rule-based Automation
class AlertRule:
    def __init__(self, metric: str, threshold: float, action: str):
        self.metric = metric
        self.threshold = threshold
        self.action = action

class RuleEngine:
    def __init__(self):
        self.rules: list[AlertRule] = []

    def add_rule(self, rule: AlertRule):
        self.rules.append(rule)

    def evaluate(self, metrics: dict[str, float]) -&gt; list[str]:
        &quot;&quot;&quot;对每条指标做规则匹配，返回触发的动作列表&quot;&quot;&quot;
        actions = []
        for rule in self.rules:
            value = metrics.get(rule.metric)
            if value is not None and value &gt; rule.threshold:
                actions.append(rule.action)
        return actions

# 使用
engine = RuleEngine()
engine.add_rule(AlertRule(&quot;cpu_usage&quot;, 90.0, &quot;scale_up&quot;))
engine.add_rule(AlertRule(&quot;error_rate&quot;, 5.0, &quot;page_oncall&quot;))
engine.add_rule(AlertRule(&quot;disk_usage&quot;, 85.0, &quot;cleanup_logs&quot;))

triggered = engine.evaluate({&quot;cpu_usage&quot;: 95.0, &quot;error_rate&quot;: 2.0})
# → [&quot;scale_up&quot;]   — 完全确定，完全可预测
</code></pre>
<h3>2.2 Workflow / DAG</h3>
<p><strong>定义</strong>：预定义步骤的有序编排。步骤之间有依赖关系，可以有条件分支，但所有可能的路径在设计时已知。</p>
<p>典型实现：Airflow、Temporal、Prefect、Step Functions、BPMN Engine。</p>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                    Workflow / DAG                        │
│                                                         │
│   Start ──→ [Step A] ──→ [Step B] ──┬──→ [Step C]      │
│                                     │                   │
│                                     └──→ [Step D]      │
│                          │                    │         │
│                          └────────┬───────────┘         │
│                                   ▼                     │
│                              [Step E] ──→ End           │
│                                                         │
│   特征：路径在设计时确定，运行时按条件选择分支           │
│   类比：地铁线路图，站点和换乘规则预先设定               │
└─────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>核心特征</strong>：</p>
<ul>
<li>步骤预定义，依赖关系显式声明</li>
<li>有条件分支，但分支的数量和逻辑在设计时确定</li>
<li>支持重试、超时、补偿（Compensation）</li>
<li>可视化程度高——DAG 本身就是文档</li>
</ul>
<pre><code class="language-python"># 典型的 Workflow / DAG 定义（伪代码，框架无关）
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Callable

class StepStatus(Enum):
    PENDING = &quot;pending&quot;
    RUNNING = &quot;running&quot;
    SUCCESS = &quot;success&quot;
    FAILED = &quot;failed&quot;
    SKIPPED = &quot;skipped&quot;

@dataclass
class Step:
    name: str
    fn: Callable
    depends_on: list[str] = field(default_factory=list)
    condition: Callable | None = None  # 条件分支
    retry_count: int = 3
    timeout_seconds: int = 300

class DAGExecutor:
    def __init__(self):
        self.steps: dict[str, Step] = {}
        self.results: dict[str, Any] = {}
        self.status: dict[str, StepStatus] = {}

    def add_step(self, step: Step):
        self.steps[step.name] = step
        self.status[step.name] = StepStatus.PENDING

    def _can_run(self, step: Step) -&gt; bool:
        &quot;&quot;&quot;检查依赖是否全部完成&quot;&quot;&quot;
        for dep in step.depends_on:
            if self.status.get(dep) != StepStatus.SUCCESS:
                return False
        return True

    def _should_run(self, step: Step) -&gt; bool:
        &quot;&quot;&quot;检查条件分支&quot;&quot;&quot;
        if step.condition is None:
            return True
        return step.condition(self.results)

    def run(self, initial_context: dict):
        self.results.update(initial_context)
        # 简化的拓扑排序执行（生产实现应支持并行）
        remaining = set(self.steps.keys())
        while remaining:
            runnable = [
                name for name in remaining
                if self._can_run(self.steps[name])
            ]
            if not runnable:
                raise RuntimeError(&quot;DAG has unresolvable dependencies&quot;)
            for name in runnable:
                step = self.steps[name]
                remaining.remove(name)
                if not self._should_run(step):
                    self.status[name] = StepStatus.SKIPPED
                    continue
                self.status[name] = StepStatus.RUNNING
                try:
                    self.results[name] = step.fn(self.results)
                    self.status[name] = StepStatus.SUCCESS
                except Exception:
                    self.status[name] = StepStatus.FAILED
                    raise
</code></pre>
<h3>2.3 Agent</h3>
<p><strong>定义</strong>：LLM 驱动的动态决策执行。每一步做什么，由 LLM 在运行时根据当前状态决定。路径不确定，在执行前无法预知。</p>
<p>典型实现：ReAct Loop、LangGraph Agent、AutoGPT、自研 Agent Runtime。</p>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                       Agent                             │
│                                                         │
│   Input ──→ [LLM: 观察+思考] ──→ [Tool A] ──┐          │
│                     ▲                        │          │
│                     │                        ▼          │
│                     │            [LLM: 观察+思考]       │
│                     │                  │     │          │
│                     │         ┌────────┘     │          │
│                     │         ▼              ▼          │
│                     ├──── [Tool C]      [Tool B]        │
│                     │         │              │          │
│                     │         ▼              ▼          │
│                     └── [LLM: 够了吗？] ──→ Output      │
│                                                         │
│   特征：路径在运行时动态生成，每一步由 LLM 决定         │
│   类比：出租车司机，根据实时路况随时调整路线             │
└─────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>核心特征</strong>：</p>
<ul>
<li>运行时决策——下一步做什么由 LLM 在当前上下文中推理得出</li>
<li>非确定性：相同输入可能走不同路径（temperature &gt; 0 时尤为明显）</li>
<li>能处理模糊、开放、未预见的输入</li>
<li>每一步决策都需要 LLM 推理，延迟和成本显著高于前两者</li>
</ul>
<pre><code class="language-python"># 典型的 Agent Loop（极简实现）
from typing import Any

class Tool:
    def __init__(self, name: str, description: str, fn: callable):
        self.name = name
        self.description = description
        self.fn = fn

class Agent:
    def __init__(self, llm_client, tools: list[Tool], max_steps: int = 10):
        self.llm = llm_client
        self.tools = {t.name: t for t in tools}
        self.max_steps = max_steps

    def run(self, user_input: str) -&gt; str:
        messages = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input}]
        tool_descriptions = [
            {&quot;name&quot;: t.name, &quot;description&quot;: t.description}
            for t in self.tools.values()
        ]

        for step in range(self.max_steps):
            # LLM 决定下一步：调用工具，还是直接回答
            response = self.llm.chat(
                messages=messages,
                tools=tool_descriptions,
            )

            if response.is_final_answer:
                return response.content

            # LLM 选择了一个工具
            tool_name = response.tool_call.name
            tool_args = response.tool_call.arguments
            tool_result = self.tools[tool_name].fn(**tool_args)

            # 将工具结果加入上下文，进入下一轮循环
            messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: response.raw})
            messages.append({&quot;role&quot;: &quot;tool&quot;, &quot;content&quot;: str(tool_result)})

        return &quot;达到最大步数限制，未能完成任务。&quot;
</code></pre>
<p>注意上面代码的关键区别：<strong>Automation 和 Workflow 的控制流是代码写死的，Agent 的控制流是 LLM 在运行时生成的。</strong> 这是三者的本质差异。</p>
<hr>
<h2>3. 决策维度分析</h2>
<h3>3.1 对比总览</h3>
<table>
<thead>
<tr>
<th>维度</th>
<th>Rule-based Automation</th>
<th>Workflow / DAG</th>
<th>Agent</th>
</tr>
</thead>
<tbody><tr>
<td><strong>确定性</strong></td>
<td>完全确定</td>
<td>路径确定，结果依赖外部</td>
<td>不确定</td>
</tr>
<tr>
<td><strong>可解释性</strong></td>
<td>极强（规则可追溯）</td>
<td>强（DAG 可视化）</td>
<td>弱（LLM 是黑盒）</td>
</tr>
<tr>
<td><strong>延迟</strong></td>
<td>μs - ms</td>
<td>ms - min（取决于步骤）</td>
<td>s - min（LLM 推理）</td>
</tr>
<tr>
<td><strong>单次成本</strong></td>
<td>几乎为零</td>
<td>低（计算资源）</td>
<td>高（Token 费用）</td>
</tr>
<tr>
<td><strong>可靠性</strong></td>
<td>极高</td>
<td>高（有重试/补偿）</td>
<td>中等（LLM 可能幻觉）</td>
</tr>
<tr>
<td><strong>可观测性</strong></td>
<td>高（日志即文档）</td>
<td>高（DAG 天然可视化）</td>
<td>低（需要额外 Trace）</td>
</tr>
<tr>
<td><strong>灵活性</strong></td>
<td>低（新规则需改代码）</td>
<td>中（新步骤需改 DAG）</td>
<td>高（Prompt 即可调整）</td>
</tr>
<tr>
<td><strong>处理模糊输入</strong></td>
<td>不支持</td>
<td>有限支持</td>
<td>原生支持</td>
</tr>
<tr>
<td><strong>开发复杂度</strong></td>
<td>低</td>
<td>中</td>
<td>高</td>
</tr>
</tbody></table>
<h3>3.2 逐维度展开</h3>
<p><strong>确定性 vs 不确定性</strong></p>
<p>这是最重要的选型维度。问自己一个问题：<strong>给定相同的输入，系统是否必须产生相同的输出？</strong></p>
<ul>
<li>如果答案是&quot;必须&quot;——不要用 Agent。Rule-based Automation 或 Workflow 是正确选择。</li>
<li>如果答案是&quot;不一定，但结果需要在合理范围内&quot;——Agent 可以考虑，但要加 Guardrail。</li>
<li>如果答案是&quot;每次可以不同，只要合理就行&quot;——Agent 是自然选择。</li>
</ul>
<p>金融交易、订单状态流转、计费逻辑——这些场景如果引入 Agent 的非确定性，后果不堪设想。</p>
<p><strong>可解释性</strong></p>
<p>生产系统出了问题，你需要回答&quot;为什么系统做了这个决策&quot;。</p>
<ul>
<li>Rule Engine：直接查看匹配了哪条规则，一目了然。</li>
<li>Workflow：查看 DAG 执行日志，哪个步骤走了哪个分支，完全透明。</li>
<li>Agent：LLM 的推理过程是一段自然语言（Chain of Thought），但它可能是事后合理化，并不一定反映真实的&quot;推理过程&quot;。</li>
</ul>
<p>在合规要求高的领域（金融、医疗、法律），可解释性不是 nice-to-have，而是硬性要求。</p>
<p><strong>成本</strong></p>
<p>这一点经常被低估。以一个中等复杂度的任务为例：</p>
<pre><code>Rule Engine:  ~0 成本（CPU 时间可忽略）
Workflow:     ~$0.001（计算资源 + 存储）
Agent:        ~$0.01 - $0.50（取决于步骤数和模型选择）
              3 步 Agent × GPT-4 级别 ≈ 每次 $0.03-0.10
              如果日调用量 100K，月成本 = $3,000 - $10,000
</code></pre>
<p>当你把 Agent 用在本该用 Rule Engine 解决的问题上，你是在用 100 倍的成本获得更差的可靠性。</p>
<p><strong>可靠性</strong></p>
<ul>
<li>Rule Engine：只要规则正确，就永远正确。故障模式是规则覆盖不全。</li>
<li>Workflow：支持重试、幂等、补偿事务。成熟的 Workflow Engine 可以做到 99.99% 可靠。</li>
<li>Agent：LLM 可能幻觉、可能选错工具、可能陷入循环。即使加了 Guardrail，端到端成功率通常在 85%-95%（复杂任务更低）。</li>
</ul>
<p><strong>可观测性</strong></p>
<ul>
<li>Rule Engine：每次执行记录匹配规则和动作，日志本身就是完整的审计轨迹。</li>
<li>Workflow：DAG 执行引擎天然提供步骤级别的状态、耗时、输入输出。Airflow 的 UI 就是最好的例子。</li>
<li>Agent：你需要自己构建 Trace 系统——记录每一轮 LLM 的输入、输出、选择的工具、工具的返回值、Token 消耗。没有这些，Agent 在生产环境中就是一个黑盒。</li>
</ul>
<hr>
<h2>4. 场景分析</h2>
<p>抽象的对比不如具体场景有说服力。下面逐个分析。</p>
<h3>4.1 数据 ETL Pipeline → Workflow</h3>
<p><strong>场景</strong>：每天从 3 个数据源抽取数据，清洗、转换、加载到数据仓库。</p>
<p><strong>选 Workflow 的理由</strong>：</p>
<ul>
<li>步骤完全确定：Extract → Transform → Load，不需要运行时决策</li>
<li>步骤间有明确的依赖关系：Transform 必须在 Extract 之后</li>
<li>需要精确的重试和失败补偿：某个数据源失败了，只重跑那个分支</li>
<li>需要调度：每天凌晨 3 点执行</li>
<li>需要回填（Backfill）：补跑历史数据</li>
</ul>
<p><strong>为什么不用 Agent</strong>：</p>
<p>ETL 不需要&quot;思考下一步做什么&quot;——步骤是固定的。用 Agent 意味着每次运行都要花 Token 让 LLM &quot;重新发现&quot;这些固定步骤，纯属浪费。更危险的是，LLM 可能在某次运行中&quot;创造性地&quot;跳过某个步骤或改变转换逻辑。</p>
<h3>4.2 客服问答 → Agent</h3>
<p><strong>场景</strong>：用户通过聊天窗口提问，系统需要理解意图、查询知识库、可能需要查订单、可能需要转人工。</p>
<p><strong>选 Agent 的理由</strong>：</p>
<ul>
<li>输入是自然语言，意图不确定，无法枚举所有可能</li>
<li>处理路径取决于用户说了什么——可能一步就能回答，也可能需要查 3 个系统</li>
<li>需要上下文理解和多轮对话能力</li>
<li>&quot;足够好&quot;的回答即可，不需要 100% 确定性</li>
</ul>
<p><strong>为什么不用 Workflow</strong>：</p>
<p>你无法预定义所有可能的对话路径。用户可能问&quot;我的订单到哪了&quot;，也可能问&quot;你们支持退款吗&quot;，也可能在同一轮对话中先问订单再问退款政策。Workflow 的路径是编译期确定的，处理不了这种运行时的动态性。</p>
<h3>4.3 定时报表生成 → Automation</h3>
<p><strong>场景</strong>：每周一早上 9 点，从数据库查询上周的销售数据，生成 Excel 报表，发送到指定邮箱。</p>
<p><strong>选 Automation 的理由</strong>：</p>
<ul>
<li>触发条件确定：Cron 定时</li>
<li>逻辑确定：SQL 查询 → 格式化 → 发送</li>
<li>不需要编排复杂依赖</li>
<li>不需要任何&quot;智能&quot;——SQL 和模板都是写死的</li>
</ul>
<p><strong>为什么不用 Workflow 或 Agent</strong>：</p>
<p>Workflow 是大炮打蚊子——这里没有复杂的步骤依赖和分支。Agent 更是离谱——你不需要 LLM 来执行 <code>SELECT SUM(amount) FROM orders WHERE date &gt;= &#39;2025-07-28&#39;</code>。</p>
<h3>4.4 代码审查助手 → Agent</h3>
<p><strong>场景</strong>：PR 提交后，自动分析代码变更，给出审查意见：安全隐患、性能问题、风格建议。</p>
<p><strong>选 Agent 的理由</strong>：</p>
<ul>
<li>代码变更是非结构化的，无法穷举所有模式</li>
<li>需要&quot;理解&quot;代码语义，而非简单的模式匹配（静态分析工具已经覆盖了模式匹配的部分）</li>
<li>审查意见需要结合上下文（这个函数在项目中是怎么用的？改动会影响什么？）</li>
<li>Agent 可以调用多种工具：读取文件、运行测试、查看 Git 历史</li>
</ul>
<p><strong>为什么不用 Rule Engine</strong>：</p>
<p>Rule Engine 只能匹配预定义模式（如&quot;函数超过 100 行&quot;），无法理解语义层面的问题（如&quot;这个 API 调用没有处理超时&quot;）。实际上，最好的方案是 <strong>Rule Engine + Agent</strong>——先用 Linter/SAST 做确定性检查，再用 Agent 做语义级审查。</p>
<h3>4.5 订单状态流转 → Workflow</h3>
<p><strong>场景</strong>：电商订单从创建到完成的状态机——待支付 → 已支付 → 拣货中 → 已发货 → 已签收 → 已完成。</p>
<p><strong>选 Workflow 的理由</strong>：</p>
<ul>
<li>状态和转换规则完全确定：已支付才能拣货，已发货才能签收</li>
<li>每个状态转换都有明确的触发条件（支付回调、物流推送）</li>
<li>需要事务保证：状态转换必须原子性，不能出现&quot;钱扣了但订单还是待支付&quot;</li>
<li>需要补偿机制：支付超时需要自动取消</li>
<li>0 容忍非确定性——用户的钱不能有任何模糊</li>
</ul>
<p><strong>为什么不用 Agent</strong>：</p>
<p>这个问题需要反复强调：<strong>涉及金钱和状态一致性的流程，绝对不能用 Agent。</strong> LLM 的幻觉在这里不是&quot;回答不太准确&quot;，而是&quot;用户的钱没了但订单没更新&quot;。</p>
<h3>4.6 智能运维（AIOps）→ 混合架构</h3>
<p><strong>场景</strong>：监控告警触发后，自动诊断根因并执行修复。</p>
<p><strong>为什么需要混合</strong>：</p>
<p>这个场景天然分为确定性部分和不确定性部分——</p>
<ul>
<li>确定性部分（Automation）：告警规则匹配、阈值判断、常见故障的自动修复（CPU 高 → 扩容，磁盘满 → 清理日志）</li>
<li>不确定性部分（Agent）：复杂故障的根因分析——Agent 可以查看日志、查询指标、检查最近的部署变更，综合判断根因</li>
<li>编排部分（Workflow）：整个处理流程的骨架——告警接收 → 去重 → 分级 → 自动修复 / 智能诊断 → 通知</li>
</ul>
<pre><code>告警触发
   │
   ▼
[Automation: 告警去重 + 分级]
   │
   ├──→ P4/P3 已知模式 ──→ [Automation: 自动修复]
   │                              │
   │                              ▼
   │                         [通知 Oncall]
   │
   └──→ P2/P1 或未知模式 ──→ [Agent: 根因分析]
                                   │
                                   ├──→ 找到根因 ──→ [Automation: 执行修复]
                                   │
                                   └──→ 无法确定 ──→ [升级到人工]
</code></pre>
<p>这才是 Agent 的正确用法——<strong>只在真正需要&quot;智能&quot;的环节使用 Agent，其余部分用更可靠、更便宜的范式处理。</strong></p>
<hr>
<h2>5. 混合架构：三者如何共存</h2>
<p>真实的生产系统很少只用一种范式。更常见的模式是：</p>
<pre><code>┌──────────────────────────────────────────────────────────────┐
│                    混合架构全景                                │
│                                                              │
│  ┌──────────────────────────────────────────────────┐        │
│  │              Workflow / DAG（骨架层）              │        │
│  │                                                  │        │
│  │  Step 1          Step 2          Step 3          │        │
│  │  ┌──────────┐   ┌──────────┐   ┌──────────┐     │        │
│  │  │Automation│──→│  Agent   │──→│Automation│     │        │
│  │  │数据预处理│   │语义分析  │   │结果写入  │     │        │
│  │  └──────────┘   └──────────┘   └──────────┘     │        │
│  │       │              │              │            │        │
│  │       ▼              ▼              ▼            │        │
│  │  确定性操作     LLM 推理       确定性操作        │        │
│  │  延迟: 10ms    延迟: 2-5s     延迟: 50ms        │        │
│  │  成本: ~0      成本: $0.02    成本: ~0           │        │
│  └──────────────────────────────────────────────────┘        │
│                                                              │
│  设计原则：                                                  │
│  1. Workflow 负责编排和容错（重试、超时、补偿）              │
│  2. Automation 处理所有确定性步骤                            │
│  3. Agent 只出现在需要&quot;理解&quot;和&quot;推理&quot;的节点                  │
│  4. Agent 的输出经过验证后才进入下一步                       │
└──────────────────────────────────────────────────────────────┘
</code></pre>
<h3>5.1 设计原则</h3>
<p><strong>原则一：Agent 是 Workflow 的节点，不是整个 Workflow</strong></p>
<p>一个常见的错误是让 Agent 控制整个流程——从数据获取到处理到存储全部由 Agent 决定。正确的做法是：Workflow 定义骨架（步骤顺序、依赖关系、容错策略），Agent 只负责其中需要推理的那一步。</p>
<pre><code class="language-python"># 错误做法：让 Agent 控制整个流程
agent.run(&quot;从数据库读取用户评论，分析情感，把结果写回数据库&quot;)
# Agent 可能：用错 SQL、忘记写回、写入格式错误...

# 正确做法：Workflow 控制流程，Agent 只做推理
def step_1_extract(ctx):
    &quot;&quot;&quot;确定性步骤：用固定 SQL 读取数据&quot;&quot;&quot;
    return db.query(&quot;SELECT id, comment FROM reviews WHERE date = %s&quot;, ctx[&quot;date&quot;])

def step_2_analyze(ctx):
    &quot;&quot;&quot;Agent 步骤：对每条评论做情感分析&quot;&quot;&quot;
    results = []
    for review in ctx[&quot;step_1_extract&quot;]:
        sentiment = agent.run(
            f&quot;分析以下评论的情感倾向(positive/negative/neutral):\n{review[&#39;comment&#39;]}&quot;
        )
        results.append({&quot;id&quot;: review[&quot;id&quot;], &quot;sentiment&quot;: sentiment})
    return results

def step_3_load(ctx):
    &quot;&quot;&quot;确定性步骤：用固定逻辑写回数据库&quot;&quot;&quot;
    for item in ctx[&quot;step_2_analyze&quot;]:
        db.execute(
            &quot;UPDATE reviews SET sentiment = %s WHERE id = %s&quot;,
            (item[&quot;sentiment&quot;], item[&quot;id&quot;])
        )

# Workflow 定义
workflow.add_step(Step(&quot;extract&quot;, step_1_extract))
workflow.add_step(Step(&quot;analyze&quot;, step_2_analyze, depends_on=[&quot;extract&quot;]))
workflow.add_step(Step(&quot;load&quot;, step_3_load, depends_on=[&quot;analyze&quot;]))
</code></pre>
<p><strong>原则二：Agent 的输出必须经过验证</strong></p>
<p>Agent 的输出是非确定性的。在混合架构中，Agent 节点和下游确定性节点之间，必须有一个验证层。</p>
<pre><code class="language-python">def step_2_analyze_with_validation(ctx):
    &quot;&quot;&quot;Agent 步骤 + 输出验证&quot;&quot;&quot;
    VALID_SENTIMENTS = {&quot;positive&quot;, &quot;negative&quot;, &quot;neutral&quot;}
    results = []
    for review in ctx[&quot;step_1_extract&quot;]:
        sentiment = agent.run(f&quot;分析情感倾向: {review[&#39;comment&#39;]}&quot;)
        # 验证 Agent 输出
        sentiment = sentiment.strip().lower()
        if sentiment not in VALID_SENTIMENTS:
            sentiment = &quot;neutral&quot;  # fallback
            log.warning(f&quot;Agent 返回了无效的情感值，已 fallback: review_id={review[&#39;id&#39;]}&quot;)
        results.append({&quot;id&quot;: review[&quot;id&quot;], &quot;sentiment&quot;: sentiment})
    return results
</code></pre>
<p><strong>原则三：确定性部分永远优先用 Automation</strong></p>
<p>如果一个步骤的输入输出可以完全预定义，就不要用 Agent。这不是技术保守，而是工程理性——用最简单的工具解决问题，把复杂性预算留给真正需要的地方。</p>
<hr>
<h2>6. Agent 的隐性成本</h2>
<p>这一节讲的是大部分&quot;Agent 教程&quot;不会告诉你的东西。</p>
<h3>6.1 Token 成本</h3>
<p>Agent 的每一步决策都需要调用 LLM。一个 5 步 Agent 执行一次任务的 Token 消耗：</p>
<pre><code>第 1 步: System Prompt (500) + User Input (200) + Response (300)    = 1,000 tokens
第 2 步: 上一轮上下文 (1,000) + Tool Result (500) + Response (400)  = 1,900 tokens
第 3 步: 上一轮上下文 (1,900) + Tool Result (300) + Response (350)  = 2,550 tokens
第 4 步: 上一轮上下文 (2,550) + Tool Result (800) + Response (400)  = 3,750 tokens
第 5 步: 上一轮上下文 (3,750) + Tool Result (200) + Response (500)  = 4,450 tokens
                                                          ─────────────────────
                                                          总计: ~13,650 tokens
</code></pre>
<p>注意上下文是累积的——每一步都要重新发送之前所有的对话历史。这意味着 <strong>Token 消耗是超线性增长的</strong>。步骤越多，后期每一步的成本越高。</p>
<p>以 GPT-4o 为例（$2.5/1M input, $10/1M output），上面这个 5 步 Agent 单次执行成本约 $0.03-0.05。看似不多，但如果日调用量 10 万次，月成本就是 $90,000-$150,000。</p>
<p><strong>优化策略</strong>：</p>
<ul>
<li>上下文压缩：每 N 步对历史做一次摘要</li>
<li>选择合适的模型：简单决策用小模型，关键决策用大模型</li>
<li>缓存：对相同输入的 Agent 结果做缓存（注意非确定性问题）</li>
<li>减少 Agent 步骤：通过更好的 Prompt 和工具设计，减少所需的推理轮次</li>
</ul>
<h3>6.2 延迟</h3>
<p>LLM 的推理延迟通常在 500ms-5s 之间（取决于模型和输出长度）。一个 5 步 Agent 的端到端延迟：</p>
<pre><code>5 步 × 平均 1.5s/步 = 7.5s

加上工具调用时间（网络请求、数据库查询等），实际延迟可能在 10-15s。
</code></pre>
<p>对比：</p>
<ul>
<li>Rule Engine 处理同样的逻辑：&lt; 10ms</li>
<li>Workflow 执行 5 个确定性步骤：&lt; 500ms</li>
</ul>
<p><strong>在延迟敏感的场景（如支付、交易、实时推荐），Agent 的延迟是不可接受的。</strong></p>
<h3>6.3 不可预测性</h3>
<p>这是 Agent 最被低估的问题。</p>
<pre><code class="language-python"># 同样的输入，Agent 可能走出完全不同的路径

# 第一次运行
# Step 1: 调用 search_database → 找到 3 条记录
# Step 2: 调用 analyze_data → 生成分析
# Step 3: 返回结果
# 总计: 3 步, 耗时 4s, 成本 $0.02

# 第二次运行（完全相同的输入）
# Step 1: 调用 search_database → 找到 3 条记录
# Step 2: 调用 search_web → 想找更多信息（为什么？LLM 这次觉得不够）
# Step 3: 调用 search_database → 用新的关键词再查一次
# Step 4: 调用 analyze_data → 生成分析
# Step 5: 觉得分析不够好，调用 analyze_data → 重新生成
# Step 6: 返回结果
# 总计: 6 步, 耗时 10s, 成本 $0.06
</code></pre>
<p>这意味着你<strong>无法预测 Agent 的执行时间和成本</strong>。在需要做容量规划和 SLA 承诺的生产系统中，这是一个严重的问题。</p>
<h3>6.4 调试困难</h3>
<p>确定性系统的 Bug 可以精确复现：相同的输入 + 相同的代码 = 相同的 Bug。</p>
<p>Agent 不行。因为：</p>
<ol>
<li>LLM 的输出本身带有随机性（即使 temperature=0，不同批次推理也可能有微小差异）</li>
<li>工具调用的结果可能随时间变化（数据库内容变了、API 返回变了）</li>
<li>上下文窗口中的信息累积，前几步的微小差异会被放大</li>
</ol>
<p><strong>调试 Agent 的正确做法</strong>：</p>
<ul>
<li>完整记录每一步的输入（包括完整的 messages 列表）和输出</li>
<li>记录每次工具调用的参数和返回值</li>
<li>记录 Token 使用量和延迟</li>
<li>支持&quot;回放&quot;——用记录的数据重新走一遍流程（但要注意，即使相同输入，LLM 也可能给出不同输出）</li>
</ul>
<hr>
<h2>7. 选型决策树</h2>
<p>面对一个具体需求，按以下流程判断：</p>
<pre><code>                    你的任务需要&quot;理解&quot;自然语言
                    或处理模糊/开放式输入吗？
                           │
                    ┌──────┴──────┐
                    │             │
                   Yes           No
                    │             │
                    ▼             ▼
             结果需要 100%     任务步骤之间有
             确定性吗？        复杂依赖关系吗？
                │                    │
          ┌─────┴─────┐        ┌─────┴─────┐
          │           │        │           │
         Yes         No       Yes         No
          │           │        │           │
          ▼           ▼        ▼           ▼
      先用规则     ┌──────┐  Workflow    Automation
      处理能处     │Agent │  / DAG      (Rule/Cron)
      理的部分     └──┬───┘
      用 Agent        │
      处理剩余        ▼
      (混合架构)   可以接受 $0.01-0.10/次
                   的成本和 2-10s 的延迟吗？
                          │
                    ┌─────┴─────┐
                    │           │
                   Yes         No
                    │           │
                    ▼           ▼
                  Agent     重新审视需求：
                            能否拆分为
                            确定性 + 模糊性部分？
                            → 混合架构
</code></pre>
<p><strong>速查表</strong>：</p>
<table>
<thead>
<tr>
<th>如果你的任务是...</th>
<th>推荐范式</th>
<th>理由</th>
</tr>
</thead>
<tbody><tr>
<td>固定逻辑 + 定时触发</td>
<td>Automation</td>
<td>无需编排，无需推理</td>
</tr>
<tr>
<td>多步骤 + 有依赖 + 确定性</td>
<td>Workflow</td>
<td>需要编排，不需要推理</td>
</tr>
<tr>
<td>理解自然语言 + 动态决策</td>
<td>Agent</td>
<td>需要推理</td>
</tr>
<tr>
<td>大部分确定 + 少量模糊</td>
<td>Workflow + Agent 节点</td>
<td>编排确定部分，推理模糊部分</td>
</tr>
<tr>
<td>简单触发 + 复杂诊断</td>
<td>Automation + Agent</td>
<td>触发用规则，诊断用推理</td>
</tr>
</tbody></table>
<hr>
<h2>8. 常见误区</h2>
<p>在结束之前，总结几个我在实际项目中反复见到的选型错误。</p>
<p><strong>误区一：因为&quot;想用 AI&quot;而选 Agent</strong></p>
<p>技术选型应该从问题出发，不是从解决方案出发。&quot;我们想用 AI&quot; 不是选 Agent 的理由，&quot;用户输入是自然语言且意图不可穷举&quot; 才是。</p>
<p><strong>误区二：用 Agent 替代状态机</strong></p>
<p>订单流转、审批流程、工单生命周期——这些有限状态机（FSM）问题有成熟的解决方案。把它们交给 Agent 不会让系统更智能，只会让它更不可靠。</p>
<p><strong>误区三：Agent 做完所有事</strong></p>
<p>让 Agent 既负责决策又负责执行。正确做法是：Agent 只负责&quot;决定做什么&quot;（What），具体的执行（How）交给确定性系统。例如 Agent 决定&quot;需要给用户退款&quot;，但实际调用退款 API 的逻辑是固定的代码，不是 Agent 自己拼 HTTP 请求。</p>
<p><strong>误区四：忽视 Agent 的失败模式</strong></p>
<p>Agent 会失败。它会幻觉、会陷入循环、会选错工具、会超时。你的系统设计必须考虑：Agent 失败了怎么办？有没有 Fallback？有没有人工兜底？最大重试次数是多少？</p>
<hr>
<h2>9. 总结</h2>
<p>回到开篇的问题：你的问题，真的需要 Agent 吗？</p>
<p>三条准则：</p>
<ol>
<li><strong>能用规则解决的，不要用 Workflow；能用 Workflow 解决的，不要用 Agent。</strong> 选择复杂度最低的范式，降低的是长期维护成本。</li>
<li><strong>Agent 的正确位置是&quot;最后一英里的模糊性&quot;。</strong> 在混合架构中，让确定性系统处理 80% 的工作，Agent 只处理那 20% 需要&quot;理解&quot;和&quot;推理&quot;的部分。</li>
<li><strong>Agent 是有代价的，而且代价比你想象的高。</strong> Token 成本、延迟、不可预测性、调试难度——这些隐性成本在规模化后会成为真实的痛点。</li>
</ol>
<p>选对抽象，才是真正的技术判断力。</p>
<hr>
<blockquote>
<p><strong>系列导航</strong>：本文是 Agentic 系列的第 03 篇。</p>
<ul>
<li>上一篇：<a href="/blog/engineering/agentic/02-From%20Prompt%20to%20Agent">02 | From Prompt to Agent</a></li>
<li>下一篇：<a href="/blog/engineering/agentic/04-The%20Agent%20Control%20Loop">04 | The Agent Control Loop</a></li>
<li>完整目录：<a href="/blog/engineering/agentic/01-From%20LLM%20to%20Agent">01 | From LLM to Agent</a></li>
</ul>
</blockquote>
1a:Td4a2,<h1>Tool Calling Deep Dive: 让 LLM 成为可编程接口</h1>
<blockquote>
<p>这是 Agentic 系列的第 05 篇。在前几篇中我们建立了 Agent 的概念模型、控制循环、以及 Agent 与 Workflow 的边界。本篇聚焦于 Agent 能力的核心支点——Tool Calling。</p>
<p>Tool Calling 不是&quot;让 AI 调 API&quot;这么简单。它是 LLM 从 <strong>Text-in/Text-out 的生成模型</strong> 变成 <strong>可编程接口</strong> 的关键转折点。理解它的工作原理、设计约束和工程实践，是构建任何 Agentic 系统的前提。</p>
</blockquote>
<hr>
<h2>1. 为什么 Tool Calling 是关键转折点</h2>
<p>一个纯粹的 LLM 只能做一件事：接受文本，生成文本。它无法查询数据库、无法读取文件、无法发送邮件、无法获取实时天气。它的知识冻结在训练数据的截止日期，它的能力边界就是 token 序列的排列组合。</p>
<p>Tool Calling 改变了这一切。</p>
<p>它的本质不是&quot;让 LLM 调用工具&quot;，而是 <strong>让 LLM 生成结构化的调用意图，由外部运行时代为执行</strong>。这个区分至关重要——LLM 从未真正&quot;执行&quot;过任何工具，它只是学会了在恰当的时机，输出一段符合约定格式的 JSON，表达&quot;我需要调用某个工具，参数是这些&quot;。</p>
<p>这意味着：</p>
<ul>
<li>LLM 变成了一个 <strong>决策引擎</strong>：决定调用什么、传什么参数</li>
<li>Runtime 变成了一个 <strong>执行引擎</strong>：负责真正的 I/O 操作</li>
<li>两者之间的契约是 <strong>JSON Schema</strong></li>
</ul>
<p>这种分离，让 LLM 从一个封闭的文本生成器，变成了一个可以与外部世界交互的可编程接口。</p>
<hr>
<h2>2. Tool Calling 的工作原理</h2>
<h3>2.1 完整流程</h3>
<pre><code>┌──────────────────────────────────────────────────────────────────────┐
│                    Tool Calling 完整序列图                            │
└──────────────────────────────────────────────────────────────────────┘

  User            LLM (API)          Runtime           Tool (Function)
   │                 │                  │                     │
   │  &quot;北京今天天气&quot;  │                  │                     │
   ├────────────────&gt;│                  │                     │
   │                 │                  │                     │
   │                 │  ┌─────────────┐ │                     │
   │                 │  │ 推理:       │ │                     │
   │                 │  │ 用户想查天气 │ │                     │
   │                 │  │ 需要调用    │ │                     │
   │                 │  │ get_weather │ │                     │
   │                 │  └─────────────┘ │                     │
   │                 │                  │                     │
   │                 │  Tool Call JSON  │                     │
   │                 │ ────────────────&gt;│                     │
   │                 │  {               │                     │
   │                 │   &quot;name&quot;:        │                     │
   │                 │    &quot;get_weather&quot; │                     │
   │                 │   &quot;arguments&quot;:   │                     │
   │                 │    {&quot;city&quot;:      │                     │
   │                 │     &quot;北京&quot;}      │                     │
   │                 │  }               │                     │
   │                 │                  │  get_weather(&quot;北京&quot;) │
   │                 │                  ├────────────────────&gt;│
   │                 │                  │                     │
   │                 │                  │  {&quot;temp&quot;: 28,       │
   │                 │                  │   &quot;condition&quot;:      │
   │                 │                  │   &quot;晴&quot;}              │
   │                 │                  │&lt;────────────────────┤
   │                 │                  │                     │
   │                 │  Tool Result     │                     │
   │                 │ &lt;────────────────│                     │
   │                 │                  │                     │
   │                 │  ┌─────────────┐ │                     │
   │                 │  │ 推理:       │ │                     │
   │                 │  │ 根据工具返回 │ │                     │
   │                 │  │ 组织回答    │ │                     │
   │                 │  └─────────────┘ │                     │
   │                 │                  │                     │
   │ &quot;北京今天28°C,晴&quot;│                  │                     │
   │&lt;────────────────│                  │                     │
   │                 │                  │                     │
</code></pre>
<h3>2.2 关键洞察</h3>
<p>从上面的序列图中，可以提炼出几个核心事实：</p>
<ol>
<li><p><strong>LLM 发起两次推理</strong>。第一次决定是否调用工具、调用哪个、传什么参数；第二次基于工具返回的结果生成最终回答。这意味着每次 Tool Calling 至少消耗两轮 LLM 调用的 token。</p>
</li>
<li><p><strong>LLM 的输出不是自然语言，而是结构化 JSON</strong>。这是模型经过专门训练（fine-tuning）才获得的能力。并非所有 LLM 都支持 Tool Calling——它需要模型在训练阶段就学会&quot;在特定上下文下输出 JSON 而非自然语言&quot;。</p>
</li>
<li><p><strong>Runtime 是不可或缺的中间层</strong>。它负责：解析 LLM 返回的 Tool Call、校验参数、路由到正确的函数、执行函数、收集结果、将结果注入下一轮对话。没有 Runtime，Tool Calling 就是一段无人执行的 JSON。</p>
</li>
<li><p><strong>整个过程对用户透明</strong>。用户看到的只是&quot;问了一个问题，得到了回答&quot;。中间的 Tool Call 调度过程完全由系统内部完成。</p>
</li>
</ol>
<hr>
<h2>3. JSON Schema 作为契约</h2>
<h3>3.1 工具定义的结构</h3>
<p>每个工具的定义由三部分组成：</p>
<pre><code class="language-python">tool_definition = {
    &quot;type&quot;: &quot;function&quot;,
    &quot;function&quot;: {
        &quot;name&quot;: &quot;get_weather&quot;,          # 工具的唯一标识
        &quot;description&quot;: &quot;...&quot;,           # 给 LLM 看的&quot;接口文档&quot;
        &quot;parameters&quot;: {                 # JSON Schema 格式的参数约束
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;city&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;城市名称，如 &#39;北京&#39;、&#39;上海&#39;&quot;
                }
            },
            &quot;required&quot;: [&quot;city&quot;]
        }
    }
}
</code></pre>
<p>这里的 <code>parameters</code> 遵循 JSON Schema 规范（Draft 2020-12 子集），它不仅定义了参数的类型，还定义了参数的约束、默认值、枚举范围等。JSON Schema 就是 LLM 与 Runtime 之间的 <strong>契约</strong>。</p>
<h3>3.2 好的描述 vs 差的描述</h3>
<p><code>description</code> 是整个工具定义中最容易被低估的字段。它不是给人类看的注释，而是 <strong>给 LLM 看的接口文档</strong>。LLM 完全依赖 description 来决定是否调用这个工具、以及如何填充参数。</p>
<p><strong>差的描述：</strong></p>
<pre><code class="language-python">{
    &quot;name&quot;: &quot;query_db&quot;,
    &quot;description&quot;: &quot;查询数据库&quot;,          # 太模糊：查什么数据库？返回什么？
    &quot;parameters&quot;: {
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;q&quot;: {                        # 参数名不直观
                &quot;type&quot;: &quot;string&quot;
            }
        }
    }
}
</code></pre>
<p><strong>好的描述：</strong></p>
<pre><code class="language-python">{
    &quot;name&quot;: &quot;query_user_orders&quot;,
    &quot;description&quot;: (
        &quot;根据用户 ID 查询该用户的历史订单列表。&quot;
        &quot;返回最近 30 天内的订单，包含订单号、金额、状态。&quot;
        &quot;如果用户不存在，返回空列表。&quot;
        &quot;不支持模糊查询，user_id 必须精确匹配。&quot;
    ),
    &quot;parameters&quot;: {
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;user_id&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;用户的唯一标识符，格式为 &#39;U&#39; + 8位数字，如 &#39;U00012345&#39;&quot;
            },
            &quot;status_filter&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;enum&quot;: [&quot;all&quot;, &quot;pending&quot;, &quot;completed&quot;, &quot;cancelled&quot;],
                &quot;description&quot;: &quot;按订单状态过滤，默认返回所有状态的订单&quot;
            }
        },
        &quot;required&quot;: [&quot;user_id&quot;]
    }
}
</code></pre>
<p>两者之间的差异在于：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>差的描述</th>
<th>好的描述</th>
</tr>
</thead>
<tbody><tr>
<td>功能边界</td>
<td>不清楚能做什么</td>
<td>明确说明查询范围和返回内容</td>
</tr>
<tr>
<td>参数语义</td>
<td><code>q</code> 是什么？</td>
<td><code>user_id</code> 含义清晰，且给出格式示例</td>
</tr>
<tr>
<td>约束条件</td>
<td>无</td>
<td>明确说明不支持模糊查询</td>
</tr>
<tr>
<td>异常行为</td>
<td>未提及</td>
<td>说明了用户不存在时的返回</td>
</tr>
<tr>
<td>枚举约束</td>
<td>无</td>
<td>用 <code>enum</code> 限定合法值</td>
</tr>
</tbody></table>
<h3>3.3 参数设计原则</h3>
<ol>
<li><strong>简单优先</strong>：参数数量尽量少。一个工具如果需要 10 个参数，说明它的职责太大，应该拆分。</li>
<li><strong>类型明确</strong>：用 <code>enum</code> 约束离散值，用 <code>pattern</code> 约束格式，用 <code>minimum</code>/<code>maximum</code> 约束数值范围。</li>
<li><strong>必选与可选分明</strong>：<code>required</code> 字段只放真正必须的参数，可选参数给默认值。</li>
<li><strong>命名即文档</strong>：<code>user_id</code> 比 <code>uid</code> 好，<code>start_date</code> 比 <code>sd</code> 好。LLM 会从参数名推断语义。</li>
<li><strong>避免嵌套过深</strong>：LLM 生成深层嵌套 JSON 的准确率会显著下降。尽量用扁平结构。</li>
</ol>
<hr>
<h2>4. Structured Output vs Free-form Output</h2>
<h3>4.1 为什么结构化输出更可靠</h3>
<p>在 Tool Calling 出现之前，让 LLM 调用工具的常见做法是：在 Prompt 中要求 LLM &quot;用特定格式输出&quot;，然后用正则或字符串解析提取调用意图。</p>
<pre><code># 旧做法（Prompt Hacking）
请用以下格式回答：
Action: &lt;工具名&gt;
Action Input: &lt;参数 JSON&gt;

# LLM 可能的输出（不可靠）
&quot;我觉得应该查一下天气。Action: get_weather Action Input: {&quot;city&quot;: &quot;北京&quot;}&quot;
                       ^^ 前面混入了自然语言，解析会出错
</code></pre>
<p>这种方式的根本问题是：LLM 的输出是 <strong>非确定性的自由文本</strong>，它可能在格式中混入自然语言、遗漏字段、搞错 JSON 语法。</p>
<p>Structured Output（结构化输出）通过 <strong>约束解码（Constrained Decoding）</strong> 从根本上解决了这个问题。模型在生成 token 时，解码器会强制输出符合预定义 JSON Schema 的 token 序列，从而保证输出 100% 可解析。</p>
<h3>4.2 三种机制的区别</h3>
<table>
<thead>
<tr>
<th>机制</th>
<th>原理</th>
<th>可靠性</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>JSON Mode</strong></td>
<td>告诉模型&quot;输出必须是合法 JSON&quot;，但不约束 schema</td>
<td>中等。JSON 语法正确，但字段可能不对</td>
<td>简单的数据提取</td>
</tr>
<tr>
<td><strong>Function Calling / Tool Use</strong></td>
<td>模型经过 fine-tuning，能在特定上下文下输出 tool call 结构</td>
<td>高。模型专门训练过</td>
<td>Agent 工具调用</td>
</tr>
<tr>
<td><strong>Structured Output</strong></td>
<td>约束解码 + JSON Schema 验证，输出严格匹配 schema</td>
<td>极高。解码层面保证</td>
<td>需要严格 schema 的场景</td>
</tr>
</tbody></table>
<h3>4.3 各大模型的实现差异</h3>
<p>不同模型提供商对 Tool Calling 的 API 设计不尽相同，但核心思想一致：</p>
<p><strong>OpenAI</strong>（GPT-4 系列）：</p>
<ul>
<li>使用 <code>tools</code> 参数传递工具定义</li>
<li>返回 <code>tool_calls</code> 数组，支持并行调用</li>
<li>支持 <code>strict: true</code> 开启 Structured Output 模式</li>
</ul>
<p><strong>Anthropic</strong>（Claude 系列）：</p>
<ul>
<li>使用 <code>tools</code> 参数传递工具定义</li>
<li>Tool Call 以 <code>tool_use</code> content block 返回</li>
<li>Tool 结果以 <code>tool_result</code> content block 传回</li>
<li>原生支持并行工具调用</li>
</ul>
<p><strong>Google</strong>（Gemini 系列）：</p>
<ul>
<li>使用 <code>tools</code> + <code>function_declarations</code> 结构</li>
<li>支持 <code>function_calling_config</code> 控制调用模式（AUTO / ANY / NONE）</li>
<li>返回 <code>function_call</code> part</li>
</ul>
<p>虽然 API 格式不同，但抽象层面是一致的：<strong>定义工具 → LLM 决定调用 → 返回结构化调用请求 → 外部执行 → 结果回传</strong>。这也是为什么我们强调框架无关的原理理解——API 会变，原理不会。</p>
<hr>
<h2>5. 工具注册与发现（Tool Registry）</h2>
<h3>5.1 静态注册</h3>
<p>最简单的方式是在代码中硬编码工具列表：</p>
<pre><code class="language-python">TOOLS = [
    get_weather_tool,
    query_db_tool,
    send_email_tool,
]

response = client.chat.completions.create(
    model=&quot;gpt-4&quot;,
    messages=messages,
    tools=TOOLS,
)
</code></pre>
<p>优点是简单直接，缺点是每次新增或修改工具都需要改代码、重新部署。适合工具数量少且稳定的场景。</p>
<h3>5.2 动态注册</h3>
<p>当工具数量增多或需要根据上下文动态调整时，需要一个 Tool Registry：</p>
<pre><code>┌────────────────────────────────────────────────┐
│                Tool Registry                    │
│                                                │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐     │
│  │ weather  │  │ database │  │  email   │     │
│  │  tool    │  │  tool    │  │  tool    │     │
│  └──────────┘  └──────────┘  └──────────┘     │
│  ┌──────────┐  ┌──────────┐                    │
│  │  calc    │  │   file   │                    │
│  │  tool    │  │  tool    │                    │
│  └──────────┘  └──────────┘                    │
│                                                │
│  register(tool) / unregister(name)             │
│  get_tools(filter?) -&gt; List[Tool]              │
│  get_tool(name) -&gt; Tool                        │
│  get_definitions() -&gt; List[Dict]               │
└────────────────────────────────────────────────┘
         │
         │  get_definitions()
         ▼
   ┌───────────┐     tools=[...]     ┌───────────┐
   │  Runtime   │ ──────────────────&gt; │  LLM API  │
   └───────────┘                     └───────────┘
</code></pre>
<h3>5.3 工具选择问题</h3>
<p>当工具数量超过一定阈值（经验值：15-20 个），LLM 的工具选择准确率会明显下降。原因有两个：</p>
<ol>
<li><strong>Context 膨胀</strong>：每个工具定义占用数百 token，20 个工具就是数千 token 的 system prompt，挤占了有效上下文空间。</li>
<li><strong>选择困难</strong>：工具越多，语义越可能重叠，LLM 越难区分应该调用哪个。</li>
</ol>
<h3>5.4 Tool Selection 策略</h3>
<p><strong>策略一：全量传递</strong></p>
<pre><code>所有工具 ──全部传递──&gt; LLM
</code></pre>
<p>适用场景：工具少于 10 个。简单暴力，无额外开销。</p>
<p><strong>策略二：语义过滤</strong></p>
<pre><code>用户输入 ──Embedding──&gt; 向量
                          │
工具描述 ──Embedding──&gt; 向量库 ──Top-K 相似──&gt; 候选工具 ──&gt; LLM
</code></pre>
<p>用 Embedding 计算用户输入与工具描述的语义相似度，只传递 Top-K 最相关的工具。缺点是可能漏掉正确工具。</p>
<p><strong>策略三：两阶段选择</strong></p>
<pre><code>阶段 1：所有工具名 + 简短描述 ──&gt; LLM ──&gt; 选出候选工具 (3-5 个)
阶段 2：候选工具的完整定义     ──&gt; LLM ──&gt; 执行 Tool Call
</code></pre>
<p>第一阶段只传递工具名和一行描述（token 消耗少），让 LLM 先做粗筛；第二阶段只传递选中工具的完整定义。这种方式在工具数量 50+ 的场景下效果最好，代价是多一轮 LLM 调用。</p>
<hr>
<h2>6. 完整代码示例</h2>
<h3>6.1 工具定义</h3>
<pre><code class="language-python">from dataclasses import dataclass, field
from typing import Any, Callable

@dataclass
class Tool:
    &quot;&quot;&quot;工具的统一抽象&quot;&quot;&quot;
    name: str
    description: str
    parameters: dict          # JSON Schema
    function: Callable        # 实际执行的函数
    requires_confirmation: bool = False  # 是否需要用户确认

    def to_openai_schema(self) -&gt; dict:
        &quot;&quot;&quot;转换为 OpenAI API 格式&quot;&quot;&quot;
        return {
            &quot;type&quot;: &quot;function&quot;,
            &quot;function&quot;: {
                &quot;name&quot;: self.name,
                &quot;description&quot;: self.description,
                &quot;parameters&quot;: self.parameters,
            }
        }

# ── 工具实现 ──────────────────────────────────────────────

def get_weather(city: str, unit: str = &quot;celsius&quot;) -&gt; dict:
    &quot;&quot;&quot;模拟天气查询&quot;&quot;&quot;
    # 实际场景中调用天气 API
    mock_data = {
        &quot;北京&quot;: {&quot;temp&quot;: 28, &quot;condition&quot;: &quot;晴&quot;, &quot;humidity&quot;: 45},
        &quot;上海&quot;: {&quot;temp&quot;: 32, &quot;condition&quot;: &quot;多云&quot;, &quot;humidity&quot;: 78},
    }
    data = mock_data.get(city, {&quot;temp&quot;: 20, &quot;condition&quot;: &quot;未知&quot;, &quot;humidity&quot;: 50})
    if unit == &quot;fahrenheit&quot;:
        data[&quot;temp&quot;] = data[&quot;temp&quot;] * 9 / 5 + 32
    return {&quot;city&quot;: city, **data}


def query_database(sql: str, database: str = &quot;default&quot;) -&gt; dict:
    &quot;&quot;&quot;模拟数据库查询&quot;&quot;&quot;
    # 实际场景中执行 SQL
    return {
        &quot;database&quot;: database,
        &quot;query&quot;: sql,
        &quot;rows&quot;: [
            {&quot;id&quot;: 1, &quot;name&quot;: &quot;Alice&quot;, &quot;amount&quot;: 100.0},
            {&quot;id&quot;: 2, &quot;name&quot;: &quot;Bob&quot;, &quot;amount&quot;: 200.0},
        ],
        &quot;row_count&quot;: 2,
    }


def calculate(expression: str) -&gt; dict:
    &quot;&quot;&quot;安全的数学计算&quot;&quot;&quot;
    allowed_chars = set(&quot;0123456789+-*/.() &quot;)
    if not all(c in allowed_chars for c in expression):
        return {&quot;error&quot;: &quot;表达式包含非法字符&quot;}
    try:
        result = eval(expression)  # 生产环境应使用 ast.literal_eval 或专用解析器
        return {&quot;expression&quot;: expression, &quot;result&quot;: result}
    except Exception as e:
        return {&quot;error&quot;: str(e)}


def read_file(file_path: str, encoding: str = &quot;utf-8&quot;) -&gt; dict:
    &quot;&quot;&quot;读取文件内容&quot;&quot;&quot;
    try:
        with open(file_path, &quot;r&quot;, encoding=encoding) as f:
            content = f.read(10000)  # 限制读取大小
        return {&quot;path&quot;: file_path, &quot;content&quot;: content, &quot;size&quot;: len(content)}
    except FileNotFoundError:
        return {&quot;error&quot;: f&quot;文件不存在: {file_path}&quot;}
    except Exception as e:
        return {&quot;error&quot;: str(e)}


def send_email(to: str, subject: str, body: str) -&gt; dict:
    &quot;&quot;&quot;模拟发送邮件&quot;&quot;&quot;
    # 实际场景中调用邮件服务
    return {&quot;status&quot;: &quot;sent&quot;, &quot;to&quot;: to, &quot;subject&quot;: subject}


# ── 工具注册 ──────────────────────────────────────────────

weather_tool = Tool(
    name=&quot;get_weather&quot;,
    description=(
        &quot;查询指定城市的当前天气信息，包括温度、天气状况和湿度。&quot;
        &quot;支持国内主要城市。如果城市不在数据库中，返回默认值。&quot;
    ),
    parameters={
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;city&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;要查询的城市名称，如 &#39;北京&#39;、&#39;上海&#39;&quot;
            },
            &quot;unit&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;],
                &quot;description&quot;: &quot;温度单位，默认摄氏度&quot;
            }
        },
        &quot;required&quot;: [&quot;city&quot;],
    },
    function=get_weather,
)

database_tool = Tool(
    name=&quot;query_database&quot;,
    description=(
        &quot;执行 SQL 查询并返回结果。仅支持 SELECT 语句，&quot;
        &quot;不允许执行 INSERT/UPDATE/DELETE 等写操作。&quot;
        &quot;返回结果包含行数据和总行数。&quot;
    ),
    parameters={
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;sql&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;要执行的 SQL SELECT 语句&quot;
            },
            &quot;database&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;enum&quot;: [&quot;default&quot;, &quot;analytics&quot;, &quot;users&quot;],
                &quot;description&quot;: &quot;目标数据库名称，默认为 &#39;default&#39;&quot;
            }
        },
        &quot;required&quot;: [&quot;sql&quot;],
    },
    function=query_database,
)

calculator_tool = Tool(
    name=&quot;calculate&quot;,
    description=(
        &quot;执行数学计算。支持加减乘除和括号。&quot;
        &quot;输入为数学表达式字符串，如 &#39;(3 + 5) * 2&#39;。&quot;
        &quot;不支持变量和函数调用，仅限纯数值运算。&quot;
    ),
    parameters={
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;expression&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;数学表达式，如 &#39;(3 + 5) * 2&#39;&quot;
            }
        },
        &quot;required&quot;: [&quot;expression&quot;],
    },
    function=calculate,
)

file_tool = Tool(
    name=&quot;read_file&quot;,
    description=(
        &quot;读取指定路径的文本文件内容。最多读取 10000 字符。&quot;
        &quot;仅支持文本文件，不支持二进制文件。&quot;
        &quot;如果文件不存在，返回错误信息。&quot;
    ),
    parameters={
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;file_path&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;文件的绝对路径或相对路径&quot;
            },
            &quot;encoding&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;文件编码，默认 utf-8&quot;
            }
        },
        &quot;required&quot;: [&quot;file_path&quot;],
    },
    function=read_file,
)

email_tool = Tool(
    name=&quot;send_email&quot;,
    description=(
        &quot;向指定收件人发送一封电子邮件。&quot;
        &quot;需要提供收件人地址、邮件主题和正文。&quot;
        &quot;正文支持纯文本格式。&quot;
    ),
    parameters={
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;to&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;收件人邮箱地址&quot;
            },
            &quot;subject&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;邮件主题&quot;
            },
            &quot;body&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;邮件正文，纯文本格式&quot;
            }
        },
        &quot;required&quot;: [&quot;to&quot;, &quot;subject&quot;, &quot;body&quot;],
    },
    function=send_email,
    requires_confirmation=True,  # 发邮件需要用户确认
)
</code></pre>
<h3>6.2 Tool Registry 实现</h3>
<pre><code class="language-python">import json
from typing import Optional

class ToolRegistry:
    &quot;&quot;&quot;工具注册中心&quot;&quot;&quot;

    def __init__(self):
        self._tools: dict[str, Tool] = {}

    def register(self, tool: Tool) -&gt; None:
        if tool.name in self._tools:
            raise ValueError(f&quot;工具 &#39;{tool.name}&#39; 已注册&quot;)
        self._tools[tool.name] = tool

    def unregister(self, name: str) -&gt; None:
        self._tools.pop(name, None)

    def get_tool(self, name: str) -&gt; Optional[Tool]:
        return self._tools.get(name)

    def get_all_tools(self) -&gt; list[Tool]:
        return list(self._tools.values())

    def get_definitions(self, names: list[str] | None = None) -&gt; list[dict]:
        &quot;&quot;&quot;获取工具定义列表（用于传递给 LLM API）&quot;&quot;&quot;
        tools = self._tools.values()
        if names:
            tools = [t for t in tools if t.name in names]
        return [t.to_openai_schema() for t in tools]

    def get_summary(self) -&gt; str:
        &quot;&quot;&quot;获取工具摘要（用于两阶段选择的第一阶段）&quot;&quot;&quot;
        lines = []
        for tool in self._tools.values():
            # 只取 description 的第一句
            short_desc = tool.description.split(&quot;。&quot;)[0] + &quot;。&quot;
            lines.append(f&quot;- {tool.name}: {short_desc}&quot;)
        return &quot;\n&quot;.join(lines)


# 初始化 Registry
registry = ToolRegistry()
for tool in [weather_tool, database_tool, calculator_tool, file_tool, email_tool]:
    registry.register(tool)
</code></pre>
<h3>6.3 Tool Dispatcher 实现</h3>
<pre><code class="language-python">import json
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed

class ToolDispatcher:
    &quot;&quot;&quot;
    工具调度器：解析 LLM 返回的 tool calls，执行对应工具，收集结果。
    &quot;&quot;&quot;

    def __init__(self, registry: ToolRegistry, max_parallel: int = 5):
        self.registry = registry
        self.max_parallel = max_parallel

    def validate_arguments(self, tool: Tool, arguments: dict) -&gt; list[str]:
        &quot;&quot;&quot;基础参数验证（生产环境建议使用 jsonschema 库）&quot;&quot;&quot;
        errors = []
        schema = tool.parameters
        required = schema.get(&quot;required&quot;, [])
        properties = schema.get(&quot;properties&quot;, {})

        # 检查必填参数
        for param in required:
            if param not in arguments:
                errors.append(f&quot;缺少必填参数: {param}&quot;)

        # 检查参数类型和枚举
        for param, value in arguments.items():
            if param not in properties:
                errors.append(f&quot;未知参数: {param}&quot;)
                continue
            prop_schema = properties[param]
            if &quot;enum&quot; in prop_schema and value not in prop_schema[&quot;enum&quot;]:
                errors.append(
                    f&quot;参数 &#39;{param}&#39; 的值 &#39;{value}&#39; &quot;
                    f&quot;不在允许范围内: {prop_schema[&#39;enum&#39;]}&quot;
                )

        return errors

    def execute_single(self, tool_call: dict) -&gt; dict:
        &quot;&quot;&quot;执行单个工具调用&quot;&quot;&quot;
        name = tool_call[&quot;function&quot;][&quot;name&quot;]
        raw_args = tool_call[&quot;function&quot;][&quot;arguments&quot;]
        call_id = tool_call.get(&quot;id&quot;, &quot;unknown&quot;)

        # 1. 查找工具
        tool = self.registry.get_tool(name)
        if not tool:
            return {
                &quot;tool_call_id&quot;: call_id,
                &quot;role&quot;: &quot;tool&quot;,
                &quot;content&quot;: json.dumps({&quot;error&quot;: f&quot;工具 &#39;{name}&#39; 不存在&quot;}),
            }

        # 2. 解析参数
        try:
            arguments = json.loads(raw_args) if isinstance(raw_args, str) else raw_args
        except json.JSONDecodeError as e:
            return {
                &quot;tool_call_id&quot;: call_id,
                &quot;role&quot;: &quot;tool&quot;,
                &quot;content&quot;: json.dumps({&quot;error&quot;: f&quot;参数 JSON 解析失败: {e}&quot;}),
            }

        # 3. 验证参数
        errors = self.validate_arguments(tool, arguments)
        if errors:
            return {
                &quot;tool_call_id&quot;: call_id,
                &quot;role&quot;: &quot;tool&quot;,
                &quot;content&quot;: json.dumps({&quot;error&quot;: &quot;参数验证失败&quot;, &quot;details&quot;: errors}),
            }

        # 4. 执行工具
        try:
            result = tool.function(**arguments)
            return {
                &quot;tool_call_id&quot;: call_id,
                &quot;role&quot;: &quot;tool&quot;,
                &quot;content&quot;: json.dumps(result, ensure_ascii=False),
            }
        except Exception as e:
            return {
                &quot;tool_call_id&quot;: call_id,
                &quot;role&quot;: &quot;tool&quot;,
                &quot;content&quot;: json.dumps({
                    &quot;error&quot;: f&quot;工具执行失败: {type(e).__name__}: {e}&quot;,
                    &quot;traceback&quot;: traceback.format_exc()[-500:],  # 截断过长的堆栈
                }),
            }

    def execute_parallel(self, tool_calls: list[dict]) -&gt; list[dict]:
        &quot;&quot;&quot;并行执行多个工具调用&quot;&quot;&quot;
        if len(tool_calls) == 1:
            return [self.execute_single(tool_calls[0])]

        results = []
        with ThreadPoolExecutor(max_workers=self.max_parallel) as executor:
            future_to_call = {
                executor.submit(self.execute_single, tc): tc
                for tc in tool_calls
            }
            for future in as_completed(future_to_call):
                results.append(future.result())

        # 按原始顺序排列结果
        id_to_result = {r[&quot;tool_call_id&quot;]: r for r in results}
        ordered = []
        for tc in tool_calls:
            call_id = tc.get(&quot;id&quot;, &quot;unknown&quot;)
            ordered.append(id_to_result.get(call_id, results.pop(0)))
        return ordered


dispatcher = ToolDispatcher(registry)
</code></pre>
<h3>6.4 完整对话循环</h3>
<pre><code class="language-python">from openai import OpenAI

def run_agent_loop(
    client: OpenAI,
    user_message: str,
    registry: ToolRegistry,
    dispatcher: ToolDispatcher,
    max_iterations: int = 10,
) -&gt; str:
    &quot;&quot;&quot;
    完整的 Agent 对话循环，支持多轮 Tool Calling。
    &quot;&quot;&quot;
    messages = [
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是一个有用的助手，可以使用工具来回答用户的问题。&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_message},
    ]
    tools = registry.get_definitions()

    for i in range(max_iterations):
        response = client.chat.completions.create(
            model=&quot;gpt-4&quot;,
            messages=messages,
            tools=tools if tools else None,
        )
        choice = response.choices[0]
        message = choice.message

        # 如果 LLM 没有调用工具，直接返回文本回答
        if not message.tool_calls:
            return message.content

        # 将 LLM 的回复（含 tool_calls）加入消息历史
        messages.append(message.model_dump())

        # 执行所有工具调用（支持并行）
        tool_calls = [tc.model_dump() for tc in message.tool_calls]
        results = dispatcher.execute_parallel(tool_calls)

        # 将工具执行结果加入消息历史
        for result in results:
            messages.append(result)

        # 继续循环，让 LLM 基于工具结果做下一步决策

    return &quot;达到最大迭代次数，对话终止。&quot;


# 使用示例
# client = OpenAI()
# answer = run_agent_loop(client, &quot;北京今天天气怎么样？然后帮我算一下 28 * 9/5 + 32&quot;, registry, dispatcher)
# print(answer)
</code></pre>
<hr>
<h2>7. 错误处理与验证</h2>
<p>Tool Calling 中的错误来源比常规 API 调用更多，因为链条更长：用户输入 → LLM 推理 → 参数生成 → 参数验证 → 工具执行 → 结果回传 → LLM 再推理。每一环都可能出错。</p>
<h3>7.1 参数验证</h3>
<p>LLM 生成的参数并不总是合法的。常见问题：</p>
<pre><code class="language-python"># LLM 可能生成的&quot;有问题&quot;的参数

# 1. 类型错误：期望 string，给了 number
{&quot;city&quot;: 123}

# 2. 枚举越界：给了不在 enum 中的值
{&quot;unit&quot;: &quot;kelvin&quot;}      # enum 里只有 celsius / fahrenheit

# 3. 格式错误：JSON 语法不对
&#39;{&quot;city&quot;: &quot;北京&quot;,}&#39;      # 尾部多余逗号（严格 JSON 不允许）

# 4. 幻觉参数：编造了不存在的参数
{&quot;city&quot;: &quot;北京&quot;, &quot;forecast_days&quot;: 7}  # 工具根本没有这个参数

# 5. 语义错误：参数值表面合法但语义错误
{&quot;sql&quot;: &quot;DROP TABLE users&quot;}  # 传了一条 DELETE 语句给 SELECT-only 工具
</code></pre>
<p>应对策略是 <strong>分层验证</strong>：</p>
<pre><code class="language-python">def validate_and_execute(tool: Tool, raw_arguments: str) -&gt; dict:
    # 第一层：JSON 语法
    try:
        args = json.loads(raw_arguments)
    except json.JSONDecodeError:
        return {&quot;error&quot;: &quot;参数不是合法的 JSON&quot;}

    # 第二层：Schema 验证（使用 jsonschema 库）
    from jsonschema import validate, ValidationError
    try:
        validate(instance=args, schema=tool.parameters)
    except ValidationError as e:
        return {&quot;error&quot;: f&quot;参数验证失败: {e.message}&quot;}

    # 第三层：业务规则验证
    if tool.name == &quot;query_database&quot;:
        sql = args.get(&quot;sql&quot;, &quot;&quot;).strip().upper()
        if not sql.startswith(&quot;SELECT&quot;):
            return {&quot;error&quot;: &quot;仅支持 SELECT 查询&quot;}

    # 执行
    return tool.function(**args)
</code></pre>
<h3>7.2 工具执行失败的反馈</h3>
<p>当工具执行失败时，最重要的原则是：<strong>将错误信息回传给 LLM，让它决定下一步</strong>。</p>
<pre><code class="language-python"># 不要这样做 —— 对用户抛出原始异常
raise RuntimeError(&quot;Connection timeout to weather API&quot;)

# 应该这样做 —— 将错误包装为工具结果，回传给 LLM
{
    &quot;tool_call_id&quot;: &quot;call_abc123&quot;,
    &quot;role&quot;: &quot;tool&quot;,
    &quot;content&quot;: json.dumps({
        &quot;error&quot;: &quot;天气 API 连接超时，请稍后重试或尝试查询其他城市&quot;,
        &quot;error_type&quot;: &quot;timeout&quot;,
        &quot;retryable&quot;: True
    })
}
</code></pre>
<p>LLM 拿到这个错误信息后，可能会：</p>
<ul>
<li>换一种方式重试（比如换个参数）</li>
<li>告知用户当前无法完成</li>
<li>尝试用其他工具达成目标</li>
</ul>
<h3>7.3 重试策略</h3>
<pre><code>                  ┌──────────────────────────┐
                  │    Tool Call 失败         │
                  └──────────┬───────────────┘
                             │
                   ┌─────────▼─────────┐
                   │  错误类型判断       │
                   └─────────┬─────────┘
                             │
              ┌──────────────┼──────────────┐
              │              │              │
        ┌─────▼─────┐ ┌─────▼─────┐ ┌─────▼─────┐
        │ 可重试     │ │ 参数错误   │ │ 不可恢复   │
        │(超时/限流) │ │(类型/格式) │ │(权限/404) │
        └─────┬─────┘ └─────┬─────┘ └─────┬─────┘
              │              │              │
        ┌─────▼─────┐ ┌─────▼─────┐ ┌─────▼─────┐
        │ Runtime    │ │ 回传 LLM  │ │ 回传 LLM  │
        │ 自动重试   │ │ 让它修正   │ │ 让它放弃   │
        │ (指数退避) │ │ 参数       │ │ 或换方案   │
        └───────────┘ └───────────┘ └───────────┘
</code></pre>
<p>核心原则：<strong>可重试的错误由 Runtime 处理，不可重试的错误交给 LLM 决策</strong>。</p>
<ul>
<li><strong>瞬时错误</strong>（网络超时、限流）：Runtime 自动重试，设置退避策略和最大重试次数，不需要浪费 LLM 的 token。</li>
<li><strong>参数错误</strong>：回传给 LLM，它可能会修正参数重新调用。</li>
<li><strong>永久错误</strong>（权限不足、资源不存在）：回传给 LLM，让它换一种方案或如实告知用户。</li>
</ul>
<h3>7.4 幂等性考量</h3>
<p>当重试机制存在时，幂等性就变得至关重要。</p>
<pre><code class="language-python"># 幂等操作 —— 重试安全
get_weather(&quot;北京&quot;)           # 多次调用结果相同
query_database(&quot;SELECT ...&quot;)  # 只读查询，天然幂等

# 非幂等操作 —— 重试危险
send_email(to=&quot;a@b.com&quot;, ...)  # 重试 = 发两封邮件
create_order(item=&quot;iPhone&quot;)    # 重试 = 创建两个订单
</code></pre>
<p>对于非幂等操作，要么禁止自动重试，要么引入幂等 key：</p>
<pre><code class="language-python">def send_email_idempotent(to: str, subject: str, body: str, idempotency_key: str) -&gt; dict:
    &quot;&quot;&quot;带幂等 key 的邮件发送&quot;&quot;&quot;
    if is_already_sent(idempotency_key):
        return {&quot;status&quot;: &quot;already_sent&quot;, &quot;message&quot;: &quot;该请求已处理，跳过重复发送&quot;}
    result = _do_send_email(to, subject, body)
    mark_as_sent(idempotency_key)
    return result
</code></pre>
<hr>
<h2>8. 安全性</h2>
<p>Tool Calling 打开了 LLM 与外部世界的通道，也同时打开了攻击面。</p>
<h3>8.1 工具权限控制</h3>
<p>不是所有工具都应该对所有用户开放。一个合理的权限模型：</p>
<pre><code class="language-python">from enum import Enum

class ToolPermission(Enum):
    READ = &quot;read&quot;        # 只读操作：查询天气、读文件
    WRITE = &quot;write&quot;      # 写操作：发邮件、创建记录
    ADMIN = &quot;admin&quot;      # 管理操作：删除数据、修改配置

class SecureToolRegistry(ToolRegistry):
    &quot;&quot;&quot;带权限控制的工具注册中心&quot;&quot;&quot;

    def __init__(self):
        super().__init__()
        self._permissions: dict[str, ToolPermission] = {}

    def register(self, tool: Tool, permission: ToolPermission = ToolPermission.READ):
        super().register(tool)
        self._permissions[tool.name] = permission

    def get_definitions(
        self,
        names: list[str] | None = None,
        max_permission: ToolPermission = ToolPermission.READ,
    ) -&gt; list[dict]:
        &quot;&quot;&quot;只返回用户权限范围内的工具&quot;&quot;&quot;
        permission_levels = {
            ToolPermission.READ: 0,
            ToolPermission.WRITE: 1,
            ToolPermission.ADMIN: 2,
        }
        max_level = permission_levels[max_permission]
        allowed = [
            t for t in self._tools.values()
            if permission_levels[self._permissions.get(t.name, ToolPermission.ADMIN)] &lt;= max_level
        ]
        if names:
            allowed = [t for t in allowed if t.name in names]
        return [t.to_openai_schema() for t in allowed]
</code></pre>
<h3>8.2 参数注入风险</h3>
<p>LLM 的参数生成可以被 Prompt Injection 操纵。考虑以下场景：</p>
<pre><code>用户输入: &quot;帮我查一下订单，user_id 是 U00012345; DROP TABLE orders; --&quot;
</code></pre>
<p>如果 <code>query_database</code> 工具直接拼接 SQL，这就变成了一次经典的 SQL 注入。防护措施：</p>
<ol>
<li><strong>参数化查询</strong>：工具内部必须使用参数化 SQL，绝不拼接。</li>
<li><strong>白名单校验</strong>：用正则或枚举限制参数值的格式。</li>
<li><strong>最小权限原则</strong>：数据库连接使用只读账号。</li>
</ol>
<h3>8.3 Sandbox 执行</h3>
<p>对于高风险工具（如代码执行、文件操作），应在隔离环境中执行：</p>
<pre><code>┌──────────────────────────────────────────────┐
│  Host Runtime                                 │
│                                              │
│   ┌─────────────┐     ┌──────────────────┐   │
│   │  Safe Tools  │     │    Sandbox       │   │
│   │  (天气/计算) │     │  ┌────────────┐  │   │
│   │  直接执行    │     │  │ Risky Tools│  │   │
│   └─────────────┘     │  │ (代码/文件) │  │   │
│                       │  │ 隔离执行    │  │   │
│                       │  └────────────┘  │   │
│                       │  - 网络受限      │   │
│                       │  - 文件系统隔离  │   │
│                       │  - 执行时间限制  │   │
│                       │  - 资源配额      │   │
│                       └──────────────────┘   │
└──────────────────────────────────────────────┘
</code></pre>
<p>Sandbox 的实现方式取决于部署环境：</p>
<ul>
<li><strong>Docker 容器</strong>：最常见，隔离性好</li>
<li><strong>gVisor / Firecracker</strong>：更强的隔离，适合多租户</li>
<li><strong>WASM</strong>：轻量级沙箱，启动快</li>
<li><strong>子进程 + seccomp</strong>：Linux 下的轻量方案</li>
</ul>
<hr>
<h2>9. Trade-off 分析</h2>
<h3>9.1 工具数量 vs 选择准确率</h3>
<pre><code>选择准确率
  100% │ ****
       │     ****
   90% │         ****
       │             ****
   80% │                 ****
       │                     ****
   70% │                         ****
       │                             ****
   60% │                                 ****
       ├───┬───┬───┬───┬───┬───┬───┬───┬───── 工具数量
       0   5  10  15  20  25  30  35  40

       |&lt;-- 全量传递 --&gt;|&lt;- 需要过滤策略 -&gt;|
</code></pre>
<ul>
<li><strong>&lt; 10 个工具</strong>：全量传递，不需要过滤。</li>
<li><strong>10-20 个工具</strong>：准确率开始下降，可通过优化 description 缓解。</li>
<li><strong>&gt; 20 个工具</strong>：必须引入 Tool Selection 策略（语义过滤或两阶段选择）。</li>
<li><strong>&gt; 50 个工具</strong>：两阶段选择几乎是唯一可行方案，或者按领域拆分为多个 Agent。</li>
</ul>
<h3>9.2 工具描述详细度 vs Token 消耗</h3>
<p>每个工具定义大约占用 100-500 token（取决于描述长度和参数数量）。20 个工具就是 2000-10000 token 的系统开销，这是每次 API 调用都要付出的 <strong>固定成本</strong>。</p>
<pre><code>                        描述详细度
                  低 ◄──────────────► 高
                  │                    │
  Token 消耗   低 │  ⚡ 省钱但模糊     │
                  │  LLM 可能误选工具  │
                  │                    │
              高 │                    │  📖 精确但昂贵
                  │                    │  LLM 选择更准确
                  │                    │
</code></pre>
<p>实践建议：</p>
<ul>
<li>工具 <code>name</code> 起好名字（零额外 token 成本，但信息量大）</li>
<li><code>description</code> 控制在 2-3 句话</li>
<li>参数的 <code>description</code> 控制在 1 句话 + 1 个示例</li>
<li>用 <code>enum</code> 和 <code>required</code> 代替冗长的文字约束</li>
</ul>
<h3>9.3 确定性执行 vs LLM 灵活性</h3>
<pre><code>确定性                                          灵活性
  │                                              │
  │  硬编码工作流           Agent Tool Calling     │
  │  if/else 分支            LLM 自由选择工具     │
  │  规则引擎                自动组合工具链        │
  │                                              │
  │  ✅ 可预测              ✅ 处理模糊意图        │
  │  ✅ 可审计              ✅ 适应新场景          │
  │  ✅ 低延迟              ✅ 用户体验自然        │
  │  ❌ 不灵活              ❌ 不可预测            │
  │  ❌ 维护成本高          ❌ 调试困难            │
  │  ❌ 无法处理长尾        ❌ 成本高              │
</code></pre>
<p>决策框架：</p>
<table>
<thead>
<tr>
<th>场景特征</th>
<th>推荐方案</th>
</tr>
</thead>
<tbody><tr>
<td>流程固定、合规要求高</td>
<td>硬编码工作流 + Tool Calling 作为执行层</td>
</tr>
<tr>
<td>意图模糊、工具组合多变</td>
<td>完全由 LLM 驱动的 Tool Calling</td>
</tr>
<tr>
<td>核心路径固定、边缘场景多</td>
<td>混合方案：主流程硬编码，长尾交给 LLM</td>
</tr>
</tbody></table>
<p>关键洞察：Tool Calling 不是非此即彼的选择。你可以让 LLM 决定 <strong>是否</strong> 调用工具，但用代码控制 <strong>调用后的流程</strong>。比如 LLM 决定&quot;需要查天气&quot;，但查完天气后的处理逻辑是确定性的代码。</p>
<hr>
<h2>10. 常见陷阱</h2>
<p>在实际工程中，以下几个坑值得提前规避：</p>
<p><strong>1. 工具描述与实际行为不一致</strong></p>
<p>工具描述说&quot;返回最近 30 天的订单&quot;，但实际实现返回所有订单。LLM 会基于描述做出错误假设，导致下游逻辑出错。<strong>描述就是契约，必须与实现严格一致</strong>。</p>
<p><strong>2. 忽略工具结果的 Token 消耗</strong></p>
<p>工具返回的结果会作为下一轮消息传给 LLM。如果一个数据库查询返回了 1000 行数据，这些数据全部变成 input token。务必在工具层面限制返回数据量。</p>
<pre><code class="language-python">def query_database(sql: str, database: str = &quot;default&quot;) -&gt; dict:
    results = _execute_query(sql, database)
    # 限制返回行数，避免 token 爆炸
    if len(results) &gt; 50:
        return {
            &quot;rows&quot;: results[:50],
            &quot;total_count&quot;: len(results),
            &quot;truncated&quot;: True,
            &quot;message&quot;: f&quot;结果共 {len(results)} 行，仅返回前 50 行&quot;
        }
    return {&quot;rows&quot;: results, &quot;total_count&quot;: len(results)}
</code></pre>
<p><strong>3. 缺少 stop condition</strong></p>
<p>如果 LLM 反复调用同一个工具（比如因为错误一直重试），而没有最大迭代次数限制，系统会陷入无限循环。前面代码中的 <code>max_iterations</code> 参数就是为此设计的。</p>
<p><strong>4. 并行调用的顺序依赖</strong></p>
<p>LLM 可能在一次回复中请求并行调用两个工具，但这两个工具之间有隐含的顺序依赖（比如先查用户 ID，再用这个 ID 查订单）。Runtime 需要能识别这种情况，或者在工具描述中引导 LLM 分步调用。</p>
<hr>
<h2>11. 总结与展望</h2>
<p>Tool Calling 的本质是一个精心设计的 <strong>协议</strong>：</p>
<pre><code>┌───────────┐    JSON Schema    ┌───────────┐    Function    ┌───────────┐
│           │    (契约)          │           │    (执行)      │           │
│    LLM    │ ◄───────────────► │  Runtime  │ ◄────────────► │   Tools   │
│  (决策层) │   Tool Call JSON   │  (调度层) │   Function     │  (能力层) │
│           │   Tool Result      │           │   Call/Return  │           │
└───────────┘                   └───────────┘                └───────────┘
</code></pre>
<ul>
<li><strong>LLM</strong> 负责理解意图、选择工具、生成参数——它是决策者。</li>
<li><strong>Runtime</strong> 负责验证、路由、执行、错误处理——它是执行者。</li>
<li><strong>Tools</strong> 是具体的能力——它们是能力的载体。</li>
<li><strong>JSON Schema</strong> 是三者之间的契约——它定义了什么可以做、怎么做。</li>
</ul>
<p>理解了这个架构，你就能在任何框架（LangChain、LlamaIndex、Semantic Kernel，或者自己写的 Runtime）上实现 Tool Calling，因为底层原理是相同的。</p>
<p>但 Tool Calling 只是让 Agent 有了&quot;手&quot;。要让 Agent 真正好用，还需要精心设计的 Prompt 来引导 LLM 的决策——什么时候该调工具、什么时候该直接回答、遇到错误该怎么处理、多个工具之间如何协调。这就是下一篇 <strong>Prompt Engineering for Agents</strong> 要深入讨论的主题。</p>
<hr>
<blockquote>
<p><strong>系列导航</strong>：本文是 Agentic 系列的第 05 篇。</p>
<ul>
<li>上一篇：<a href="/blog/engineering/agentic/04-The%20Agent%20Control%20Loop">04 | The Agent Control Loop</a></li>
<li>下一篇：<a href="/blog/engineering/agentic/06-Prompt%20Engineering%20for%20Agents">06 | Prompt Engineering for Agents</a></li>
<li>完整目录：<a href="/blog/engineering/agentic/01-From%20LLM%20to%20Agent">01 | From LLM to Agent</a></li>
</ul>
</blockquote>
1b:T9c80,<h1>Agent Runtime from Scratch: 不依赖框架构建 Agent</h1>
<blockquote>
<p>框架是加速器，不是知识的替代品。</p>
<p>本文是 Agentic 系列第 07 篇，也是 Phase 2 的收官之作。我们将抛开所有框架，用纯 Python 从零构建一个功能完整的 Agent Runtime。这是系列中代码量最大的一篇——每一行代码都指向同一个目标：让你彻底理解 Agent 的运行本质。</p>
</blockquote>
<hr>
<h2>1. 为什么要自己写 Agent Runtime？</h2>
<p>前几篇我们理解了控制循环（第 04 篇）、Tool Calling（第 05 篇）、Prompt 工程（第 06 篇）。但这些还停留在概念层面。现在的问题是：<strong>不用 LangChain、不用 LangGraph——你能写出一个 Agent 吗？</strong></p>
<p>自建 Runtime 的价值：</p>
<ul>
<li><strong>透明性</strong>：每一行代码你都清楚，出了问题知道往哪里看</li>
<li><strong>可控性</strong>：精确控制重试策略、超时机制、消息压缩、工具调度，而不被框架的默认行为绑架</li>
<li><strong>本质理解</strong>：理解了 Runtime 本质，用任何框架时都能一眼看出它在做什么、哪里做得不好</li>
</ul>
<p>更现实的原因：<strong>生产环境中很多 Agent 系统最终都走向了自研</strong>。框架在 PoC 阶段很方便，但到了需要精细控制 Token 成本、自定义 Observability、与内部基础设施深度集成时，框架往往成为障碍。</p>
<hr>
<h2>2. 架构设计</h2>
<pre><code>┌───────────────────────────────────────────────────┐
│                   AgentRuntime                     │
│                (Core Control Loop)                 │
│                                                    │
│  ┌────────────┐  ┌──────────────┐  ┌───────────┐ │
│  │ LLMClient  │  │MessageManager│  │ StateStore │ │
│  │ chat()     │  │ append()     │  │ save()     │ │
│  │ stream()   │  │ compress()   │  │ load()     │ │
│  │ retry()    │  │ count_tokens │  │ clear()    │ │
│  └─────┬──────┘  └──────┬───────┘  └───────────┘ │
│        │                │                          │
│        ▼                ▼                          │
│  ┌────────────────────────────────────┐            │
│  │          Runtime Loop              │            │
│  │  while not done and turns &lt; max:   │            │
│  │    response = llm.chat(messages)   │            │
│  │    if tool_calls:                  │            │
│  │      results = executor.run()      │            │
│  │    else: done = True               │            │
│  └──────────┬─────────────────────────┘            │
│       ┌─────┴──────┐                               │
│       ▼            ▼                                │
│  ┌──────────┐ ┌────────────┐                       │
│  │ToolRegist│ │ToolExecutor│                       │
│  │ register │ │ execute()  │                       │
│  │ schema() │ │ parallel() │                       │
│  └──────────┘ └────────────┘                       │
└───────────────────────────────────────────────────┘
</code></pre>
<p><strong>核心设计原则——职责分离</strong>：</p>
<table>
<thead>
<tr>
<th>模块</th>
<th>职责</th>
<th>边界</th>
</tr>
</thead>
<tbody><tr>
<td><code>LLMClient</code></td>
<td>封装模型调用，处理重试</td>
<td>只管&quot;调 API&quot;，不管消息历史</td>
</tr>
<tr>
<td><code>ToolRegistry</code></td>
<td>注册工具，生成 JSON Schema</td>
<td>只管&quot;有哪些工具&quot;，不管怎么调</td>
</tr>
<tr>
<td><code>ToolExecutor</code></td>
<td>解析 tool_calls，分发执行</td>
<td>只管&quot;执行工具&quot;，不管谁触发的</td>
</tr>
<tr>
<td><code>MessageManager</code></td>
<td>管理消息列表，Token 计数和压缩</td>
<td>只管&quot;消息&quot;，不管消息从哪来</td>
</tr>
<tr>
<td><code>AgentRuntime</code></td>
<td>组装一切，驱动控制循环</td>
<td>只管&quot;编排&quot;，不自己做具体事</td>
</tr>
</tbody></table>
<p>任何模块可独立替换。换 Anthropic API？只改 <code>LLMClient</code>。状态存 Redis？只改 <code>StateStore</code>。Runtime 本身不需要变动。</p>
<hr>
<h2>3. 逐步构建</h2>
<h3>Step 1: LLMClient — 封装模型调用</h3>
<p>封装 OpenAI 兼容接口，支持 <code>tools</code> / <code>tool_choice</code>，处理流式/非流式，实现指数退避重试。</p>
<pre><code class="language-python"># llm_client.py
import time, json, logging
from dataclasses import dataclass, field
from typing import Optional, Generator
from openai import OpenAI, APIError, RateLimitError, APITimeoutError

logger = logging.getLogger(__name__)

@dataclass
class ToolCall:
    id: str
    name: str
    arguments: dict

@dataclass
class LLMResponse:
    content: Optional[str] = None
    tool_calls: list[ToolCall] = field(default_factory=list)
    usage: dict = field(default_factory=dict)
    finish_reason: str = &quot;&quot;

    @property
    def has_tool_calls(self) -&gt; bool:
        return len(self.tool_calls) &gt; 0

class LLMClient:
    RETRYABLE_ERRORS = (RateLimitError, APITimeoutError, APIError)

    def __init__(self, model=&quot;gpt-4o&quot;, base_url=None, api_key=None,
                 max_retries=3, retry_base_delay=1.0, timeout=60.0):
        self.model = model
        self.max_retries = max_retries
        self.retry_base_delay = retry_base_delay
        self.client = OpenAI(base_url=base_url, api_key=api_key, timeout=timeout)

    def chat(self, messages, tools=None, tool_choice=&quot;auto&quot;, temperature=0.0):
        kwargs = {&quot;model&quot;: self.model, &quot;messages&quot;: messages,
                  &quot;temperature&quot;: temperature}
        if tools:
            kwargs[&quot;tools&quot;] = tools
            kwargs[&quot;tool_choice&quot;] = tool_choice
        raw = self._call_with_retry(**kwargs)
        return self._parse_response(raw)

    def stream(self, messages, tools=None, tool_choice=&quot;auto&quot;,
               temperature=0.0) -&gt; Generator[LLMResponse, None, None]:
        kwargs = {&quot;model&quot;: self.model, &quot;messages&quot;: messages,
                  &quot;temperature&quot;: temperature, &quot;stream&quot;: True}
        if tools:
            kwargs[&quot;tools&quot;] = tools
            kwargs[&quot;tool_choice&quot;] = tool_choice

        accumulated_tool_calls: dict[int, dict] = {}
        for chunk in self._call_with_retry(**kwargs):
            delta = chunk.choices[0].delta if chunk.choices else None
            if not delta:
                continue
            if delta.content:
                yield LLMResponse(content=delta.content)
            # 流式下 tool_calls 分片到达，需要累积拼装
            if delta.tool_calls:
                for tc in delta.tool_calls:
                    idx = tc.index
                    if idx not in accumulated_tool_calls:
                        accumulated_tool_calls[idx] = {
                            &quot;id&quot;: &quot;&quot;, &quot;name&quot;: &quot;&quot;, &quot;arguments&quot;: &quot;&quot;}
                    if tc.id: accumulated_tool_calls[idx][&quot;id&quot;] = tc.id
                    if tc.function.name:
                        accumulated_tool_calls[idx][&quot;name&quot;] = tc.function.name
                    if tc.function.arguments:
                        accumulated_tool_calls[idx][&quot;arguments&quot;] += \
                            tc.function.arguments

        if accumulated_tool_calls:
            tool_calls = []
            for d in accumulated_tool_calls.values():
                args = json.loads(d[&quot;arguments&quot;]) if d[&quot;arguments&quot;] else {}
                tool_calls.append(ToolCall(d[&quot;id&quot;], d[&quot;name&quot;], args))
            yield LLMResponse(tool_calls=tool_calls)

    def _call_with_retry(self, **kwargs):
        last_error = None
        for attempt in range(self.max_retries + 1):
            try:
                return self.client.chat.completions.create(**kwargs)
            except self.RETRYABLE_ERRORS as e:
                last_error = e
                if attempt &lt; self.max_retries:
                    delay = self.retry_base_delay * (2 ** attempt)
                    logger.warning(f&quot;Retry {attempt+1} in {delay}s: {e}&quot;)
                    time.sleep(delay)
        raise last_error

    def _parse_response(self, raw) -&gt; LLMResponse:
        choice = raw.choices[0]
        msg = choice.message
        tool_calls = []
        if msg.tool_calls:
            for tc in msg.tool_calls:
                args = json.loads(tc.function.arguments) \
                    if tc.function.arguments else {}
                tool_calls.append(ToolCall(tc.id, tc.function.name, args))
        return LLMResponse(
            content=msg.content, tool_calls=tool_calls,
            usage={&quot;prompt_tokens&quot;: raw.usage.prompt_tokens,
                   &quot;completion_tokens&quot;: raw.usage.completion_tokens,
                   &quot;total_tokens&quot;: raw.usage.total_tokens},
            finish_reason=choice.finish_reason)
</code></pre>
<p><strong>关键设计决策</strong>：</p>
<ol>
<li><strong>统一 <code>LLMResponse</code></strong>：无论底层用什么模型，Runtime 只看到同一结构——适配器模式。</li>
<li><strong>重试只针对可恢复错误</strong>：<code>RateLimitError</code> 值得重试，<code>AuthenticationError</code> 重试一万次也没用。</li>
<li><strong>流式 tool_calls 累积拼装</strong>：OpenAI 把 tool_calls 拆成多个 chunk（先发 name，再逐步发 arguments），必须在客户端拼装。这是容易踩的坑。</li>
</ol>
<hr>
<h3>Step 2: ToolRegistry — 工具注册与发现</h3>
<p>用装饰器注册函数，通过 type hints 和 docstring 自动生成 OpenAI 格式的 JSON Schema。</p>
<pre><code class="language-python"># tool_registry.py
import inspect, json
from typing import Any, Callable, Optional, get_type_hints

TYPE_MAP = {str: &quot;string&quot;, int: &quot;integer&quot;, float: &quot;number&quot;,
            bool: &quot;boolean&quot;, list: &quot;array&quot;, dict: &quot;object&quot;}

class ToolRegistry:
    def __init__(self):
        self._tools: dict[str, Callable] = {}
        self._schemas: dict[str, dict] = {}

    def tool(self, name=None, description=None):
        &quot;&quot;&quot;装饰器注册工具&quot;&quot;&quot;
        def decorator(func):
            n = name or func.__name__
            d = description or (func.__doc__ or &quot;&quot;).strip().split(&quot;\n&quot;)[0]
            self._tools[n] = func
            self._schemas[n] = self._gen_schema(func, n, d)
            return func
        return decorator

    def register(self, func, name=None, description=None):
        &quot;&quot;&quot;命令式注册（适用于无法加装饰器的场景）&quot;&quot;&quot;
        n = name or func.__name__
        d = description or (func.__doc__ or &quot;&quot;).strip().split(&quot;\n&quot;)[0]
        self._tools[n] = func
        self._schemas[n] = self._gen_schema(func, n, d)

    def get_function(self, name): return self._tools.get(name)
    def get_all_schemas(self): return list(self._schemas.values())
    def list_tools(self): return list(self._tools.keys())

    def _gen_schema(self, func, name, description):
        sig = inspect.signature(func)
        hints = get_type_hints(func)
        properties, required = {}, []
        for pname, param in sig.parameters.items():
            if pname in (&quot;self&quot;, &quot;cls&quot;): continue
            ptype = hints.get(pname, str)
            prop = {&quot;type&quot;: TYPE_MAP.get(ptype, &quot;string&quot;)}
            # 从 Google 风格 docstring 提取参数描述
            pdesc = self._param_desc(func, pname)
            if pdesc: prop[&quot;description&quot;] = pdesc
            properties[pname] = prop
            if param.default is inspect.Parameter.empty:
                required.append(pname)
        return {&quot;type&quot;: &quot;function&quot;, &quot;function&quot;: {
            &quot;name&quot;: name, &quot;description&quot;: description,
            &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;,
                           &quot;properties&quot;: properties, &quot;required&quot;: required}}}

    @staticmethod
    def _param_desc(func, param_name):
        doc = func.__doc__ or &quot;&quot;
        in_args = False
        for line in doc.split(&quot;\n&quot;):
            s = line.strip()
            if s.lower().startswith(&quot;args:&quot;): in_args = True; continue
            if in_args and param_name + &quot;:&quot; in s:
                return s.split(&quot;:&quot;, 1)[1].strip()
        return &quot;&quot;
</code></pre>
<p>验证效果：</p>
<pre><code class="language-python">registry = ToolRegistry()

@registry.tool()
def web_search(query: str, max_results: int = 5) -&gt; str:
    &quot;&quot;&quot;搜索网页内容
    Args:
        query: 搜索关键词
        max_results: 最大返回结果数量
    &quot;&quot;&quot;
    return f&quot;Results for: {query}&quot;

# 输出 OpenAI 格式的 tool schema
# {&quot;type&quot;:&quot;function&quot;,&quot;function&quot;:{&quot;name&quot;:&quot;web_search&quot;,&quot;description&quot;:&quot;搜索网页内容&quot;,
#  &quot;parameters&quot;:{&quot;type&quot;:&quot;object&quot;,&quot;properties&quot;:{&quot;query&quot;:{&quot;type&quot;:&quot;string&quot;,
#  &quot;description&quot;:&quot;搜索关键词&quot;},&quot;max_results&quot;:{&quot;type&quot;:&quot;integer&quot;,
#  &quot;description&quot;:&quot;最大返回结果数量&quot;}},&quot;required&quot;:[&quot;query&quot;]}}}
</code></pre>
<hr>
<h3>Step 3: ToolExecutor — 工具执行与结果处理</h3>
<p>接收 LLM 返回的 <code>tool_calls</code>，分发执行，收集结果，处理异常。支持串行和并行两种模式。</p>
<pre><code class="language-python"># tool_executor.py
import json, time, logging, traceback
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FTE
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class ToolResult:
    tool_call_id: str
    name: str
    result: str
    success: bool
    duration_ms: float = 0.0

class ToolExecutor:
    def __init__(self, registry, default_timeout=30.0, max_workers=4):
        self.registry = registry
        self.default_timeout = default_timeout
        self.max_workers = max_workers

    def execute(self, tool_calls) -&gt; list[ToolResult]:
        &quot;&quot;&quot;串行执行&quot;&quot;&quot;
        return [self._run_one(tc) for tc in tool_calls]

    def execute_parallel(self, tool_calls) -&gt; list[ToolResult]:
        &quot;&quot;&quot;并行执行（LLM 一次返回多个 tool_calls 时使用）&quot;&quot;&quot;
        if len(tool_calls) &lt;= 1:
            return self.execute(tool_calls)
        results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as pool:
            futures = {pool.submit(self._run_one, tc): tc for tc in tool_calls}
            for fut in futures:
                try:
                    results.append(fut.result(timeout=self.default_timeout))
                except FTE:
                    tc = futures[fut]
                    results.append(ToolResult(
                        tc.id, tc.name,
                        f&quot;Error: &#39;{tc.name}&#39; timed out after &quot;
                        f&quot;{self.default_timeout}s&quot;, False))
        return results

    def _run_one(self, tool_call) -&gt; ToolResult:
        start = time.monotonic()
        func = self.registry.get_function(tool_call.name)
        if not func:
            return ToolResult(tool_call.id, tool_call.name,
                f&quot;Error: Unknown tool &#39;{tool_call.name}&#39;. &quot;
                f&quot;Available: {self.registry.list_tools()}&quot;, False)
        try:
            result = func(**tool_call.arguments)
            if not isinstance(result, str):
                result = json.dumps(result, ensure_ascii=False, default=str)
            ms = (time.monotonic() - start) * 1000
            logger.info(f&quot;Tool &#39;{tool_call.name}&#39; OK in {ms:.0f}ms&quot;)
            return ToolResult(tool_call.id, tool_call.name, result, True, ms)
        except Exception as e:
            ms = (time.monotonic() - start) * 1000
            msg = f&quot;Error: {type(e).__name__}: {e}&quot;
            logger.error(f&quot;{msg}\n{traceback.format_exc()}&quot;)
            return ToolResult(tool_call.id, tool_call.name, msg, False, ms)

    @staticmethod
    def results_to_messages(results):
        return [{&quot;role&quot;: &quot;tool&quot;, &quot;tool_call_id&quot;: r.tool_call_id,
                 &quot;content&quot;: r.result} for r in results]
</code></pre>
<p><strong>串行 vs 并行的 Trade-off</strong>：串行简单可调试；并行在 LLM 同时返回多个独立 tool_calls 时显著降低延迟。LLM 在一次响应中返回多个 tool_calls 本身就隐含了&quot;它们之间无依赖&quot;——否则它会分成多轮调用。</p>
<hr>
<h3>Step 4: MessageManager — 消息历史管理与压缩</h3>
<p>解决 Agent 长对话中最常遇到的问题：<strong>消息越来越多，Context Window 不够用了</strong>。</p>
<pre><code class="language-python"># message_manager.py
import json, logging, tiktoken
from typing import Optional
from copy import deepcopy

logger = logging.getLogger(__name__)

class MessageManager:
    def __init__(self, system_prompt=&quot;&quot;, model=&quot;gpt-4o&quot;,
                 max_tokens=120000, compression_threshold=0.75):
        self.system_prompt = system_prompt
        self.max_tokens = max_tokens
        self.compression_threshold = compression_threshold
        try: self.enc = tiktoken.encoding_for_model(model)
        except KeyError: self.enc = tiktoken.get_encoding(&quot;cl100k_base&quot;)
        self._messages: list[dict] = []

    def append(self, msg):
        self._messages.append(msg)
        self._maybe_compress()

    def extend(self, msgs):
        self._messages.extend(msgs)
        self._maybe_compress()

    def get_messages(self):
        out = []
        if self.system_prompt:
            out.append({&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: self.system_prompt})
        out.extend(deepcopy(self._messages))
        return out

    def count_tokens(self, msgs=None):
        msgs = msgs or self.get_messages()
        total = 2  # priming tokens
        for m in msgs:
            total += 4  # per-message overhead
            for v in m.values():
                if isinstance(v, str): total += len(self.enc.encode(v))
                elif isinstance(v, list):
                    total += len(self.enc.encode(json.dumps(v)))
        return total

    def _maybe_compress(self):
        threshold = int(self.max_tokens * self.compression_threshold)
        if self.count_tokens() &lt;= threshold: return
        logger.info(&quot;Token threshold exceeded, compressing...&quot;)
        self._sliding_window_compress(threshold)

    def _sliding_window_compress(self, target):
        &quot;&quot;&quot;从最早的消息移除，保持 tool_call 对完整性。

        关键约束：assistant(tool_calls) 后面的 tool(result) 消息必须
        一起移除，否则 OpenAI API 会报错。
        &quot;&quot;&quot;
        msgs, i = self._messages, 0
        while i &lt; len(msgs):
            remaining = msgs[i:]
            sys_msgs = ([{&quot;role&quot;:&quot;system&quot;,&quot;content&quot;:self.system_prompt}]
                        if self.system_prompt else [])
            if self.count_tokens(sys_msgs + remaining) &lt;= target: break
            i += 1
            # 如果刚移除的是含 tool_calls 的 assistant，连续移除后续 tool 消息
            if (i &gt; 0 and msgs[i-1].get(&quot;role&quot;) == &quot;assistant&quot;
                    and msgs[i-1].get(&quot;tool_calls&quot;)):
                while i &lt; len(msgs) and msgs[i].get(&quot;role&quot;) == &quot;tool&quot;:
                    i += 1
        if i &gt; 0:
            summary = {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;:
                f&quot;[{i} earlier messages removed to fit context window.]&quot;}
            self._messages = [summary] + msgs[i:]
            logger.info(f&quot;Removed {i} msgs, tokens: {self.count_tokens()}&quot;)
</code></pre>
<p><strong>三个关键点</strong>：System Prompt 始终保留不参与压缩；tool_call 对必须保持完整（<code>assistant</code> + 后续 <code>tool</code> 消息一起删或一起留）；在 75% 时就触发压缩，给回复留够空间。</p>
<hr>
<h3>Step 5: StateStore — 状态持久化</h3>
<p>简单的键值存储，生产中替换为 Redis 或数据库即可。</p>
<pre><code class="language-python"># state_store.py
import json
from typing import Any, Optional
from pathlib import Path

class StateStore:
    def __init__(self, store_dir=&quot;.agent_state&quot;):
        self.dir = Path(store_dir)
        self.dir.mkdir(parents=True, exist_ok=True)
        self._cache: dict[str, Any] = {}

    def save(self, key, value):
        self._cache[key] = value
        (self.dir / f&quot;{key}.json&quot;).write_text(
            json.dumps(value, ensure_ascii=False, indent=2, default=str))

    def load(self, key, default=None):
        if key in self._cache: return self._cache[key]
        f = self.dir / f&quot;{key}.json&quot;
        if f.exists():
            v = json.loads(f.read_text())
            self._cache[key] = v
            return v
        return default

    def clear(self, key=None):
        if key:
            self._cache.pop(key, None)
            (self.dir / f&quot;{key}.json&quot;).unlink(missing_ok=True)
        else:
            self._cache.clear()
            for f in self.dir.glob(&quot;*.json&quot;): f.unlink()
</code></pre>
<hr>
<h2>4. 核心 Runtime Loop</h2>
<p>所有模块就绪，组装成完整的 <code>AgentRuntime</code>。这是整篇文章的核心。</p>
<pre><code class="language-python"># agent_runtime.py
import json, time, logging
from dataclasses import dataclass, field
from typing import Optional, Callable
from collections import Counter

from llm_client import LLMClient, LLMResponse
from tool_registry import ToolRegistry
from tool_executor import ToolExecutor
from message_manager import MessageManager
from state_store import StateStore

logger = logging.getLogger(__name__)

@dataclass
class RuntimeConfig:
    max_turns: int = 20               # 最大循环轮次
    max_total_time: float = 300.0     # 最大总执行时间（秒）
    parallel_tool_calls: bool = True  # 是否并行执行工具
    loop_detection_window: int = 4    # 死循环检测窗口
    loop_detection_threshold: int = 3 # 相同调用出现次数阈值

@dataclass
class AgentResult:
    content: str
    turns: int = 0
    total_tokens: int = 0
    tool_calls_made: list[dict] = field(default_factory=list)
    duration_ms: float = 0.0
    stopped_reason: str = &quot;&quot;

class AgentRuntime:
    def __init__(self, llm: LLMClient, registry: ToolRegistry,
                 system_prompt=&quot;You are a helpful assistant.&quot;,
                 config: Optional[RuntimeConfig] = None):
        self.llm = llm
        self.registry = registry
        self.executor = ToolExecutor(registry)
        self.config = config or RuntimeConfig()
        self.messages = MessageManager(system_prompt=system_prompt,
                                       model=llm.model)
        self.state = StateStore()
        self.on_tool_start: Optional[Callable] = None
        self.on_tool_end: Optional[Callable] = None

    def run(self, user_input: str) -&gt; AgentResult:
        start_time = time.monotonic()
        self.messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input})
        tools = self.registry.get_all_schemas() or None

        turns, total_tokens, all_tc = 0, 0, []
        tc_history: list[str] = []
        final_content, stopped = &quot;&quot;, &quot;completed&quot;

        while turns &lt; self.config.max_turns:
            turns += 1

            # ── 全局超时检查 ─────────────────────────────
            if time.monotonic() - start_time &gt; self.config.max_total_time:
                stopped = f&quot;timeout ({self.config.max_total_time}s)&quot;
                break

            # ── 调用 LLM ────────────────────────────────
            logger.info(f&quot;Turn {turns}: calling LLM...&quot;)
            resp = self.llm.chat(self.messages.get_messages(), tools=tools)
            total_tokens += resp.usage.get(&quot;total_tokens&quot;, 0)

            # ── 情况 1: 有 tool_calls → 执行工具 ────────
            if resp.has_tool_calls:
                # 构建 assistant 消息（必须包含 tool_calls 字段）
                asst = {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: resp.content,
                        &quot;tool_calls&quot;: [
                    {&quot;id&quot;: tc.id, &quot;type&quot;: &quot;function&quot;,
                     &quot;function&quot;: {&quot;name&quot;: tc.name,
                                  &quot;arguments&quot;: json.dumps(tc.arguments)}}
                    for tc in resp.tool_calls]}
                self.messages.append(asst)

                # 死循环检测
                sig = json.dumps([(tc.name, tc.arguments)
                                  for tc in resp.tool_calls], sort_keys=True)
                tc_history.append(sig)
                if self._detect_loop(tc_history):
                    stopped = &quot;loop_detected&quot;
                    final_content = (&quot;I&#39;m repeating the same actions. &quot;
                                     &quot;Stopping to summarize findings.&quot;)
                    break

                # 执行
                if self.on_tool_start: self.on_tool_start(resp.tool_calls)
                if self.config.parallel_tool_calls and len(resp.tool_calls) &gt; 1:
                    results = self.executor.execute_parallel(resp.tool_calls)
                else:
                    results = self.executor.execute(resp.tool_calls)
                if self.on_tool_end: self.on_tool_end(results)

                for tc, r in zip(resp.tool_calls, results):
                    all_tc.append({&quot;turn&quot;: turns, &quot;name&quot;: tc.name,
                        &quot;arguments&quot;: tc.arguments,
                        &quot;success&quot;: r.success, &quot;duration_ms&quot;: r.duration_ms})

                self.messages.extend(ToolExecutor.results_to_messages(results))

            # ── 情况 2: 纯文本 → 任务完成 ───────────────
            else:
                final_content = resp.content or &quot;&quot;
                self.messages.append(
                    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: final_content})
                break
        else:
            stopped = f&quot;max_turns ({self.config.max_turns})&quot;

        return AgentResult(
            content=final_content, turns=turns, total_tokens=total_tokens,
            tool_calls_made=all_tc,
            duration_ms=(time.monotonic() - start_time) * 1000,
            stopped_reason=stopped)

    def _detect_loop(self, history):
        &quot;&quot;&quot;滑动窗口 + 频次统计，同时捕获连续重复和交替重复&quot;&quot;&quot;
        w = self.config.loop_detection_window
        t = self.config.loop_detection_threshold
        if len(history) &lt; t: return False
        return any(c &gt;= t for c in Counter(history[-w:]).values())
</code></pre>
<h3>核心循环解读</h3>
<p><strong>两种退出路径</strong>——这是 Agent 与 Workflow 的本质区别：</p>
<pre><code>resp.has_tool_calls == True   → 继续（还有事要做）
resp.has_tool_calls == False  → break（LLM 认为任务完成了）
</code></pre>
<p><strong>为什么 assistant 消息必须包含 tool_calls 字段？</strong> 这是 OpenAI API 的协议约束。消息流必须是：<code>user</code> → <code>assistant(tool_calls)</code> → <code>tool(result)</code> → <code>assistant(final)</code>。打破这个顺序会报错。</p>
<p><strong>死循环检测</strong>用滑动窗口而非简单的&quot;连续 N 次相同&quot;，因为 LLM 有时会在两个工具间交替调用（A→B→A→B→...），这也是死循环，但不是&quot;连续相同&quot;。</p>
<hr>
<h2>5. 高级特性</h2>
<h3>5.1 Streaming 支持</h3>
<p>流式模式下需要边输出文本、边判断是否有 tool_calls：</p>
<pre><code class="language-python"># 添加到 AgentRuntime
def run_stream(self, user_input: str):
    self.messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input})
    tools = self.registry.get_all_schemas() or None
    turns = 0

    while turns &lt; self.config.max_turns:
        turns += 1
        content, final_tc = &quot;&quot;, None

        for chunk in self.llm.stream(self.messages.get_messages(), tools=tools):
            if chunk.content:
                content += chunk.content
                yield {&quot;type&quot;: &quot;text&quot;, &quot;content&quot;: chunk.content}
            if chunk.tool_calls:
                final_tc = chunk.tool_calls

        if final_tc:
            yield {&quot;type&quot;: &quot;tool_start&quot;,
                   &quot;calls&quot;: [{&quot;name&quot;:tc.name} for tc in final_tc]}
            asst = {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: content,
                    &quot;tool_calls&quot;: [
                {&quot;id&quot;:tc.id, &quot;type&quot;:&quot;function&quot;,
                 &quot;function&quot;:{&quot;name&quot;:tc.name,
                             &quot;arguments&quot;:json.dumps(tc.arguments)}}
                for tc in final_tc]}
            self.messages.append(asst)
            results = self.executor.execute(final_tc)
            self.messages.extend(ToolExecutor.results_to_messages(results))
            yield {&quot;type&quot;: &quot;tool_end&quot;,
                   &quot;results&quot;: [{&quot;name&quot;:r.name, &quot;ok&quot;:r.success} for r in results]}
        else:
            self.messages.append({&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:content})
            yield {&quot;type&quot;: &quot;done&quot;, &quot;content&quot;: content}
            break
</code></pre>
<h3>5.2 超时控制的两层设计</h3>
<pre><code>┌──────────────────────────────────────┐
│ 全局超时 (max_total_time = 300s)     │
│  ┌──────┐ ┌──────┐ ┌──────┐        │
│  │Tool 1│ │Tool 2│ │Tool 3│        │
│  │30s   │ │30s   │ │30s   │        │
│  └──────┘ └──────┘ └──────┘        │
│ 单工具超时 (default_timeout = 30s)   │
└──────────────────────────────────────┘
</code></pre>
<p>单工具超时在 <code>ToolExecutor</code> 中通过 <code>ThreadPoolExecutor.result(timeout=30)</code> 控制；全局超时在 Runtime 每轮循环开始时检查 elapsed time。</p>
<hr>
<h2>6. 完整示例：研究助手 Agent</h2>
<pre><code class="language-python"># research_agent.py
import json, os, logging
from agent_runtime import AgentRuntime, RuntimeConfig
from llm_client import LLMClient
from tool_registry import ToolRegistry

logging.basicConfig(level=logging.INFO,
    format=&quot;%(asctime)s [%(levelname)s] %(name)s: %(message)s&quot;)

registry = ToolRegistry()

@registry.tool()
def web_search(query: str, max_results: int = 5) -&gt; str:
    &quot;&quot;&quot;搜索网页内容
    Args:
        query: 搜索关键词
        max_results: 最大返回数量
    &quot;&quot;&quot;
    # 生产环境替换为 SerpAPI / Bing API
    return json.dumps([{&quot;title&quot;: f&quot;Result {i+1} for &#39;{query}&#39;&quot;,
        &quot;url&quot;: f&quot;https://example.com/article-{i+1}&quot;,
        &quot;snippet&quot;: f&quot;Detailed article about {query}, section {i+1}...&quot;}
        for i in range(min(max_results, 3))], ensure_ascii=False)

@registry.tool()
def read_url(url: str) -&gt; str:
    &quot;&quot;&quot;读取网页内容
    Args:
        url: 网页地址
    &quot;&quot;&quot;
    # 生产环境替换为 requests + BeautifulSoup
    return (f&quot;[Content from {url}]\n&quot;
            f&quot;Key points: 1) Fundamental concepts 2) Best practices &quot;
            f&quot;3) Common pitfalls 4) Case studies and benchmarks&quot;)

@registry.tool()
def write_file(filename: str, content: str) -&gt; str:
    &quot;&quot;&quot;写入文件
    Args:
        filename: 文件名
        content: 文本内容
    &quot;&quot;&quot;
    os.makedirs(&quot;output&quot;, exist_ok=True)
    path = os.path.join(&quot;output&quot;, os.path.basename(filename))
    with open(path, &quot;w&quot;) as f: f.write(content)
    return f&quot;Wrote {len(content)} chars to {path}&quot;

@registry.tool()
def ask_user(question: str) -&gt; str:
    &quot;&quot;&quot;向用户提问
    Args:
        question: 问题
    &quot;&quot;&quot;
    print(f&quot;\nAgent asks: {question}&quot;)
    return input(&quot;Your answer: &quot;)

SYSTEM_PROMPT = &quot;&quot;&quot;You are a research assistant. Workflow:
1. Search for information using web_search
2. Read promising articles using read_url (at least 2 sources)
3. Synthesize into a report and save with write_file
4. Present a summary. Use ask_user if the topic is unclear.&quot;&quot;&quot;

agent = AgentRuntime(
    llm=LLMClient(model=&quot;gpt-4o&quot;, api_key=os.environ.get(&quot;OPENAI_API_KEY&quot;)),
    registry=registry,
    system_prompt=SYSTEM_PROMPT,
    config=RuntimeConfig(max_turns=15, max_total_time=120.0))

if __name__ == &quot;__main__&quot;:
    result = agent.run(&quot;研究 Python asyncio 最佳实践，整理成技术报告并保存。&quot;)
    print(f&quot;\n{&#39;=&#39;*50}\nTurns: {result.turns} | Tokens: {result.total_tokens} &quot;
          f&quot;| {result.duration_ms:.0f}ms | {result.stopped_reason}&quot;)
    for tc in result.tool_calls_made:
        print(f&quot;  Turn {tc[&#39;turn&#39;]}: {tc[&#39;name&#39;]}() &quot;
              f&quot;{&#39;OK&#39; if tc[&#39;success&#39;] else &#39;FAIL&#39;} {tc[&#39;duration_ms&#39;]:.0f}ms&quot;)
    print(f&quot;\n{result.content[:300]}&quot;)
</code></pre>
<h3>执行 Trace</h3>
<pre><code>Turn 1: calling LLM...  → web_search(&quot;Python asyncio best practices&quot;)
Turn 2: calling LLM...  → read_url(url1) + read_url(url2)  [parallel]
Turn 3: calling LLM...  → web_search(&quot;asyncio common pitfalls&quot;)
Turn 4: calling LLM...  → read_url(url3)
Turn 5: calling LLM...  → write_file(&quot;asyncio-report.md&quot;, ...)
Turn 6: calling LLM...  → [no tool_calls] → Done

==================================================
Turns: 6 | Tokens: 8432 | 13245ms | completed
  Turn 1: web_search() OK 45ms
  Turn 2: read_url() OK 120ms
  Turn 2: read_url() OK 135ms
  Turn 3: web_search() OK 38ms
  Turn 4: read_url() OK 110ms
  Turn 5: write_file() OK 5ms
</code></pre>
<p>注意 Turn 2：LLM 返回了两个 <code>read_url</code>，Runtime 自动并行执行。</p>
<hr>
<h2>7. 与框架对比</h2>
<h3>自建 vs 框架</h3>
<table>
<thead>
<tr>
<th>维度</th>
<th>自建 Runtime</th>
<th>框架（LangChain 等）</th>
</tr>
</thead>
<tbody><tr>
<td><strong>透明性</strong></td>
<td>完全透明</td>
<td>需要读框架源码</td>
</tr>
<tr>
<td><strong>调试</strong></td>
<td>直接 breakpoint</td>
<td>需要理解框架抽象层</td>
</tr>
<tr>
<td><strong>定制</strong></td>
<td>任何行为可改</td>
<td>受 API 设计约束</td>
</tr>
<tr>
<td><strong>依赖</strong></td>
<td><code>openai</code> + <code>tiktoken</code></td>
<td>几十个传递依赖</td>
</tr>
<tr>
<td><strong>边界情况</strong></td>
<td>自己发现和处理</td>
<td>社区帮你踩过坑</td>
</tr>
<tr>
<td><strong>生态集成</strong></td>
<td>每个都要自己写</td>
<td>现成的 VectorStore/Retriever</td>
</tr>
<tr>
<td><strong>开发速度</strong></td>
<td>初期更慢</td>
<td>有模板更快</td>
</tr>
</tbody></table>
<h3>决策建议</h3>
<ul>
<li><strong>学习阶段</strong>：一定要自建一次。不理解原理就用框架，永远无法判断框架是否在坑你。</li>
<li><strong>PoC / Hackathon</strong>：用框架，速度第一。</li>
<li><strong>生产系统</strong>：自建核心 Runtime + 选择性使用框架组件（如只用 LangChain 的 Retriever）。</li>
<li><strong>基础设施团队</strong>：自建。你们的需求框架大概率满足不了。</li>
</ul>
<hr>
<h2>8. 结语：Phase 2 完成</h2>
<p>到这里，Phase 2 四篇文章全部完成：</p>
<ul>
<li><strong>第 04 篇</strong>：理解控制循环 — Observe → Think → Act → Reflect</li>
<li><strong>第 05 篇</strong>：深入 Tool Calling — JSON Schema、Function Calling、Structured Output</li>
<li><strong>第 06 篇</strong>：Prompt Engineering — System Prompt 设计、工具选择引导、Reflection Prompt</li>
<li><strong>第 07 篇（本篇）</strong>：把以上所有知识组装成可运行的 Agent Runtime</li>
</ul>
<p>此刻你有能力<strong>不依赖任何框架，从零构建功能完整的 Agent 系统</strong>。</p>
<p>但如果你运行过这个 Agent，会很快发现几个问题：</p>
<ol>
<li><strong>没有记忆</strong>：每次启动都是白纸，不记得上次的对话</li>
<li><strong>不会计划</strong>：面对复杂任务只是一步步试，没有全局规划</li>
<li><strong>一个不够用</strong>：有些任务需要不同角色的 Agent 协作</li>
</ol>
<p>这就是 Phase 3 要解决的问题：</p>
<ul>
<li><strong>第 08 篇</strong>：Memory Architecture — Agent 的状态与记忆体系</li>
<li><strong>第 09 篇</strong>：RAG as Cognitive Memory — 检索增强生成的工程实践</li>
<li><strong>第 10 篇</strong>：Planning and Reflection — 从 ReAct 到分层规划</li>
<li><strong>第 11 篇</strong>：Multi-Agent Collaboration — 多 Agent 协作</li>
</ul>
<p>Phase 2 给了你造一把锤子的能力。Phase 3 将教你如何造一个工具箱。</p>
<hr>
<blockquote>
<p><strong>系列导航</strong>：本文是 Agentic 系列的第 07 篇。</p>
<ul>
<li>上一篇：<a href="/blog/engineering/agentic/06-Prompt%20Engineering%20for%20Agents">06 | Prompt Engineering for Agents</a></li>
<li>下一篇：<a href="/blog/engineering/agentic/08-Memory%20Architecture">08 | Memory Architecture</a></li>
<li>完整目录：<a href="/blog/engineering/agentic/01-From%20LLM%20to%20Agent">01 | From LLM to Agent</a></li>
</ul>
</blockquote>
5:["$","article",null,{"className":"min-h-screen","children":["$","div",null,{"className":"mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12","children":[["$","header",null,{"className":"mb-8","children":[["$","nav",null,{"className":"flex items-center gap-1 text-sm mb-4","children":[["$","$L13",null,{"href":"/blog/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"博客"}],["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"Engineering"}],[["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/agentic/page/1","className":"text-blue-600 hover:text-blue-700 transition-colors","children":"Agentic 系统"}]]]}],["$","div",null,{"className":"flex items-center mb-6","children":["$","div",null,{"className":"inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal","children":[["$","svg",null,{"className":"w-4 h-4 mr-2 text-gray-400","fill":"none","stroke":"currentColor","viewBox":"0 0 24 24","children":["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","strokeWidth":2,"d":"M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z"}]}],["$","time",null,{"dateTime":"2025-12-14","children":"2025年12月14日"}]]}]}],["$","h1",null,{"className":"text-4xl font-bold text-gray-900 mb-6 text-center","children":"The Agent Control Loop: Agent 运行时的核心抽象"}],["$","div",null,{"className":"flex flex-wrap gap-2 mb-6 justify-center","children":[["$","$L13","Agentic",{"href":"/blog/tag/Agentic/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"Agentic"}],["$","$L13","AI Engineering",{"href":"/blog/tag/AI%20Engineering/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"AI Engineering"}],["$","$L13","Runtime",{"href":"/blog/tag/Runtime/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"Runtime"}]]}]]}],["$","div",null,{"className":"max-w-5xl mx-auto","children":["$","$L14",null,{"content":"$15"}]}],["$","$10",null,{"fallback":["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":"加载导航中..."}],"children":["$","$L16",null,{"globalNav":{"prev":{"slug":"engineering/architecture/微服务架构落地指南：从核心模式到技术选型","title":"微服务架构落地指南：从核心模式到技术选型","description":"系统性地探讨微服务架构设计的核心关注点，包括服务注册发现、API 网关、服务容错、基础设施选型、CI/CD 流水线和可观测性体系，帮助你从 0 到 1 构建一套完整的微服务技术栈。","pubDate":"2025-12-12","tags":["架构设计","微服务","分布式系统","技术选型"],"heroImage":"$undefined","content":"$17"},"next":{"slug":"engineering/architecture/高并发系统设计：原理、策略与工程实践","title":"高并发系统设计：原理、策略与工程实践","description":"系统梳理高并发架构的核心设计策略，从计算层、数据层、流量层到容错层，逐一分析每种策略的适用原理、决策依据与工程实践，构建可落地的高并发设计知识体系。","pubDate":"2025-12-15","tags":["高并发","系统架构","性能优化","分布式系统"],"heroImage":"$undefined","content":"$18"}},"tagNav":{"Agentic":{"prev":{"slug":"engineering/agentic/03-Agent vs Workflow vs Automation","title":"Agent vs Workflow vs Automation: 选对抽象才是关键","description":"不是所有问题都需要 Agent。本文系统比较 Rule-based Automation、Workflow/DAG、Agent 三种执行范式，从确定性、成本、可观测性等维度给出选型框架，帮助工程师在真实场景中选对抽象层次。","pubDate":"2025-12-09","tags":["Agentic","AI Engineering","Architecture"],"heroImage":"$undefined","content":"$19"},"next":{"slug":"engineering/agentic/05-Tool Calling Deep Dive","title":"Tool Calling Deep Dive: 让 LLM 成为可编程接口","description":"Tool Calling 是 LLM 从「对话机器」变成「可编程接口」的关键转折点。本文从底层原理出发，系统拆解 Tool Calling 的工作机制、JSON Schema 契约设计、工具注册与发现策略、错误处理、安全性考量及关键 Trade-off，附带完整可运行代码。","pubDate":"2025-12-18","tags":["Agentic","AI Engineering","Tool Calling"],"heroImage":"$undefined","content":"$1a"}},"AI Engineering":{"prev":"$5:props:children:props:children:props:children:2:props:children:props:tagNav:Agentic:prev","next":"$5:props:children:props:children:props:children:2:props:children:props:tagNav:Agentic:next"},"Runtime":{"prev":null,"next":{"slug":"engineering/agentic/07-Agent Runtime from Scratch","title":"Agent Runtime from Scratch: 不依赖框架构建 Agent","description":"不依赖 LangChain 等框架，从零实现一个功能完整的 Agent Runtime。逐模块构建 LLMClient、ToolRegistry、ToolExecutor、MessageManager 和核心控制循环，包含并行工具调用、Streaming、超时控制、死循环检测等高级特性，附完整可运行代码。","pubDate":"2025-12-28","tags":["Agentic","AI Engineering","Runtime"],"heroImage":"$undefined","content":"$1b"}}}}]}],["$","$L1c",null,{}]]}]}]}]
8:null
c:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
7:null
a:{"metadata":[["$","title","0",{"children":"The Agent Control Loop: Agent 运行时的核心抽象 - Skyfalling Blog"}],["$","meta","1",{"name":"description","content":"Agent 的本质不是一次函数调用，而是一个可中断的控制循环。本文从状态机模型出发，深入剖析 Agent Control Loop 的每个阶段——OBSERVE、THINK、ACT、REFLECT，对比 ReAct 与 Plan-then-Execute 两种主流模式，讨论状态管理、错误处理与性能优化策略，并给出一个不依赖任何框架的完整 Python 实现。"}],["$","meta","2",{"property":"og:title","content":"The Agent Control Loop: Agent 运行时的核心抽象"}],["$","meta","3",{"property":"og:description","content":"Agent 的本质不是一次函数调用，而是一个可中断的控制循环。本文从状态机模型出发，深入剖析 Agent Control Loop 的每个阶段——OBSERVE、THINK、ACT、REFLECT，对比 ReAct 与 Plan-then-Execute 两种主流模式，讨论状态管理、错误处理与性能优化策略，并给出一个不依赖任何框架的完整 Python 实现。"}],["$","meta","4",{"property":"og:type","content":"article"}],["$","meta","5",{"property":"article:published_time","content":"2025-12-14"}],["$","meta","6",{"property":"article:author","content":"Skyfalling"}],["$","meta","7",{"name":"twitter:card","content":"summary"}],["$","meta","8",{"name":"twitter:title","content":"The Agent Control Loop: Agent 运行时的核心抽象"}],["$","meta","9",{"name":"twitter:description","content":"Agent 的本质不是一次函数调用，而是一个可中断的控制循环。本文从状态机模型出发，深入剖析 Agent Control Loop 的每个阶段——OBSERVE、THINK、ACT、REFLECT，对比 ReAct 与 Plan-then-Execute 两种主流模式，讨论状态管理、错误处理与性能优化策略，并给出一个不依赖任何框架的完整 Python 实现。"}],["$","link","10",{"rel":"shortcut icon","href":"/favicon.png"}],["$","link","11",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","link","12",{"rel":"icon","href":"/favicon.png"}],["$","link","13",{"rel":"apple-touch-icon","href":"/favicon.png"}]],"error":null,"digest":"$undefined"}
12:{"metadata":"$a:metadata","error":null,"digest":"$undefined"}
