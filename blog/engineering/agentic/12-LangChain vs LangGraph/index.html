<!DOCTYPE html><html lang="zh-CN"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/7dd6b3ec14b0b1d8.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-42d55485b4428e47.js"/><script src="/_next/static/chunks/4bd1b696-8ec333fca6b38e39.js" async=""></script><script src="/_next/static/chunks/1684-a2aac8a674e5d38c.js" async=""></script><script src="/_next/static/chunks/main-app-2791dc86ed05573e.js" async=""></script><script src="/_next/static/chunks/6874-7791217feaf05c17.js" async=""></script><script src="/_next/static/chunks/app/layout-142e67ac4336647c.js" async=""></script><script src="/_next/static/chunks/968-d7155a2506e36f1d.js" async=""></script><script src="/_next/static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js" async=""></script><meta name="next-size-adjust" content=""/><title>LangChain vs LangGraph: 框架的价值与边界 - Skyfalling Blog</title><meta name="description" content="Agentic 系列第 12 篇。客观审视 AI Agent 框架的价值与局限。深入分析 LangChain 的抽象模型与陷阱、LangGraph 的状态机优势与学习曲线，横向对比 CrewAI、AutoGen、Semantic Kernel 等框架，最终给出框架 vs 自研的决策矩阵。核心立场：理解原理再用框架，框架是加速器而非必需品。"/><meta property="og:title" content="LangChain vs LangGraph: 框架的价值与边界"/><meta property="og:description" content="Agentic 系列第 12 篇。客观审视 AI Agent 框架的价值与局限。深入分析 LangChain 的抽象模型与陷阱、LangGraph 的状态机优势与学习曲线，横向对比 CrewAI、AutoGen、Semantic Kernel 等框架，最终给出框架 vs 自研的决策矩阵。核心立场：理解原理再用框架，框架是加速器而非必需品。"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2026-01-22"/><meta property="article:author" content="Skyfalling"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="LangChain vs LangGraph: 框架的价值与边界"/><meta name="twitter:description" content="Agentic 系列第 12 篇。客观审视 AI Agent 框架的价值与局限。深入分析 LangChain 的抽象模型与陷阱、LangGraph 的状态机优势与学习曲线，横向对比 CrewAI、AutoGen、Semantic Kernel 等框架，最终给出框架 vs 自研的决策矩阵。核心立场：理解原理再用框架，框架是加速器而非必需品。"/><link rel="shortcut icon" href="/favicon.png"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><link rel="icon" href="/favicon.png"/><link rel="apple-touch-icon" href="/favicon.png"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_f367f3"><div hidden=""><!--$--><!--/$--></div><div class="min-h-screen flex flex-col"><header class="bg-[var(--background)]"><nav class="mx-auto flex max-w-7xl items-center justify-between p-6 lg:px-8" aria-label="Global"><div class="flex lg:flex-1"><a class="-m-1.5 p-1.5" href="/"><span class="sr-only">Skyfalling Blog</span><span class="text-2xl font-bold text-gray-900">Skyfalling</span></a></div><div class="flex lg:hidden"><button type="button" class="-m-2.5 inline-flex items-center justify-center rounded-md p-2.5 text-gray-700"><span class="sr-only">打开主菜单</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></button></div><div class="hidden lg:flex lg:gap-x-12"><a class="text-base font-semibold leading-6 transition-colors text-gray-900 hover:text-blue-600" href="/">首页</a><a class="text-base font-semibold leading-6 transition-colors text-blue-600 border-b-2 border-blue-600 pb-1" href="/blog/">博客</a><a class="text-base font-semibold leading-6 transition-colors text-gray-900 hover:text-blue-600" href="/about/">关于</a></div><div class="hidden lg:flex lg:flex-1 lg:justify-end"></div></nav></header><main class="flex-1"><article class="min-h-screen"><div class="mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8"><div class="rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12"><header class="mb-8"><nav class="flex items-center gap-1 text-sm mb-4"><a class="text-gray-500 hover:text-blue-600 transition-colors" href="/blog/page/1/">博客</a><span class="text-gray-300">/</span><a class="text-gray-500 hover:text-blue-600 transition-colors" href="/blog/category/engineering/page/1/">Engineering</a><span class="text-gray-300">/</span><a class="text-blue-600 hover:text-blue-700 transition-colors" href="/blog/category/engineering/agentic/page/1/">Agentic 系统</a></nav><div class="flex items-center mb-6"><div class="inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal"><svg class="w-4 h-4 mr-2 text-gray-400" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z"></path></svg><time dateTime="2026-01-22">2026年01月22日</time></div></div><h1 class="text-4xl font-bold text-gray-900 mb-6 text-center">LangChain vs LangGraph: 框架的价值与边界</h1><div class="flex flex-wrap gap-2 mb-6 justify-center"><a class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors" href="/blog/tag/Agentic/page/1/">Agentic</a><a class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors" href="/blog/tag/AI%20Engineering/page/1/">AI Engineering</a><a class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors" href="/blog/tag/Framework/page/1/">Framework</a></div></header><div class="max-w-5xl mx-auto"><div class="prose prose-lg prose-gray mx-auto max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:text-blue-700 prose-strong:text-gray-900 prose-strong:font-semibold prose-li:text-gray-700 prose-hr:border-gray-300"><h1>LangChain vs LangGraph: 框架的价值与边界</h1>
<blockquote>
<p>框架是加速器，不是必需品。它替你做了决策——有些决策是好的，有些会在深夜的生产事故中反噬你。</p>
<p>本文是 Agentic 系列第 12 篇。前面 11 篇我们从零构建了 Agent 的每一个组件——控制循环、工具调用、记忆、规划、多 Agent 协作。现在是时候回过头来，以工程师的视角冷静审视：框架提供了什么，隐藏了什么，限制了什么。</p>
</blockquote>
<hr>
<h2>1. 开篇：你真的需要框架吗？</h2>
<p>这个问题的答案不是&quot;需要&quot;或&quot;不需要&quot;，而是&quot;取决于&quot;。</p>
<p>如果你已经读完本系列前 7 篇文章（从控制循环到自研 Runtime），你已经具备了从零构建一个 Agent 系统的能力。你知道 Tool Calling 的 JSON Schema 契约，知道控制循环的 Observe-Think-Plan-Act-Reflect-Update 六阶段，知道 Memory 的短期/长期分层，知道 Planner 的 ReAct 与分层规划。</p>
<p>这时候你面临一个决策：</p>
<pre><code>选择 A：自己实现所有组件，完全掌控
选择 B：使用框架，快速启动，接受其抽象和约束
选择 C：理解框架的实现，选择性地借鉴或使用其部分模块
</code></pre>
<p>大多数成熟的工程团队最终会走向选择 C。但要做到选择 C，你必须先深入理解框架到底在做什么。这就是本文的目的。</p>
<hr>
<h2>2. 为什么需要框架</h2>
<p>框架存在是有道理的。在深入批判之前，先公正地承认它们解决了哪些真实的工程问题。</p>
<h3>2.1 减少重复代码</h3>
<p>每一个 Agent 系统都需要处理以下样板代码：</p>
<ul>
<li><strong>工具注册与调度</strong>：维护一个 <code>tool_name → callable</code> 的映射表，处理参数校验和错误捕获</li>
<li><strong>消息格式管理</strong>：构造和维护 <code>messages</code> 列表，处理不同角色（system/user/assistant/tool）的消息格式</li>
<li><strong>LLM 调用封装</strong>：处理 API 差异（OpenAI、Anthropic、本地模型的接口都不同）、流式输出、重试、降级</li>
<li><strong>状态序列化</strong>：将 Agent 的运行状态持久化到数据库或文件系统</li>
</ul>
<p>这些代码在每个项目中高度相似，但又充满细节（比如 OpenAI 的 <code>tool_calls</code> 和 Anthropic 的 <code>tool_use</code> 格式差异）。框架把这些细节屏蔽了。</p>
<h3>2.2 社区生态</h3>
<p>成熟框架最大的资产不是代码，而是生态：</p>
<ul>
<li><strong>预置 Tool 集成</strong>：搜索引擎（Tavily、SerpAPI）、数据库（SQL、MongoDB）、文件系统、浏览器等，开箱即用</li>
<li><strong>预置 Retriever</strong>：支持各种向量数据库（Pinecone、Weaviate、Chroma、FAISS）的统一接口</li>
<li><strong>文档与教程</strong>：从入门到进阶的学习路径</li>
<li><strong>社区问答</strong>：遇到问题时有人讨论、有 issue 可以搜索</li>
</ul>
<h3>2.3 最佳实践封装</h3>
<p>框架将社区沉淀的设计模式编码为默认行为：</p>
<ul>
<li>ReAct 模式的标准实现</li>
<li>Retrieval-Augmented Generation 的标准 pipeline</li>
<li>对话记忆的滑动窗口管理</li>
<li>工具调用的错误处理和重试</li>
</ul>
<p>对于刚接触 Agent 开发的团队，这些封装可以避免很多常见的设计错误。</p>
<h3>2.4 快速原型验证</h3>
<p>当你需要在两天内验证一个想法是否可行时，框架的价值最大化。10 行代码就能跑通一个带工具调用的 Agent 原型，比从零实现快一个数量级。</p>
<pre><code class="language-python"># 10 行代码验证一个想法——这是框架的甜蜜点
from langchain_openai import ChatOpenAI
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.tools.tavily_search import TavilySearchResults

llm = ChatOpenAI(model=&quot;gpt-4o&quot;)
tools = [TavilySearchResults(max_results=3)]
prompt = ChatPromptTemplate.from_messages([
    (&quot;system&quot;, &quot;You are a helpful research assistant.&quot;),
    (&quot;human&quot;, &quot;{input}&quot;),
    (&quot;placeholder&quot;, &quot;{agent_scratchpad}&quot;),
])
agent = create_tool_calling_agent(llm, tools, prompt)
executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
result = executor.invoke({&quot;input&quot;: &quot;2025 年 AI Agent 领域有哪些重要进展？&quot;})
</code></pre>
<p>这段代码在 5 分钟内就能跑通。但如果你打算把它部署到生产环境——请继续往下读。</p>
<hr>
<h2>3. LangChain 深入分析</h2>
<p>LangChain 是 AI Agent 领域生态最大的框架，也是争议最多的框架。我们不吹不黑，从架构和工程两个维度来分析。</p>
<h3>3.1 核心抽象</h3>
<p>LangChain 的设计围绕四个核心抽象：</p>
<table>
<thead>
<tr>
<th>抽象</th>
<th>本质</th>
<th>职责</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Chain</strong></td>
<td>链式调用</td>
<td>将多个步骤串联为顺序执行的管道</td>
</tr>
<tr>
<td><strong>Agent</strong></td>
<td>工具选择 + 循环</td>
<td>LLM 自主决定调用哪个工具，循环直到完成</td>
</tr>
<tr>
<td><strong>Memory</strong></td>
<td>对话状态管理</td>
<td>维护对话历史，支持滑动窗口、摘要等策略</td>
</tr>
<tr>
<td><strong>Retriever</strong></td>
<td>知识检索</td>
<td>从向量数据库或其他数据源检索相关文档</td>
</tr>
</tbody></table>
<p>这四个抽象之间的关系可以用下图表示：</p>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                    LangChain Architecture                │
├─────────────────────────────────────────────────────────┤
│                                                         │
│   ┌──────────┐    ┌──────────┐    ┌──────────────────┐  │
│   │  Chain   │    │  Agent   │    │  AgentExecutor   │  │
│   │          │    │          │    │  (Control Loop)  │  │
│   │ step1 →  │    │ LLM +   │    │                  │  │
│   │ step2 →  │    │ Tools +  │    │  while not done: │  │
│   │ step3    │    │ Prompt   │    │    plan()        │  │
│   └────┬─────┘    └────┬─────┘    │    execute()     │  │
│        │               │          │    observe()     │  │
│        │               └──────────┤                  │  │
│        │                          └────────┬─────────┘  │
│        │                                   │            │
│   ┌────▼───────────────────────────────────▼─────────┐  │
│   │              LLM Abstraction Layer               │  │
│   │  ChatOpenAI │ ChatAnthropic │ ChatOllama │ ...   │  │
│   └────────────────────┬─────────────────────────────┘  │
│                        │                                │
│   ┌────────────────────▼─────────────────────────────┐  │
│   │                  Memory                          │  │
│   │  ConversationBufferMemory │ ConversationSummary  │  │
│   │  VectorStoreMemory │ EntityMemory │ ...          │  │
│   └──────────────────────────────────────────────────┘  │
│                                                         │
│   ┌──────────────────────────────────────────────────┐  │
│   │                  Retriever                       │  │
│   │  VectorStoreRetriever │ BM25 │ MultiQuery │ ... │  │
│   └──────────────────────────────────────────────────┘  │
│                                                         │
│   ┌──────────────────────────────────────────────────┐  │
│   │                  Tools                           │  │
│   │  Search │ Calculator │ SQL │ FileSystem │ ...    │  │
│   └──────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────┘
</code></pre>
<h3>3.2 代码示例：用 LangChain 实现工具调用 Agent</h3>
<p>下面用 LangChain 实现一个能查天气和创建日程的 Agent，同时标注每一层抽象的存在：</p>
<pre><code class="language-python">from langchain_openai import ChatOpenAI
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool

# --- 第 1 层抽象：@tool 装饰器 ---
# LangChain 用装饰器将普通函数包装为 Tool 对象
# 自动从类型注解和 docstring 生成 JSON Schema
@tool
def get_weather(city: str, date: str) -&gt; str:
    &quot;&quot;&quot;获取指定城市在指定日期的天气预报。

    Args:
        city: 城市名称，例如 &quot;北京&quot;
        date: 日期，格式 YYYY-MM-DD
    &quot;&quot;&quot;
    # 实际调用天气 API
    return f&#39;{{&quot;city&quot;: &quot;{city}&quot;, &quot;date&quot;: &quot;{date}&quot;, &quot;temp&quot;: &quot;31°C&quot;, &quot;condition&quot;: &quot;多云转雷阵雨&quot;}}&#39;

@tool
def create_reminder(title: str, time: str, note: str) -&gt; str:
    &quot;&quot;&quot;创建一个日程提醒。

    Args:
        title: 提醒标题
        time: 提醒时间，ISO 8601 格式
        note: 提醒备注内容
    &quot;&quot;&quot;
    return f&#39;{{&quot;status&quot;: &quot;created&quot;, &quot;title&quot;: &quot;{title}&quot;, &quot;time&quot;: &quot;{time}&quot;}}&#39;

# --- 第 2 层抽象：LLM 封装 ---
# ChatOpenAI 封装了 OpenAI API 的调用细节
llm = ChatOpenAI(model=&quot;gpt-4o&quot;, temperature=0)

# --- 第 3 层抽象：Prompt Template ---
# ChatPromptTemplate 管理消息的组装逻辑
prompt = ChatPromptTemplate.from_messages([
    (&quot;system&quot;, &quot;你是一个智能助手，可以查询天气和管理日程。今天是 2025-09-01。&quot;),
    (&quot;human&quot;, &quot;{input}&quot;),
    MessagesPlaceholder(variable_name=&quot;agent_scratchpad&quot;),  # Agent 的工作记忆
])

# --- 第 4 层抽象：Agent 构造 ---
# create_tool_calling_agent 将 LLM + Tools + Prompt 组合为一个 Agent
tools = [get_weather, create_reminder]
agent = create_tool_calling_agent(llm, tools, prompt)

# --- 第 5 层抽象：AgentExecutor ---
# AgentExecutor 提供控制循环：调用 Agent → 执行工具 → 反馈结果 → 循环
executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,       # 输出每一步的推理过程
    max_iterations=10,  # 最大循环次数
    handle_parsing_errors=True,  # 自动处理 LLM 输出格式错误
)

# --- 运行 ---
result = executor.invoke({&quot;input&quot;: &quot;帮我查看明天北京的天气，然后创建一个提醒&quot;})
print(result[&quot;output&quot;])
</code></pre>
<p>数一数：从你的业务逻辑（两个工具函数）到最终执行，经过了 <strong>5 层抽象</strong>。每一层都在&quot;帮你做决策&quot;——消息格式、工具注册方式、控制循环策略、错误处理逻辑、输出解析方式。</p>
<h3>3.3 优点</h3>
<p><strong>1. 生态最大、集成最多</strong></p>
<p>截至 2025 年，LangChain 拥有 AI Agent 框架领域最庞大的集成生态：</p>
<ul>
<li>70+ LLM 提供商（OpenAI、Anthropic、Google、Mistral、本地模型等）</li>
<li>50+ 向量数据库</li>
<li>100+ 预置工具</li>
<li>30+ Document Loader（PDF、HTML、CSV、Notion、Confluence 等）</li>
</ul>
<p><strong>2. 社区活跃</strong></p>
<p>GitHub 上最活跃的 AI 项目之一。遇到问题时，StackOverflow 和 GitHub Issues 中大概率能找到讨论。</p>
<p><strong>3. 上手快</strong></p>
<p>对于 PoC（Proof of Concept）和原型验证，LangChain 能让你在几小时内从零到一跑通一个完整的 Agent。</p>
<p><strong>4. 抽象统一</strong></p>
<p>不同 LLM 提供商的 API 差异被封装在统一接口下。切换 OpenAI → Anthropic 只需要换一行代码（理论上如此，实际上有细微差异）。</p>
<h3>3.4 问题</h3>
<p>以下不是主观吐槽，而是在生产环境中反复遇到的工程问题。</p>
<p><strong>问题 1：过度抽象——简单的事情被包了太多层</strong></p>
<p>考虑一个最基本的需求：调用 LLM 并获取结构化输出。</p>
<pre><code class="language-python"># 不用框架：3 行代码，直白清晰
import openai
response = openai.chat.completions.create(
    model=&quot;gpt-4o&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;分析这段文本的情感&quot;}],
    response_format={&quot;type&quot;: &quot;json_object&quot;},
)
result = json.loads(response.choices[0].message.content)

# 用 LangChain：需要理解 ChatOpenAI、BaseOutputParser、RunnableSequence、
# StrOutputParser vs JsonOutputParser、LCEL 管道语法...
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template(&quot;分析这段文本的情感: {text}&quot;)
llm = ChatOpenAI(model=&quot;gpt-4o&quot;)
parser = JsonOutputParser()
chain = prompt | llm | parser  # LCEL 管道语法
result = chain.invoke({&quot;text&quot;: &quot;这个产品太棒了&quot;})
</code></pre>
<p>LangChain 版本代码量更多不是问题——问题在于它引入了多个你需要理解的新概念（<code>ChatPromptTemplate</code>、<code>JsonOutputParser</code>、LCEL 管道操作符 <code>|</code>），而这些概念只是在封装原本就很简单的操作。</p>
<p><strong>问题 2：调试困难——错误信息穿过多层封装后难以定位</strong></p>
<p>当 LangChain 链条中的某一环出错时，错误堆栈可能长达 20-30 层，涉及 <code>RunnableSequence</code>、<code>RunnableParallel</code>、<code>RunnableLambda</code> 等内部抽象。你需要在这些框架内部类之间导航，才能找到真正的错误源。</p>
<pre><code># 真实场景中的错误堆栈（简化版）
Traceback:
  langchain_core/runnables/base.py      RunnableSequence.invoke()
  langchain_core/runnables/base.py      RunnableSequence._invoke()
  langchain_core/runnables/base.py      Runnable.invoke()
  langchain_core/runnables/base.py      RunnableLambda.invoke()
  langchain/agents/output_parsers.py    ToolsAgentOutputParser.parse()
  ...
  # 15 层之后...
  你的代码.py                            你的函数()   ← 真正的问题在这里
</code></pre>
<p>在生产环境的 3 AM 报警中，这种调试体验是痛苦的。</p>
<p><strong>问题 3：版本混乱——API 变动频繁</strong></p>
<p>LangChain 在快速迭代中经历了多次重大 API 变更：</p>
<ul>
<li><code>langchain</code> → <code>langchain-core</code> + <code>langchain-community</code> 的包拆分</li>
<li><code>LLMChain</code> → LCEL（LangChain Expression Language）的范式转换</li>
<li><code>initialize_agent</code> → <code>create_tool_calling_agent</code> 的 Agent 创建方式变更</li>
<li>Memory 接口的多次重构</li>
</ul>
<p>6 个月前写的代码，今天大概率跑不通。网上的教程和 StackOverflow 答案大量过时。对于需要长期维护的生产系统，这是一个严重的风险。</p>
<p><strong>问题 4：&quot;Chain&quot; 思维的局限——线性链无法表达复杂的分支和循环</strong></p>
<p>LangChain 的核心抽象是 &quot;Chain&quot;——链式调用。这个模型对于线性流水线（A → B → C）非常优雅，但现实中的 Agent 逻辑往往是非线性的：</p>
<pre><code>线性 Chain 能表达的：

    A ──→ B ──→ C ──→ D
    (检索)  (摘要)  (格式化) (输出)


现实中 Agent 需要的：

    A ──→ B ──→ C ──→ D
    │     │     ▲     │
    │     ├─→ E ─┘     │     ← 条件分支
    │     │             │
    │     └─→ F ──→ G ──┘     ← 并行执行
    │           │
    └───────────┘              ← 循环重试
</code></pre>
<p>LangChain 的 LCEL 可以通过 <code>RunnableBranch</code> 和 <code>RunnableParallel</code> 实现一些分支和并行，但语法变得复杂且不直观。这正是 LangGraph 诞生的原因。</p>
<hr>
<h2>4. LangGraph 深入分析</h2>
<p>LangGraph 是 LangChain 团队推出的下一代框架，核心思想是用<strong>有向图（Directed Graph）</strong> 替代<strong>链（Chain）</strong> 作为基础抽象。这不是一个小改动——它从根本上改变了 Agent 逻辑的表达方式。</p>
<h3>4.1 核心抽象</h3>
<p>LangGraph 的设计围绕四个概念：</p>
<table>
<thead>
<tr>
<th>抽象</th>
<th>本质</th>
<th>对应的计算模型</th>
</tr>
</thead>
<tbody><tr>
<td><strong>State</strong></td>
<td>共享状态对象</td>
<td>状态机的 State</td>
</tr>
<tr>
<td><strong>Node</strong></td>
<td>一个函数</td>
<td>状态机的 State Handler</td>
</tr>
<tr>
<td><strong>Edge</strong></td>
<td>节点间的连接</td>
<td>状态机的 Transition</td>
</tr>
<tr>
<td><strong>Graph</strong></td>
<td>节点和边的组合</td>
<td>有限状态机（FSM）</td>
</tr>
</tbody></table>
<p>核心思想：<strong>Agent 的执行流程就是一个状态机。</strong> 每个节点是一个处理函数，每条边是一个转移条件，整个图定义了 Agent 的所有可能执行路径。</p>
<pre><code>┌─────────────────────────────────────────────────────────────┐
│                   LangGraph State Machine                    │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   ┌─────────────────────────────────────────────────────┐   │
│   │                  Shared State                       │   │
│   │  {messages: [...], tool_results: {...}, plan: [...]} │   │
│   └────────────────────────┬────────────────────────────┘   │
│                            │                                │
│               ┌────────────▼────────────┐                   │
│               │       START             │                   │
│               └────────────┬────────────┘                   │
│                            │                                │
│               ┌────────────▼────────────┐                   │
│               │      agent_node         │                   │
│               │   (LLM Reasoning)       │                   │
│               └────────────┬────────────┘                   │
│                            │                                │
│               ┌────────────▼────────────┐                   │
│              ╱    should_continue?       ╲                   │
│             ╱  (Conditional Edge)         ╲                  │
│            ╱                               ╲                 │
│      tool_calls?                      no tool_calls?        │
│           │                                │                │
│  ┌────────▼─────────┐          ┌──────────▼──────────┐     │
│  │    tool_node      │          │       END            │     │
│  │  (Execute Tools)  │          │   (Return Result)    │     │
│  └────────┬──────────┘          └─────────────────────┘     │
│           │                                                 │
│           └──────────────────┐                              │
│                              │ (feed tool results back)     │
│               ┌──────────────▼──────────┐                   │
│               │      agent_node         │ ← 回到推理节点    │
│               └─────────────────────────┘                   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<p>这个图可以清晰地表达：</p>
<ul>
<li><strong>循环</strong>：<code>agent_node → tool_node → agent_node</code>（工具调用循环）</li>
<li><strong>分支</strong>：<code>should_continue?</code> 条件路由</li>
<li><strong>终止</strong>：到达 <code>END</code> 节点时退出</li>
</ul>
<h3>4.2 代码示例：用 LangGraph 实现同一个 Agent</h3>
<p>用 LangGraph 实现与上文 LangChain 相同的天气查询 + 日程创建 Agent：</p>
<pre><code class="language-python">from typing import Annotated, TypedDict
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode


# ============================================================
# Step 1: 定义共享状态（State）
# ============================================================
# 这是 LangGraph 与 LangChain 的核心差异：
# 显式定义 Agent 的完整状态结构
class AgentState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]  # 消息列表，自动追加


# ============================================================
# Step 2: 定义工具（和 LangChain 相同）
# ============================================================
@tool
def get_weather(city: str, date: str) -&gt; str:
    &quot;&quot;&quot;获取指定城市在指定日期的天气预报。&quot;&quot;&quot;
    return f&#39;{{&quot;city&quot;: &quot;{city}&quot;, &quot;date&quot;: &quot;{date}&quot;, &quot;temp&quot;: &quot;31°C&quot;, &quot;condition&quot;: &quot;多云转雷阵雨&quot;}}&#39;

@tool
def create_reminder(title: str, time: str, note: str) -&gt; str:
    &quot;&quot;&quot;创建一个日程提醒。&quot;&quot;&quot;
    return f&#39;{{&quot;status&quot;: &quot;created&quot;, &quot;title&quot;: &quot;{title}&quot;, &quot;time&quot;: &quot;{time}&quot;}}&#39;

tools = [get_weather, create_reminder]


# ============================================================
# Step 3: 定义节点（Node）
# ============================================================
llm = ChatOpenAI(model=&quot;gpt-4o&quot;, temperature=0).bind_tools(tools)

def agent_node(state: AgentState) -&gt; dict:
    &quot;&quot;&quot;推理节点：LLM 根据当前状态决定下一步&quot;&quot;&quot;
    system_message = {
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: &quot;你是一个智能助手，可以查询天气和管理日程。今天是 2025-09-01。&quot;
    }
    messages = [system_message] + state[&quot;messages&quot;]
    response = llm.invoke(messages)
    return {&quot;messages&quot;: [response]}

# ToolNode 是 LangGraph 的内置节点，自动执行工具调用
tool_node = ToolNode(tools)


# ============================================================
# Step 4: 定义边（Edge）—— 条件路由
# ============================================================
def should_continue(state: AgentState) -&gt; str:
    &quot;&quot;&quot;条件路由：检查最后一条消息是否包含工具调用&quot;&quot;&quot;
    last_message = state[&quot;messages&quot;][-1]
    if hasattr(last_message, &quot;tool_calls&quot;) and last_message.tool_calls:
        return &quot;tools&quot;     # 有工具调用 → 去 tool_node
    return &quot;end&quot;           # 无工具调用 → 任务完成


# ============================================================
# Step 5: 构建图（Graph）
# ============================================================
graph_builder = StateGraph(AgentState)

# 添加节点
graph_builder.add_node(&quot;agent&quot;, agent_node)
graph_builder.add_node(&quot;tools&quot;, tool_node)

# 添加边
graph_builder.add_edge(START, &quot;agent&quot;)                        # 入口 → 推理
graph_builder.add_conditional_edges(&quot;agent&quot;, should_continue, {
    &quot;tools&quot;: &quot;tools&quot;,                                         # 推理 → 工具执行
    &quot;end&quot;: END,                                               # 推理 → 结束
})
graph_builder.add_edge(&quot;tools&quot;, &quot;agent&quot;)                      # 工具执行 → 回到推理

# 编译图
graph = graph_builder.compile()


# ============================================================
# Step 6: 运行
# ============================================================
result = graph.invoke({
    &quot;messages&quot;: [HumanMessage(content=&quot;帮我查看明天北京的天气，然后创建一个提醒&quot;)]
})

# 输出最终结果
for message in result[&quot;messages&quot;]:
    print(f&quot;[{message.type}] {message.content}&quot;)
</code></pre>
<p>对比 LangChain 版本，LangGraph 的关键差异：</p>
<ol>
<li><strong>显式状态定义</strong>：<code>AgentState</code> 明确声明了 Agent 运行时的完整状态</li>
<li><strong>显式控制流</strong>：<code>add_edge</code> 和 <code>add_conditional_edges</code> 让执行路径一目了然</li>
<li><strong>图可视化</strong>：编译后的 <code>graph</code> 可以直接渲染为流程图，便于理解和调试</li>
<li><strong>没有隐藏的循环</strong>：循环通过 <code>tools → agent</code> 的边显式定义，而不是藏在 <code>AgentExecutor</code> 内部</li>
</ol>
<h3>4.3 优点</h3>
<p><strong>1. 状态机模型比 Chain 更强大</strong></p>
<p>Chain 只能表达线性流水线。Graph 可以表达任意拓扑——分支、循环、并行、条件汇聚。这与现实中 Agent 的执行逻辑天然匹配。</p>
<p><strong>2. 确定性的控制流 + 非确定性的 LLM 决策</strong></p>
<p>这是 LangGraph 最精妙的设计哲学：</p>
<pre><code>确定性（代码定义）：            非确定性（LLM 决定）：
├── 有哪些节点                 ├── 每个节点内部的推理
├── 节点间如何连接              ├── 工具选择和参数
├── 条件路由的判断逻辑          ├── 是否继续循环
└── 状态的数据结构              └── 最终输出内容
</code></pre>
<p>图的拓扑结构是确定性的（你在编译时就知道所有可能的执行路径），但每一步走哪条路径是 LLM 在运行时决定的。这实现了<strong>可预测的系统行为</strong>与<strong>灵活的智能决策</strong>之间的平衡。</p>
<p><strong>3. Checkpoint 支持——暂停、恢复、Time-Travel</strong></p>
<p>LangGraph 内置了状态检查点机制。这意味着：</p>
<pre><code class="language-python">from langgraph.checkpoint.memory import MemorySaver

# 带 checkpoint 的图
checkpointer = MemorySaver()
graph = graph_builder.compile(checkpointer=checkpointer)

# 运行时传入 thread_id
config = {&quot;configurable&quot;: {&quot;thread_id&quot;: &quot;user-123&quot;}}
result = graph.invoke({&quot;messages&quot;: [HumanMessage(content=&quot;查天气&quot;)]}, config)

# 可以暂停、恢复、回放
# - 暂停：interrupt_before=[&quot;tool_node&quot;] 在工具执行前暂停，等待人类审批
# - 恢复：再次 invoke 同一个 thread_id，从上次中断点继续
# - Time-travel：回滚到任意 checkpoint，重新执行
</code></pre>
<p>这在 Human-in-the-Loop（人机协作）场景中极其有价值——Agent 可以在执行敏感操作前暂停，等待人类确认。</p>
<p><strong>4. 可以表达复杂的多 Agent 架构</strong></p>
<p>上一篇我们讨论的 Supervisor/Worker 模式、并行 Agent 协作，在 LangGraph 中可以自然地表达为图结构：</p>
<pre><code>                 ┌──────────────┐
                 │  Supervisor  │
                 └──────┬───────┘
                        │
              ┌─────────┼─────────┐
              ▼         ▼         ▼
        ┌──────────┐ ┌──────┐ ┌──────────┐
        │ Researcher│ │Coder │ │ Reviewer │
        └──────────┘ └──────┘ └──────────┘
              │         │         │
              └─────────┼─────────┘
                        ▼
                 ┌──────────────┐
                 │  Supervisor  │ ← 回到 Supervisor 决定是否继续
                 └──────────────┘
</code></pre>
<h3>4.4 问题</h3>
<p><strong>问题 1：学习曲线较陡</strong></p>
<p>LangGraph 要求你理解状态机、有向图、条件路由等概念。对于习惯了&quot;调用一个函数就能跑&quot;的开发者来说，需要一段适应期。</p>
<p>特别是 <code>Annotated[list[BaseMessage], add_messages]</code> 这样的状态定义语法（使用 <code>Annotated</code> 类型指定 reducer 函数），对 Python 类型系统不熟悉的开发者可能感到困惑。</p>
<p><strong>问题 2：状态定义需要提前规划</strong></p>
<p>在 LangChain 中，你可以随意传递数据，框架会帮你管理。在 LangGraph 中，所有状态必须在 <code>AgentState</code> 中预先定义。这意味着你需要在写代码之前就想清楚 Agent 需要哪些状态。</p>
<pre><code class="language-python"># 如果开发到一半发现需要新的状态字段，
# 你需要修改 State 定义，并确保所有节点兼容
class AgentState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    plan: list[str]                    # 后来加的
    current_step: int                  # 后来加的
    tool_results: dict[str, str]       # 后来加的
    retry_count: int                   # 后来加的
    # ... 状态会越来越复杂
</code></pre>
<p>对于探索性的开发来说，这种&quot;先定义后使用&quot;的约束会拖慢迭代速度。</p>
<p><strong>问题 3：小任务过度工程化</strong></p>
<p>如果你的 Agent 逻辑就是&quot;调用 LLM → 可能调用工具 → 返回结果&quot;这个简单循环，用 LangGraph 定义 State、Node、Edge、Conditional Edge 就像是用大炮打蚊子。</p>
<pre><code class="language-python"># 一个简单的 ReAct Agent，用 LangGraph 需要 40+ 行图定义代码
# 用原生 Python 只需要一个 while 循环：
while True:
    response = llm.chat(messages, tools=tools)
    if not response.tool_calls:
        return response.content
    for tc in response.tool_calls:
        result = execute_tool(tc)
        messages.append(tool_message(tc.id, result))
</code></pre>
<p>当你的 Agent 逻辑不涉及复杂的分支和并行时，LangGraph 的开销不值得。</p>
<hr>
<h2>5. 其他框架概览</h2>
<p>除了 LangChain 和 LangGraph，AI Agent 领域还有多个值得关注的框架。以下不深入展开，重点给出定位和适用场景。</p>
<h3>5.1 框架定位速览</h3>
<table>
<thead>
<tr>
<th>框架</th>
<th>开发者</th>
<th>核心抽象</th>
<th>定位</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>LangChain</strong></td>
<td>LangChain Inc.</td>
<td>Chain（链式调用）</td>
<td>通用 AI 应用框架</td>
<td>原型验证、RAG、简单 Agent</td>
</tr>
<tr>
<td><strong>LangGraph</strong></td>
<td>LangChain Inc.</td>
<td>Graph（状态机）</td>
<td>复杂 Agent 编排</td>
<td>多步推理、Human-in-the-Loop、多 Agent</td>
</tr>
<tr>
<td><strong>CrewAI</strong></td>
<td>CrewAI Inc.</td>
<td>Crew + Agent + Task</td>
<td>多 Agent 协作</td>
<td>角色扮演式多 Agent 工作流</td>
</tr>
<tr>
<td><strong>AutoGen</strong></td>
<td>Microsoft</td>
<td>Agent + Conversation</td>
<td>多 Agent 对话</td>
<td>研究型多 Agent 系统、代码生成</td>
</tr>
<tr>
<td><strong>Semantic Kernel</strong></td>
<td>Microsoft</td>
<td>Kernel + Plugin + Planner</td>
<td>企业级 AI 编排</td>
<td>企业应用集成、.NET 生态</td>
</tr>
<tr>
<td><strong>Haystack</strong></td>
<td>deepset</td>
<td>Pipeline + Component</td>
<td>RAG 专用</td>
<td>文档检索、知识问答</td>
</tr>
<tr>
<td><strong>DSPy</strong></td>
<td>Stanford NLP</td>
<td>Module + Signature + Optimizer</td>
<td>Prompt 优化</td>
<td>需要自动调优 Prompt 的系统</td>
</tr>
</tbody></table>
<h3>5.2 简要点评</h3>
<p><strong>CrewAI</strong> 的核心思路是&quot;角色扮演&quot;——你定义多个 Agent，每个 Agent 有一个角色（Researcher、Writer、Reviewer），然后把一个任务分配给这个&quot;团队&quot;。这个抽象直观好懂，但在复杂场景中角色定义和任务分配的灵活性不足。</p>
<pre><code class="language-python"># CrewAI 的核心抽象：角色 + 任务 + 团队
from crewai import Agent, Task, Crew

researcher = Agent(role=&quot;Researcher&quot;, goal=&quot;查找相关信息&quot;, ...)
writer = Agent(role=&quot;Writer&quot;, goal=&quot;撰写报告&quot;, ...)
task1 = Task(description=&quot;研究 AI Agent 的最新进展&quot;, agent=researcher)
task2 = Task(description=&quot;基于研究结果撰写报告&quot;, agent=writer)
crew = Crew(agents=[researcher, writer], tasks=[task1, task2])
result = crew.kickoff()
</code></pre>
<p><strong>AutoGen</strong>（Microsoft）强调多 Agent 之间的对话作为协作机制。Agent 之间通过消息传递交互，可以构建复杂的对话流程。适合研究和实验性项目，生产部署的工程支持较弱。</p>
<p><strong>Semantic Kernel</strong>（Microsoft）面向企业用户，强调与现有企业系统的集成。如果你的技术栈是 .NET/C#，或者需要与 Microsoft 365/Azure 深度集成，Semantic Kernel 是更自然的选择。</p>
<p><strong>Haystack</strong>（deepset）不试图做通用 Agent 框架，而是专注于 RAG pipeline。如果你的核心需求是文档检索和知识问答（而不是 Agent 的自主决策和工具调用），Haystack 的 Pipeline 抽象比 LangChain 更干净。</p>
<p><strong>DSPy</strong>（Stanford NLP）走了一条完全不同的路——它不是一个 Agent 运行时框架，而是一个 Prompt 优化框架。核心思想是把 Prompt 当作可学习的参数，通过编译和优化自动找到最佳 Prompt。适合对 Prompt 质量有极高要求的场景。</p>
<h3>5.3 框架选型决策树</h3>
<pre><code>你的核心需求是什么？
│
├─── 快速原型 / PoC
│    └─→ LangChain（生态最大，上手最快）
│
├─── 复杂 Agent 逻辑（分支/循环/并行）
│    └─→ LangGraph（状态机模型天然适合）
│
├─── 多 Agent 协作
│    ├─── 角色扮演式 → CrewAI
│    ├─── 对话式协作 → AutoGen
│    └─── 图编排式   → LangGraph
│
├─── RAG / 知识问答
│    ├─── 需要灵活性  → LangChain + Retriever
│    └─── 需要干净抽象 → Haystack
│
├─── 企业级集成（.NET / Azure）
│    └─→ Semantic Kernel
│
├─── Prompt 自动优化
│    └─→ DSPy
│
└─── 生产系统（需要精细控制）
     └─→ 自研，或只使用框架的底层模块
</code></pre>
<hr>
<h2>6. 框架 vs 自研的决策矩阵</h2>
<p>这是本文最重要的一节。不存在&quot;框架一定好&quot;或&quot;自研一定好&quot;的结论——关键是根据你的具体场景做出理性决策。</p>
<h3>6.1 决策矩阵</h3>
<table>
<thead>
<tr>
<th>考量因素</th>
<th>倾向选框架</th>
<th>倾向选自研</th>
</tr>
</thead>
<tbody><tr>
<td><strong>项目阶段</strong></td>
<td>原型验证、MVP</td>
<td>生产系统、需要长期维护</td>
</tr>
<tr>
<td><strong>团队规模</strong></td>
<td>1-3 人小团队</td>
<td>5+ 人专职 AI 团队</td>
</tr>
<tr>
<td><strong>定制化程度</strong></td>
<td>标准 ReAct/RAG 模式</td>
<td>有独特的控制流或状态管理需求</td>
</tr>
<tr>
<td><strong>调试要求</strong></td>
<td>能接受黑盒</td>
<td>需要完全可观测、可追踪</td>
</tr>
<tr>
<td><strong>性能要求</strong></td>
<td>对 latency 不敏感</td>
<td>需要极致优化每一毫秒</td>
</tr>
<tr>
<td><strong>依赖容忍度</strong></td>
<td>能接受第三方依赖的版本变化</td>
<td>需要完全掌控依赖</td>
</tr>
<tr>
<td><strong>上线时间</strong></td>
<td>2 周内上线</td>
<td>3 个月以上的工程周期</td>
</tr>
<tr>
<td><strong>团队 AI 经验</strong></td>
<td>初次接触 Agent 开发</td>
<td>对 Agent 架构有深入理解</td>
</tr>
</tbody></table>
<h3>6.2 常见场景分析</h3>
<p><strong>场景 1：初创团队做 AI 产品的 MVP</strong></p>
<p>推荐：LangChain（快速原型）→ 验证产品方向 → 决定是否重写</p>
<p>理由：此时最大的风险不是技术债，而是方向错误。花 3 个月自研一个完美的 Agent Runtime，结果发现用户不需要 Agent——这才是最大的浪费。用框架在 2 周内验证想法，确认方向后再决定技术路线。</p>
<p><strong>场景 2：大厂 AI 平台团队</strong></p>
<p>推荐：自研核心 Runtime + 选择性使用框架的底层模块</p>
<p>理由：大厂有足够的工程资源，且对可靠性、可观测性、安全性的要求远超框架的默认支持。自研 Runtime 可以完全掌控控制循环、状态管理、错误处理、日志追踪。但可以借鉴框架的设计模式，或使用框架的工具集成层（比如 LangChain 的 Tool/Retriever 集成）。</p>
<p><strong>场景 3：企业内部的 AI 助手</strong></p>
<p>推荐：LangGraph（如果逻辑复杂）或 LangChain（如果逻辑简单）</p>
<p>理由：企业内部项目通常有明确的需求边界和合理的 SLA 要求，框架能满足大部分需求。LangGraph 的 Human-in-the-Loop 支持对企业审批流程特别有用。</p>
<p><strong>场景 4：研究实验</strong></p>
<p>推荐：AutoGen 或自研轻量框架</p>
<p>理由：研究需要最大的灵活性来尝试新想法。框架的抽象可能限制实验空间。但如果实验涉及多 Agent 交互，AutoGen 的对话式抽象可以减少样板代码。</p>
<h3>6.3 一个务实的折中方案</h3>
<p>在实践中，最常见的成熟方案是<strong>分层使用框架</strong>：</p>
<pre><code>┌─────────────────────────────────────────────────┐
│              你的应用层代码                        │
│         (业务逻辑、API 接口、用户交互)              │
├─────────────────────────────────────────────────┤
│              自研 Agent Runtime                    │
│    (控制循环、状态管理、错误处理、可观测性)          │
├───────────────┬─────────────────────────────────┤
│  自研工具调度   │   框架的集成模块（可选使用）       │
│  自研消息管理   │   LangChain Tool/Retriever       │
│  自研状态存储   │   LangChain Document Loader      │
│               │   LangChain Embedding 接口        │
├───────────────┴─────────────────────────────────┤
│              LLM Provider SDK                     │
│         (openai, anthropic, etc.)                 │
└─────────────────────────────────────────────────┘
</code></pre>
<p>核心思路：</p>
<ul>
<li><strong>控制循环自研</strong>：这是 Agent 最核心的逻辑，也是最需要定制的部分。用 40-60 行 Python 就能实现一个健壮的控制循环（回顾第 07 篇）</li>
<li><strong>LLM 调用用原生 SDK</strong>：OpenAI SDK 和 Anthropic SDK 本身就很好用，不需要再包一层</li>
<li><strong>工具集成可以借用框架</strong>：LangChain 的 Tool 生态确实强大。你可以只 <code>pip install langchain-community</code> 来使用其预置工具，而不用采纳整个框架</li>
<li><strong>状态管理自研</strong>：根据你的持久化需求（Redis、PostgreSQL、内存）定制</li>
</ul>
<p>这个方案的好处是：你在最关键的层面保留了完全掌控力，同时在最不需要掌控的层面（第三方服务的集成）借助了框架的生态。</p>
<hr>
<h2>7. 框架的正确使用姿势</h2>
<p>无论你最终选择什么方案，以下原则都适用。</p>
<h3>7.1 理解原理再用框架</h3>
<p>这正是本系列前 7 篇文章的价值。当你理解了控制循环的六个阶段、Tool Calling 的 JSON Schema 契约、Memory 的分层架构之后，框架在你眼中就不再是黑盒——它只是这些原理的一种实现。</p>
<pre><code>不理解原理时使用框架：
    框架 = 黑魔法（出错时手足无措）

理解原理后使用框架：
    框架 = 已知原理的一种实现（出错时知道去哪里找原因）
</code></pre>
<p>具体来说：</p>
<ul>
<li>当 LangChain 的 <code>AgentExecutor</code> 出错时，你知道它内部在跑一个控制循环，可以猜测问题出在哪个阶段</li>
<li>当 LangGraph 的状态转移出现异常时，你知道这本质上是一个状态机的转移条件判断错误</li>
<li>当框架的 Memory 管理不符合你的需求时，你知道自己需要什么样的记忆架构，可以替换或扩展</li>
</ul>
<h3>7.2 不要被框架限制思维</h3>
<p>框架提供了一组默认的设计模式。这些模式覆盖了 80% 的常见场景，但你的场景可能落在剩下的 20%。</p>
<p><strong>反模式</strong>：为了适配框架的抽象而扭曲自己的业务逻辑。</p>
<pre><code class="language-python"># 反模式：业务逻辑需要 Agent 在两个工具的结果之间做比较，
# 但框架不直接支持，于是你&quot;发明&quot;了一个假工具来绕过限制

@tool
def compare_results(result_a: str, result_b: str) -&gt; str:
    &quot;&quot;&quot;比较两个结果（实际上这应该是 Agent 内部的推理逻辑，不是工具）&quot;&quot;&quot;
    # 这不应该是一个 Tool —— 这是把框架的抽象当成了唯一的解法
    return llm.invoke(f&quot;比较: {result_a} vs {result_b}&quot;)
</code></pre>
<p><strong>正确做法</strong>：框架不支持的逻辑，用原生代码实现，然后插入到框架的流程中（或者干脆不用框架处理这部分）。</p>
<h3>7.3 框架代码是最好的学习材料</h3>
<p>即使你决定自研，框架的源码仍然是宝贵的学习资源。以下是几个值得阅读的代码文件：</p>
<ul>
<li><strong>LangGraph 的 <code>StateGraph</code></strong>：理解如何用 Python 实现一个状态机运行时</li>
<li><strong>LangChain 的 <code>ToolNode</code></strong>：理解如何将 LLM 的 tool_call 输出映射为实际的函数调用</li>
<li><strong>LangChain 的 <code>ChatOpenAI</code></strong>：理解如何封装 LLM Provider 的 API 差异</li>
<li><strong>LangGraph 的 <code>MemorySaver</code></strong>：理解 checkpoint 和状态持久化的实现</li>
</ul>
<p>阅读源码时，关注的不是具体的 API，而是<strong>设计决策</strong>：为什么这样抽象？这个 trade-off 是什么？有没有更好的方案？</p>
<h3>7.4 随时准备好替换或去掉框架</h3>
<p>一个健康的架构应该允许你在不重写业务逻辑的情况下替换底层框架。实现方式：</p>
<pre><code class="language-python"># 定义你自己的接口（不依赖任何框架）
from abc import ABC, abstractmethod

class BaseLLM(ABC):
    @abstractmethod
    def chat(self, messages: list[dict], tools: list[dict] | None = None) -&gt; dict:
        ...

class BaseToolExecutor(ABC):
    @abstractmethod
    def execute(self, tool_name: str, args: dict) -&gt; str:
        ...

class BaseMemory(ABC):
    @abstractmethod
    def get_messages(self, limit: int = 20) -&gt; list[dict]:
        ...
    @abstractmethod
    def add_message(self, message: dict) -&gt; None:
        ...


# 框架实现（可替换）
class LangChainLLM(BaseLLM):
    def __init__(self):
        from langchain_openai import ChatOpenAI
        self._llm = ChatOpenAI(model=&quot;gpt-4o&quot;)

    def chat(self, messages, tools=None):
        # 将你的接口适配为 LangChain 接口
        ...

# 原生实现（可替换）
class NativeLLM(BaseLLM):
    def __init__(self):
        import openai
        self._client = openai.OpenAI()

    def chat(self, messages, tools=None):
        response = self._client.chat.completions.create(
            model=&quot;gpt-4o&quot;, messages=messages, tools=tools
        )
        ...


# 你的 Agent 代码只依赖自己的接口
class MyAgent:
    def __init__(self, llm: BaseLLM, tools: BaseToolExecutor, memory: BaseMemory):
        self.llm = llm
        self.tools = tools
        self.memory = memory

    def run(self, user_input: str) -&gt; str:
        # 业务逻辑不依赖任何框架
        ...
</code></pre>
<p>这不是过度设计——这是<strong>依赖倒置原则</strong>在 Agent 架构中的直接应用。当框架发生 breaking change（LangChain 几乎每季度都有）时，你只需要修改适配层，而不是重写整个系统。</p>
<hr>
<h2>8. LangChain vs LangGraph：直接对比</h2>
<p>最后，用一张表格直接对比 LangChain 和 LangGraph 在各维度的差异：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>LangChain</th>
<th>LangGraph</th>
</tr>
</thead>
<tbody><tr>
<td><strong>核心抽象</strong></td>
<td>Chain（线性管道）</td>
<td>Graph（有向状态机）</td>
</tr>
<tr>
<td><strong>控制流表达</strong></td>
<td>线性为主，分支/循环需要 hack</td>
<td>天然支持分支、循环、并行</td>
</tr>
<tr>
<td><strong>状态管理</strong></td>
<td>隐式（框架内部管理）</td>
<td>显式（开发者定义 State 类型）</td>
</tr>
<tr>
<td><strong>学习曲线</strong></td>
<td>低（上手快）</td>
<td>中等（需要理解状态机概念）</td>
</tr>
<tr>
<td><strong>调试体验</strong></td>
<td>差（多层抽象遮蔽错误源）</td>
<td>中等（图结构可视化，但状态流转需追踪）</td>
</tr>
<tr>
<td><strong>适合场景</strong></td>
<td>简单 Agent、RAG、原型验证</td>
<td>复杂 Agent、多 Agent、Human-in-the-Loop</td>
</tr>
<tr>
<td><strong>生态集成</strong></td>
<td>最丰富</td>
<td>继承 LangChain 生态</td>
</tr>
<tr>
<td><strong>Human-in-the-Loop</strong></td>
<td>不原生支持</td>
<td>原生 Checkpoint + Interrupt 支持</td>
</tr>
<tr>
<td><strong>多 Agent</strong></td>
<td>需要自行编排</td>
<td>原生支持子图嵌套</td>
</tr>
<tr>
<td><strong>生产就绪度</strong></td>
<td>中等（需要大量自定义）</td>
<td>较高（状态持久化、检查点内置）</td>
</tr>
<tr>
<td><strong>灵活性</strong></td>
<td>框架约束多，突破框架难</td>
<td>图定义灵活，但需要提前规划</td>
</tr>
<tr>
<td><strong>版本稳定性</strong></td>
<td>差（API 频繁变更）</td>
<td>较好（API 相对稳定）</td>
</tr>
</tbody></table>
<p><strong>总结</strong>：如果 LangChain 是一条<strong>传送带</strong>（把东西从 A 运到 B），那么 LangGraph 就是一张<strong>铁路网</strong>（可以在任意站点之间调度列车）。传送带简单高效，铁路网灵活强大——选哪个取决于你要运的东西有多复杂。</p>
<hr>
<h2>9. 结语与进一步思考</h2>
<h3>核心立场回顾</h3>
<p>本文的核心立场可以用三句话概括：</p>
<ol>
<li><p><strong>框架是加速器，不是必需品。</strong> 它加速了开发，但也隐藏了复杂性。当隐藏的复杂性成为你的瓶颈时，框架就从加速器变成了减速器。</p>
</li>
<li><p><strong>理解原理比掌握框架更重要。</strong> 框架会变（LangChain 已经经历了多次 API 大改），但控制循环、状态管理、工具调用的基本原理不会变。前 7 篇文章构建的知识，是你评估和使用任何框架的基础。</p>
</li>
<li><p><strong>最好的架构是&quot;框架可替换&quot;的架构。</strong> 把框架当作可插拔的实现层，而不是系统的骨架。你的业务逻辑应该依赖自己定义的接口，而不是某个框架的 API。</p>
</li>
</ol>
<h3>框架解决了&quot;怎么写&quot;，协议解决&quot;怎么连接&quot;</h3>
<p>框架帮你解决了一个 Agent 内部的组件编排问题：如何组织 LLM 调用、工具执行、状态管理。但当你有多个 Agent、多个工具提供者、多个模型时，一个更根本的问题浮现出来：</p>
<blockquote>
<p>这些组件之间用什么协议通信？工具如何被发现和注册？能力如何被声明和协商？</p>
</blockquote>
<p>这不是框架能解决的问题——这需要<strong>协议（Protocol）</strong>。下一篇我们将讨论 MCP（Model Context Protocol），看看 Agent 工具生态的协议化未来。</p>
<h3>留给读者的思考</h3>
<p><strong>关于框架的未来</strong>：LLM 本身的能力在快速增强。当模型原生支持复杂的多步推理（如 o1/o3 的 chain-of-thought）、原生支持长对话记忆（如 Gemini 的长上下文窗口）、原生支持工具调用时，框架的价值会被压缩还是放大？换句话说——当 LLM 足够强时，我们还需要框架在中间做多少事？</p>
<p><strong>关于抽象的代价</strong>：每一层抽象都在隐藏复杂性。隐藏复杂性是好事（让你专注于业务逻辑），但也是坏事（让你在出问题时无法理解系统行为）。在 Agent 这样本身就充满不确定性的系统中，你能接受多少&quot;隐藏的复杂性&quot;？</p>
<p><strong>关于生态锁定</strong>：选择一个框架意味着接受它的抽象、它的生态、它的更新节奏、它的设计理念。当框架的方向与你的需求分叉时，迁移的成本有多高？这个成本是否在你的决策时被低估了？</p>
<p>这些问题没有标准答案。但作为 AI 工程师，能够清晰地提出这些问题，本身就是一种重要的能力。</p>
<hr>
<blockquote>
<p><strong>系列导航</strong>：本文是 Agentic 系列的第 12 篇。</p>
<ul>
<li>上一篇：<a href="/blog/engineering/agentic/11-Multi-Agent%20Collaboration">11 | Multi-Agent Collaboration</a></li>
<li>下一篇：<a href="/blog/engineering/agentic/13-MCP%20and%20Tool%20Protocol">13 | MCP and Tool Protocol</a></li>
<li>完整目录：<a href="/blog/engineering/agentic/01-From%20LLM%20to%20Agent">01 | From LLM to Agent</a></li>
</ul>
</blockquote>
</div></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><div class="mt-12 pt-8 border-t border-gray-200">加载导航中...</div><!--/$--><div class="mt-16 border-t border-gray-200 pt-8"><div class="mx-auto max-w-3xl"><h3 class="text-2xl font-bold text-gray-900 mb-8">评论</h3></div></div></div></div></article><!--$--><!--/$--></main><footer class="bg-[var(--background)]"><div class="mx-auto max-w-7xl px-6 py-12 lg:px-8"><p class="text-center text-xs leading-5 text-gray-400">© <!-- -->2026<!-- --> Skyfalling</p></div></footer></div><script src="/_next/static/chunks/webpack-42d55485b4428e47.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[10616,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"7177\",\"static/chunks/app/layout-142e67ac4336647c.js\"],\"default\"]\n3:I[87555,[],\"\"]\n4:I[31295,[],\"\"]\n6:I[59665,[],\"OutletBoundary\"]\n9:I[74911,[],\"AsyncMetadataOutlet\"]\nb:I[59665,[],\"ViewportBoundary\"]\nd:I[59665,[],\"MetadataBoundary\"]\nf:I[26614,[],\"\"]\n:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/7dd6b3ec14b0b1d8.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"2rrmzfsoknNGuymzsZdxz\",\"p\":\"\",\"c\":[\"\",\"blog\",\"engineering\",\"agentic\",\"12-LangChain%20vs%20LangGraph\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"engineering/agentic/12-LangChain%20vs%20LangGraph\",\"c\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/7dd6b3ec14b0b1d8.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"zh-CN\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_f367f3\",\"children\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex flex-col\",\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"main\",null,{\"className\":\"flex-1\",\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"footer\",null,{\"className\":\"bg-[var(--background)]\",\"children\":[\"$\",\"div\",null,{\"className\":\"mx-auto max-w-7xl px-6 py-12 lg:px-8\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-center text-xs leading-5 text-gray-400\",\"children\":[\"© \",2026,\" Skyfalling\"]}]}]}]]}]}]}]]}],{\"children\":[\"blog\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"engineering/agentic/12-LangChain%20vs%20LangGraph\",\"c\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",null,[\"$\",\"$L6\",null,{\"children\":[\"$L7\",\"$L8\",[\"$\",\"$L9\",null,{\"promise\":\"$@a\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"z9DDnIJi1PDLG-vt6xuQ4v\",{\"children\":[[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],[\"$\",\"$Ld\",null,{\"children\":\"$Le\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$f\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"10:\"$Sreact.suspense\"\n11:I[74911,[],\"AsyncMetadata\"]\n13:I[6874,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"\"]\n14:I[32923,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"default\"]\n16:I[40780,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"default\"]\n19:I[85300,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"default\"]\ne:[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$10\",null,{\"fallback\":null,\"children\":[\"$\",\"$L11\",null,{\"promise\":\"$@12\"}]}]}]\n15:Td1b4,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eLangChain vs LangGraph: 框架的价值与边界\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e框架是加速器，不是必需品。它替你做了决策——有些决策是好的，有些会在深夜的生产事故中反噬你。\u003c/p\u003e\n\u003cp\u003e本文是 Agentic 系列第 12 篇。前面 11 篇我们从零构建了 Agent 的每一个组件——控制循环、工具调用、记忆、规划、多 Agent 协作。现在是时候回过头来，以工程师的视角冷静审视：框架提供了什么，隐藏了什么，限制了什么。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch2\u003e1. 开篇：你真的需要框架吗？\u003c/h2\u003e\n\u003cp\u003e这个问题的答案不是\u0026quot;需要\u0026quot;或\u0026quot;不需要\u0026quot;，而是\u0026quot;取决于\u0026quot;。\u003c/p\u003e\n\u003cp\u003e如果你已经读完本系列前 7 篇文章（从控制循环到自研 Runtime），你已经具备了从零构建一个 Agent 系统的能力。你知道 Tool Calling 的 JSON Schema 契约，知道控制循环的 Observe-Think-Plan-Act-Reflect-Update 六阶段，知道 Memory 的短期/长期分层，知道 Planner 的 ReAct 与分层规划。\u003c/p\u003e\n\u003cp\u003e这时候你面临一个决策：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e选择 A：自己实现所有组件，完全掌控\n选择 B：使用框架，快速启动，接受其抽象和约束\n选择 C：理解框架的实现，选择性地借鉴或使用其部分模块\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e大多数成熟的工程团队最终会走向选择 C。但要做到选择 C，你必须先深入理解框架到底在做什么。这就是本文的目的。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e2. 为什么需要框架\u003c/h2\u003e\n\u003cp\u003e框架存在是有道理的。在深入批判之前，先公正地承认它们解决了哪些真实的工程问题。\u003c/p\u003e\n\u003ch3\u003e2.1 减少重复代码\u003c/h3\u003e\n\u003cp\u003e每一个 Agent 系统都需要处理以下样板代码：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e工具注册与调度\u003c/strong\u003e：维护一个 \u003ccode\u003etool_name → callable\u003c/code\u003e 的映射表，处理参数校验和错误捕获\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e消息格式管理\u003c/strong\u003e：构造和维护 \u003ccode\u003emessages\u003c/code\u003e 列表，处理不同角色（system/user/assistant/tool）的消息格式\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLLM 调用封装\u003c/strong\u003e：处理 API 差异（OpenAI、Anthropic、本地模型的接口都不同）、流式输出、重试、降级\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e状态序列化\u003c/strong\u003e：将 Agent 的运行状态持久化到数据库或文件系统\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这些代码在每个项目中高度相似，但又充满细节（比如 OpenAI 的 \u003ccode\u003etool_calls\u003c/code\u003e 和 Anthropic 的 \u003ccode\u003etool_use\u003c/code\u003e 格式差异）。框架把这些细节屏蔽了。\u003c/p\u003e\n\u003ch3\u003e2.2 社区生态\u003c/h3\u003e\n\u003cp\u003e成熟框架最大的资产不是代码，而是生态：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e预置 Tool 集成\u003c/strong\u003e：搜索引擎（Tavily、SerpAPI）、数据库（SQL、MongoDB）、文件系统、浏览器等，开箱即用\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e预置 Retriever\u003c/strong\u003e：支持各种向量数据库（Pinecone、Weaviate、Chroma、FAISS）的统一接口\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e文档与教程\u003c/strong\u003e：从入门到进阶的学习路径\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e社区问答\u003c/strong\u003e：遇到问题时有人讨论、有 issue 可以搜索\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e2.3 最佳实践封装\u003c/h3\u003e\n\u003cp\u003e框架将社区沉淀的设计模式编码为默认行为：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eReAct 模式的标准实现\u003c/li\u003e\n\u003cli\u003eRetrieval-Augmented Generation 的标准 pipeline\u003c/li\u003e\n\u003cli\u003e对话记忆的滑动窗口管理\u003c/li\u003e\n\u003cli\u003e工具调用的错误处理和重试\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e对于刚接触 Agent 开发的团队，这些封装可以避免很多常见的设计错误。\u003c/p\u003e\n\u003ch3\u003e2.4 快速原型验证\u003c/h3\u003e\n\u003cp\u003e当你需要在两天内验证一个想法是否可行时，框架的价值最大化。10 行代码就能跑通一个带工具调用的 Agent 原型，比从零实现快一个数量级。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 10 行代码验证一个想法——这是框架的甜蜜点\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_tool_calling_agent, AgentExecutor\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\nllm = ChatOpenAI(model=\u0026quot;gpt-4o\u0026quot;)\ntools = [TavilySearchResults(max_results=3)]\nprompt = ChatPromptTemplate.from_messages([\n    (\u0026quot;system\u0026quot;, \u0026quot;You are a helpful research assistant.\u0026quot;),\n    (\u0026quot;human\u0026quot;, \u0026quot;{input}\u0026quot;),\n    (\u0026quot;placeholder\u0026quot;, \u0026quot;{agent_scratchpad}\u0026quot;),\n])\nagent = create_tool_calling_agent(llm, tools, prompt)\nexecutor = AgentExecutor(agent=agent, tools=tools, verbose=True)\nresult = executor.invoke({\u0026quot;input\u0026quot;: \u0026quot;2025 年 AI Agent 领域有哪些重要进展？\u0026quot;})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这段代码在 5 分钟内就能跑通。但如果你打算把它部署到生产环境——请继续往下读。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e3. LangChain 深入分析\u003c/h2\u003e\n\u003cp\u003eLangChain 是 AI Agent 领域生态最大的框架，也是争议最多的框架。我们不吹不黑，从架构和工程两个维度来分析。\u003c/p\u003e\n\u003ch3\u003e3.1 核心抽象\u003c/h3\u003e\n\u003cp\u003eLangChain 的设计围绕四个核心抽象：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e抽象\u003c/th\u003e\n\u003cth\u003e本质\u003c/th\u003e\n\u003cth\u003e职责\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eChain\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e链式调用\u003c/td\u003e\n\u003ctd\u003e将多个步骤串联为顺序执行的管道\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eAgent\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e工具选择 + 循环\u003c/td\u003e\n\u003ctd\u003eLLM 自主决定调用哪个工具，循环直到完成\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eMemory\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e对话状态管理\u003c/td\u003e\n\u003ctd\u003e维护对话历史，支持滑动窗口、摘要等策略\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eRetriever\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e知识检索\u003c/td\u003e\n\u003ctd\u003e从向量数据库或其他数据源检索相关文档\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e这四个抽象之间的关系可以用下图表示：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌─────────────────────────────────────────────────────────┐\n│                    LangChain Architecture                │\n├─────────────────────────────────────────────────────────┤\n│                                                         │\n│   ┌──────────┐    ┌──────────┐    ┌──────────────────┐  │\n│   │  Chain   │    │  Agent   │    │  AgentExecutor   │  │\n│   │          │    │          │    │  (Control Loop)  │  │\n│   │ step1 →  │    │ LLM +   │    │                  │  │\n│   │ step2 →  │    │ Tools +  │    │  while not done: │  │\n│   │ step3    │    │ Prompt   │    │    plan()        │  │\n│   └────┬─────┘    └────┬─────┘    │    execute()     │  │\n│        │               │          │    observe()     │  │\n│        │               └──────────┤                  │  │\n│        │                          └────────┬─────────┘  │\n│        │                                   │            │\n│   ┌────▼───────────────────────────────────▼─────────┐  │\n│   │              LLM Abstraction Layer               │  │\n│   │  ChatOpenAI │ ChatAnthropic │ ChatOllama │ ...   │  │\n│   └────────────────────┬─────────────────────────────┘  │\n│                        │                                │\n│   ┌────────────────────▼─────────────────────────────┐  │\n│   │                  Memory                          │  │\n│   │  ConversationBufferMemory │ ConversationSummary  │  │\n│   │  VectorStoreMemory │ EntityMemory │ ...          │  │\n│   └──────────────────────────────────────────────────┘  │\n│                                                         │\n│   ┌──────────────────────────────────────────────────┐  │\n│   │                  Retriever                       │  │\n│   │  VectorStoreRetriever │ BM25 │ MultiQuery │ ... │  │\n│   └──────────────────────────────────────────────────┘  │\n│                                                         │\n│   ┌──────────────────────────────────────────────────┐  │\n│   │                  Tools                           │  │\n│   │  Search │ Calculator │ SQL │ FileSystem │ ...    │  │\n│   └──────────────────────────────────────────────────┘  │\n└─────────────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e3.2 代码示例：用 LangChain 实现工具调用 Agent\u003c/h3\u003e\n\u003cp\u003e下面用 LangChain 实现一个能查天气和创建日程的 Agent，同时标注每一层抽象的存在：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_tool_calling_agent, AgentExecutor\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.tools import tool\n\n# --- 第 1 层抽象：@tool 装饰器 ---\n# LangChain 用装饰器将普通函数包装为 Tool 对象\n# 自动从类型注解和 docstring 生成 JSON Schema\n@tool\ndef get_weather(city: str, date: str) -\u0026gt; str:\n    \u0026quot;\u0026quot;\u0026quot;获取指定城市在指定日期的天气预报。\n\n    Args:\n        city: 城市名称，例如 \u0026quot;北京\u0026quot;\n        date: 日期，格式 YYYY-MM-DD\n    \u0026quot;\u0026quot;\u0026quot;\n    # 实际调用天气 API\n    return f\u0026#39;{{\u0026quot;city\u0026quot;: \u0026quot;{city}\u0026quot;, \u0026quot;date\u0026quot;: \u0026quot;{date}\u0026quot;, \u0026quot;temp\u0026quot;: \u0026quot;31°C\u0026quot;, \u0026quot;condition\u0026quot;: \u0026quot;多云转雷阵雨\u0026quot;}}\u0026#39;\n\n@tool\ndef create_reminder(title: str, time: str, note: str) -\u0026gt; str:\n    \u0026quot;\u0026quot;\u0026quot;创建一个日程提醒。\n\n    Args:\n        title: 提醒标题\n        time: 提醒时间，ISO 8601 格式\n        note: 提醒备注内容\n    \u0026quot;\u0026quot;\u0026quot;\n    return f\u0026#39;{{\u0026quot;status\u0026quot;: \u0026quot;created\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;{title}\u0026quot;, \u0026quot;time\u0026quot;: \u0026quot;{time}\u0026quot;}}\u0026#39;\n\n# --- 第 2 层抽象：LLM 封装 ---\n# ChatOpenAI 封装了 OpenAI API 的调用细节\nllm = ChatOpenAI(model=\u0026quot;gpt-4o\u0026quot;, temperature=0)\n\n# --- 第 3 层抽象：Prompt Template ---\n# ChatPromptTemplate 管理消息的组装逻辑\nprompt = ChatPromptTemplate.from_messages([\n    (\u0026quot;system\u0026quot;, \u0026quot;你是一个智能助手，可以查询天气和管理日程。今天是 2025-09-01。\u0026quot;),\n    (\u0026quot;human\u0026quot;, \u0026quot;{input}\u0026quot;),\n    MessagesPlaceholder(variable_name=\u0026quot;agent_scratchpad\u0026quot;),  # Agent 的工作记忆\n])\n\n# --- 第 4 层抽象：Agent 构造 ---\n# create_tool_calling_agent 将 LLM + Tools + Prompt 组合为一个 Agent\ntools = [get_weather, create_reminder]\nagent = create_tool_calling_agent(llm, tools, prompt)\n\n# --- 第 5 层抽象：AgentExecutor ---\n# AgentExecutor 提供控制循环：调用 Agent → 执行工具 → 反馈结果 → 循环\nexecutor = AgentExecutor(\n    agent=agent,\n    tools=tools,\n    verbose=True,       # 输出每一步的推理过程\n    max_iterations=10,  # 最大循环次数\n    handle_parsing_errors=True,  # 自动处理 LLM 输出格式错误\n)\n\n# --- 运行 ---\nresult = executor.invoke({\u0026quot;input\u0026quot;: \u0026quot;帮我查看明天北京的天气，然后创建一个提醒\u0026quot;})\nprint(result[\u0026quot;output\u0026quot;])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e数一数：从你的业务逻辑（两个工具函数）到最终执行，经过了 \u003cstrong\u003e5 层抽象\u003c/strong\u003e。每一层都在\u0026quot;帮你做决策\u0026quot;——消息格式、工具注册方式、控制循环策略、错误处理逻辑、输出解析方式。\u003c/p\u003e\n\u003ch3\u003e3.3 优点\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e1. 生态最大、集成最多\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e截至 2025 年，LangChain 拥有 AI Agent 框架领域最庞大的集成生态：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e70+ LLM 提供商（OpenAI、Anthropic、Google、Mistral、本地模型等）\u003c/li\u003e\n\u003cli\u003e50+ 向量数据库\u003c/li\u003e\n\u003cli\u003e100+ 预置工具\u003c/li\u003e\n\u003cli\u003e30+ Document Loader（PDF、HTML、CSV、Notion、Confluence 等）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e2. 社区活跃\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eGitHub 上最活跃的 AI 项目之一。遇到问题时，StackOverflow 和 GitHub Issues 中大概率能找到讨论。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. 上手快\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e对于 PoC（Proof of Concept）和原型验证，LangChain 能让你在几小时内从零到一跑通一个完整的 Agent。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4. 抽象统一\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e不同 LLM 提供商的 API 差异被封装在统一接口下。切换 OpenAI → Anthropic 只需要换一行代码（理论上如此，实际上有细微差异）。\u003c/p\u003e\n\u003ch3\u003e3.4 问题\u003c/h3\u003e\n\u003cp\u003e以下不是主观吐槽，而是在生产环境中反复遇到的工程问题。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e问题 1：过度抽象——简单的事情被包了太多层\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e考虑一个最基本的需求：调用 LLM 并获取结构化输出。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 不用框架：3 行代码，直白清晰\nimport openai\nresponse = openai.chat.completions.create(\n    model=\u0026quot;gpt-4o\u0026quot;,\n    messages=[{\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;分析这段文本的情感\u0026quot;}],\n    response_format={\u0026quot;type\u0026quot;: \u0026quot;json_object\u0026quot;},\n)\nresult = json.loads(response.choices[0].message.content)\n\n# 用 LangChain：需要理解 ChatOpenAI、BaseOutputParser、RunnableSequence、\n# StrOutputParser vs JsonOutputParser、LCEL 管道语法...\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_template(\u0026quot;分析这段文本的情感: {text}\u0026quot;)\nllm = ChatOpenAI(model=\u0026quot;gpt-4o\u0026quot;)\nparser = JsonOutputParser()\nchain = prompt | llm | parser  # LCEL 管道语法\nresult = chain.invoke({\u0026quot;text\u0026quot;: \u0026quot;这个产品太棒了\u0026quot;})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLangChain 版本代码量更多不是问题——问题在于它引入了多个你需要理解的新概念（\u003ccode\u003eChatPromptTemplate\u003c/code\u003e、\u003ccode\u003eJsonOutputParser\u003c/code\u003e、LCEL 管道操作符 \u003ccode\u003e|\u003c/code\u003e），而这些概念只是在封装原本就很简单的操作。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e问题 2：调试困难——错误信息穿过多层封装后难以定位\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e当 LangChain 链条中的某一环出错时，错误堆栈可能长达 20-30 层，涉及 \u003ccode\u003eRunnableSequence\u003c/code\u003e、\u003ccode\u003eRunnableParallel\u003c/code\u003e、\u003ccode\u003eRunnableLambda\u003c/code\u003e 等内部抽象。你需要在这些框架内部类之间导航，才能找到真正的错误源。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# 真实场景中的错误堆栈（简化版）\nTraceback:\n  langchain_core/runnables/base.py      RunnableSequence.invoke()\n  langchain_core/runnables/base.py      RunnableSequence._invoke()\n  langchain_core/runnables/base.py      Runnable.invoke()\n  langchain_core/runnables/base.py      RunnableLambda.invoke()\n  langchain/agents/output_parsers.py    ToolsAgentOutputParser.parse()\n  ...\n  # 15 层之后...\n  你的代码.py                            你的函数()   ← 真正的问题在这里\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e在生产环境的 3 AM 报警中，这种调试体验是痛苦的。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e问题 3：版本混乱——API 变动频繁\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLangChain 在快速迭代中经历了多次重大 API 变更：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003elangchain\u003c/code\u003e → \u003ccode\u003elangchain-core\u003c/code\u003e + \u003ccode\u003elangchain-community\u003c/code\u003e 的包拆分\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eLLMChain\u003c/code\u003e → LCEL（LangChain Expression Language）的范式转换\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003einitialize_agent\u003c/code\u003e → \u003ccode\u003ecreate_tool_calling_agent\u003c/code\u003e 的 Agent 创建方式变更\u003c/li\u003e\n\u003cli\u003eMemory 接口的多次重构\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e6 个月前写的代码，今天大概率跑不通。网上的教程和 StackOverflow 答案大量过时。对于需要长期维护的生产系统，这是一个严重的风险。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e问题 4：\u0026quot;Chain\u0026quot; 思维的局限——线性链无法表达复杂的分支和循环\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLangChain 的核心抽象是 \u0026quot;Chain\u0026quot;——链式调用。这个模型对于线性流水线（A → B → C）非常优雅，但现实中的 Agent 逻辑往往是非线性的：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e线性 Chain 能表达的：\n\n    A ──→ B ──→ C ──→ D\n    (检索)  (摘要)  (格式化) (输出)\n\n\n现实中 Agent 需要的：\n\n    A ──→ B ──→ C ──→ D\n    │     │     ▲     │\n    │     ├─→ E ─┘     │     ← 条件分支\n    │     │             │\n    │     └─→ F ──→ G ──┘     ← 并行执行\n    │           │\n    └───────────┘              ← 循环重试\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLangChain 的 LCEL 可以通过 \u003ccode\u003eRunnableBranch\u003c/code\u003e 和 \u003ccode\u003eRunnableParallel\u003c/code\u003e 实现一些分支和并行，但语法变得复杂且不直观。这正是 LangGraph 诞生的原因。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e4. LangGraph 深入分析\u003c/h2\u003e\n\u003cp\u003eLangGraph 是 LangChain 团队推出的下一代框架，核心思想是用\u003cstrong\u003e有向图（Directed Graph）\u003c/strong\u003e 替代\u003cstrong\u003e链（Chain）\u003c/strong\u003e 作为基础抽象。这不是一个小改动——它从根本上改变了 Agent 逻辑的表达方式。\u003c/p\u003e\n\u003ch3\u003e4.1 核心抽象\u003c/h3\u003e\n\u003cp\u003eLangGraph 的设计围绕四个概念：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e抽象\u003c/th\u003e\n\u003cth\u003e本质\u003c/th\u003e\n\u003cth\u003e对应的计算模型\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eState\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e共享状态对象\u003c/td\u003e\n\u003ctd\u003e状态机的 State\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eNode\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e一个函数\u003c/td\u003e\n\u003ctd\u003e状态机的 State Handler\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eEdge\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e节点间的连接\u003c/td\u003e\n\u003ctd\u003e状态机的 Transition\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eGraph\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e节点和边的组合\u003c/td\u003e\n\u003ctd\u003e有限状态机（FSM）\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e核心思想：\u003cstrong\u003eAgent 的执行流程就是一个状态机。\u003c/strong\u003e 每个节点是一个处理函数，每条边是一个转移条件，整个图定义了 Agent 的所有可能执行路径。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌─────────────────────────────────────────────────────────────┐\n│                   LangGraph State Machine                    │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│   ┌─────────────────────────────────────────────────────┐   │\n│   │                  Shared State                       │   │\n│   │  {messages: [...], tool_results: {...}, plan: [...]} │   │\n│   └────────────────────────┬────────────────────────────┘   │\n│                            │                                │\n│               ┌────────────▼────────────┐                   │\n│               │       START             │                   │\n│               └────────────┬────────────┘                   │\n│                            │                                │\n│               ┌────────────▼────────────┐                   │\n│               │      agent_node         │                   │\n│               │   (LLM Reasoning)       │                   │\n│               └────────────┬────────────┘                   │\n│                            │                                │\n│               ┌────────────▼────────────┐                   │\n│              ╱    should_continue?       ╲                   │\n│             ╱  (Conditional Edge)         ╲                  │\n│            ╱                               ╲                 │\n│      tool_calls?                      no tool_calls?        │\n│           │                                │                │\n│  ┌────────▼─────────┐          ┌──────────▼──────────┐     │\n│  │    tool_node      │          │       END            │     │\n│  │  (Execute Tools)  │          │   (Return Result)    │     │\n│  └────────┬──────────┘          └─────────────────────┘     │\n│           │                                                 │\n│           └──────────────────┐                              │\n│                              │ (feed tool results back)     │\n│               ┌──────────────▼──────────┐                   │\n│               │      agent_node         │ ← 回到推理节点    │\n│               └─────────────────────────┘                   │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这个图可以清晰地表达：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e循环\u003c/strong\u003e：\u003ccode\u003eagent_node → tool_node → agent_node\u003c/code\u003e（工具调用循环）\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e分支\u003c/strong\u003e：\u003ccode\u003eshould_continue?\u003c/code\u003e 条件路由\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e终止\u003c/strong\u003e：到达 \u003ccode\u003eEND\u003c/code\u003e 节点时退出\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e4.2 代码示例：用 LangGraph 实现同一个 Agent\u003c/h3\u003e\n\u003cp\u003e用 LangGraph 实现与上文 LangChain 相同的天气查询 + 日程创建 Agent：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom typing import Annotated, TypedDict\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\nfrom langchain_core.messages import BaseMessage, HumanMessage, AIMessage\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode\n\n\n# ============================================================\n# Step 1: 定义共享状态（State）\n# ============================================================\n# 这是 LangGraph 与 LangChain 的核心差异：\n# 显式定义 Agent 的完整状态结构\nclass AgentState(TypedDict):\n    messages: Annotated[list[BaseMessage], add_messages]  # 消息列表，自动追加\n\n\n# ============================================================\n# Step 2: 定义工具（和 LangChain 相同）\n# ============================================================\n@tool\ndef get_weather(city: str, date: str) -\u0026gt; str:\n    \u0026quot;\u0026quot;\u0026quot;获取指定城市在指定日期的天气预报。\u0026quot;\u0026quot;\u0026quot;\n    return f\u0026#39;{{\u0026quot;city\u0026quot;: \u0026quot;{city}\u0026quot;, \u0026quot;date\u0026quot;: \u0026quot;{date}\u0026quot;, \u0026quot;temp\u0026quot;: \u0026quot;31°C\u0026quot;, \u0026quot;condition\u0026quot;: \u0026quot;多云转雷阵雨\u0026quot;}}\u0026#39;\n\n@tool\ndef create_reminder(title: str, time: str, note: str) -\u0026gt; str:\n    \u0026quot;\u0026quot;\u0026quot;创建一个日程提醒。\u0026quot;\u0026quot;\u0026quot;\n    return f\u0026#39;{{\u0026quot;status\u0026quot;: \u0026quot;created\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;{title}\u0026quot;, \u0026quot;time\u0026quot;: \u0026quot;{time}\u0026quot;}}\u0026#39;\n\ntools = [get_weather, create_reminder]\n\n\n# ============================================================\n# Step 3: 定义节点（Node）\n# ============================================================\nllm = ChatOpenAI(model=\u0026quot;gpt-4o\u0026quot;, temperature=0).bind_tools(tools)\n\ndef agent_node(state: AgentState) -\u0026gt; dict:\n    \u0026quot;\u0026quot;\u0026quot;推理节点：LLM 根据当前状态决定下一步\u0026quot;\u0026quot;\u0026quot;\n    system_message = {\n        \u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;,\n        \u0026quot;content\u0026quot;: \u0026quot;你是一个智能助手，可以查询天气和管理日程。今天是 2025-09-01。\u0026quot;\n    }\n    messages = [system_message] + state[\u0026quot;messages\u0026quot;]\n    response = llm.invoke(messages)\n    return {\u0026quot;messages\u0026quot;: [response]}\n\n# ToolNode 是 LangGraph 的内置节点，自动执行工具调用\ntool_node = ToolNode(tools)\n\n\n# ============================================================\n# Step 4: 定义边（Edge）—— 条件路由\n# ============================================================\ndef should_continue(state: AgentState) -\u0026gt; str:\n    \u0026quot;\u0026quot;\u0026quot;条件路由：检查最后一条消息是否包含工具调用\u0026quot;\u0026quot;\u0026quot;\n    last_message = state[\u0026quot;messages\u0026quot;][-1]\n    if hasattr(last_message, \u0026quot;tool_calls\u0026quot;) and last_message.tool_calls:\n        return \u0026quot;tools\u0026quot;     # 有工具调用 → 去 tool_node\n    return \u0026quot;end\u0026quot;           # 无工具调用 → 任务完成\n\n\n# ============================================================\n# Step 5: 构建图（Graph）\n# ============================================================\ngraph_builder = StateGraph(AgentState)\n\n# 添加节点\ngraph_builder.add_node(\u0026quot;agent\u0026quot;, agent_node)\ngraph_builder.add_node(\u0026quot;tools\u0026quot;, tool_node)\n\n# 添加边\ngraph_builder.add_edge(START, \u0026quot;agent\u0026quot;)                        # 入口 → 推理\ngraph_builder.add_conditional_edges(\u0026quot;agent\u0026quot;, should_continue, {\n    \u0026quot;tools\u0026quot;: \u0026quot;tools\u0026quot;,                                         # 推理 → 工具执行\n    \u0026quot;end\u0026quot;: END,                                               # 推理 → 结束\n})\ngraph_builder.add_edge(\u0026quot;tools\u0026quot;, \u0026quot;agent\u0026quot;)                      # 工具执行 → 回到推理\n\n# 编译图\ngraph = graph_builder.compile()\n\n\n# ============================================================\n# Step 6: 运行\n# ============================================================\nresult = graph.invoke({\n    \u0026quot;messages\u0026quot;: [HumanMessage(content=\u0026quot;帮我查看明天北京的天气，然后创建一个提醒\u0026quot;)]\n})\n\n# 输出最终结果\nfor message in result[\u0026quot;messages\u0026quot;]:\n    print(f\u0026quot;[{message.type}] {message.content}\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e对比 LangChain 版本，LangGraph 的关键差异：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e显式状态定义\u003c/strong\u003e：\u003ccode\u003eAgentState\u003c/code\u003e 明确声明了 Agent 运行时的完整状态\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e显式控制流\u003c/strong\u003e：\u003ccode\u003eadd_edge\u003c/code\u003e 和 \u003ccode\u003eadd_conditional_edges\u003c/code\u003e 让执行路径一目了然\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e图可视化\u003c/strong\u003e：编译后的 \u003ccode\u003egraph\u003c/code\u003e 可以直接渲染为流程图，便于理解和调试\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e没有隐藏的循环\u003c/strong\u003e：循环通过 \u003ccode\u003etools → agent\u003c/code\u003e 的边显式定义，而不是藏在 \u003ccode\u003eAgentExecutor\u003c/code\u003e 内部\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e4.3 优点\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e1. 状态机模型比 Chain 更强大\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eChain 只能表达线性流水线。Graph 可以表达任意拓扑——分支、循环、并行、条件汇聚。这与现实中 Agent 的执行逻辑天然匹配。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. 确定性的控制流 + 非确定性的 LLM 决策\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e这是 LangGraph 最精妙的设计哲学：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e确定性（代码定义）：            非确定性（LLM 决定）：\n├── 有哪些节点                 ├── 每个节点内部的推理\n├── 节点间如何连接              ├── 工具选择和参数\n├── 条件路由的判断逻辑          ├── 是否继续循环\n└── 状态的数据结构              └── 最终输出内容\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e图的拓扑结构是确定性的（你在编译时就知道所有可能的执行路径），但每一步走哪条路径是 LLM 在运行时决定的。这实现了\u003cstrong\u003e可预测的系统行为\u003c/strong\u003e与\u003cstrong\u003e灵活的智能决策\u003c/strong\u003e之间的平衡。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. Checkpoint 支持——暂停、恢复、Time-Travel\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLangGraph 内置了状态检查点机制。这意味着：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom langgraph.checkpoint.memory import MemorySaver\n\n# 带 checkpoint 的图\ncheckpointer = MemorySaver()\ngraph = graph_builder.compile(checkpointer=checkpointer)\n\n# 运行时传入 thread_id\nconfig = {\u0026quot;configurable\u0026quot;: {\u0026quot;thread_id\u0026quot;: \u0026quot;user-123\u0026quot;}}\nresult = graph.invoke({\u0026quot;messages\u0026quot;: [HumanMessage(content=\u0026quot;查天气\u0026quot;)]}, config)\n\n# 可以暂停、恢复、回放\n# - 暂停：interrupt_before=[\u0026quot;tool_node\u0026quot;] 在工具执行前暂停，等待人类审批\n# - 恢复：再次 invoke 同一个 thread_id，从上次中断点继续\n# - Time-travel：回滚到任意 checkpoint，重新执行\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这在 Human-in-the-Loop（人机协作）场景中极其有价值——Agent 可以在执行敏感操作前暂停，等待人类确认。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4. 可以表达复杂的多 Agent 架构\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e上一篇我们讨论的 Supervisor/Worker 模式、并行 Agent 协作，在 LangGraph 中可以自然地表达为图结构：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e                 ┌──────────────┐\n                 │  Supervisor  │\n                 └──────┬───────┘\n                        │\n              ┌─────────┼─────────┐\n              ▼         ▼         ▼\n        ┌──────────┐ ┌──────┐ ┌──────────┐\n        │ Researcher│ │Coder │ │ Reviewer │\n        └──────────┘ └──────┘ └──────────┘\n              │         │         │\n              └─────────┼─────────┘\n                        ▼\n                 ┌──────────────┐\n                 │  Supervisor  │ ← 回到 Supervisor 决定是否继续\n                 └──────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e4.4 问题\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e问题 1：学习曲线较陡\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLangGraph 要求你理解状态机、有向图、条件路由等概念。对于习惯了\u0026quot;调用一个函数就能跑\u0026quot;的开发者来说，需要一段适应期。\u003c/p\u003e\n\u003cp\u003e特别是 \u003ccode\u003eAnnotated[list[BaseMessage], add_messages]\u003c/code\u003e 这样的状态定义语法（使用 \u003ccode\u003eAnnotated\u003c/code\u003e 类型指定 reducer 函数），对 Python 类型系统不熟悉的开发者可能感到困惑。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e问题 2：状态定义需要提前规划\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e在 LangChain 中，你可以随意传递数据，框架会帮你管理。在 LangGraph 中，所有状态必须在 \u003ccode\u003eAgentState\u003c/code\u003e 中预先定义。这意味着你需要在写代码之前就想清楚 Agent 需要哪些状态。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 如果开发到一半发现需要新的状态字段，\n# 你需要修改 State 定义，并确保所有节点兼容\nclass AgentState(TypedDict):\n    messages: Annotated[list[BaseMessage], add_messages]\n    plan: list[str]                    # 后来加的\n    current_step: int                  # 后来加的\n    tool_results: dict[str, str]       # 后来加的\n    retry_count: int                   # 后来加的\n    # ... 状态会越来越复杂\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e对于探索性的开发来说，这种\u0026quot;先定义后使用\u0026quot;的约束会拖慢迭代速度。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e问题 3：小任务过度工程化\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e如果你的 Agent 逻辑就是\u0026quot;调用 LLM → 可能调用工具 → 返回结果\u0026quot;这个简单循环，用 LangGraph 定义 State、Node、Edge、Conditional Edge 就像是用大炮打蚊子。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 一个简单的 ReAct Agent，用 LangGraph 需要 40+ 行图定义代码\n# 用原生 Python 只需要一个 while 循环：\nwhile True:\n    response = llm.chat(messages, tools=tools)\n    if not response.tool_calls:\n        return response.content\n    for tc in response.tool_calls:\n        result = execute_tool(tc)\n        messages.append(tool_message(tc.id, result))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e当你的 Agent 逻辑不涉及复杂的分支和并行时，LangGraph 的开销不值得。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e5. 其他框架概览\u003c/h2\u003e\n\u003cp\u003e除了 LangChain 和 LangGraph，AI Agent 领域还有多个值得关注的框架。以下不深入展开，重点给出定位和适用场景。\u003c/p\u003e\n\u003ch3\u003e5.1 框架定位速览\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e框架\u003c/th\u003e\n\u003cth\u003e开发者\u003c/th\u003e\n\u003cth\u003e核心抽象\u003c/th\u003e\n\u003cth\u003e定位\u003c/th\u003e\n\u003cth\u003e适用场景\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eLangChain\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eLangChain Inc.\u003c/td\u003e\n\u003ctd\u003eChain（链式调用）\u003c/td\u003e\n\u003ctd\u003e通用 AI 应用框架\u003c/td\u003e\n\u003ctd\u003e原型验证、RAG、简单 Agent\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eLangGraph\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eLangChain Inc.\u003c/td\u003e\n\u003ctd\u003eGraph（状态机）\u003c/td\u003e\n\u003ctd\u003e复杂 Agent 编排\u003c/td\u003e\n\u003ctd\u003e多步推理、Human-in-the-Loop、多 Agent\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eCrewAI\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eCrewAI Inc.\u003c/td\u003e\n\u003ctd\u003eCrew + Agent + Task\u003c/td\u003e\n\u003ctd\u003e多 Agent 协作\u003c/td\u003e\n\u003ctd\u003e角色扮演式多 Agent 工作流\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eAutoGen\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eMicrosoft\u003c/td\u003e\n\u003ctd\u003eAgent + Conversation\u003c/td\u003e\n\u003ctd\u003e多 Agent 对话\u003c/td\u003e\n\u003ctd\u003e研究型多 Agent 系统、代码生成\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eSemantic Kernel\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eMicrosoft\u003c/td\u003e\n\u003ctd\u003eKernel + Plugin + Planner\u003c/td\u003e\n\u003ctd\u003e企业级 AI 编排\u003c/td\u003e\n\u003ctd\u003e企业应用集成、.NET 生态\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eHaystack\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003edeepset\u003c/td\u003e\n\u003ctd\u003ePipeline + Component\u003c/td\u003e\n\u003ctd\u003eRAG 专用\u003c/td\u003e\n\u003ctd\u003e文档检索、知识问答\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eDSPy\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eStanford NLP\u003c/td\u003e\n\u003ctd\u003eModule + Signature + Optimizer\u003c/td\u003e\n\u003ctd\u003ePrompt 优化\u003c/td\u003e\n\u003ctd\u003e需要自动调优 Prompt 的系统\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch3\u003e5.2 简要点评\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eCrewAI\u003c/strong\u003e 的核心思路是\u0026quot;角色扮演\u0026quot;——你定义多个 Agent，每个 Agent 有一个角色（Researcher、Writer、Reviewer），然后把一个任务分配给这个\u0026quot;团队\u0026quot;。这个抽象直观好懂，但在复杂场景中角色定义和任务分配的灵活性不足。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# CrewAI 的核心抽象：角色 + 任务 + 团队\nfrom crewai import Agent, Task, Crew\n\nresearcher = Agent(role=\u0026quot;Researcher\u0026quot;, goal=\u0026quot;查找相关信息\u0026quot;, ...)\nwriter = Agent(role=\u0026quot;Writer\u0026quot;, goal=\u0026quot;撰写报告\u0026quot;, ...)\ntask1 = Task(description=\u0026quot;研究 AI Agent 的最新进展\u0026quot;, agent=researcher)\ntask2 = Task(description=\u0026quot;基于研究结果撰写报告\u0026quot;, agent=writer)\ncrew = Crew(agents=[researcher, writer], tasks=[task1, task2])\nresult = crew.kickoff()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eAutoGen\u003c/strong\u003e（Microsoft）强调多 Agent 之间的对话作为协作机制。Agent 之间通过消息传递交互，可以构建复杂的对话流程。适合研究和实验性项目，生产部署的工程支持较弱。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSemantic Kernel\u003c/strong\u003e（Microsoft）面向企业用户，强调与现有企业系统的集成。如果你的技术栈是 .NET/C#，或者需要与 Microsoft 365/Azure 深度集成，Semantic Kernel 是更自然的选择。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHaystack\u003c/strong\u003e（deepset）不试图做通用 Agent 框架，而是专注于 RAG pipeline。如果你的核心需求是文档检索和知识问答（而不是 Agent 的自主决策和工具调用），Haystack 的 Pipeline 抽象比 LangChain 更干净。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDSPy\u003c/strong\u003e（Stanford NLP）走了一条完全不同的路——它不是一个 Agent 运行时框架，而是一个 Prompt 优化框架。核心思想是把 Prompt 当作可学习的参数，通过编译和优化自动找到最佳 Prompt。适合对 Prompt 质量有极高要求的场景。\u003c/p\u003e\n\u003ch3\u003e5.3 框架选型决策树\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e你的核心需求是什么？\n│\n├─── 快速原型 / PoC\n│    └─→ LangChain（生态最大，上手最快）\n│\n├─── 复杂 Agent 逻辑（分支/循环/并行）\n│    └─→ LangGraph（状态机模型天然适合）\n│\n├─── 多 Agent 协作\n│    ├─── 角色扮演式 → CrewAI\n│    ├─── 对话式协作 → AutoGen\n│    └─── 图编排式   → LangGraph\n│\n├─── RAG / 知识问答\n│    ├─── 需要灵活性  → LangChain + Retriever\n│    └─── 需要干净抽象 → Haystack\n│\n├─── 企业级集成（.NET / Azure）\n│    └─→ Semantic Kernel\n│\n├─── Prompt 自动优化\n│    └─→ DSPy\n│\n└─── 生产系统（需要精细控制）\n     └─→ 自研，或只使用框架的底层模块\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e6. 框架 vs 自研的决策矩阵\u003c/h2\u003e\n\u003cp\u003e这是本文最重要的一节。不存在\u0026quot;框架一定好\u0026quot;或\u0026quot;自研一定好\u0026quot;的结论——关键是根据你的具体场景做出理性决策。\u003c/p\u003e\n\u003ch3\u003e6.1 决策矩阵\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e考量因素\u003c/th\u003e\n\u003cth\u003e倾向选框架\u003c/th\u003e\n\u003cth\u003e倾向选自研\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e项目阶段\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e原型验证、MVP\u003c/td\u003e\n\u003ctd\u003e生产系统、需要长期维护\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e团队规模\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e1-3 人小团队\u003c/td\u003e\n\u003ctd\u003e5+ 人专职 AI 团队\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e定制化程度\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e标准 ReAct/RAG 模式\u003c/td\u003e\n\u003ctd\u003e有独特的控制流或状态管理需求\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e调试要求\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e能接受黑盒\u003c/td\u003e\n\u003ctd\u003e需要完全可观测、可追踪\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e性能要求\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e对 latency 不敏感\u003c/td\u003e\n\u003ctd\u003e需要极致优化每一毫秒\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e依赖容忍度\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e能接受第三方依赖的版本变化\u003c/td\u003e\n\u003ctd\u003e需要完全掌控依赖\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e上线时间\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e2 周内上线\u003c/td\u003e\n\u003ctd\u003e3 个月以上的工程周期\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e团队 AI 经验\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e初次接触 Agent 开发\u003c/td\u003e\n\u003ctd\u003e对 Agent 架构有深入理解\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch3\u003e6.2 常见场景分析\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e场景 1：初创团队做 AI 产品的 MVP\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e推荐：LangChain（快速原型）→ 验证产品方向 → 决定是否重写\u003c/p\u003e\n\u003cp\u003e理由：此时最大的风险不是技术债，而是方向错误。花 3 个月自研一个完美的 Agent Runtime，结果发现用户不需要 Agent——这才是最大的浪费。用框架在 2 周内验证想法，确认方向后再决定技术路线。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e场景 2：大厂 AI 平台团队\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e推荐：自研核心 Runtime + 选择性使用框架的底层模块\u003c/p\u003e\n\u003cp\u003e理由：大厂有足够的工程资源，且对可靠性、可观测性、安全性的要求远超框架的默认支持。自研 Runtime 可以完全掌控控制循环、状态管理、错误处理、日志追踪。但可以借鉴框架的设计模式，或使用框架的工具集成层（比如 LangChain 的 Tool/Retriever 集成）。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e场景 3：企业内部的 AI 助手\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e推荐：LangGraph（如果逻辑复杂）或 LangChain（如果逻辑简单）\u003c/p\u003e\n\u003cp\u003e理由：企业内部项目通常有明确的需求边界和合理的 SLA 要求，框架能满足大部分需求。LangGraph 的 Human-in-the-Loop 支持对企业审批流程特别有用。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e场景 4：研究实验\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e推荐：AutoGen 或自研轻量框架\u003c/p\u003e\n\u003cp\u003e理由：研究需要最大的灵活性来尝试新想法。框架的抽象可能限制实验空间。但如果实验涉及多 Agent 交互，AutoGen 的对话式抽象可以减少样板代码。\u003c/p\u003e\n\u003ch3\u003e6.3 一个务实的折中方案\u003c/h3\u003e\n\u003cp\u003e在实践中，最常见的成熟方案是\u003cstrong\u003e分层使用框架\u003c/strong\u003e：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌─────────────────────────────────────────────────┐\n│              你的应用层代码                        │\n│         (业务逻辑、API 接口、用户交互)              │\n├─────────────────────────────────────────────────┤\n│              自研 Agent Runtime                    │\n│    (控制循环、状态管理、错误处理、可观测性)          │\n├───────────────┬─────────────────────────────────┤\n│  自研工具调度   │   框架的集成模块（可选使用）       │\n│  自研消息管理   │   LangChain Tool/Retriever       │\n│  自研状态存储   │   LangChain Document Loader      │\n│               │   LangChain Embedding 接口        │\n├───────────────┴─────────────────────────────────┤\n│              LLM Provider SDK                     │\n│         (openai, anthropic, etc.)                 │\n└─────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e核心思路：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e控制循环自研\u003c/strong\u003e：这是 Agent 最核心的逻辑，也是最需要定制的部分。用 40-60 行 Python 就能实现一个健壮的控制循环（回顾第 07 篇）\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLLM 调用用原生 SDK\u003c/strong\u003e：OpenAI SDK 和 Anthropic SDK 本身就很好用，不需要再包一层\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e工具集成可以借用框架\u003c/strong\u003e：LangChain 的 Tool 生态确实强大。你可以只 \u003ccode\u003epip install langchain-community\u003c/code\u003e 来使用其预置工具，而不用采纳整个框架\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e状态管理自研\u003c/strong\u003e：根据你的持久化需求（Redis、PostgreSQL、内存）定制\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这个方案的好处是：你在最关键的层面保留了完全掌控力，同时在最不需要掌控的层面（第三方服务的集成）借助了框架的生态。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e7. 框架的正确使用姿势\u003c/h2\u003e\n\u003cp\u003e无论你最终选择什么方案，以下原则都适用。\u003c/p\u003e\n\u003ch3\u003e7.1 理解原理再用框架\u003c/h3\u003e\n\u003cp\u003e这正是本系列前 7 篇文章的价值。当你理解了控制循环的六个阶段、Tool Calling 的 JSON Schema 契约、Memory 的分层架构之后，框架在你眼中就不再是黑盒——它只是这些原理的一种实现。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e不理解原理时使用框架：\n    框架 = 黑魔法（出错时手足无措）\n\n理解原理后使用框架：\n    框架 = 已知原理的一种实现（出错时知道去哪里找原因）\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e具体来说：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e当 LangChain 的 \u003ccode\u003eAgentExecutor\u003c/code\u003e 出错时，你知道它内部在跑一个控制循环，可以猜测问题出在哪个阶段\u003c/li\u003e\n\u003cli\u003e当 LangGraph 的状态转移出现异常时，你知道这本质上是一个状态机的转移条件判断错误\u003c/li\u003e\n\u003cli\u003e当框架的 Memory 管理不符合你的需求时，你知道自己需要什么样的记忆架构，可以替换或扩展\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e7.2 不要被框架限制思维\u003c/h3\u003e\n\u003cp\u003e框架提供了一组默认的设计模式。这些模式覆盖了 80% 的常见场景，但你的场景可能落在剩下的 20%。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e反模式\u003c/strong\u003e：为了适配框架的抽象而扭曲自己的业务逻辑。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 反模式：业务逻辑需要 Agent 在两个工具的结果之间做比较，\n# 但框架不直接支持，于是你\u0026quot;发明\u0026quot;了一个假工具来绕过限制\n\n@tool\ndef compare_results(result_a: str, result_b: str) -\u0026gt; str:\n    \u0026quot;\u0026quot;\u0026quot;比较两个结果（实际上这应该是 Agent 内部的推理逻辑，不是工具）\u0026quot;\u0026quot;\u0026quot;\n    # 这不应该是一个 Tool —— 这是把框架的抽象当成了唯一的解法\n    return llm.invoke(f\u0026quot;比较: {result_a} vs {result_b}\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e正确做法\u003c/strong\u003e：框架不支持的逻辑，用原生代码实现，然后插入到框架的流程中（或者干脆不用框架处理这部分）。\u003c/p\u003e\n\u003ch3\u003e7.3 框架代码是最好的学习材料\u003c/h3\u003e\n\u003cp\u003e即使你决定自研，框架的源码仍然是宝贵的学习资源。以下是几个值得阅读的代码文件：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLangGraph 的 \u003ccode\u003eStateGraph\u003c/code\u003e\u003c/strong\u003e：理解如何用 Python 实现一个状态机运行时\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLangChain 的 \u003ccode\u003eToolNode\u003c/code\u003e\u003c/strong\u003e：理解如何将 LLM 的 tool_call 输出映射为实际的函数调用\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLangChain 的 \u003ccode\u003eChatOpenAI\u003c/code\u003e\u003c/strong\u003e：理解如何封装 LLM Provider 的 API 差异\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLangGraph 的 \u003ccode\u003eMemorySaver\u003c/code\u003e\u003c/strong\u003e：理解 checkpoint 和状态持久化的实现\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e阅读源码时，关注的不是具体的 API，而是\u003cstrong\u003e设计决策\u003c/strong\u003e：为什么这样抽象？这个 trade-off 是什么？有没有更好的方案？\u003c/p\u003e\n\u003ch3\u003e7.4 随时准备好替换或去掉框架\u003c/h3\u003e\n\u003cp\u003e一个健康的架构应该允许你在不重写业务逻辑的情况下替换底层框架。实现方式：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 定义你自己的接口（不依赖任何框架）\nfrom abc import ABC, abstractmethod\n\nclass BaseLLM(ABC):\n    @abstractmethod\n    def chat(self, messages: list[dict], tools: list[dict] | None = None) -\u0026gt; dict:\n        ...\n\nclass BaseToolExecutor(ABC):\n    @abstractmethod\n    def execute(self, tool_name: str, args: dict) -\u0026gt; str:\n        ...\n\nclass BaseMemory(ABC):\n    @abstractmethod\n    def get_messages(self, limit: int = 20) -\u0026gt; list[dict]:\n        ...\n    @abstractmethod\n    def add_message(self, message: dict) -\u0026gt; None:\n        ...\n\n\n# 框架实现（可替换）\nclass LangChainLLM(BaseLLM):\n    def __init__(self):\n        from langchain_openai import ChatOpenAI\n        self._llm = ChatOpenAI(model=\u0026quot;gpt-4o\u0026quot;)\n\n    def chat(self, messages, tools=None):\n        # 将你的接口适配为 LangChain 接口\n        ...\n\n# 原生实现（可替换）\nclass NativeLLM(BaseLLM):\n    def __init__(self):\n        import openai\n        self._client = openai.OpenAI()\n\n    def chat(self, messages, tools=None):\n        response = self._client.chat.completions.create(\n            model=\u0026quot;gpt-4o\u0026quot;, messages=messages, tools=tools\n        )\n        ...\n\n\n# 你的 Agent 代码只依赖自己的接口\nclass MyAgent:\n    def __init__(self, llm: BaseLLM, tools: BaseToolExecutor, memory: BaseMemory):\n        self.llm = llm\n        self.tools = tools\n        self.memory = memory\n\n    def run(self, user_input: str) -\u0026gt; str:\n        # 业务逻辑不依赖任何框架\n        ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这不是过度设计——这是\u003cstrong\u003e依赖倒置原则\u003c/strong\u003e在 Agent 架构中的直接应用。当框架发生 breaking change（LangChain 几乎每季度都有）时，你只需要修改适配层，而不是重写整个系统。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e8. LangChain vs LangGraph：直接对比\u003c/h2\u003e\n\u003cp\u003e最后，用一张表格直接对比 LangChain 和 LangGraph 在各维度的差异：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e维度\u003c/th\u003e\n\u003cth\u003eLangChain\u003c/th\u003e\n\u003cth\u003eLangGraph\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e核心抽象\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eChain（线性管道）\u003c/td\u003e\n\u003ctd\u003eGraph（有向状态机）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e控制流表达\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e线性为主，分支/循环需要 hack\u003c/td\u003e\n\u003ctd\u003e天然支持分支、循环、并行\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e状态管理\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e隐式（框架内部管理）\u003c/td\u003e\n\u003ctd\u003e显式（开发者定义 State 类型）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e学习曲线\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e低（上手快）\u003c/td\u003e\n\u003ctd\u003e中等（需要理解状态机概念）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e调试体验\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e差（多层抽象遮蔽错误源）\u003c/td\u003e\n\u003ctd\u003e中等（图结构可视化，但状态流转需追踪）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e适合场景\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e简单 Agent、RAG、原型验证\u003c/td\u003e\n\u003ctd\u003e复杂 Agent、多 Agent、Human-in-the-Loop\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e生态集成\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e最丰富\u003c/td\u003e\n\u003ctd\u003e继承 LangChain 生态\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eHuman-in-the-Loop\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e不原生支持\u003c/td\u003e\n\u003ctd\u003e原生 Checkpoint + Interrupt 支持\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e多 Agent\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e需要自行编排\u003c/td\u003e\n\u003ctd\u003e原生支持子图嵌套\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e生产就绪度\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e中等（需要大量自定义）\u003c/td\u003e\n\u003ctd\u003e较高（状态持久化、检查点内置）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e灵活性\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e框架约束多，突破框架难\u003c/td\u003e\n\u003ctd\u003e图定义灵活，但需要提前规划\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e版本稳定性\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e差（API 频繁变更）\u003c/td\u003e\n\u003ctd\u003e较好（API 相对稳定）\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e总结\u003c/strong\u003e：如果 LangChain 是一条\u003cstrong\u003e传送带\u003c/strong\u003e（把东西从 A 运到 B），那么 LangGraph 就是一张\u003cstrong\u003e铁路网\u003c/strong\u003e（可以在任意站点之间调度列车）。传送带简单高效，铁路网灵活强大——选哪个取决于你要运的东西有多复杂。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e9. 结语与进一步思考\u003c/h2\u003e\n\u003ch3\u003e核心立场回顾\u003c/h3\u003e\n\u003cp\u003e本文的核心立场可以用三句话概括：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e框架是加速器，不是必需品。\u003c/strong\u003e 它加速了开发，但也隐藏了复杂性。当隐藏的复杂性成为你的瓶颈时，框架就从加速器变成了减速器。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e理解原理比掌握框架更重要。\u003c/strong\u003e 框架会变（LangChain 已经经历了多次 API 大改），但控制循环、状态管理、工具调用的基本原理不会变。前 7 篇文章构建的知识，是你评估和使用任何框架的基础。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e最好的架构是\u0026quot;框架可替换\u0026quot;的架构。\u003c/strong\u003e 把框架当作可插拔的实现层，而不是系统的骨架。你的业务逻辑应该依赖自己定义的接口，而不是某个框架的 API。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e框架解决了\u0026quot;怎么写\u0026quot;，协议解决\u0026quot;怎么连接\u0026quot;\u003c/h3\u003e\n\u003cp\u003e框架帮你解决了一个 Agent 内部的组件编排问题：如何组织 LLM 调用、工具执行、状态管理。但当你有多个 Agent、多个工具提供者、多个模型时，一个更根本的问题浮现出来：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e这些组件之间用什么协议通信？工具如何被发现和注册？能力如何被声明和协商？\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e这不是框架能解决的问题——这需要\u003cstrong\u003e协议（Protocol）\u003c/strong\u003e。下一篇我们将讨论 MCP（Model Context Protocol），看看 Agent 工具生态的协议化未来。\u003c/p\u003e\n\u003ch3\u003e留给读者的思考\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e关于框架的未来\u003c/strong\u003e：LLM 本身的能力在快速增强。当模型原生支持复杂的多步推理（如 o1/o3 的 chain-of-thought）、原生支持长对话记忆（如 Gemini 的长上下文窗口）、原生支持工具调用时，框架的价值会被压缩还是放大？换句话说——当 LLM 足够强时，我们还需要框架在中间做多少事？\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e关于抽象的代价\u003c/strong\u003e：每一层抽象都在隐藏复杂性。隐藏复杂性是好事（让你专注于业务逻辑），但也是坏事（让你在出问题时无法理解系统行为）。在 Agent 这样本身就充满不确定性的系统中，你能接受多少\u0026quot;隐藏的复杂性\u0026quot;？\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e关于生态锁定\u003c/strong\u003e：选择一个框架意味着接受它的抽象、它的生态、它的更新节奏、它的设计理念。当框架的方向与你的需求分叉时，迁移的成本有多高？这个成本是否在你的决策时被低估了？\u003c/p\u003e\n\u003cp\u003e这些问题没有标准答案。但作为 AI 工程师，能够清晰地提出这些问题，本身就是一种重要的能力。\u003c/p\u003e\n\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e系列导航\u003c/strong\u003e：本文是 Agentic 系列的第 12 篇。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e上一篇：\u003ca href=\"/blog/engineering/agentic/11-Multi-Agent%20Collaboration\"\u003e11 | Multi-Agent Collaboration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下一篇：\u003ca href=\"/blog/engineering/agentic/13-MCP%20and%20Tool%20Protocol\"\u003e13 | MCP and Tool Protocol\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e完整目录：\u003ca href=\"/blog/engineering/agentic/01-From%20LLM%20to%20Agent\"\u003e01 | From LLM to Agent\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"17:Tf833,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eMulti-Agent Collaboration: 多 Agent 协作模式与架构\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e一个人可以走得很快，但一群人才能走得很远。Agent 也是如此。\u003c/p\u003e\n\u003cp\u003e本文是 Agentic 系列第 11 篇。前 10 篇我们一直在讨论单个 Agent 如何更聪明——更好的记忆、更强的工具、更深的规划。这一篇，我们把视角从\u0026quot;个体智能\u0026quot;拉升到\u0026quot;集体智能\u0026quot;：当一个 Agent 不够用时，多个 Agent 如何协作？\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch2\u003e1. 为什么单 Agent 不够\u003c/h2\u003e\n\u003ch3\u003e1.1 一个类比：从独立开发者到工程团队\u003c/h3\u003e\n\u003cp\u003e想象你是一个全栈工程师，独自完成一个项目。前端、后端、数据库、DevOps、测试、文档——全部一个人扛。小项目可以，但当系统规模增长到一定程度，你会发现：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e注意力是瓶颈\u003c/strong\u003e：你不可能同时想着 CSS 布局和数据库索引优化\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e专业化有上限\u003c/strong\u003e：一个人很难同时成为安全专家、性能专家和 UX 专家\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e效率有天花板\u003c/strong\u003e：就算你是 10x 工程师，你的时间也是串行的\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e单点风险\u003c/strong\u003e：你生病了，整个项目就停了\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这就是人类发明\u0026quot;团队协作\u0026quot;的原因。Agent 面临完全相同的结构性限制。\u003c/p\u003e\n\u003ch3\u003e1.2 Single-Agent 的四个天花板\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e天花板一：Context Window 限制\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e一个 Agent 的 System Prompt 需要包含：角色定义、工具描述、输出格式约束、领域知识、示例。当你试图让一个 Agent 同时承担搜索、分析、写作、代码生成、数据可视化等多个职能时，光是工具描述就可能占据数万 token。留给实际任务执行的上下文空间被严重压缩。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e一个\u0026quot;全能\u0026quot; Agent 的 Context 分配：\n\n┌─────────────────────────────────────────────────┐\n│ System Prompt (角色 + 规则)         ~2,000 tokens │\n│ Tool Schemas (15 个工具)            ~6,000 tokens │\n│ 领域知识 (RAG 检索结果)             ~4,000 tokens │\n│ 对话历史                            ~8,000 tokens │\n│ 当前任务 + 中间状态                 ~3,000 tokens │\n├─────────────────────────────────────────────────┤\n│ 剩余可用空间                        ~9,000 tokens │ ← 越来越捉襟见肘\n│ (128K 窗口下比例更好，但工具越多问题越突出)         │\n└─────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e更关键的是，研究表明 LLM 在超长上下文中存在\u0026quot;Lost in the Middle\u0026quot;问题——中间位置的信息检索准确率显著下降。塞得越多，每条信息被有效利用的概率越低。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e天花板二：专业化限制\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e一个 System Prompt 很难让 LLM 同时扮演好多个角色。你告诉它\u0026quot;你是一个严谨的数据分析师\u0026quot;，它分析数据时很好；但同一个 prompt 里你又说\u0026quot;你也是一个有创意的文案写手\u0026quot;，这两种人格的行为模式是矛盾的。严谨和创意在同一个 prompt 中互相干扰，最终两个角色都做不好。\u003c/p\u003e\n\u003cp\u003e这不是 prompt engineering 的技巧问题，而是注意力分配的结构性问题——一个 LLM 调用只有一个 attention 分布，强调了分析的严谨性，就必然削弱了文案的创造性。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e天花板三：可靠性限制\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e单 Agent 是一个 Single Point of Failure。如果它在第 5 步推理出错（比如工具调用参数写错），整个任务链路都会受到污染。虽然我们在第 10 篇讨论了 Reflection 和自我纠错，但自我纠错的前提是\u0026quot;能发现自己错了\u0026quot;——而 LLM 对自身错误的检测能力是有限的。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e天花板四：并行度限制\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e单 Agent 的执行是串行的——一次 LLM 调用，等待结果，再进行下一次。如果一个任务可以分解为三个独立子任务（比如同时搜索三个数据源），单 Agent 只能顺序执行，浪费了大量时间。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSingle-Agent 串行执行：\n\n  Task ──→ [Search A] ──→ [Search B] ──→ [Search C] ──→ [Synthesize]\n                                                         Total: ~40s\n\nMulti-Agent 并行执行：\n\n           ┌─→ [Search A] ─┐\n  Task ──→ ├─→ [Search B] ─┼──→ [Synthesize]\n           └─→ [Search C] ─┘\n                              Total: ~15s\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e2. Multi-Agent 的四种协作模式\u003c/h2\u003e\n\u003cp\u003e当我们决定使用多个 Agent 时，第一个架构问题是：\u003cstrong\u003e它们之间的协作关系是什么？\u003c/strong\u003e 不同的关系模式适用于不同的场景，选错模式比用错框架更致命。\u003c/p\u003e\n\u003ch3\u003e2.1 模式一：Supervisor-Worker（上级分配型）\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e                    ┌──────────────────┐\n                    │    Supervisor    │\n                    │   (任务分解 +    │\n                    │    结果合成)     │\n                    └──────┬───────────┘\n                           │\n              ┌────────────┼────────────┐\n              │            │            │\n              ▼            ▼            ▼\n       ┌──────────┐ ┌──────────┐ ┌──────────┐\n       │ Worker A │ │ Worker B │ │ Worker C │\n       │ (搜索)   │ │ (分析)   │ │ (写作)   │\n       └──────────┘ └──────────┘ └──────────┘\n              │            │            │\n              └────────────┼────────────┘\n                           │\n                           ▼\n                    ┌──────────────────┐\n                    │    Supervisor    │\n                    │   (收集 + 合成   │\n                    │    最终输出)     │\n                    └──────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e工作流程\u003c/strong\u003e：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSupervisor Agent 接收用户任务\u003c/li\u003e\n\u003cli\u003eSupervisor 将任务分解为子任务，分配给不同的 Worker Agent\u003c/li\u003e\n\u003cli\u003e每个 Worker 独立执行各自的子任务\u003c/li\u003e\n\u003cli\u003eSupervisor 收集所有 Worker 的结果，合成最终输出\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003e核心特征\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e有一个明确的中央协调者\u003c/li\u003e\n\u003cli\u003eWorker 之间不直接通信，只与 Supervisor 交互\u003c/li\u003e\n\u003cli\u003eSupervisor 负责全局决策，Worker 负责局部执行\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e适用场景\u003c/strong\u003e：任务可以明确分解的场景。比如撰写一篇技术调研报告：Search Agent 负责信息搜集，Analyze Agent 负责数据分析，Write Agent 负责报告撰写。Supervisor 负责协调整个流程。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTrade-off\u003c/strong\u003e：Supervisor 是单点——如果 Supervisor 对任务的分解不合理，所有 Worker 的努力都会被浪费。此外，Supervisor 本身也是一个 LLM 调用，它对任务的理解能力决定了整个系统的上限。\u003c/p\u003e\n\u003ch3\u003e2.2 模式二：Peer-to-Peer（平等协商型）\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e       ┌──────────┐          ┌──────────┐\n       │ Agent A  │◀────────▶│ Agent B  │\n       │ (作者)   │          │ (审稿人) │\n       └────┬─────┘          └────┬─────┘\n            │                     │\n            │    ┌──────────┐     │\n            └───▶│ Agent C  │◀────┘\n                 │ (编辑)   │\n                 └──────────┘\n\n       消息流是双向的，没有固定的上下级关系\n       每个 Agent 都可以发起对话、提出意见、做出决策\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e工作流程\u003c/strong\u003e：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e多个 Agent 地位平等，通过消息传递进行协商\u003c/li\u003e\n\u003cli\u003e没有中央协调者——Agent 之间直接通信\u003c/li\u003e\n\u003cli\u003e通过多轮对话达成共识或完成任务\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003e核心特征\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e去中心化\u003c/li\u003e\n\u003cli\u003eAgent 之间直接消息传递\u003c/li\u003e\n\u003cli\u003e适合需要多视角碰撞的任务\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e适用场景\u003c/strong\u003e：辩论式分析（多个 Agent 从不同立场论证）、代码审查（Author Agent 写代码，Reviewer Agent 审查，双方来回沟通直到代码质量达标）、多角度决策（乐观分析师 + 悲观分析师 + 风险评估师共同评估一个投资决策）。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTrade-off\u003c/strong\u003e：没有中央协调意味着可能出现无限循环（两个 Agent 互相不同意，永远达不成共识）。需要额外的终止机制——最大轮次限制、外部仲裁者、投票制度等。调试也更困难，因为没有一个中心节点可以观察全局状态。\u003c/p\u003e\n\u003ch3\u003e2.3 模式三：Pipeline（流水线型）\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e  Input                                                          Output\n    │                                                              ▲\n    ▼                                                              │\n┌────────┐    ┌────────┐    ┌────────┐    ┌────────┐    ┌────────┐\n│ Draft  │───▶│ Review │───▶│  Edit  │───▶│  Fact  │───▶│ Format │\n│ Agent  │    │ Agent  │    │ Agent  │    │ Check  │    │ Agent  │\n│        │    │        │    │        │    │ Agent  │    │        │\n└────────┘    └────────┘    └────────┘    └────────┘    └────────┘\n\n  Stage 1       Stage 2       Stage 3       Stage 4       Stage 5\n  生成初稿      审查质量       修改完善      事实核查       格式化输出\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e工作流程\u003c/strong\u003e：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAgent 按顺序串联，形成流水线\u003c/li\u003e\n\u003cli\u003e上游 Agent 的输出是下游 Agent 的输入\u003c/li\u003e\n\u003cli\u003e每个 Agent 专注于一个处理阶段\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003e核心特征\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e类似 Unix 管道：\u003ccode\u003ecmd1 | cmd2 | cmd3\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e数据单向流动\u003c/li\u003e\n\u003cli\u003e每个阶段的 Agent 有明确、单一的职责\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e适用场景\u003c/strong\u003e：内容生产流水线（起草 -\u0026gt; 审查 -\u0026gt; 编辑 -\u0026gt; 排版）、数据处理管道（提取 -\u0026gt; 清洗 -\u0026gt; 转换 -\u0026gt; 加载）、多阶段审批（初审 -\u0026gt; 复审 -\u0026gt; 终审）。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTrade-off\u003c/strong\u003e：流水线是严格串行的——上游不完成，下游无法开始。如果中间某个 Agent 输出质量差，后续所有阶段都会受影响（错误传播）。但好处是架构简单、易于理解和调试、每个阶段可以独立优化。\u003c/p\u003e\n\u003ch3\u003e2.4 模式四：Dynamic Routing（动态路由型）\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e                    ┌──────────────────┐\n                    │   Router Agent   │\n                    │ (意图识别 + 路由) │\n                    └──────┬───────────┘\n                           │\n              ┌────────────┼────────────┐\n              │            │            │\n              ▼            ▼            ▼\n       ┌──────────┐ ┌──────────┐ ┌──────────┐\n       │ 技术支持  │ │ 售后服务  │ │ 销售咨询  │\n       │ Agent    │ │ Agent    │ │ Agent    │\n       │          │ │          │ │          │\n       │ 处理技术  │ │ 处理退款  │ │ 处理购买  │\n       │ 故障排查  │ │ 换货投诉  │ │ 产品推荐  │\n       └──────────┘ └──────────┘ └──────────┘\n\n  路由依据：用户输入的意图分类\n  每个专家 Agent 有独立的 System Prompt、Tools、知识库\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e工作流程\u003c/strong\u003e：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eRouter Agent 接收用户输入\u003c/li\u003e\n\u003cli\u003e根据意图分类，将请求路由到对应的专家 Agent\u003c/li\u003e\n\u003cli\u003e专家 Agent 处理请求并返回结果\u003c/li\u003e\n\u003cli\u003e必要时 Router 可以在专家之间进行二次路由\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003e核心特征\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e一个轻量级的 Router 做决策\u003c/li\u003e\n\u003cli\u003e多个重量级的专家 Agent 做执行\u003c/li\u003e\n\u003cli\u003eRouter 可以用简单模型（快速、便宜），专家用强大模型（准确、深入）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e适用场景\u003c/strong\u003e：客服系统（技术问题 -\u0026gt; 技术 Agent，退款问题 -\u0026gt; 售后 Agent）、多领域知识问答（医疗问题 -\u0026gt; 医疗 Agent，法律问题 -\u0026gt; 法律 Agent）、代码助手（Python 问题 -\u0026gt; Python 专家，Rust 问题 -\u0026gt; Rust 专家）。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTrade-off\u003c/strong\u003e：路由准确率是整个系统的瓶颈——路由错了，后面再专业也没用。模糊意图（\u0026quot;我买的东西有技术问题\u0026quot;——这是技术支持还是售后？）需要特殊处理。一种常见策略是允许 Router 在不确定时同时咨询多个专家，再综合判断。\u003c/p\u003e\n\u003ch3\u003e2.5 四种模式的对比决策\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e维度\u003c/th\u003e\n\u003cth\u003eSupervisor-Worker\u003c/th\u003e\n\u003cth\u003ePeer-to-Peer\u003c/th\u003e\n\u003cth\u003ePipeline\u003c/th\u003e\n\u003cth\u003eDynamic Routing\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e控制结构\u003c/td\u003e\n\u003ctd\u003e中心化\u003c/td\u003e\n\u003ctd\u003e去中心化\u003c/td\u003e\n\u003ctd\u003e线性\u003c/td\u003e\n\u003ctd\u003e分发型\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e通信模式\u003c/td\u003e\n\u003ctd\u003e星形\u003c/td\u003e\n\u003ctd\u003e网状\u003c/td\u003e\n\u003ctd\u003e链式\u003c/td\u003e\n\u003ctd\u003e扇出\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e并行度\u003c/td\u003e\n\u003ctd\u003e高（Worker 并行）\u003c/td\u003e\n\u003ctd\u003e中\u003c/td\u003e\n\u003ctd\u003e低（严格串行）\u003c/td\u003e\n\u003ctd\u003e高（请求级并行）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e适用复杂度\u003c/td\u003e\n\u003ctd\u003e高\u003c/td\u003e\n\u003ctd\u003e中\u003c/td\u003e\n\u003ctd\u003e中\u003c/td\u003e\n\u003ctd\u003e低-中\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e调试难度\u003c/td\u003e\n\u003ctd\u003e中\u003c/td\u003e\n\u003ctd\u003e高\u003c/td\u003e\n\u003ctd\u003e低\u003c/td\u003e\n\u003ctd\u003e低\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e典型场景\u003c/td\u003e\n\u003ctd\u003e报告生成、项目规划\u003c/td\u003e\n\u003ctd\u003e辩论、审查\u003c/td\u003e\n\u003ctd\u003e内容流水线\u003c/td\u003e\n\u003ctd\u003e客服、问答路由\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e决策原则\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e任务可以并行分解 -\u0026gt; Supervisor-Worker\u003c/li\u003e\n\u003cli\u003e需要多视角碰撞 -\u0026gt; Peer-to-Peer\u003c/li\u003e\n\u003cli\u003e处理有明确阶段 -\u0026gt; Pipeline\u003c/li\u003e\n\u003cli\u003e请求类型多样，专家各有擅长 -\u0026gt; Dynamic Routing\u003c/li\u003e\n\u003cli\u003e不确定？先从最简单的 Pipeline 开始，逐步演进\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e3. Agent 间通信机制\u003c/h2\u003e\n\u003cp\u003e多个 Agent 之间需要交换信息，通信机制的选择直接影响系统的可扩展性、耦合度和调试难度。\u003c/p\u003e\n\u003ch3\u003e3.1 共享内存（Blackboard Pattern）\u003c/h3\u003e\n\u003cp\u003e所有 Agent 读写同一个共享状态存储。这是最简单直接的通信方式。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e       ┌──────────┐   ┌──────────┐   ┌──────────┐\n       │ Agent A  │   │ Agent B  │   │ Agent C  │\n       └────┬─────┘   └────┬─────┘   └────┬─────┘\n            │  read/write   │  read/write   │\n            ▼              ▼              ▼\n       ┌──────────────────────────────────────────┐\n       │           Shared Blackboard              │\n       │                                          │\n       │  { \u0026quot;search_results\u0026quot;: [...],              │\n       │    \u0026quot;analysis\u0026quot;: {...},                    │\n       │    \u0026quot;draft\u0026quot;: \u0026quot;...\u0026quot;,                       │\n       │    \u0026quot;status\u0026quot;: {\u0026quot;search\u0026quot;: \u0026quot;done\u0026quot;, ...} }   │\n       └──────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom dataclasses import dataclass, field\nfrom typing import Any\nimport threading\n\n\n@dataclass\nclass Blackboard:\n    \u0026quot;\u0026quot;\u0026quot;共享黑板：所有 Agent 的公共状态空间\u0026quot;\u0026quot;\u0026quot;\n    _state: dict[str, Any] = field(default_factory=dict)\n    _lock: threading.Lock = field(default_factory=threading.Lock)\n    _history: list[dict] = field(default_factory=list)\n\n    def read(self, key: str) -\u0026gt; Any:\n        with self._lock:\n            return self._state.get(key)\n\n    def write(self, key: str, value: Any, author: str = \u0026quot;unknown\u0026quot;):\n        with self._lock:\n            self._history.append({\n                \u0026quot;action\u0026quot;: \u0026quot;write\u0026quot;,\n                \u0026quot;key\u0026quot;: key,\n                \u0026quot;author\u0026quot;: author,\n                \u0026quot;old_value\u0026quot;: self._state.get(key),\n                \u0026quot;new_value\u0026quot;: value,\n            })\n            self._state[key] = value\n\n    def read_all(self) -\u0026gt; dict[str, Any]:\n        with self._lock:\n            return dict(self._state)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e优点\u003c/strong\u003e：实现简单，Agent 之间完全解耦（不需要知道彼此的存在），天然支持任意读写模式。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e缺点\u003c/strong\u003e：共享状态意味着潜在的竞争条件——两个 Agent 同时写同一个 key 怎么办？需要锁机制或更复杂的冲突解决策略。随着 Agent 数量增加，Blackboard 可能成为瓶颈。\u003c/p\u003e\n\u003ch3\u003e3.2 消息传递（Message Passing）\u003c/h3\u003e\n\u003cp\u003eAgent 之间通过显式的消息进行通信。每个 Agent 有自己的收件箱。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e       ┌──────────┐         ┌──────────┐\n       │ Agent A  │──msg───▶│ Agent B  │\n       │          │◀──msg───│          │\n       └──────────┘         └──────────┘\n            │                     ▲\n            │         msg         │\n            ▼                     │\n       ┌──────────┐              │\n       │ Agent C  │──────msg─────┘\n       └──────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom dataclasses import dataclass, field\nfrom collections import defaultdict\nfrom queue import Queue\n\n\n@dataclass\nclass Message:\n    sender: str\n    receiver: str\n    content: Any\n    msg_type: str = \u0026quot;default\u0026quot;  # \u0026quot;task\u0026quot;, \u0026quot;result\u0026quot;, \u0026quot;feedback\u0026quot;, \u0026quot;error\u0026quot;\n\n\nclass MessageBus:\n    \u0026quot;\u0026quot;\u0026quot;点对点消息传递\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self):\n        self._queues: dict[str, Queue] = defaultdict(Queue)\n\n    def send(self, message: Message):\n        self._queues[message.receiver].put(message)\n\n    def receive(self, agent_id: str, timeout: float = None) -\u0026gt; Message | None:\n        try:\n            return self._queues[agent_id].get(timeout=timeout)\n        except Exception:\n            return None\n\n    def has_messages(self, agent_id: str) -\u0026gt; bool:\n        return not self._queues[agent_id].empty()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e优点\u003c/strong\u003e：通信关系显式、可追踪、可审计。每条消息都有明确的发送者和接收者。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e缺点\u003c/strong\u003e：Agent 需要知道其他 Agent 的存在（至少知道 ID），耦合度比 Blackboard 高。如果通信拓扑复杂（多对多），消息管理会变得困难。\u003c/p\u003e\n\u003ch3\u003e3.3 事件驱动（Event Bus）\u003c/h3\u003e\n\u003cp\u003eAgent 通过发布/订阅事件进行间接通信。Agent 不需要知道谁会消费它的事件。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e       ┌──────────┐   ┌──────────┐   ┌──────────┐\n       │ Agent A  │   │ Agent B  │   │ Agent C  │\n       │ pub: X   │   │ sub: X   │   │ sub: X,Y │\n       └────┬─────┘   └────┬─────┘   └────┬─────┘\n            │  publish      │  subscribe   │\n            ▼              ▼              ▼\n       ┌──────────────────────────────────────────┐\n       │              Event Bus                    │\n       │                                          │\n       │  topic \u0026quot;search_done\u0026quot;  → [Agent B, C]     │\n       │  topic \u0026quot;analysis_done\u0026quot; → [Agent C]        │\n       │  topic \u0026quot;error\u0026quot;        → [Supervisor]      │\n       └──────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom collections import defaultdict\nfrom typing import Callable\n\n\nclass EventBus:\n    \u0026quot;\u0026quot;\u0026quot;发布/订阅事件总线\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self):\n        self._subscribers: dict[str, list[Callable]] = defaultdict(list)\n        self._event_log: list[dict] = []\n\n    def subscribe(self, topic: str, handler: Callable):\n        self._subscribers[topic].append(handler)\n\n    def publish(self, topic: str, data: Any, publisher: str = \u0026quot;unknown\u0026quot;):\n        event = {\u0026quot;topic\u0026quot;: topic, \u0026quot;data\u0026quot;: data, \u0026quot;publisher\u0026quot;: publisher}\n        self._event_log.append(event)\n        for handler in self._subscribers.get(topic, []):\n            handler(event)\n\n    def get_event_log(self) -\u0026gt; list[dict]:\n        return list(self._event_log)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e优点\u003c/strong\u003e：Agent 之间完全解耦——发布者不知道有谁在监听，订阅者不知道事件从哪里来。扩展性好，新增 Agent 只需订阅相关事件。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e缺点\u003c/strong\u003e：事件流难以追踪——\u0026quot;这个事件是谁发的？谁处理了？处理结果在哪里？\u0026quot;调试时需要完整的事件日志。事件顺序可能不确定，需要额外的排序机制。\u003c/p\u003e\n\u003ch3\u003e3.4 通信机制对比\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e维度\u003c/th\u003e\n\u003cth\u003eBlackboard\u003c/th\u003e\n\u003cth\u003eMessage Passing\u003c/th\u003e\n\u003cth\u003eEvent Bus\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e耦合度\u003c/td\u003e\n\u003ctd\u003e低（通过 key 间接通信）\u003c/td\u003e\n\u003ctd\u003e中（需要知道目标 Agent）\u003c/td\u003e\n\u003ctd\u003e低（通过 topic 间接通信）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e实现复杂度\u003c/td\u003e\n\u003ctd\u003e低\u003c/td\u003e\n\u003ctd\u003e中\u003c/td\u003e\n\u003ctd\u003e中\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e调试友好度\u003c/td\u003e\n\u003ctd\u003e中（看状态快照）\u003c/td\u003e\n\u003ctd\u003e高（消息链路清晰）\u003c/td\u003e\n\u003ctd\u003e低（事件流分散）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e并发安全\u003c/td\u003e\n\u003ctd\u003e需要锁/MVCC\u003c/td\u003e\n\u003ctd\u003e天然安全（队列隔离）\u003c/td\u003e\n\u003ctd\u003e需要考虑处理顺序\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e适用模式\u003c/td\u003e\n\u003ctd\u003eSupervisor-Worker\u003c/td\u003e\n\u003ctd\u003ePeer-to-Peer\u003c/td\u003e\n\u003ctd\u003ePipeline, 事件驱动架构\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e可观测性\u003c/td\u003e\n\u003ctd\u003e状态快照\u003c/td\u003e\n\u003ctd\u003e消息轨迹\u003c/td\u003e\n\u003ctd\u003e事件日志\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e实践建议\u003c/strong\u003e：大多数 Multi-Agent 系统可以从 Blackboard 开始——它最简单，且对 Supervisor-Worker 模式特别友好。当系统复杂度增长到需要解耦 Agent 间关系时，再考虑 Event Bus。Message Passing 适合 Agent 之间有明确的、频繁的双向交互的场景。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e4. 完整实现：Supervisor-Worker 协作框架\u003c/h2\u003e\n\u003cp\u003e下面用 Python 从零实现一个 Supervisor-Worker 框架。这不依赖任何 Agent 框架，完全基于第一性原理构建。\u003c/p\u003e\n\u003ch3\u003e4.1 基础抽象\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport json\nimport asyncio\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom typing import Any\n\n\n# ---- LLM 调用抽象（与具体 SDK 解耦）----\n\nasync def call_llm(\n    messages: list[dict],\n    model: str = \u0026quot;gpt-4o\u0026quot;,\n    response_format: dict | None = None,\n) -\u0026gt; str:\n    \u0026quot;\u0026quot;\u0026quot;LLM 调用的统一接口（简化版，生产中替换为真实 SDK 调用）\u0026quot;\u0026quot;\u0026quot;\n    import openai\n    client = openai.AsyncOpenAI()\n    kwargs = {\u0026quot;model\u0026quot;: model, \u0026quot;messages\u0026quot;: messages}\n    if response_format:\n        kwargs[\u0026quot;response_format\u0026quot;] = response_format\n    response = await client.chat.completions.create(**kwargs)\n    return response.choices[0].message.content\n\n\n# ---- 任务与结果的数据结构 ----\n\n@dataclass\nclass Task:\n    \u0026quot;\u0026quot;\u0026quot;一个可执行的子任务\u0026quot;\u0026quot;\u0026quot;\n    task_id: str\n    description: str\n    assigned_to: str = \u0026quot;\u0026quot;          # Worker Agent 名称\n    context: dict = field(default_factory=dict)  # 来自上游的上下文\n    status: str = \u0026quot;pending\u0026quot;        # pending | running | done | failed\n    result: str = \u0026quot;\u0026quot;\n    error: str = \u0026quot;\u0026quot;\n\n\n@dataclass\nclass TeamResult:\n    \u0026quot;\u0026quot;\u0026quot;团队执行的最终结果\u0026quot;\u0026quot;\u0026quot;\n    success: bool\n    output: str\n    tasks: list[Task]\n    total_tokens: int = 0\n    total_llm_calls: int = 0\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e4.2 Worker Agent\u003c/h3\u003e\n\u003cp\u003e每个 Worker 是一个专注于特定领域的 Agent，拥有独立的 System Prompt 和能力边界。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass WorkerAgent:\n    \u0026quot;\u0026quot;\u0026quot;Worker Agent：接收子任务，独立执行，返回结果\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, name: str, system_prompt: str, model: str = \u0026quot;gpt-4o\u0026quot;):\n        self.name = name\n        self.system_prompt = system_prompt\n        self.model = model\n        self._call_count = 0\n\n    async def execute(self, task: Task) -\u0026gt; Task:\n        \u0026quot;\u0026quot;\u0026quot;执行一个子任务\u0026quot;\u0026quot;\u0026quot;\n        task.status = \u0026quot;running\u0026quot;\n        task.assigned_to = self.name\n\n        messages = [\n            {\u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;, \u0026quot;content\u0026quot;: self.system_prompt},\n            {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: self._build_prompt(task)},\n        ]\n\n        try:\n            result = await call_llm(messages, model=self.model)\n            self._call_count += 1\n            task.result = result\n            task.status = \u0026quot;done\u0026quot;\n        except Exception as e:\n            task.error = str(e)\n            task.status = \u0026quot;failed\u0026quot;\n\n        return task\n\n    def _build_prompt(self, task: Task) -\u0026gt; str:\n        prompt = f\u0026quot;## 任务\\n{task.description}\\n\u0026quot;\n        if task.context:\n            prompt += f\u0026quot;\\n## 上下文信息\\n{json.dumps(task.context, ensure_ascii=False, indent=2)}\\n\u0026quot;\n        prompt += \u0026quot;\\n请完成上述任务，直接输出结果。\u0026quot;\n        return prompt\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e4.3 Supervisor Agent\u003c/h3\u003e\n\u003cp\u003eSupervisor 负责三件事：任务分解、任务分配、结果合成。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eDECOMPOSE_PROMPT = \u0026quot;\u0026quot;\u0026quot;你是一个任务分解专家。给定一个复杂任务，将其分解为可以独立执行的子任务。\n\n可用的 Worker 及其能力：\n{workers_description}\n\n请将任务分解为子任务，并指定每个子任务应该分配给哪个 Worker。\n输出 JSON 格式：\n{{\n  \u0026quot;subtasks\u0026quot;: [\n    {{\n      \u0026quot;task_id\u0026quot;: \u0026quot;task_1\u0026quot;,\n      \u0026quot;description\u0026quot;: \u0026quot;具体的子任务描述\u0026quot;,\n      \u0026quot;assigned_to\u0026quot;: \u0026quot;worker 名称\u0026quot;,\n      \u0026quot;depends_on\u0026quot;: []\n    }}\n  ]\n}}\n\n注意：\n- 每个子任务应该足够具体，让 Worker 能独立完成\n- depends_on 标明依赖关系（某个子任务需要等另一个完成后才能开始）\n- 尽可能让子任务并行执行以提高效率\n\u0026quot;\u0026quot;\u0026quot;\n\nSYNTHESIZE_PROMPT = \u0026quot;\u0026quot;\u0026quot;你是一个结果合成专家。多个专业 Agent 已经分别完成了子任务。\n请根据它们的结果，合成一个完整、连贯、高质量的最终输出。\n\n原始任务：{original_task}\n\n各子任务的执行结果：\n{subtask_results}\n\n请整合以上信息，生成最终的完整输出。确保：\n1. 信息完整，没有遗漏\n2. 逻辑连贯，前后一致\n3. 去除重复内容\n4. 保持专业质量\n\u0026quot;\u0026quot;\u0026quot;\n\n\nclass SupervisorAgent:\n    \u0026quot;\u0026quot;\u0026quot;Supervisor Agent：任务分解、分配、合成\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, model: str = \u0026quot;gpt-4o\u0026quot;):\n        self.model = model\n        self._call_count = 0\n\n    async def decompose(\n        self, task: str, workers: dict[str, WorkerAgent]\n    ) -\u0026gt; list[Task]:\n        \u0026quot;\u0026quot;\u0026quot;将复杂任务分解为子任务\u0026quot;\u0026quot;\u0026quot;\n        workers_desc = \u0026quot;\\n\u0026quot;.join(\n            f\u0026quot;- {name}: {w.system_prompt[:200]}\u0026quot;\n            for name, w in workers.items()\n        )\n\n        messages = [\n            {\n                \u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;,\n                \u0026quot;content\u0026quot;: DECOMPOSE_PROMPT.format(\n                    workers_description=workers_desc\n                ),\n            },\n            {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: task},\n        ]\n\n        result = await call_llm(\n            messages,\n            model=self.model,\n            response_format={\u0026quot;type\u0026quot;: \u0026quot;json_object\u0026quot;},\n        )\n        self._call_count += 1\n\n        parsed = json.loads(result)\n        tasks = []\n        for st in parsed.get(\u0026quot;subtasks\u0026quot;, []):\n            tasks.append(Task(\n                task_id=st[\u0026quot;task_id\u0026quot;],\n                description=st[\u0026quot;description\u0026quot;],\n                assigned_to=st.get(\u0026quot;assigned_to\u0026quot;, \u0026quot;\u0026quot;),\n            ))\n        return tasks\n\n    async def synthesize(\n        self, original_task: str, completed_tasks: list[Task]\n    ) -\u0026gt; str:\n        \u0026quot;\u0026quot;\u0026quot;合成所有 Worker 的结果\u0026quot;\u0026quot;\u0026quot;\n        results_text = \u0026quot;\\n\\n\u0026quot;.join(\n            f\u0026quot;### {t.task_id} ({t.assigned_to})\\n{t.result}\u0026quot;\n            for t in completed_tasks\n            if t.status == \u0026quot;done\u0026quot;\n        )\n\n        messages = [\n            {\n                \u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;,\n                \u0026quot;content\u0026quot;: SYNTHESIZE_PROMPT.format(\n                    original_task=original_task,\n                    subtask_results=results_text,\n                ),\n            },\n            {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;请合成最终结果。\u0026quot;},\n        ]\n\n        result = await call_llm(messages, model=self.model)\n        self._call_count += 1\n        return result\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e4.4 AgentTeam：编排层\u003c/h3\u003e\n\u003cp\u003eAgentTeam 管理多个 Agent 的生命周期、通信和执行流程。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass AgentTeam:\n    \u0026quot;\u0026quot;\u0026quot;Agent 团队：管理 Supervisor + Workers 的协作\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, supervisor: SupervisorAgent):\n        self.supervisor = supervisor\n        self.workers: dict[str, WorkerAgent] = {}\n        self.blackboard = Blackboard()\n        self.execution_log: list[dict] = []\n\n    def add_worker(self, worker: WorkerAgent):\n        self.workers[worker.name] = worker\n\n    async def run(self, task: str, max_retries: int = 2) -\u0026gt; TeamResult:\n        \u0026quot;\u0026quot;\u0026quot;执行完整的 Multi-Agent 协作流程\u0026quot;\u0026quot;\u0026quot;\n        self._log(\u0026quot;team\u0026quot;, f\u0026quot;接收任务: {task[:100]}...\u0026quot;)\n\n        # Phase 1: Supervisor 分解任务\n        self._log(\u0026quot;supervisor\u0026quot;, \u0026quot;开始任务分解\u0026quot;)\n        subtasks = await self.supervisor.decompose(task, self.workers)\n        self._log(\u0026quot;supervisor\u0026quot;, f\u0026quot;分解为 {len(subtasks)} 个子任务\u0026quot;)\n\n        for st in subtasks:\n            self._log(\u0026quot;supervisor\u0026quot;, f\u0026quot;  {st.task_id} -\u0026gt; {st.assigned_to}: {st.description[:80]}\u0026quot;)\n\n        # Phase 2: Workers 并行执行（考虑依赖关系）\n        completed = await self._execute_tasks(subtasks, max_retries)\n\n        # Phase 3: Supervisor 合成结果\n        self._log(\u0026quot;supervisor\u0026quot;, \u0026quot;开始合成结果\u0026quot;)\n        final_output = await self.supervisor.synthesize(task, completed)\n        self._log(\u0026quot;supervisor\u0026quot;, \u0026quot;合成完成\u0026quot;)\n\n        # 汇总统计\n        total_calls = self.supervisor._call_count + sum(\n            w._call_count for w in self.workers.values()\n        )\n\n        return TeamResult(\n            success=all(t.status == \u0026quot;done\u0026quot; for t in completed),\n            output=final_output,\n            tasks=completed,\n            total_llm_calls=total_calls,\n        )\n\n    async def _execute_tasks(\n        self, tasks: list[Task], max_retries: int\n    ) -\u0026gt; list[Task]:\n        \u0026quot;\u0026quot;\u0026quot;执行子任务，支持并行和重试\u0026quot;\u0026quot;\u0026quot;\n        completed = []\n        pending = list(tasks)\n\n        while pending:\n            # 找出当前可以执行的任务（依赖已满足）\n            ready = []\n            still_pending = []\n            completed_ids = {t.task_id for t in completed}\n\n            for task in pending:\n                deps = task.context.get(\u0026quot;depends_on\u0026quot;, [])\n                if all(d in completed_ids for d in deps):\n                    ready.append(task)\n                else:\n                    still_pending.append(task)\n\n            if not ready:\n                # 没有可执行的任务但还有待处理的 -\u0026gt; 可能存在循环依赖\n                self._log(\u0026quot;team\u0026quot;, \u0026quot;警告: 检测到无法满足的依赖关系\u0026quot;)\n                break\n\n            # 并行执行所有就绪的任务\n            results = await asyncio.gather(*[\n                self._execute_single(task, max_retries)\n                for task in ready\n            ])\n\n            for task in results:\n                completed.append(task)\n                # 将结果写入 Blackboard，供后续任务使用\n                if task.status == \u0026quot;done\u0026quot;:\n                    self.blackboard.write(\n                        task.task_id, task.result, author=task.assigned_to\n                    )\n\n            pending = still_pending\n\n        return completed\n\n    async def _execute_single(\n        self, task: Task, max_retries: int\n    ) -\u0026gt; Task:\n        \u0026quot;\u0026quot;\u0026quot;执行单个任务，带重试\u0026quot;\u0026quot;\u0026quot;\n        worker = self.workers.get(task.assigned_to)\n        if not worker:\n            task.status = \u0026quot;failed\u0026quot;\n            task.error = f\u0026quot;未找到 Worker: {task.assigned_to}\u0026quot;\n            return task\n\n        # 将 Blackboard 上的相关信息注入任务上下文\n        task.context[\u0026quot;blackboard\u0026quot;] = self.blackboard.read_all()\n\n        for attempt in range(max_retries + 1):\n            self._log(worker.name, f\u0026quot;执行 {task.task_id} (尝试 {attempt + 1})\u0026quot;)\n            result = await worker.execute(task)\n\n            if result.status == \u0026quot;done\u0026quot;:\n                self._log(worker.name, f\u0026quot;{task.task_id} 完成\u0026quot;)\n                return result\n\n            self._log(worker.name, f\u0026quot;{task.task_id} 失败: {result.error}\u0026quot;)\n\n            if attempt \u0026lt; max_retries:\n                self._log(worker.name, f\u0026quot;准备重试 {task.task_id}\u0026quot;)\n\n        return result\n\n    def _log(self, source: str, message: str):\n        entry = {\u0026quot;source\u0026quot;: source, \u0026quot;message\u0026quot;: message}\n        self.execution_log.append(entry)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e4.5 组装示例：技术调研报告\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def main():\n    \u0026quot;\u0026quot;\u0026quot;示例：用 Multi-Agent 团队撰写一篇技术调研报告\u0026quot;\u0026quot;\u0026quot;\n\n    # 创建 Supervisor\n    supervisor = SupervisorAgent(model=\u0026quot;gpt-4o\u0026quot;)\n\n    # 创建专业化的 Worker Agent\n    search_agent = WorkerAgent(\n        name=\u0026quot;searcher\u0026quot;,\n        system_prompt=(\n            \u0026quot;你是一个信息搜索专家。你的任务是根据给定的主题，\u0026quot;\n            \u0026quot;整理出全面的信息摘要，包括关键事实、数据、案例。\u0026quot;\n            \u0026quot;输出结构化的搜索结果，标注来源和可信度。\u0026quot;\n        ),\n    )\n\n    analyze_agent = WorkerAgent(\n        name=\u0026quot;analyst\u0026quot;,\n        system_prompt=(\n            \u0026quot;你是一个技术分析专家。你的任务是根据搜索结果和原始数据，\u0026quot;\n            \u0026quot;进行深度分析，提炼洞察，识别趋势、风险和机会。\u0026quot;\n            \u0026quot;输出包含数据支撑的分析报告。\u0026quot;\n        ),\n    )\n\n    write_agent = WorkerAgent(\n        name=\u0026quot;writer\u0026quot;,\n        system_prompt=(\n            \u0026quot;你是一个技术写作专家。你的任务是根据分析结果，\u0026quot;\n            \u0026quot;撰写结构清晰、逻辑严谨、可读性强的技术报告。\u0026quot;\n            \u0026quot;确保使用专业术语，并配有合适的章节结构。\u0026quot;\n        ),\n    )\n\n    # 组建团队\n    team = AgentTeam(supervisor=supervisor)\n    team.add_worker(search_agent)\n    team.add_worker(analyze_agent)\n    team.add_worker(write_agent)\n\n    # 执行任务\n    result = await team.run(\n        \u0026quot;撰写一篇关于 LLM Agent 在企业客服场景落地的技术调研报告，\u0026quot;\n        \u0026quot;包括行业现状、主流技术方案对比、落地挑战和建议。\u0026quot;\n    )\n\n    print(f\u0026quot;成功: {result.success}\u0026quot;)\n    print(f\u0026quot;LLM 调用次数: {result.total_llm_calls}\u0026quot;)\n    print(f\u0026quot;\\n最终输出:\\n{result.output[:500]}...\u0026quot;)\n\n    # 查看执行日志\n    print(\u0026quot;\\n执行链路:\u0026quot;)\n    for entry in team.execution_log:\n        print(f\u0026quot;  [{entry[\u0026#39;source\u0026#39;]}] {entry[\u0026#39;message\u0026#39;]}\u0026quot;)\n\n\n# asyncio.run(main())\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这段代码展示了核心的协作模式。生产系统中还需要补充：Token 用量追踪、超时控制、Worker 健康检查、结果缓存等。但架构骨架已经清晰——Supervisor 负责全局调度，Worker 负责局部执行，Blackboard 负责状态共享，AgentTeam 负责生命周期管理。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e5. 状态管理的复杂性\u003c/h2\u003e\n\u003cp\u003eMulti-Agent 系统的状态管理比 Single-Agent 复杂一个数量级。核心难题在于：多个 Agent 同时操作状态，如何保证一致性。\u003c/p\u003e\n\u003ch3\u003e5.1 共享状态 vs 独立状态\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e方案 A：共享状态                     方案 B：独立状态\n┌─────────────────┐                ┌──────────┐  ┌──────────┐  ┌──────────┐\n│  Global State   │                │ State A  │  │ State B  │  │ State C  │\n│                 │                │ (Agent A │  │ (Agent B │  │ (Agent C │\n│ Agent A ──write │                │  独占)   │  │  独占)   │  │  独占)   │\n│ Agent B ──write │                └──────────┘  └──────────┘  └──────────┘\n│ Agent C ──write │                      │              │              │\n└─────────────────┘                      └──────────────┼──────────────┘\n                                                        ▼\n                                                  合并/同步层\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e共享状态\u003c/strong\u003e的优点是 Agent 之间信息同步即时，任何 Agent 都能看到最新全局状态。缺点是需要处理并发冲突。适合 Supervisor-Worker 模式——Supervisor 需要看到所有 Worker 的进度。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e独立状态\u003c/strong\u003e的优点是无并发问题，每个 Agent 完全自主。缺点是 Agent 之间信息同步有延迟，需要显式的合并机制。适合 Pipeline 模式——每个阶段独立处理，只在交接时传递状态。\u003c/p\u003e\n\u003ch3\u003e5.2 冲突解决策略\u003c/h3\u003e\n\u003cp\u003e当两个 Agent 同时修改同一个状态时，需要冲突解决。常见策略：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass ConflictResolver:\n    \u0026quot;\u0026quot;\u0026quot;状态冲突解决器\u0026quot;\u0026quot;\u0026quot;\n\n    @staticmethod\n    def last_writer_wins(old_value, new_value_a, new_value_b, timestamp_a, timestamp_b):\n        \u0026quot;\u0026quot;\u0026quot;最后写入者胜出——简单但可能丢失数据\u0026quot;\u0026quot;\u0026quot;\n        return new_value_a if timestamp_a \u0026gt; timestamp_b else new_value_b\n\n    @staticmethod\n    def merge_append(old_value, new_value_a, new_value_b):\n        \u0026quot;\u0026quot;\u0026quot;合并追加——适用于列表类型的状态\u0026quot;\u0026quot;\u0026quot;\n        if isinstance(old_value, list):\n            merged = list(old_value)\n            if isinstance(new_value_a, list):\n                merged.extend(new_value_a)\n            if isinstance(new_value_b, list):\n                merged.extend(new_value_b)\n            return merged\n        return new_value_b  # fallback\n\n    @staticmethod\n    async def llm_resolve(old_value, new_value_a, new_value_b, context: str):\n        \u0026quot;\u0026quot;\u0026quot;用 LLM 判断如何合并冲突——最灵活但最贵\u0026quot;\u0026quot;\u0026quot;\n        prompt = (\n            f\u0026quot;两个 Agent 同时修改了同一个状态。\\n\u0026quot;\n            f\u0026quot;原始值: {old_value}\\n\u0026quot;\n            f\u0026quot;Agent A 的修改: {new_value_a}\\n\u0026quot;\n            f\u0026quot;Agent B 的修改: {new_value_b}\\n\u0026quot;\n            f\u0026quot;上下文: {context}\\n\u0026quot;\n            f\u0026quot;请决定最终值应该是什么，并解释原因。\u0026quot;\n        )\n        return await call_llm([{\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: prompt}])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e实践中，大多数 Multi-Agent 系统通过架构设计来避免冲突，而不是在运行时解决冲突。最有效的方法是\u003cstrong\u003e状态分区\u003c/strong\u003e——每个 Agent 只写自己负责的状态区域，避免多 Agent 写同一个 key。这也是 Supervisor-Worker 模式天然的优势：每个 Worker 写自己的结果 key，只有 Supervisor 读所有 key。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e6. 错误处理与容错\u003c/h2\u003e\n\u003cp\u003eMulti-Agent 系统的错误处理比 Single-Agent 更复杂，因为错误的传播路径更多。\u003c/p\u003e\n\u003ch3\u003e6.1 Worker 失败\u003c/h3\u003e\n\u003cp\u003eWorker 失败是最常见的情况。处理策略按优先级：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eWorker 失败处理决策树：\n\n  Worker 执行失败\n       │\n       ▼\n  ┌─ 是否可重试？ ─── 是 ──→ 重试（最多 N 次）──→ 成功？──→ 继续\n  │      │                                          │\n  │     否                                         否\n  │      │                                          │\n  │      ▼                                          ▼\n  │  ┌─ 有替代 Worker？ ─── 是 ──→ 分配给替代 Worker\n  │  │      │\n  │  │     否\n  │  │      │\n  │  │      ▼\n  │  │  ┌─ 该子任务是关键路径？\n  │  │  │      │            │\n  │  │  │     是           否\n  │  │  │      │            │\n  │  │  │      ▼            ▼\n  │  │  │  整体任务失败   降级处理（跳过该子任务，\n  │  │  │                 标记结果为不完整）\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass ResilientAgentTeam(AgentTeam):\n    \u0026quot;\u0026quot;\u0026quot;增强容错能力的 Agent 团队\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, supervisor: SupervisorAgent):\n        super().__init__(supervisor)\n        self.fallback_workers: dict[str, list[str]] = {}  # Worker 降级链\n\n    def set_fallback(self, worker_name: str, fallbacks: list[str]):\n        \u0026quot;\u0026quot;\u0026quot;设置 Worker 的降级替代链\u0026quot;\u0026quot;\u0026quot;\n        self.fallback_workers[worker_name] = fallbacks\n\n    async def _execute_single(self, task: Task, max_retries: int) -\u0026gt; Task:\n        \u0026quot;\u0026quot;\u0026quot;增强版：支持 Worker 降级\u0026quot;\u0026quot;\u0026quot;\n        # 尝试主 Worker\n        result = await super()._execute_single(task, max_retries)\n        if result.status == \u0026quot;done\u0026quot;:\n            return result\n\n        # 主 Worker 失败，尝试降级 Worker\n        fallbacks = self.fallback_workers.get(task.assigned_to, [])\n        for fb_name in fallbacks:\n            self._log(\u0026quot;team\u0026quot;, f\u0026quot;降级: {task.assigned_to} -\u0026gt; {fb_name}\u0026quot;)\n            task.assigned_to = fb_name\n            task.status = \u0026quot;pending\u0026quot;\n            task.error = \u0026quot;\u0026quot;\n            result = await super()._execute_single(task, max_retries=1)\n            if result.status == \u0026quot;done\u0026quot;:\n                return result\n\n        return result\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e6.2 Supervisor 失败\u003c/h3\u003e\n\u003cp\u003eSupervisor 失败更严重——它是中央协调者，失败意味着整个任务无法继续。处理策略：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e外部监控\u003c/strong\u003e：在 AgentTeam 之上设置一个非 LLM 的监控层，检测 Supervisor 的健康状态\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervisor 冗余\u003c/strong\u003e：准备一个备用 Supervisor（可以用不同的模型），主 Supervisor 失败时切换\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCheckpoint 机制\u003c/strong\u003e：Supervisor 在每个决策点保存状态快照，失败后从最近的 Checkpoint 恢复\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def run_with_checkpoint(self, task: str) -\u0026gt; TeamResult:\n    \u0026quot;\u0026quot;\u0026quot;带 Checkpoint 的执行流程\u0026quot;\u0026quot;\u0026quot;\n    checkpoint = {\u0026quot;phase\u0026quot;: \u0026quot;init\u0026quot;, \u0026quot;subtasks\u0026quot;: [], \u0026quot;completed\u0026quot;: []}\n\n    try:\n        # Phase 1: 分解\n        checkpoint[\u0026quot;phase\u0026quot;] = \u0026quot;decompose\u0026quot;\n        subtasks = await self.supervisor.decompose(task, self.workers)\n        checkpoint[\u0026quot;subtasks\u0026quot;] = subtasks\n\n        # Phase 2: 执行\n        checkpoint[\u0026quot;phase\u0026quot;] = \u0026quot;execute\u0026quot;\n        completed = await self._execute_tasks(subtasks, max_retries=2)\n        checkpoint[\u0026quot;completed\u0026quot;] = completed\n\n        # Phase 3: 合成\n        checkpoint[\u0026quot;phase\u0026quot;] = \u0026quot;synthesize\u0026quot;\n        output = await self.supervisor.synthesize(task, completed)\n\n        return TeamResult(success=True, output=output, tasks=completed)\n\n    except Exception as e:\n        self._log(\u0026quot;team\u0026quot;, f\u0026quot;失败于阶段 {checkpoint[\u0026#39;phase\u0026#39;]}: {e}\u0026quot;)\n        # 可以从 checkpoint 恢复，跳过已完成的阶段\n        return TeamResult(\n            success=False,\n            output=f\u0026quot;任务在 {checkpoint[\u0026#39;phase\u0026#39;]} 阶段失败: {e}\u0026quot;,\n            tasks=checkpoint.get(\u0026quot;completed\u0026quot;, []),\n        )\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e6.3 死锁检测\u003c/h3\u003e\n\u003cp\u003e在 Peer-to-Peer 模式中，两个 Agent 可能互相等待对方的回复，形成死锁。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e死锁场景：\n\n  Agent A: \u0026quot;请 Agent B 先确认方案\u0026quot;\n           ↓ 等待 B\n  Agent B: \u0026quot;请 Agent A 先提供数据\u0026quot;\n           ↓ 等待 A\n  → 无限等待\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e解决方案：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass DeadlockDetector:\n    \u0026quot;\u0026quot;\u0026quot;简单的死锁检测器\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, timeout_seconds: float = 60):\n        self.timeout = timeout_seconds\n        self._waiting: dict[str, str] = {}  # agent_id -\u0026gt; waiting_for_agent_id\n\n    def register_wait(self, agent_id: str, waiting_for: str):\n        self._waiting[agent_id] = waiting_for\n        # 检测环形等待\n        if self._has_cycle(agent_id):\n            raise DeadlockError(\n                f\u0026quot;检测到死锁: {self._trace_cycle(agent_id)}\u0026quot;\n            )\n\n    def _has_cycle(self, start: str) -\u0026gt; bool:\n        visited = set()\n        current = start\n        while current in self._waiting:\n            if current in visited:\n                return True\n            visited.add(current)\n            current = self._waiting[current]\n        return False\n\n    def _trace_cycle(self, start: str) -\u0026gt; str:\n        chain = [start]\n        current = self._waiting.get(start, \u0026quot;\u0026quot;)\n        while current != start and current:\n            chain.append(current)\n            current = self._waiting.get(current, \u0026quot;\u0026quot;)\n        chain.append(start)\n        return \u0026quot; -\u0026gt; \u0026quot;.join(chain)\n\n\nclass DeadlockError(Exception):\n    pass\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e7. Multi-Agent 的成本问题\u003c/h2\u003e\n\u003cp\u003e成本是 Multi-Agent 系统必须正视的问题。它不只是\u0026quot;贵一点\u0026quot;的问题——可能是\u0026quot;贵一个数量级\u0026quot;的问题。\u003c/p\u003e\n\u003ch3\u003e7.1 成本模型\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eSingle-Agent 执行一个任务的 Token 消耗：\n\n  1 x System Prompt   +  N x (Context + Response)\n  ~1,000 tokens          ~3,000 tokens x 5 iterations\n                         = ~16,000 tokens\n\n\nMulti-Agent (Supervisor + 3 Workers) 的 Token 消耗：\n\n  Supervisor 分解:   ~4,000 tokens   (System Prompt + 任务分解)\n  Worker A 执行:     ~8,000 tokens   (System Prompt + 执行)\n  Worker B 执行:     ~8,000 tokens   (System Prompt + 执行)\n  Worker C 执行:     ~8,000 tokens   (System Prompt + 执行)\n  Supervisor 合成:   ~6,000 tokens   (收集所有结果 + 合成)\n                     ──────────────\n  Total:             ~34,000 tokens   ← 约 2x Single-Agent\n\n  如果 Worker 内部也有多轮迭代，消耗会更高。\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e7.2 什么时候 Multi-Agent 的收益大于成本\u003c/h3\u003e\n\u003cp\u003e不是所有场景都值得用 Multi-Agent。一个简单的决策框架：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e                        任务复杂度\n                    低 ─────────── 高\n                    │               │\n  专业化需求  低    │  Single-Agent │  Single-Agent\n              │    │  (够用)       │  + Better Prompt\n              │    │               │\n              高    │  Single-Agent │  Multi-Agent ✓\n                    │  + Tools      │  (值得投入)\n                    │               │\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMulti-Agent 在以下条件下收益最大：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e任务天然可并行\u003c/strong\u003e：子任务之间独立性高，Multi-Agent 通过并行执行缩短总耗时，即使 token 消耗增加，时间成本下降\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e专业化收益显著\u003c/strong\u003e：专家 Agent 在自己的领域比通用 Agent 的输出质量显著更高，质量提升值得额外成本\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSingle-Agent 已经到达能力瓶颈\u003c/strong\u003e：Context Window 不够、单个 prompt 角色冲突、输出质量不稳定\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e任务的商业价值足够高\u003c/strong\u003e：生成一份价值数万元的分析报告，多花几美元的 API 费用是可以接受的\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e7.3 成本优化策略\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass CostAwareTeam(AgentTeam):\n    \u0026quot;\u0026quot;\u0026quot;成本感知的 Agent 团队\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, supervisor, token_budget: int = 100_000):\n        super().__init__(supervisor)\n        self.token_budget = token_budget\n        self.token_used = 0\n\n    def _select_model_for_task(self, task: Task) -\u0026gt; str:\n        \u0026quot;\u0026quot;\u0026quot;根据任务复杂度选择模型——不是所有子任务都需要最强模型\u0026quot;\u0026quot;\u0026quot;\n        if task.context.get(\u0026quot;complexity\u0026quot;) == \u0026quot;low\u0026quot;:\n            return \u0026quot;gpt-4o-mini\u0026quot;     # 简单任务用小模型\n        elif task.context.get(\u0026quot;complexity\u0026quot;) == \u0026quot;high\u0026quot;:\n            return \u0026quot;gpt-4o\u0026quot;          # 复杂任务用大模型\n        else:\n            return \u0026quot;gpt-4o-mini\u0026quot;     # 默认用小模型，够用即可\n\n    def _should_continue(self) -\u0026gt; bool:\n        \u0026quot;\u0026quot;\u0026quot;预算检查\u0026quot;\u0026quot;\u0026quot;\n        if self.token_used \u0026gt;= self.token_budget:\n            self._log(\u0026quot;team\u0026quot;, f\u0026quot;Token 预算耗尽 ({self.token_used}/{self.token_budget})\u0026quot;)\n            return False\n        return True\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e关键原则：\u003cstrong\u003eRouter 和 Supervisor 可以用轻量模型，只有需要深度推理的 Worker 才用重量级模型。\u003c/strong\u003e 这类似人类组织中，项目经理不需要是技术最强的人，但专家必须在各自领域足够专业。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e8. Multi-Agent 的调试挑战\u003c/h2\u003e\n\u003cp\u003eMulti-Agent 系统的调试难度是 Single-Agent 的平方级增长——不仅每个 Agent 内部可能出错，Agent 之间的交互也可能出错。\u003c/p\u003e\n\u003ch3\u003e8.1 执行链路追踪\u003c/h3\u003e\n\u003cp\u003e每次 Multi-Agent 执行都应该生成一个完整的 Trace，记录每个 Agent 的每次 LLM 调用、输入、输出和耗时。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport time\nimport uuid\nfrom dataclasses import dataclass, field\n\n\n@dataclass\nclass Span:\n    \u0026quot;\u0026quot;\u0026quot;一个执行跨度（对应一次 Agent 操作）\u0026quot;\u0026quot;\u0026quot;\n    span_id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])\n    parent_id: str = \u0026quot;\u0026quot;\n    agent_name: str = \u0026quot;\u0026quot;\n    operation: str = \u0026quot;\u0026quot;          # \u0026quot;decompose\u0026quot;, \u0026quot;execute\u0026quot;, \u0026quot;synthesize\u0026quot;\n    input_summary: str = \u0026quot;\u0026quot;\n    output_summary: str = \u0026quot;\u0026quot;\n    start_time: float = 0.0\n    end_time: float = 0.0\n    token_count: int = 0\n    status: str = \u0026quot;running\u0026quot;      # running | done | failed\n    children: list = field(default_factory=list)\n\n    @property\n    def duration_ms(self) -\u0026gt; float:\n        return (self.end_time - self.start_time) * 1000\n\n\nclass Tracer:\n    \u0026quot;\u0026quot;\u0026quot;Multi-Agent 执行链路追踪器\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self):\n        self.root_span: Span | None = None\n        self._span_stack: list[Span] = []\n\n    def start_span(self, agent_name: str, operation: str, input_summary: str = \u0026quot;\u0026quot;) -\u0026gt; Span:\n        span = Span(\n            agent_name=agent_name,\n            operation=operation,\n            input_summary=input_summary[:200],\n            start_time=time.time(),\n        )\n        if self._span_stack:\n            parent = self._span_stack[-1]\n            span.parent_id = parent.span_id\n            parent.children.append(span)\n        else:\n            self.root_span = span\n\n        self._span_stack.append(span)\n        return span\n\n    def end_span(self, output_summary: str = \u0026quot;\u0026quot;, status: str = \u0026quot;done\u0026quot;):\n        if self._span_stack:\n            span = self._span_stack.pop()\n            span.end_time = time.time()\n            span.output_summary = output_summary[:200]\n            span.status = status\n\n    def print_trace(self, span: Span = None, indent: int = 0):\n        \u0026quot;\u0026quot;\u0026quot;打印可视化的执行链路\u0026quot;\u0026quot;\u0026quot;\n        span = span or self.root_span\n        if not span:\n            return\n\n        prefix = \u0026quot;  \u0026quot; * indent\n        status_icon = \u0026quot;OK\u0026quot; if span.status == \u0026quot;done\u0026quot; else \u0026quot;FAIL\u0026quot;\n        print(\n            f\u0026quot;{prefix}[{status_icon}] {span.agent_name}.{span.operation} \u0026quot;\n            f\u0026quot;({span.duration_ms:.0f}ms)\u0026quot;\n        )\n        if span.input_summary:\n            print(f\u0026quot;{prefix}  IN:  {span.input_summary[:80]}\u0026quot;)\n        if span.output_summary:\n            print(f\u0026quot;{prefix}  OUT: {span.output_summary[:80]}\u0026quot;)\n\n        for child in span.children:\n            self.print_trace(child, indent + 1)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e输出示例：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[OK] supervisor.decompose (2340ms)\n  IN:  撰写一篇关于 LLM Agent 在企业客服场景落地的技术调研报告...\n  OUT: {\u0026quot;subtasks\u0026quot;: [{\u0026quot;task_id\u0026quot;: \u0026quot;task_1\u0026quot;, ...}, ...]}\n  [OK] searcher.execute (5120ms)\n    IN:  搜索 LLM Agent 客服场景的行业现状和主流方案...\n    OUT: ## 行业现状\\n1. 2024 年全球智能客服市场规模...\n  [OK] analyst.execute (4800ms)\n    IN:  分析搜索结果，提炼关键洞察和趋势...\n    OUT: ## 分析结论\\n1. 技术成熟度：LLM 客服处于...\n  [OK] writer.execute (6200ms)\n    IN:  根据分析结果撰写完整的技术调研报告...\n    OUT: # LLM Agent 企业客服落地技术调研报告\\n\\n## 1. 执行摘要...\n[OK] supervisor.synthesize (3100ms)\n  IN:  请合成最终结果。\n  OUT: # LLM Agent 企业客服落地技术调研报告（终稿）...\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e8.2 Bug 复现\u003c/h3\u003e\n\u003cp\u003eMulti-Agent 场景的 bug 复现特别困难，因为：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLLM 输出是非确定性的——相同输入可能产生不同输出\u003c/li\u003e\n\u003cli\u003eAgent 之间的交互是动态的——执行路径取决于中间结果\u003c/li\u003e\n\u003cli\u003e并发执行的时序不确定——Worker A 和 B 谁先完成可能影响最终结果\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e应对策略：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e记录完整的 LLM 输入/输出\u003c/strong\u003e：在 Trace 中保存每次 LLM 调用的完整 messages 和 response，不只是摘要\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDeterministic Replay\u003c/strong\u003e：用固定的 seed 和 temperature=0 复现执行，或者直接 mock LLM 响应\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e快照式调试\u003c/strong\u003e：在每个 Agent 决策点保存完整的 Blackboard 状态快照，出问题时可以回溯到任意时间点\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass ReplayableTeam(AgentTeam):\n    \u0026quot;\u0026quot;\u0026quot;可回放的 Agent 团队——记录完整的 LLM 交互供复现\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, supervisor):\n        super().__init__(supervisor)\n        self._llm_recordings: list[dict] = []\n\n    def record_llm_call(self, agent_name: str, messages: list[dict], response: str):\n        self._llm_recordings.append({\n            \u0026quot;agent\u0026quot;: agent_name,\n            \u0026quot;messages\u0026quot;: messages,\n            \u0026quot;response\u0026quot;: response,\n            \u0026quot;timestamp\u0026quot;: time.time(),\n        })\n\n    def save_recording(self, path: str):\n        \u0026quot;\u0026quot;\u0026quot;保存录制数据，用于后续回放和调试\u0026quot;\u0026quot;\u0026quot;\n        with open(path, \u0026quot;w\u0026quot;) as f:\n            json.dump(self._llm_recordings, f, ensure_ascii=False, indent=2)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e8.3 可观测性设计\u003c/h3\u003e\n\u003cp\u003e一个生产级 Multi-Agent 系统至少需要以下可观测性指标：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e指标类别\u003c/th\u003e\n\u003cth\u003e具体指标\u003c/th\u003e\n\u003cth\u003e目的\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e延迟\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e每个 Agent 的执行时间、端到端总时间\u003c/td\u003e\n\u003ctd\u003e定位性能瓶颈\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e成本\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e每个 Agent 的 Token 消耗、总消耗\u003c/td\u003e\n\u003ctd\u003e成本监控和预算控制\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e质量\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e任务成功率、重试次数、降级次数\u003c/td\u003e\n\u003ctd\u003e评估系统可靠性\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e链路\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e完整的 Trace（Agent、操作、输入、输出）\u003c/td\u003e\n\u003ctd\u003e问题排查\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e状态\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eBlackboard 的状态变更历史\u003c/td\u003e\n\u003ctd\u003e数据流追踪\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e通信\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eAgent 间消息数量、消息大小\u003c/td\u003e\n\u003ctd\u003e通信效率分析\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003chr\u003e\n\u003ch2\u003e9. 设计 Multi-Agent 系统的决策清单\u003c/h2\u003e\n\u003cp\u003e在你决定构建 Multi-Agent 系统之前，逐一回答以下问题：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e必要性验证\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e单个 Agent 真的不够吗？是否尝试过优化 prompt、增加工具、使用更强的模型？\u003c/li\u003e\n\u003cli\u003e任务是否天然需要多角色/多视角？还是只是因为你觉得\u0026quot;多 Agent 更酷\u0026quot;？\u003c/li\u003e\n\u003cli\u003e团队的 LLM API 预算能否支撑多 Agent 的额外消耗？\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e架构选择\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e任务结构更接近哪种模式？Supervisor-Worker / Peer-to-Peer / Pipeline / Dynamic Routing？\u003c/li\u003e\n\u003cli\u003eAgent 之间需要什么样的通信？单向传递 / 双向协商 / 广播通知？\u003c/li\u003e\n\u003cli\u003e状态应该共享还是独立？冲突解决策略是什么？\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e工程保障\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e每个 Agent 的失败影响范围是什么？有降级方案吗？\u003c/li\u003e\n\u003cli\u003e如何追踪一个请求在多个 Agent 之间的完整执行链路？\u003c/li\u003e\n\u003cli\u003e如何测试多 Agent 协作的正确性——单元测试（单个 Agent）+ 集成测试（Agent 交互）？\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e10. 结语与展望\u003c/h2\u003e\n\u003cp\u003e本文是 Phase 3（How to Scale Agent Intelligence）的最后一篇。在 Phase 3 的四篇文章中，我们从单个 Agent 的四个维度进行了升级：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ePhase 3 知识路线：\n\n  第 08 篇 Memory       → Agent 有了\u0026quot;记忆\u0026quot;\n  第 09 篇 RAG          → Agent 有了\u0026quot;外部知识\u0026quot;\n  第 10 篇 Planning     → Agent 有了\u0026quot;规划和反思\u0026quot;\n  第 11 篇 Multi-Agent  → Agent 有了\u0026quot;团队协作\u0026quot;（本文）\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e至此，我们已经拥有构建一个\u0026quot;聪明的\u0026quot; Agent 系统所需的全部核心概念。但\u0026quot;聪明\u0026quot;不等于\u0026quot;可用\u0026quot;。一个在本地跑通 demo 的 Multi-Agent 系统，距离生产环境还有巨大的鸿沟——框架选型、协议标准化、可观测性、安全性、成本控制、评估体系。\u003c/p\u003e\n\u003cp\u003e这正是 Phase 4（How to Ship Agents to Production）要解决的问题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e下一篇（12）\u003c/strong\u003e：LangChain vs LangGraph —— 你应该用框架还是自己写？框架的价值边界在哪里？我们会从 Chain 和 Graph 两种抽象出发，讨论框架在什么时候是加速器，什么时候是束缚。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e第 13 篇\u003c/strong\u003e：MCP and Tool Protocol —— Agent 的工具需要标准化。MCP 协议如何让不同 Agent 共享工具？工具的发现、声明、权限控制。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e第 14 篇\u003c/strong\u003e：Production-Grade Agent Systems —— 最后一篇，打通最后一公里：评估、安全、成本、灰度、监控。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e进一步思考\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e关于协作模式的演化\u003c/strong\u003e：本文介绍的四种模式是\u0026quot;纯模式\u0026quot;。真实系统中，你很可能需要混合模式——比如 Supervisor-Worker 的 Worker 内部用 Pipeline，或者 Dynamic Routing 的专家 Agent 内部用 Peer-to-Peer 辩论。如何设计这种嵌套的多层协作结构，是一个值得深入探索的方向。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e关于 Agent 的涌现行为\u003c/strong\u003e：当多个 Agent 协作时，是否会出现超越单个 Agent 能力的\u0026quot;涌现行为\u0026quot;？还是说 Multi-Agent 的上限永远被最强的那个 Agent 决定？这个问题在学术界尚无定论，但从实践角度看，好的协作架构确实能产出超越任何单个 Agent 的结果——正如一个好的工程团队能完成任何个人都无法独自完成的项目。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e关于 Human-in-the-Loop\u003c/strong\u003e：本文讨论的全是 Agent-to-Agent 的协作。但在生产环境中，最重要的\u0026quot;Agent\u0026quot;可能是人类。如何设计一个 Multi-Agent 系统，让人类能在关键节点介入、审核和纠正？Human-Agent 协作可能比 Agent-Agent 协作更有实用价值，也更有挑战性。\u003c/p\u003e\n\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e系列导航\u003c/strong\u003e：本文是 Agentic 系列的第 11 篇。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e上一篇：\u003ca href=\"/blog/engineering/agentic/10-Planning%20and%20Reflection\"\u003e10 | Planning and Reflection\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下一篇：\u003ca href=\"/blog/engineering/agentic/12-LangChain%20vs%20LangGraph\"\u003e12 | LangChain vs LangGraph\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e完整目录：\u003ca href=\"/blog/engineering/agentic/01-From%20LLM%20to%20Agent\"\u003e01 | From LLM to Agent\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"18:Taba2,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eMCP and Tool Protocol: Agent 工具的协议化未来\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e每一次技术生态的成熟，都伴随着协议的诞生。Web 有 HTTP，邮件有 SMTP，实时通信有 WebSocket。当 Agent 从实验走向生产，工具调用也必然需要自己的协议层。\u003c/p\u003e\n\u003cp\u003e本文是 Agentic 系列第 13 篇。我们将从当前工具集成的痛点出发，深入分析 MCP（Model Context Protocol）的设计哲学与技术细节，探讨工具协议化对 Agent 生态的深远影响。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch2\u003e1. 开篇：重复造轮子的困境\u003c/h2\u003e\n\u003cp\u003e假设你正在构建一个 Agent，需要它能够：查询 Jira 工单、读取 GitHub PR、搜索 Confluence 文档、发送 Slack 消息。\u003c/p\u003e\n\u003cp\u003e如果你用 LangChain，你需要找到或编写四个 LangChain Tool wrapper。如果明天切换到 LlamaIndex，这四个 wrapper 全部作废。如果后天决定用 OpenAI Assistants API，又得按 Function Calling 的 schema 再来一遍。\u003cstrong\u003e同样的能力，被实现了三遍。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e这个问题并不新鲜。Web 技术演进史上，我们见过完全相同的模式：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e早期 Web：每个 CGI 脚本都有自己的通信方式\n  → HTTP 统一了通信 → REST 统一了风格 → OpenAPI 统一了描述\n\nAgent 工具（当前）：每个框架都有自己的工具定义格式\n  → ??? 统一工具通信 → ??? 统一工具描述 → ??? 统一工具发现\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e从 CGI 到 HTTP，Web 用了十年。Agent 工具生态能更快吗？MCP 正在尝试回答这个问题。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e2. 工具集成的现状与问题\u003c/h2\u003e\n\u003ch3\u003e2.1 五大痛点\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e硬编码模式\u003c/strong\u003e：工具在代码中写死，新增工具需要改代码、重新部署。\u003cstrong\u003e框架绑定\u003c/strong\u003e：LangChain Tool、OpenAI Function、Anthropic Tool 各有格式，互不兼容——工具提供者要么选边站，要么维护三份代码。\u003cstrong\u003e缺乏发现机制\u003c/strong\u003e：Agent 不知道有哪些工具可用。\u003cstrong\u003e缺乏权限控制\u003c/strong\u003e：Agent 可以调用任何已注册的工具。\u003cstrong\u003e缺乏版本管理\u003c/strong\u003e：工具升级可能静默破坏 Agent 行为。\u003c/p\u003e\n\u003ch3\u003e2.2 N x M 集成问题\u003c/h3\u003e\n\u003cp\u003e这些痛点的根源，是经典的 \u003cstrong\u003eN x M 集成问题\u003c/strong\u003e：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e当前：N 个框架 × M 个工具 = N×M 个适配器\n\n  ┌──────────┐   ┌──────────┐   ┌──────────┐\n  │LangChain │   │LlamaIndex│   │  OpenAI  │\n  └──┬─┬─┬───┘   └──┬─┬─┬───┘   └──┬─┬─┬───┘\n     │ │ │          │ │ │          │ │ │\n     ▼ ▼ ▼          ▼ ▼ ▼          ▼ ▼ ▼       ← 15 个适配器\n  ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐\n  │ Jira │ │GitHub│ │Slack │ │  DB  │ │Search│\n  └──────┘ └──────┘ └──────┘ └──────┘ └──────┘\n\n期望：通过协议层解耦，N + M\n\n  ┌──────────┐   ┌──────────┐   ┌──────────┐\n  │LangChain │   │LlamaIndex│   │  OpenAI  │\n  └────┬─────┘   └────┬─────┘   └────┬─────┘\n       ▼               ▼               ▼\n  ┌──────────────────────────────────────────┐\n  │           标准化协议层（MCP）              │\n  └──┬───────┬───────┬───────┬───────┬───────┘\n     ▼       ▼       ▼       ▼       ▼         ← 8 个实现\n  ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐\n  │ Jira │ │GitHub│ │Slack │ │  DB  │ │Search│\n  └──────┘ └──────┘ └──────┘ └──────┘ └──────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e将 N x M 降为 N + M\u003c/strong\u003e——这正是 MCP 试图解决的核心问题。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e3. MCP 深入分析\u003c/h2\u003e\n\u003ch3\u003e3.1 什么是 MCP\u003c/h3\u003e\n\u003cp\u003eMCP（Model Context Protocol）是 Anthropic 于 2024 年末提出的开放协议，定义了 AI 应用与外部工具/数据源之间的标准化通信方式。不绑定任何特定 LLM 或框架。\u003c/p\u003e\n\u003cp\u003e类比：\u003cstrong\u003eUSB-C 之于硬件外设，正如 MCP 之于 Agent 工具。\u003c/strong\u003e 没有 USB-C 时，每个设备一种接口；有了 USB-C，一个接口连接一切。MCP 的目标是同样的——一个协议连接所有工具。\u003c/p\u003e\n\u003ch3\u003e3.2 核心架构：Host → Client → Server\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e┌─────────────────────────────────────────────────────┐\n│  Host (Claude Desktop / IDE / 自定义 Agent)           │\n│                                                      │\n│  ┌────────────────────────────────────────────────┐  │\n│  │              MCP Client                        │  │\n│  └───┬──────────────┬──────────────┬──────────────┘  │\n└──────┼──────────────┼──────────────┼─────────────────┘\n       │              │              │\n       ▼              ▼              ▼\n┌────────────┐ ┌────────────┐ ┌────────────┐\n│ MCP Server │ │ MCP Server │ │ MCP Server │\n│  (GitHub)  │ │  (Slack)   │ │ (Database) │\n│ Tools:     │ │ Tools:     │ │ Tools:     │\n│ -search    │ │ -send_msg  │ │ -query     │\n│ -create_pr │ │ -list_ch   │ │ -insert    │\n└────────────┘ └────────────┘ └────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHost\u003c/strong\u003e：最终用户面对的应用，创建和管理 MCP Client 实例。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eClient\u003c/strong\u003e：协议客户端，与 Server 保持一对一连接，负责能力协商与请求路由。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eServer\u003c/strong\u003e：工具/数据提供者，暴露 Tools、Resources 和 Prompts。轻量级，不需了解 LLM。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e3.3 三大原语\u003c/h3\u003e\n\u003cp\u003eMCP 定义了三种核心原语，覆盖 Agent 与外部世界交互的主要模式：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌────────────┬──────────────┬──────────────────────────────┐\n│   原语      │  控制权归属    │  语义                        │\n├────────────┼──────────────┼──────────────────────────────┤\n│  Tools     │  Model 控制   │  可执行操作，LLM 自主决定调用  │\n│  Resources │  App 控制     │  可读数据源，Host 决定读取     │\n│  Prompts   │  User 控制    │  交互模板，用户显式选择        │\n└────────────┴──────────────┴──────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这种\u003cstrong\u003e分层控制\u003c/strong\u003e是 MCP 设计中最精妙的部分——避免\u0026quot;一切交给 LLM\u0026quot;的风险，保留人类最终控制权。Tools 是 Agent 的\u0026quot;手\u0026quot;，Resources 是\u0026quot;眼\u0026quot;，Prompts 是\u0026quot;工作手册\u0026quot;。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e4. 通信机制\u003c/h2\u003e\n\u003ch3\u003e4.1 传输层\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003estdio\u003c/strong\u003e：本地进程间通信。零网络开销、简单可靠，但仅限同一台机器。\u003cbr\u003e\u003cstrong\u003eHTTP + SSE\u003c/strong\u003e：远程服务通信。Client 通过 HTTP POST 发请求，Server 通过 SSE 推响应。2025 年的 Streamable HTTP 更新进一步统一了远程传输层。\u003c/p\u003e\n\u003ch3\u003e4.2 消息格式：JSON-RPC 2.0\u003c/h3\u003e\n\u003cp\u003eMCP 使用成熟的 JSON-RPC 2.0（2010 年发布，大量现成实现）：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e// 请求\n{\u0026quot;jsonrpc\u0026quot;: \u0026quot;2.0\u0026quot;, \u0026quot;id\u0026quot;: 1, \u0026quot;method\u0026quot;: \u0026quot;tools/call\u0026quot;,\n \u0026quot;params\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;query_db\u0026quot;, \u0026quot;arguments\u0026quot;: {\u0026quot;sql\u0026quot;: \u0026quot;SELECT * FROM users\u0026quot;}}}\n\n// 响应\n{\u0026quot;jsonrpc\u0026quot;: \u0026quot;2.0\u0026quot;, \u0026quot;id\u0026quot;: 1,\n \u0026quot;result\u0026quot;: {\u0026quot;content\u0026quot;: [{\u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Found 42 users...\u0026quot;}]}}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e4.3 生命周期\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eClient                                       Server\n  │  ① initialize (clientInfo, capabilities)    │\n  │ ───────────────────────────────────────────▶│\n  │  ② response (serverInfo, capabilities)      │\n  │◀─────────────────────────────────────────── │\n  │  ③ notifications/initialized                │\n  │ ───────────────────────────────────────────▶│\n  │  ④ Normal: tools/list, tools/call ...       │\n  │◀───────────────────────────────────────────▶│\n  │  ⑤ Shutdown                                 │\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e初始化阶段的\u003cstrong\u003e能力协商\u003c/strong\u003e是关键设计——Client 和 Server 各自声明支持的能力，只使用交集。这使得旧 Client 可以连新 Server，只是无法使用新功能。\u003c/p\u003e\n\u003ch3\u003e4.4 一次完整的工具调用\u003c/h3\u003e\n\u003cp\u003e关键设计：\u003cstrong\u003eLLM 不直接与 MCP Server 通信\u003c/strong\u003e。LLM 只表达\u0026quot;我想调用某工具\u0026quot;，Host 运行时执行实际 MCP 调用。这层间接性让 Host 可以在调用前进行权限检查、参数验证、用户确认。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eUser → Host: \u0026quot;查询活跃用户\u0026quot;\nHost → LLM:  消息 + 可用工具列表\nLLM  → Host: tool_use: query_db(sql=\u0026quot;...\u0026quot;)\nHost → MCP Client → MCP Server: tools/call\nMCP Server → MCP Client → Host: 结果\nHost → LLM:  工具结果 + 继续对话\nLLM  → Host: \u0026quot;共 42 个活跃用户\u0026quot;\nHost → User: 最终回答\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e5. 实现一个 MCP Server\u003c/h2\u003e\n\u003cp\u003e使用官方 \u003ccode\u003emcp\u003c/code\u003e Python SDK 实现一个项目管理工具 Server：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom mcp.server import Server\nfrom mcp.server.stdio import stdio_server\nfrom mcp.types import Tool, TextContent, Resource\nimport json, asyncio\n\nserver = Server(\u0026quot;project-manager\u0026quot;)\nTASKS = {\n    \u0026quot;TASK-001\u0026quot;: {\u0026quot;title\u0026quot;: \u0026quot;实现用户认证\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;done\u0026quot;, \u0026quot;assignee\u0026quot;: \u0026quot;alice\u0026quot;},\n    \u0026quot;TASK-002\u0026quot;: {\u0026quot;title\u0026quot;: \u0026quot;设计 DB schema\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;in_progress\u0026quot;, \u0026quot;assignee\u0026quot;: \u0026quot;bob\u0026quot;},\n    \u0026quot;TASK-003\u0026quot;: {\u0026quot;title\u0026quot;: \u0026quot;编写 API 文档\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;todo\u0026quot;, \u0026quot;assignee\u0026quot;: \u0026quot;alice\u0026quot;},\n}\n\n@server.list_tools()\nasync def list_tools() -\u0026gt; list[Tool]:\n    return [\n        Tool(name=\u0026quot;list_tasks\u0026quot;,\n             description=\u0026quot;列出项目任务，可按状态和负责人筛选\u0026quot;,\n             inputSchema={\u0026quot;type\u0026quot;: \u0026quot;object\u0026quot;, \u0026quot;properties\u0026quot;: {\n                 \u0026quot;status\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;enum\u0026quot;: [\u0026quot;todo\u0026quot;, \u0026quot;in_progress\u0026quot;, \u0026quot;done\u0026quot;]},\n                 \u0026quot;assignee\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;},\n             }}),\n        Tool(name=\u0026quot;update_task_status\u0026quot;,\n             description=\u0026quot;更新任务状态\u0026quot;,\n             inputSchema={\u0026quot;type\u0026quot;: \u0026quot;object\u0026quot;, \u0026quot;properties\u0026quot;: {\n                 \u0026quot;task_id\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;},\n                 \u0026quot;new_status\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;enum\u0026quot;: [\u0026quot;todo\u0026quot;, \u0026quot;in_progress\u0026quot;, \u0026quot;done\u0026quot;]},\n             }, \u0026quot;required\u0026quot;: [\u0026quot;task_id\u0026quot;, \u0026quot;new_status\u0026quot;]}),\n    ]\n\n@server.call_tool()\nasync def call_tool(name: str, arguments: dict) -\u0026gt; list[TextContent]:\n    if name == \u0026quot;list_tasks\u0026quot;:\n        results = {tid: t for tid, t in TASKS.items()\n                   if (not arguments.get(\u0026quot;status\u0026quot;) or t[\u0026quot;status\u0026quot;] == arguments[\u0026quot;status\u0026quot;])\n                   and (not arguments.get(\u0026quot;assignee\u0026quot;) or t[\u0026quot;assignee\u0026quot;] == arguments[\u0026quot;assignee\u0026quot;])}\n        return [TextContent(type=\u0026quot;text\u0026quot;, text=json.dumps(results, ensure_ascii=False, indent=2))]\n    elif name == \u0026quot;update_task_status\u0026quot;:\n        tid, ns = arguments[\u0026quot;task_id\u0026quot;], arguments[\u0026quot;new_status\u0026quot;]\n        if tid not in TASKS:\n            return [TextContent(type=\u0026quot;text\u0026quot;, text=f\u0026quot;任务 {tid} 不存在\u0026quot;)]\n        old = TASKS[tid][\u0026quot;status\u0026quot;]\n        TASKS[tid][\u0026quot;status\u0026quot;] = ns\n        return [TextContent(type=\u0026quot;text\u0026quot;, text=f\u0026quot;已将 {tid} 从 {old} 更新为 {ns}\u0026quot;)]\n    return [TextContent(type=\u0026quot;text\u0026quot;, text=f\u0026quot;未知工具: {name}\u0026quot;)]\n\n@server.list_resources()\nasync def list_resources() -\u0026gt; list[Resource]:\n    return [Resource(uri=\u0026quot;project://tasks/summary\u0026quot;, name=\u0026quot;项目任务总览\u0026quot;,\n                     description=\u0026quot;任务统计摘要\u0026quot;, mimeType=\u0026quot;application/json\u0026quot;)]\n\n@server.read_resource()\nasync def read_resource(uri: str) -\u0026gt; str:\n    if str(uri) == \u0026quot;project://tasks/summary\u0026quot;:\n        summary = {\u0026quot;total\u0026quot;: len(TASKS), \u0026quot;by_status\u0026quot;: {}, \u0026quot;by_assignee\u0026quot;: {}}\n        for t in TASKS.values():\n            summary[\u0026quot;by_status\u0026quot;][t[\u0026quot;status\u0026quot;]] = summary[\u0026quot;by_status\u0026quot;].get(t[\u0026quot;status\u0026quot;], 0) + 1\n            summary[\u0026quot;by_assignee\u0026quot;][t[\u0026quot;assignee\u0026quot;]] = summary[\u0026quot;by_assignee\u0026quot;].get(t[\u0026quot;assignee\u0026quot;], 0) + 1\n        return json.dumps(summary, ensure_ascii=False, indent=2)\n    raise ValueError(f\u0026quot;未知资源: {uri}\u0026quot;)\n\nasync def main():\n    async with stdio_server() as (read_stream, write_stream):\n        await server.run(read_stream, write_stream, server.create_initialization_options())\n\nif __name__ == \u0026quot;__main__\u0026quot;:\n    asyncio.run(main())\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e核心模式：\u003cstrong\u003e声明式工具注册\u003c/strong\u003e（\u003ccode\u003elist_tools\u003c/code\u003e 返回名称、描述、JSON Schema）→ \u003cstrong\u003e请求路由\u003c/strong\u003e（\u003ccode\u003ecall_tool\u003c/code\u003e 根据工具名分发）→ \u003cstrong\u003e资源暴露\u003c/strong\u003e（URI 标识的可读数据源）→ \u003cstrong\u003e传输透明\u003c/strong\u003e（同一份代码可跑 stdio 或 HTTP）。\u003c/p\u003e\n\u003cp\u003eHost 通过配置文件声明连接：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e{\n    \u0026quot;mcpServers\u0026quot;: {\n        \u0026quot;project-manager\u0026quot;: {\n            \u0026quot;command\u0026quot;: \u0026quot;python\u0026quot;,\n            \u0026quot;args\u0026quot;: [\u0026quot;path/to/server.py\u0026quot;]\n        },\n        \u0026quot;github\u0026quot;: {\n            \u0026quot;command\u0026quot;: \u0026quot;npx\u0026quot;,\n            \u0026quot;args\u0026quot;: [\u0026quot;-y\u0026quot;, \u0026quot;@modelcontextprotocol/server-github\u0026quot;],\n            \u0026quot;env\u0026quot;: {\u0026quot;GITHUB_TOKEN\u0026quot;: \u0026quot;ghp_xxxx\u0026quot;}\n        }\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e5.1 实现一个 MCP Client\u003c/h3\u003e\n\u003cp\u003e上面实现了 Server 端。现在看另一半——Client 如何连接 Server、发现工具、并与 LLM Agent 循环集成。\u003c/p\u003e\n\u003cp\u003e以下代码展示一个完整的 MCP Client，它连接 Server、获取工具列表、将工具转换为 LLM Function Calling 格式、并在 Agent 循环中路由 LLM 的 \u003ccode\u003etool_use\u003c/code\u003e 请求回 MCP：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom mcp import ClientSession\nfrom mcp.client.stdio import stdio_client, StdioServerParameters\nimport json, asyncio\n\nclass MCPAgentClient:\n    \u0026quot;\u0026quot;\u0026quot;MCP Client：连接 Server，桥接 LLM Function Calling\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, server_command: str, server_args: list[str]):\n        self.server_params = StdioServerParameters(\n            command=server_command, args=server_args\n        )\n        self.session: ClientSession | None = None\n        self._tools_cache: list[dict] = []\n\n    async def connect(self, read_stream, write_stream):\n        \u0026quot;\u0026quot;\u0026quot;建立连接并完成初始化握手\u0026quot;\u0026quot;\u0026quot;\n        self.session = ClientSession(read_stream, write_stream)\n        await self.session.initialize()\n        # 初始化后立即拉取工具列表\n        await self.refresh_tools()\n\n    async def refresh_tools(self):\n        \u0026quot;\u0026quot;\u0026quot;从 Server 获取最新工具列表\u0026quot;\u0026quot;\u0026quot;\n        result = await self.session.list_tools()\n        self._tools_cache = [\n            {\n                \u0026quot;name\u0026quot;: tool.name,\n                \u0026quot;description\u0026quot;: tool.description,\n                \u0026quot;input_schema\u0026quot;: tool.inputSchema\n            }\n            for tool in result.tools\n        ]\n\n    def get_tools_for_llm(self) -\u0026gt; list[dict]:\n        \u0026quot;\u0026quot;\u0026quot;将 MCP 工具转换为 LLM Function Calling 格式\n\n        关键桥接：MCP 工具描述 → LLM 能理解的 function schema\n        不同 LLM 的格式略有差异，这里以常见格式为例。\n        \u0026quot;\u0026quot;\u0026quot;\n        return [\n            {\n                \u0026quot;type\u0026quot;: \u0026quot;function\u0026quot;,\n                \u0026quot;function\u0026quot;: {\n                    \u0026quot;name\u0026quot;: tool[\u0026quot;name\u0026quot;],\n                    \u0026quot;description\u0026quot;: tool[\u0026quot;description\u0026quot;],\n                    \u0026quot;parameters\u0026quot;: tool[\u0026quot;input_schema\u0026quot;]\n                }\n            }\n            for tool in self._tools_cache\n        ]\n\n    async def route_tool_call(self, tool_name: str, arguments: dict) -\u0026gt; str:\n        \u0026quot;\u0026quot;\u0026quot;将 LLM 的 tool_use 请求路由到 MCP Server\u0026quot;\u0026quot;\u0026quot;\n        result = await self.session.call_tool(tool_name, arguments)\n        # 提取文本内容返回给 LLM\n        return \u0026quot;\\n\u0026quot;.join(\n            block.text for block in result.content\n            if hasattr(block, \u0026quot;text\u0026quot;)\n        )\n\n\nasync def agent_loop(llm_client, mcp_client: MCPAgentClient):\n    \u0026quot;\u0026quot;\u0026quot;Agent 主循环：LLM 决策 → MCP 执行 → 结果反馈\u0026quot;\u0026quot;\u0026quot;\n    tools = mcp_client.get_tools_for_llm()\n    messages = [{\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;帮我看看 alice 有哪些进行中的任务\u0026quot;}]\n\n    while True:\n        response = await llm_client.chat(messages=messages, tools=tools)\n\n        # LLM 没有调用工具，对话结束\n        if not response.tool_calls:\n            print(f\u0026quot;Agent: {response.content}\u0026quot;)\n            break\n\n        # LLM 请求调用工具 → 路由到 MCP Server\n        for call in response.tool_calls:\n            tool_result = await mcp_client.route_tool_call(\n                call.function.name,\n                json.loads(call.function.arguments)\n            )\n            messages.append({\n                \u0026quot;role\u0026quot;: \u0026quot;tool\u0026quot;,\n                \u0026quot;tool_call_id\u0026quot;: call.id,\n                \u0026quot;content\u0026quot;: tool_result\n            })\n\n\nasync def main():\n    client = MCPAgentClient(\u0026quot;python\u0026quot;, [\u0026quot;server.py\u0026quot;])\n    async with stdio_client(client.server_params) as (read, write):\n        await client.connect(read, write)\n        print(f\u0026quot;已连接，发现 {len(client._tools_cache)} 个工具：\u0026quot;)\n        for t in client._tools_cache:\n            print(f\u0026quot;  - {t[\u0026#39;name\u0026#39;]}: {t[\u0026#39;description\u0026#39;]}\u0026quot;)\n        # await agent_loop(llm_client, client)\n\nif __name__ == \u0026quot;__main__\u0026quot;:\n    asyncio.run(main())\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e核心模式总结：\u003cstrong\u003e连接与握手\u003c/strong\u003e（\u003ccode\u003einitialize\u003c/code\u003e 完成能力协商）→ \u003cstrong\u003e工具发现\u003c/strong\u003e（\u003ccode\u003elist_tools\u003c/code\u003e 获取 Server 暴露的所有工具）→ \u003cstrong\u003e格式转换\u003c/strong\u003e（MCP Tool schema → LLM Function Calling schema，这是 Client 的关键职责）→ \u003cstrong\u003e请求路由\u003c/strong\u003e（LLM 输出 \u003ccode\u003etool_use\u003c/code\u003e → Client 调用 \u003ccode\u003ecall_tool\u003c/code\u003e → 结果回填到对话上下文）。\u003c/p\u003e\n\u003cp\u003e注意 Client 在架构中的定位：它是 \u003cstrong\u003eLLM 世界与 MCP 世界之间的翻译层\u003c/strong\u003e。LLM 不知道 MCP 的存在，MCP Server 不知道 LLM 的存在。Client 把两边连接起来，同时也是插入权限检查、参数验证、超时控制等逻辑的最佳位置。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e6. 工具发现与动态注册\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e静态发现\u003c/strong\u003e：配置文件声明所有 Server，Host 启动时初始化。简单可靠，但新增 Server 需重启。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e动态发现\u003c/strong\u003e：MCP 支持 \u003ccode\u003enotifications/tools/list_changed\u003c/code\u003e 通知——Server 可在运行时告知 Client 工具列表变更，无需重启连接。\u003c/p\u003e\n\u003cp\u003e更大的愿景是\u003cstrong\u003e工具注册中心（Tool Registry）\u003c/strong\u003e——Agent 在运行时查询\u0026quot;有哪些 MCP Server 可用\u0026quot;，按需连接。本质上是 Agent 版的 Service Discovery。\u003c/p\u003e\n\u003cp\u003e与传统 Service Discovery 的核心区别：传统消费者是确定性代码（知道要调哪个 API），MCP 消费者是 LLM（根据自然语言意图选工具）。因此工具描述的\u003cstrong\u003e语义质量\u003c/strong\u003e至关重要——模糊的 description 会导致 LLM 误选工具。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e7. 安全与权限控制\u003c/h2\u003e\n\u003ch3\u003e7.1 威胁模型\u003c/h3\u003e\n\u003cp\u003eAgent 工具调用面临五类威胁：\u003cstrong\u003ePrompt Injection\u003c/strong\u003e（诱导调用不该调用的工具）、\u003cstrong\u003e权限越权\u003c/strong\u003e（只读 Agent 执行写入）、\u003cstrong\u003e数据泄露\u003c/strong\u003e（敏感数据通过 LLM 响应外泄）、\u003cstrong\u003e恶意 Server\u003c/strong\u003e（第三方 Server 返回恶意内容）、\u003cstrong\u003e参数篡改\u003c/strong\u003e（被诱导传入 SQL 注入等恶意参数）。\u003c/p\u003e\n\u003ch3\u003e7.2 防护策略\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e工具级 ACL\u003c/strong\u003e：在 Host 层实现访问控制——白名单/黑名单决定哪些 Agent 可调用哪些工具。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e参数级约束\u003c/strong\u003e：即使允许调用，也限制参数范围（如 SQL 工具只允许 SELECT、禁止 DROP/DELETE）。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHuman-in-the-Loop\u003c/strong\u003e：高风险操作（写入、删除、发送消息）要求用户显式确认后再执行。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e审计日志\u003c/strong\u003e：记录所有工具调用的时间戳、Agent ID、工具名、参数、结果、耗时、状态。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 工具级 ACL 示例\nasync def guarded_tool_call(agent_id: str, tool_name: str, arguments: dict):\n    perms = TOOL_PERMISSIONS[agent_id]\n    if tool_name in perms[\u0026quot;denied\u0026quot;]:\n        raise PermissionError(f\u0026quot;{agent_id} cannot call {tool_name}\u0026quot;)\n    # 参数验证\n    validate_arguments(tool_name, arguments)\n    # 高风险确认\n    if tool_name in HIGH_RISK_TOOLS:\n        if not await prompt_user(f\u0026quot;允许调用 {tool_name}? [y/n]\u0026quot;):\n            return {\u0026quot;error\u0026quot;: \u0026quot;用户拒绝\u0026quot;}\n    return await mcp_client.call_tool(tool_name, arguments)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e7.3 Sandbox 执行\u003c/h3\u003e\n\u003cp\u003eMCP 的 stdio 模式天然提供进程级隔离。更严格的方案：容器隔离（Docker）→ VM 隔离（Firecracker）→ WASM 沙箱。执行不可信代码的 Server，容器隔离是最低要求。\u003c/p\u003e\n\u003ch3\u003e7.4 错误处理与容错\u003c/h3\u003e\n\u003cp\u003eMCP Server 的错误最终会进入 LLM 的上下文窗口。这意味着错误信息的设计有双重读者——\u003cstrong\u003e人类开发者\u003c/strong\u003e需要 debug 信息，\u003cstrong\u003eLLM\u003c/strong\u003e 需要可理解、可行动的恢复指引。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e错误传播设计原则\u003c/strong\u003e：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e❌ 糟糕的错误：  \u0026quot;Internal Server Error\u0026quot;\n   → LLM 无法理解原因，只能对用户说 \u0026quot;出了点问题\u0026quot;\n\n❌ 过于技术化：  \u0026quot;psycopg2.OperationalError: connection refused on port 5432\u0026quot;\n   → LLM 不知道该重试还是放弃\n\n✅ 面向 LLM 的错误：  \u0026quot;数据库连接暂时不可用。这是临时性故障，建议等待 30 秒后重试。\n   如果多次重试仍失败，请告知用户数据库服务可能在维护中。\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e核心思路：错误信息中要包含\u003cstrong\u003e原因分类\u003c/strong\u003e（临时故障/参数错误/权限不足）、\u003cstrong\u003e建议动作\u003c/strong\u003e（重试/换参数/告知用户），以及\u003cstrong\u003e足够的上下文\u003c/strong\u003e让 LLM 能生成有意义的回复。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTimeout 与 Retry 策略\u003c/strong\u003e：MCP 工具调用需要明确的超时边界。没有 timeout 的工具调用可能永远挂起，阻塞整个 Agent 循环。Retry 应使用 exponential backoff，且只对临时性故障重试（网络超时、服务暂时不可用），对确定性错误（参数无效、权限不足）不应重试。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCircuit Breaker 模式\u003c/strong\u003e：对于不可靠的外部 Server，连续失败应触发熔断，避免浪费 LLM tokens 反复尝试一个已知不可用的服务。\u003c/p\u003e\n\u003cp\u003e以下是一个整合 timeout、retry 和 circuit breaker 的 MCP Client 容错封装：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport asyncio\nimport time\nfrom dataclasses import dataclass, field\nfrom mcp import ClientSession\n\n@dataclass\nclass CircuitBreaker:\n    \u0026quot;\u0026quot;\u0026quot;简单的 Circuit Breaker：连续失败超过阈值则熔断\u0026quot;\u0026quot;\u0026quot;\n    failure_threshold: int = 5\n    recovery_timeout: float = 60.0  # 熔断恢复等待时间（秒）\n    _failure_count: int = field(default=0, init=False)\n    _last_failure_time: float = field(default=0.0, init=False)\n    _state: str = field(default=\u0026quot;closed\u0026quot;, init=False)  # closed / open / half_open\n\n    def record_success(self):\n        self._failure_count = 0\n        self._state = \u0026quot;closed\u0026quot;\n\n    def record_failure(self):\n        self._failure_count += 1\n        self._last_failure_time = time.time()\n        if self._failure_count \u0026gt;= self.failure_threshold:\n            self._state = \u0026quot;open\u0026quot;\n\n    def allow_request(self) -\u0026gt; bool:\n        if self._state == \u0026quot;closed\u0026quot;:\n            return True\n        if self._state == \u0026quot;open\u0026quot;:\n            if time.time() - self._last_failure_time \u0026gt; self.recovery_timeout:\n                self._state = \u0026quot;half_open\u0026quot;\n                return True  # 允许试探性请求\n            return False\n        return True  # half_open: 允许一次试探\n\nclass ResilientMCPClient:\n    \u0026quot;\u0026quot;\u0026quot;带容错能力的 MCP Client 封装\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, session: ClientSession, timeout: float = 30.0,\n                 max_retries: int = 3, base_delay: float = 1.0):\n        self.session = session\n        self.timeout = timeout\n        self.max_retries = max_retries\n        self.base_delay = base_delay\n        self._breakers: dict[str, CircuitBreaker] = {}\n\n    def _get_breaker(self, tool_name: str) -\u0026gt; CircuitBreaker:\n        if tool_name not in self._breakers:\n            self._breakers[tool_name] = CircuitBreaker()\n        return self._breakers[tool_name]\n\n    async def call_tool(self, tool_name: str, arguments: dict) -\u0026gt; dict:\n        breaker = self._get_breaker(tool_name)\n\n        if not breaker.allow_request():\n            return {\n                \u0026quot;error\u0026quot;: f\u0026quot;工具 {tool_name} 当前不可用（连续失败已触发熔断）。\u0026quot;\n                         f\u0026quot;请告知用户该服务暂时不可用，大约 {breaker.recovery_timeout} 秒后可重试。\u0026quot;\n            }\n\n        last_error = None\n        for attempt in range(self.max_retries):\n            try:\n                result = await asyncio.wait_for(\n                    self.session.call_tool(tool_name, arguments),\n                    timeout=self.timeout\n                )\n                breaker.record_success()\n                return {\u0026quot;content\u0026quot;: result.content}\n\n            except asyncio.TimeoutError:\n                last_error = f\u0026quot;工具 {tool_name} 调用超时（\u0026gt;{self.timeout}s）\u0026quot;\n                breaker.record_failure()\n            except Exception as e:\n                if _is_permanent_error(e):\n                    # 参数错误、权限不足等确定性失败，不重试\n                    return {\u0026quot;error\u0026quot;: f\u0026quot;工具调用失败：{e}。请检查参数后重新尝试。\u0026quot;}\n                last_error = str(e)\n                breaker.record_failure()\n\n            if attempt \u0026lt; self.max_retries - 1:\n                delay = self.base_delay * (2 ** attempt)  # exponential backoff\n                await asyncio.sleep(delay)\n\n        return {\u0026quot;error\u0026quot;: f\u0026quot;工具 {tool_name} 在 {self.max_retries} 次重试后仍然失败：{last_error}。\u0026quot;\n                         f\u0026quot;这可能是临时性故障，建议稍后重试或告知用户。\u0026quot;}\n\ndef _is_permanent_error(e: Exception) -\u0026gt; bool:\n    \u0026quot;\u0026quot;\u0026quot;判断是否为确定性错误（不应重试）\u0026quot;\u0026quot;\u0026quot;\n    permanent_types = (ValueError, PermissionError, KeyError)\n    return isinstance(e, permanent_types)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这个封装的设计思路：\u003cstrong\u003etimeout 防挂起\u003c/strong\u003e（每次调用有明确的时间上限）→ \u003cstrong\u003eretry 抗抖动\u003c/strong\u003e（临时性故障用 exponential backoff 重试）→ \u003cstrong\u003ecircuit breaker 防雪崩\u003c/strong\u003e（连续失败后快速失败，避免反复调用一个已知坏掉的服务）→ \u003cstrong\u003eLLM 友好的错误信息\u003c/strong\u003e（每个错误路径都返回 LLM 可理解的文本描述）。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e8. MCP 之外的协议探索\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eOpenAI Function Calling\u003c/strong\u003e：定义了工具描述格式，但更多是 API 特性而非通信协议——没有定义工具发现、连接管理、生命周期。MCP 是完整的端到端协议。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eGoogle Genkit\u003c/strong\u003e：跨语言 Agent 开发框架。注意区分：\u003cstrong\u003e框架绑定实现\u003c/strong\u003e（你的代码运行在框架中），\u003cstrong\u003e协议解耦实现\u003c/strong\u003e（你的代码遵循协议通信，实现自由选择）。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAgent Protocol（by e2b）\u003c/strong\u003e：标准化 Agent 本身的通信接口，与 MCP（Agent 与工具的通信）互补。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eOpenAPI / AsyncAPI\u003c/strong\u003e：可用于工具描述，但缺少面向 LLM 优化的语义——工具描述需要让模型\u0026quot;理解\u0026quot;何时该用，而非只让人类开发者读懂。\u003c/p\u003e\n\u003cp\u003e趋势清晰：\u003cstrong\u003e工具协议化正在发生\u003c/strong\u003e。MCP 目前的优势在于开放协议、社区快速增长、设计简洁实用。\u003c/p\u003e\n\u003ch3\u003e8.1 协议对比矩阵\u003c/h3\u003e\n\u003cp\u003e以下从六个维度横向对比当前主要的工具/Agent 协议方案：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e维度\u003c/th\u003e\n\u003cth\u003eMCP\u003c/th\u003e\n\u003cth\u003eOpenAI Function Calling\u003c/th\u003e\n\u003cth\u003eGoogle Genkit\u003c/th\u003e\n\u003cth\u003eAgent Protocol (e2b)\u003c/th\u003e\n\u003cth\u003eOpenAPI\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e工具发现\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e动态发现，\u003ccode\u003etools/list\u003c/code\u003e + \u003ccode\u003elist_changed\u003c/code\u003e 通知\u003c/td\u003e\n\u003ctd\u003e无，工具需在请求中硬编码传入\u003c/td\u003e\n\u003ctd\u003e框架内注册，支持反射式发现\u003c/td\u003e\n\u003ctd\u003e无工具发现，聚焦 Agent 任务管理\u003c/td\u003e\n\u003ctd\u003e静态，通过 spec 文件描述\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e通信方式\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eJSON-RPC 2.0 over stdio / HTTP+SSE\u003c/td\u003e\n\u003ctd\u003eHTTP API（嵌入 Chat Completion 请求）\u003c/td\u003e\n\u003ctd\u003e框架内函数调用（Go/JS）\u003c/td\u003e\n\u003ctd\u003eREST API（HTTP）\u003c/td\u003e\n\u003ctd\u003eREST / HTTP\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e安全模型\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eHost 层 ACL + 参数约束 + Human-in-the-Loop\u003c/td\u003e\n\u003ctd\u003eAPI Key 级别，无工具粒度控制\u003c/td\u003e\n\u003ctd\u003e框架内中间件\u003c/td\u003e\n\u003ctd\u003eAPI Token 认证\u003c/td\u003e\n\u003ctd\u003eOAuth / API Key\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e多语言支持\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003ePython, TypeScript, Java, Kotlin 等 SDK\u003c/td\u003e\n\u003ctd\u003e任何能发 HTTP 的语言\u003c/td\u003e\n\u003ctd\u003eGo, JavaScript/TypeScript\u003c/td\u003e\n\u003ctd\u003e任何能发 HTTP 的语言\u003c/td\u003e\n\u003ctd\u003e语言无关（spec 是 YAML/JSON）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e生态成熟度\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e快速增长，1000+ 社区 Server\u003c/td\u003e\n\u003ctd\u003e最大用户基数，但非独立协议\u003c/td\u003e\n\u003ctd\u003e较新，Google 生态内使用\u003c/td\u003e\n\u003ctd\u003e小众，e2b 社区为主\u003c/td\u003e\n\u003ctd\u003e极成熟，但非 AI 原生\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e适用场景\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eAgent ↔ 工具的标准化通信\u003c/td\u003e\n\u003ctd\u003e单一 LLM 的工具调用\u003c/td\u003e\n\u003ctd\u003eGoogle 生态内的全栈 AI 应用\u003c/td\u003e\n\u003ctd\u003eAgent 间的任务编排与通信\u003c/td\u003e\n\u003ctd\u003e传统 API 描述与集成\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e几个关键观察：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMCP 是唯一面向 Agent 工具设计的完整协议\u003c/strong\u003e。Function Calling 只解决了\u0026quot;LLM 怎么表达想调用工具\u0026quot;，但没有解决\u0026quot;工具怎么被发现、怎么连接、怎么管理生命周期\u0026quot;。MCP 覆盖了从发现到调用到关闭的完整链路。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eOpenAPI 有潜力但缺 AI 语义\u003c/strong\u003e。OpenAPI spec 描述了 API 的结构，但缺少面向 LLM 优化的语义层——什么时候该用这个 API？参数的哪些组合是有意义的？错误时该怎么恢复？这些信息在 OpenAPI spec 中要么缺失，要么只面向人类开发者。已有项目尝试将 OpenAPI spec 自动转换为 MCP Server，桥接两个生态。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAgent Protocol 与 MCP 是互补关系\u003c/strong\u003e。MCP 标准化 Agent 与工具的通信，Agent Protocol 标准化 Agent 与 Agent（或 Agent 与编排器）的通信。未来的 Multi-Agent 系统可能同时需要两者。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e9. Trade-off 分析\u003c/h2\u003e\n\u003ch3\u003e9.1 标准化 vs 灵活性\u003c/h3\u003e\n\u003cp\u003e标准化收益显而易见（生态共享、减少重复、互操作），代价是表达力受限和演进惯性。关键判断：\u003cstrong\u003eMCP 的抽象层次选得好\u003c/strong\u003e。它定义通信方式但不限制工具实现——类似 HTTP 定义请求-响应模式但不限制 body 内容。\u003c/p\u003e\n\u003ch3\u003e9.2 额外复杂度\u003c/h3\u003e\n\u003cp\u003e没有 MCP 时工具就是函数调用。有了 MCP 需要进程管理、连接维护、序列化。决策框架：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e工具少（\u0026lt; 5）且团队单一   → 直接硬编码\n工具多（\u0026gt; 10）且跨团队    → MCP 收益显现\n工具需被多 Agent 共享     → MCP 几乎必需\n工具需独立部署和升级      → MCP 最佳选择\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e9.3 生态依赖\u003c/h3\u003e\n\u003cp\u003eMCP 由 Anthropic 主导——缓解策略：MIT 开源可 fork、Server 是独立进程（最坏只需换 Client）、核心业务逻辑应与协议层分离。\u003cstrong\u003e投入合理，但要做好隔离。\u003c/strong\u003e\u003c/p\u003e\n\u003ch3\u003e9.4 性能\u003c/h3\u003e\n\u003cp\u003estdio 通信 0.1-1ms，HTTP 通信 1-50ms，连接初始化 100ms-2s。相比 LLM 推理耗时（100ms-10s），\u003cstrong\u003eMCP 性能开销可忽略\u003c/strong\u003e。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e10. 实践建议\u003c/h2\u003e\n\u003ch3\u003e10.1 工具描述的最佳实践\u003c/h3\u003e\n\u003cp\u003e这是最影响效果的环节。工具描述不是给人类读的 API 文档——它是 LLM 的决策依据。描述质量直接决定 Agent 选对工具的概率。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e反面示例\u003c/strong\u003e：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eTool(\n    name=\u0026quot;search\u0026quot;,\n    description=\u0026quot;Search for things\u0026quot;,\n    inputSchema={\u0026quot;type\u0026quot;: \u0026quot;object\u0026quot;, \u0026quot;properties\u0026quot;: {\n        \u0026quot;q\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;},\n    }}\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e问题：\u003ccode\u003esearch\u003c/code\u003e 搜什么？\u0026quot;things\u0026quot; 是什么？参数 \u003ccode\u003eq\u003c/code\u003e 代表什么？LLM 无法准确判断何时应该调用这个工具。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e正面示例\u003c/strong\u003e：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eTool(\n    name=\u0026quot;search_jira_issues\u0026quot;,\n    description=(\n        \u0026quot;在 Jira 中搜索 issue。适用场景：用户想查找 bug、需求、任务等工单。\u0026quot;\n        \u0026quot;支持 JQL 语法。不适用于搜索 Confluence 文档或代码仓库。\u0026quot;\n        \u0026quot;返回匹配的 issue 列表，包含 key、标题、状态、负责人。\u0026quot;\n        \u0026quot;最多返回 50 条结果。\u0026quot;\n    ),\n    inputSchema={\u0026quot;type\u0026quot;: \u0026quot;object\u0026quot;, \u0026quot;properties\u0026quot;: {\n        \u0026quot;jql\u0026quot;: {\n            \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;,\n            \u0026quot;description\u0026quot;: \u0026quot;Jira Query Language 查询语句，例如: \u0026#39;project = BACKEND AND status = Open\u0026#39;\u0026quot;\n        },\n        \u0026quot;max_results\u0026quot;: {\n            \u0026quot;type\u0026quot;: \u0026quot;integer\u0026quot;,\n            \u0026quot;description\u0026quot;: \u0026quot;最大返回条数，默认 20，最大 50\u0026quot;,\n            \u0026quot;default\u0026quot;: 20\n        },\n    }, \u0026quot;required\u0026quot;: [\u0026quot;jql\u0026quot;]}\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e关键原则：\u003cstrong\u003e名称具体\u003c/strong\u003e（\u003ccode\u003esearch_jira_issues\u003c/code\u003e 而非 \u003ccode\u003esearch\u003c/code\u003e）、\u003cstrong\u003e描述含边界\u003c/strong\u003e（说清楚能做什么和不能做什么）、\u003cstrong\u003e参数有示例\u003c/strong\u003e（LLM 看到 JQL 示例才知道该用什么语法）、\u003cstrong\u003e返回值说明\u003c/strong\u003e（LLM 知道能拿到什么，才能决定要不要调用）。\u003c/p\u003e\n\u003ch3\u003e10.2 Server 粒度设计\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e保持 Server 单一职责\u003c/strong\u003e：\u003ccode\u003egithub-server\u003c/code\u003e、\u003ccode\u003edatabase-server\u003c/code\u003e、\u003ccode\u003eslack-server\u003c/code\u003e 而非 \u003ccode\u003eall-tools-server\u003c/code\u003e——独立升级、细粒度权限、缩小故障面。\u003c/p\u003e\n\u003cp\u003e但\u0026quot;单一职责\u0026quot;的粒度怎么把握？以下是决策框架：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e何时拆分 Server：\n  - 工具属于不同领域（GitHub vs Slack）         → 拆\n  - 工具需要不同权限凭证                        → 拆\n  - 工具有不同的故障域（一个挂了不该影响另一个）  → 拆\n  - 工具需要独立的部署和升级周期                 → 拆\n\n何时合并 Server：\n  - 工具间共享状态（同一数据库连接）             → 合\n  - 工具总是一起使用（read_file + write_file）   → 合\n  - 工具数量少（\u0026lt; 3）且属于同一上下文            → 合\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e实际案例——一个数据分析场景：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e❌ 过细：query-server, chart-server, export-server  （3 个进程管理成本高，且紧耦合）\n❌ 过粗：analytics-server（含 20 个工具，LLM 选择困难）\n✅ 合适：data-query-server（查询+聚合）, visualization-server（图表+导出）\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e10.3 测试策略\u003c/h3\u003e\n\u003cp\u003eMCP Server 本质是一个暴露工具的进程，需要三层测试覆盖：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e单元测试\u003c/strong\u003e：测试工具的核心逻辑，不涉及 MCP 协议。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport pytest\n\n# 直接测试业务逻辑函数，不通过 MCP 协议\nasync def test_list_tasks_filter_by_status():\n    result = filter_tasks(TASKS, status=\u0026quot;in_progress\u0026quot;)\n    assert len(result) == 1\n    assert \u0026quot;TASK-002\u0026quot; in result\n\nasync def test_update_task_nonexistent():\n    with pytest.raises(TaskNotFoundError):\n        update_task_status(\u0026quot;TASK-999\u0026quot;, \u0026quot;done\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e集成测试\u003c/strong\u003e：通过 MCP Client 连接 Server，测试完整的协议交互。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom mcp import ClientSession\nfrom mcp.client.stdio import stdio_client, StdioServerParameters\n\nasync def test_mcp_tool_call():\n    \u0026quot;\u0026quot;\u0026quot;通过 MCP 协议发起完整的工具调用\u0026quot;\u0026quot;\u0026quot;\n    params = StdioServerParameters(command=\u0026quot;python\u0026quot;, args=[\u0026quot;server.py\u0026quot;])\n    async with stdio_client(params) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n\n            # 验证工具列表\n            tools = await session.list_tools()\n            tool_names = [t.name for t in tools.tools]\n            assert \u0026quot;list_tasks\u0026quot; in tool_names\n\n            # 验证工具调用\n            result = await session.call_tool(\u0026quot;list_tasks\u0026quot;, {\u0026quot;status\u0026quot;: \u0026quot;todo\u0026quot;})\n            assert \u0026quot;TASK-003\u0026quot; in result.content[0].text\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eLLM 端到端测试\u003c/strong\u003e：验证 LLM 在给定上下文中能正确选择和使用工具。这类测试成本高、有非确定性，但对关键流程不可或缺。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def test_llm_selects_correct_tool():\n    \u0026quot;\u0026quot;\u0026quot;验证 LLM 面对用户意图时选择正确的工具\u0026quot;\u0026quot;\u0026quot;\n    tools = await get_tool_definitions()  # 从 MCP Server 获取\n    response = await llm.chat(\n        messages=[{\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;帮我看看 alice 有哪些待做的任务\u0026quot;}],\n        tools=tools\n    )\n    # 断言 LLM 选择了 list_tasks 而非 update_task_status\n    assert response.tool_calls[0].name == \u0026quot;list_tasks\u0026quot;\n    assert response.tool_calls[0].arguments[\u0026quot;assignee\u0026quot;] == \u0026quot;alice\u0026quot;\n    assert response.tool_calls[0].arguments[\u0026quot;status\u0026quot;] == \u0026quot;todo\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e做好错误处理\u003c/strong\u003e：MCP Server 的错误会进入 LLM 上下文。清晰的错误信息（\u0026quot;任务 TASK-999 不存在，请用 list_tasks 查看可用任务\u0026quot;）能帮助 LLM 自我纠正。详见 7.4 节的错误处理设计。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e11. 进一步思考\u003c/h2\u003e\n\u003cp\u003eMCP 正在快速演进，几个未解问题值得关注：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e工具组合\u003c/strong\u003e：工具 A 输出作为工具 B 输入时，由 LLM 串联（灵活但低效）还是协议层支持工具链（高效但复杂）？\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e有状态交互\u003c/strong\u003e：当前每次调用独立。但数据库事务、多步操作需要跨调用的状态。如何在协议层表达？\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e工具质量评估\u003c/strong\u003e：Agent 如何判断 MCP Server 的描述是否准确、响应是否可靠？需要\u0026quot;工具信誉系统\u0026quot;。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e多模态工具\u003c/strong\u003e：MCP 已支持 \u003ccode\u003eImageContent\u003c/code\u003e，但多模态生态仍在早期。\u003c/p\u003e\n\u003cp\u003e长远来看，工具协议化的终局可能是一个\u003cstrong\u003e去中心化的 Agent 工具市场\u003c/strong\u003e——发布 MCP Server 如同发布 npm 包，Agent 在运行时动态发现、评估、连接、使用工具。协议保证互操作性，市场机制保证质量。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e12. 总结\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e当前工具集成不可持续\u003c/strong\u003e。标准化协议将 N x M 降为 N + M。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMCP 设计务实\u003c/strong\u003e。三大原语覆盖主要交互模式，JSON-RPC 2.0 成熟可靠，双传输层适配不同场景。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e安全不是事后补丁\u003c/strong\u003e。ACL、参数约束、Human-in-the-Loop、审计日志需在架构设计阶段考虑。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e协议化成本可控\u003c/strong\u003e。性能可忽略，规模增长时收益迅速超过成本。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e保持务实的乐观\u003c/strong\u003e。MCP 目前最有前途，但要做好业务逻辑与协议层的解耦。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e工具协议化是 Agent 生态从\u0026quot;手工作坊\u0026quot;走向\u0026quot;工业化\u0026quot;的关键一步。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e系列导航\u003c/strong\u003e：本文是 Agentic 系列的第 13 篇。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e上一篇：\u003ca href=\"/blog/engineering/agentic/12-LangChain%20vs%20LangGraph\"\u003e12 | LangChain vs LangGraph\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下一篇：\u003ca href=\"/blog/engineering/agentic/14-Production-Grade%20Agent%20Systems\"\u003e14 | Production-Grade Agent Systems\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e完整目录：\u003ca href=\"/blog/engineering/agentic/01-From%20LLM%20to%20Agent\"\u003e01 | From LLM to Agent\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"article\",null,{\"className\":\"min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"nav\",null,{\"className\":\"flex items-center gap-1 text-sm mb-4\",\"children\":[[\"$\",\"$L13\",null,{\"href\":\"/blog/page/1\",\"className\":\"text-gray-500 hover:text-blue-600 transition-colors\",\"children\":\"博客\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"/\"}],[\"$\",\"$L13\",null,{\"href\":\"/blog/category/engineering/page/1\",\"className\":\"text-gray-500 hover:text-blue-600 transition-colors\",\"children\":\"Engineering\"}],[[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"/\"}],[\"$\",\"$L13\",null,{\"href\":\"/blog/category/engineering/agentic/page/1\",\"className\":\"text-blue-600 hover:text-blue-700 transition-colors\",\"children\":\"Agentic 系统\"}]]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center mb-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"w-4 h-4 mr-2 text-gray-400\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z\"}]}],[\"$\",\"time\",null,{\"dateTime\":\"2026-01-22\",\"children\":\"2026年01月22日\"}]]}]}],[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold text-gray-900 mb-6 text-center\",\"children\":\"LangChain vs LangGraph: 框架的价值与边界\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2 mb-6 justify-center\",\"children\":[[\"$\",\"$L13\",\"Agentic\",{\"href\":\"/blog/tag/Agentic/page/1/\",\"className\":\"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors\",\"children\":\"Agentic\"}],[\"$\",\"$L13\",\"AI Engineering\",{\"href\":\"/blog/tag/AI%20Engineering/page/1/\",\"className\":\"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors\",\"children\":\"AI Engineering\"}],[\"$\",\"$L13\",\"Framework\",{\"href\":\"/blog/tag/Framework/page/1/\",\"className\":\"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors\",\"children\":\"Framework\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"max-w-5xl mx-auto\",\"children\":[\"$\",\"$L14\",null,{\"content\":\"$15\"}]}],[\"$\",\"$10\",null,{\"fallback\":[\"$\",\"div\",null,{\"className\":\"mt-12 pt-8 border-t border-gray-200\",\"children\":\"加载导航中...\"}],\"children\":[\"$\",\"$L16\",null,{\"globalNav\":{\"prev\":{\"slug\":\"engineering/agentic/11-Multi-Agent Collaboration\",\"title\":\"Multi-Agent Collaboration: 多 Agent 协作模式与架构\",\"description\":\"单个 Agent 的能力有天花板——Context Window 有限、专业化受限、单点故障、串行瓶颈。本文系统拆解多 Agent 协作的四种核心模式（Supervisor-Worker、Peer-to-Peer、Pipeline、Dynamic Routing），深入 Agent 间通信机制、状态管理、错误处理与成本控制，并用 Python 从零实现一个 Supervisor-Worker 协作框架。\",\"pubDate\":\"2026-01-17\",\"tags\":[\"Agentic\",\"AI Engineering\",\"Multi-Agent\"],\"heroImage\":\"$undefined\",\"content\":\"$17\"},\"next\":{\"slug\":\"engineering/agentic/13-MCP and Tool Protocol\",\"title\":\"MCP and Tool Protocol: Agent 工具的协议化未来\",\"description\":\"当前 Agent 工具集成面临 N×M 问题：每个框架、每个应用都在重复造轮子。MCP（Model Context Protocol）正在尝试成为 Agent 工具世界的 HTTP——一个标准化的通信协议。本文深入剖析 MCP 的架构设计、通信机制与安全模型，探讨工具协议化的趋势、trade-off 与未来走向。\",\"pubDate\":\"2026-01-27\",\"tags\":[\"Agentic\",\"AI Engineering\",\"MCP\"],\"heroImage\":\"$undefined\",\"content\":\"$18\"}},\"tagNav\":{\"Agentic\":{\"prev\":\"$5:props:children:props:children:props:children:2:props:children:props:globalNav:prev\",\"next\":\"$5:props:children:props:children:props:children:2:props:children:props:globalNav:next\"},\"AI Engineering\":{\"prev\":\"$5:props:children:props:children:props:children:2:props:children:props:globalNav:prev\",\"next\":\"$5:props:children:props:children:props:children:2:props:children:props:globalNav:next\"},\"Framework\":{\"prev\":null,\"next\":null}}}]}],[\"$\",\"$L19\",null,{}]]}]}]}]\n"])</script><script>self.__next_f.push([1,"8:null\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n7:null\n"])</script><script>self.__next_f.push([1,"a:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"LangChain vs LangGraph: 框架的价值与边界 - Skyfalling Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Agentic 系列第 12 篇。客观审视 AI Agent 框架的价值与局限。深入分析 LangChain 的抽象模型与陷阱、LangGraph 的状态机优势与学习曲线，横向对比 CrewAI、AutoGen、Semantic Kernel 等框架，最终给出框架 vs 自研的决策矩阵。核心立场：理解原理再用框架，框架是加速器而非必需品。\"}],[\"$\",\"meta\",\"2\",{\"property\":\"og:title\",\"content\":\"LangChain vs LangGraph: 框架的价值与边界\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:description\",\"content\":\"Agentic 系列第 12 篇。客观审视 AI Agent 框架的价值与局限。深入分析 LangChain 的抽象模型与陷阱、LangGraph 的状态机优势与学习曲线，横向对比 CrewAI、AutoGen、Semantic Kernel 等框架，最终给出框架 vs 自研的决策矩阵。核心立场：理解原理再用框架，框架是加速器而非必需品。\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"5\",{\"property\":\"article:published_time\",\"content\":\"2026-01-22\"}],[\"$\",\"meta\",\"6\",{\"property\":\"article:author\",\"content\":\"Skyfalling\"}],[\"$\",\"meta\",\"7\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:title\",\"content\":\"LangChain vs LangGraph: 框架的价值与边界\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:description\",\"content\":\"Agentic 系列第 12 篇。客观审视 AI Agent 框架的价值与局限。深入分析 LangChain 的抽象模型与陷阱、LangGraph 的状态机优势与学习曲线，横向对比 CrewAI、AutoGen、Semantic Kernel 等框架，最终给出框架 vs 自研的决策矩阵。核心立场：理解原理再用框架，框架是加速器而非必需品。\"}],[\"$\",\"link\",\"10\",{\"rel\":\"shortcut icon\",\"href\":\"/favicon.png\"}],[\"$\",\"link\",\"11\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"link\",\"12\",{\"rel\":\"icon\",\"href\":\"/favicon.png\"}],[\"$\",\"link\",\"13\",{\"rel\":\"apple-touch-icon\",\"href\":\"/favicon.png\"}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"12:{\"metadata\":\"$a:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>