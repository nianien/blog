1:"$Sreact.fragment"
2:I[10616,["6874","static/chunks/6874-7791217feaf05c17.js","7177","static/chunks/app/layout-142e67ac4336647c.js"],"default"]
3:I[87555,[],""]
4:I[31295,[],""]
6:I[59665,[],"OutletBoundary"]
9:I[74911,[],"AsyncMetadataOutlet"]
b:I[59665,[],"ViewportBoundary"]
d:I[59665,[],"MetadataBoundary"]
f:I[26614,[],""]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/232416e7c3a1ca7e.css","style"]
0:{"P":null,"b":"Ugi13mxW3xqx8EX5Ds9lw","p":"","c":["","blog","engineering","agentic","01-From%20LLM%20to%20Agent",""],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","engineering/agentic/01-From%20LLM%20to%20Agent","c"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/232416e7c3a1ca7e.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"zh-CN","children":["$","body",null,{"className":"__className_f367f3","children":["$","div",null,{"className":"min-h-screen flex flex-col","children":[["$","$L2",null,{}],["$","main",null,{"className":"flex-1","children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"bg-[var(--background)]","children":["$","div",null,{"className":"mx-auto max-w-7xl px-6 py-12 lg:px-8","children":["$","p",null,{"className":"text-center text-xs leading-5 text-gray-400","children":["© ",2026," Skyfalling"]}]}]}]]}]}]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","engineering/agentic/01-From%20LLM%20to%20Agent","c"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L5",null,["$","$L6",null,{"children":["$L7","$L8",["$","$L9",null,{"promise":"$@a"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","WgPzrbhwCUhkKfXv7dY52v",{"children":[["$","$Lb",null,{"children":"$Lc"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$Ld",null,{"children":"$Le"}]]}],false]],"m":"$undefined","G":["$f","$undefined"],"s":false,"S":true}
10:"$Sreact.suspense"
11:I[74911,[],"AsyncMetadata"]
13:I[6874,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],""]
14:I[32923,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
16:I[40780,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
1a:I[85300,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
e:["$","div",null,{"hidden":true,"children":["$","$10",null,{"fallback":null,"children":["$","$L11",null,{"promise":"$@12"}]}]}]
15:T9629,<h1>From LLM to Agent: Agentic 系统的知识地图</h1>
<blockquote>
<p>大语言模型是一个令人惊叹的函数：Text In, Text Out。但函数不等于系统，生成不等于行动，回答不等于解决。</p>
<p>本文是 Agentic 系列 14 篇文章的开篇。我们将从&quot;LLM 能做什么&quot;出发，推导出&quot;Agent 必须做什么&quot;，然后为整个系列绘制一张完整的知识地图。</p>
</blockquote>
<hr>
<h2>1. 为什么需要从 LLM 走向 Agent</h2>
<h3>1.1 LLM 是一个了不起的函数</h3>
<p>2022 年底以来，以 GPT-4、Claude、Gemini 为代表的大语言模型展示了令人印象深刻的能力：理解自然语言、生成结构化文本、进行多步推理、甚至通过各类考试。但如果我们冷静地回到工程视角，LLM 本质上是一个<strong>无状态的文本映射函数</strong>：</p>
<pre><code>f(prompt: str, context: str) → response: str
</code></pre>
<p>它接收一段文本，返回一段文本。仅此而已。</p>
<h3>1.2 LLM 的五个结构性局限</h3>
<p>当你试图用 LLM 解决真实世界的任务时，会迅速撞上以下墙壁：</p>
<table>
<thead>
<tr>
<th>局限</th>
<th>本质原因</th>
<th>后果</th>
</tr>
</thead>
<tbody><tr>
<td><strong>知识静态</strong></td>
<td>训练数据有截止日期</td>
<td>无法回答实时问题，产生幻觉</td>
</tr>
<tr>
<td><strong>无法行动</strong></td>
<td>输出是文本，不是可执行指令</td>
<td>不能查数据库、调 API、操作文件</td>
</tr>
<tr>
<td><strong>记忆易失</strong></td>
<td>上下文窗口有限且无持久状态</td>
<td>长对话丢失信息，跨会话失忆</td>
</tr>
<tr>
<td><strong>单步思维</strong></td>
<td>一次 completion 只做一次推理</td>
<td>复杂任务无法分解、无法迭代</td>
</tr>
<tr>
<td><strong>不会反思</strong></td>
<td>不检查自己的输出质量</td>
<td>错误会被自信地传递下去</td>
</tr>
</tbody></table>
<p>这五个局限不是&quot;模型不够大&quot;能解决的问题——它们是<strong>架构层面的缺失</strong>。更大的模型只是让函数 <code>f</code> 更强，但不会让函数变成系统。</p>
<h3>1.3 从函数到系统的必然性</h3>
<p>真实世界的任务天然具有以下特征：</p>
<ul>
<li><strong>需要多步执行</strong>：完成一次数据分析需要查询 → 清洗 → 计算 → 可视化</li>
<li><strong>需要外部交互</strong>：查实时数据、调第三方 API、读写文件</li>
<li><strong>需要持久记忆</strong>：记住用户偏好、历史决策、领域知识</li>
<li><strong>需要自我纠错</strong>：发现错误后能回退、重试、换策略</li>
<li><strong>需要可靠执行</strong>：有超时、有重试、有降级、有审计</li>
</ul>
<p>当这些需求叠加在一起，你需要的不再是一个&quot;更好的 prompt&quot;，而是一个<strong>围绕 LLM 构建的系统</strong>。这个系统，就是 Agent。</p>
<hr>
<h2>2. 定义 Agent</h2>
<h3>2.1 一个精确的定义</h3>
<p><strong>Agent = LLM + Memory + Tools + Planner + Runtime</strong></p>
<p>这不是随意的拼凑，而是对上一节五个局限的逐一回应：</p>
<pre><code>局限：知识静态     → 解法：Memory（外部知识 + RAG）
局限：无法行动     → 解法：Tools（函数调用 + 外部接口）
局限：记忆易失     → 解法：Memory（会话状态 + 持久化记忆）
局限：单步思维     → 解法：Planner（任务分解 + 多步规划）
局限：不会反思     → 解法：Runtime（控制循环 + 反思机制）
</code></pre>
<p>每个组件都有明确的职责：</p>
<ul>
<li><strong>LLM</strong>：核心推理引擎。理解意图、生成计划、选择工具、产出结果。它是&quot;大脑&quot;，但不是全部。</li>
<li><strong>Memory</strong>：分为短期记忆（当前对话上下文、工作区状态）和长期记忆（向量数据库中的文档、用户画像、历史经验）。短期记忆保证连贯性，长期记忆突破知识边界。</li>
<li><strong>Tools</strong>：Agent 与外部世界的接口。一个 Tool 就是一个带有 JSON Schema 描述的可调用函数。搜索引擎、数据库查询、代码执行器、API 网关——都是 Tool。</li>
<li><strong>Planner</strong>：将复杂任务分解为可执行的子步骤。从简单的 ReAct（交替推理和行动）到复杂的分层规划（Hierarchical Planning），Planner 决定了 Agent 的&quot;智商上限&quot;。</li>
<li><strong>Runtime</strong>：Agent 的执行环境。负责控制循环的调度、工具调用的执行、错误处理、超时控制、状态持久化。没有 Runtime，前面四个组件只是散落的零件。</li>
</ul>
<h3>2.2 Agent 与 LLM 的本质差异</h3>
<p>用一个类比来强化理解：</p>
<pre><code>LLM  ≈ CPU             —— 强大的计算单元，但单独无法工作
Agent ≈ Operating System —— 围绕 CPU 构建的完整运行时

LLM  是 Pure Function   —— 相同输入，相同输出，无副作用
Agent 是 Stateful System —— 有状态、有副作用、有执行循环
</code></pre>
<p>这个区分极其重要。很多团队把 LLM 当 Agent 用（期望一次 prompt 解决所有问题），或者把 Agent 当 LLM 用（忽略控制循环和状态管理），都会走进死胡同。</p>
<hr>
<h2>3. Agent 的核心控制循环</h2>
<p>Agent 之所以能完成复杂任务，核心在于它运行一个<strong>持续的控制循环</strong>。这个循环可以抽象为六个阶段：</p>
<pre><code>                    ┌──────────────────────────────────┐
                    │         Agent Control Loop        │
                    └──────────────────────────────────┘

                           ┌─────────────┐
                     ┌────▶│   Observe   │─────┐
                     │     │ (感知输入)   │     │
                     │     └─────────────┘     │
                     │                          ▼
              ┌──────┴──────┐           ┌─────────────┐
              │    Update   │           │    Think    │
              │ (更新状态)   │           │ (理解意图)   │
              └──────┬──────┘           └──────┬──────┘
                     ▲                          │
                     │                          ▼
              ┌──────┴──────┐           ┌─────────────┐
              │   Reflect   │           │    Plan     │
              │ (评估结果)   │◀──────────│ (制定计划)   │
              └─────────────┘           └──────┬──────┘
                                               │
                                               ▼
                                        ┌─────────────┐
                                        │     Act     │
                                        │ (执行动作)   │
                                        └─────────────┘
</code></pre>
<p>各阶段职责：</p>
<ol>
<li><strong>Observe（感知）</strong>：接收用户输入或环境变化。不仅是文本——可能是工具返回的结果、系统事件、定时触发。</li>
<li><strong>Think（思考）</strong>：LLM 理解当前状态和目标。这一步对应 prompt 中的 System Message 和上下文组装。</li>
<li><strong>Plan（规划）</strong>：决定下一步做什么。可能是调用工具、请求更多信息、或直接回答。ReAct 框架在此步生成 Thought + Action。</li>
<li><strong>Act（执行）</strong>：真正执行动作。调用 API、查询数据库、运行代码、生成文件。这一步有<strong>副作用</strong>。</li>
<li><strong>Reflect（反思）</strong>：检查执行结果是否符合预期。结果有错误？重试。结果不完整？补充。任务完成？退出循环。</li>
<li><strong>Update（更新）</strong>：将本轮的观察、决策、结果写入记忆。更新会话上下文，可能也写入长期记忆。</li>
</ol>
<p><strong>关键设计决策：何时退出循环？</strong></p>
<p>这是 Agent 设计中最容易被忽视的问题。常见策略：</p>
<ul>
<li><strong>Max Iterations</strong>：硬性限制最大循环次数（防止无限循环和 token 爆炸）</li>
<li><strong>Goal Completion</strong>：LLM 判断任务已完成（但 LLM 判断可能不准）</li>
<li><strong>Confidence Threshold</strong>：当 Reflect 阶段的置信度低于阈值时，请求人类介入</li>
<li><strong>Token Budget</strong>：累计 token 消耗达到上限时强制退出</li>
</ul>
<p>在生产系统中，通常需要<strong>组合多种策略</strong>，以 Max Iterations 作为保底。</p>
<hr>
<h2>4. Agentic 系统的全景架构</h2>
<p>下面这张图展示了一个完整的 Agentic 系统的分层架构。它是整个系列 14 篇文章的&quot;地图&quot;：</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                     Production Layer                                │
│  Observability │ Evaluation │ Security │ Cost Control │ Deployment  │
├─────────────────────────────────────────────────────────────────────┤
│                     Protocol Layer                                  │
│         MCP (Model Context Protocol) │ Tool Registry               │
│         Capability Declaration │ Permission Control                 │
├─────────────────────────────────────────────────────────────────────┤
│                     Multi-Agent Layer                               │
│    Supervisor/Worker │ Peer-to-Peer │ Graph-based Orchestration    │
│    Message Passing │ Shared State │ Agent Registry                  │
├─────────────────────────────────────────────────────────────────────┤
│                     Planner Layer                                   │
│    ReAct │ Chain-of-Thought │ Tree-of-Thought │ Hierarchical Plan  │
│    Task Decomposition │ Self-Evaluation │ Retry Budget              │
├─────────────────────────────────────────────────────────────────────┤
│                     Memory Layer                                    │
│    Short-term: Conversation State │ Working Memory                  │
│    Long-term: Vector DB │ Knowledge Graph │ User Profile            │
│    RAG Pipeline: Chunk → Embed → Index → Retrieve → Rerank         │
├─────────────────────────────────────────────────────────────────────┤
│                     Tool Layer                                      │
│    Function Calling │ JSON Schema │ Structured Output               │
│    Tool Validation │ Sandbox Execution │ Error Handling             │
├─────────────────────────────────────────────────────────────────────┤
│                     Control Loop Layer                              │
│    Observe → Think → Plan → Act → Reflect → Update                 │
│    State Machine │ Execution Engine │ Interrupt &amp; Resume            │
├─────────────────────────────────────────────────────────────────────┤
│                     LLM Runtime Layer                               │
│    ChatCompletion API │ Streaming │ Token Management                │
│    Model Router │ Fallback │ Rate Limiting │ Caching               │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>架构解读</strong>：</p>
<ul>
<li><strong>自底向上</strong>：每一层为上一层提供能力。LLM Runtime 提供推理能力，Control Loop 提供执行循环，Tool 提供行动能力，Memory 提供持久化，Planner 提供智能规划，Multi-Agent 提供协作，Protocol 提供互操作性，Production 提供生产级保障。</li>
<li><strong>耦合方向</strong>：上层依赖下层，但下层不应感知上层。Tool Layer 不需要知道自己被 Multi-Agent 调用还是 Single-Agent 调用。</li>
<li><strong>灵活组合</strong>：不是每个系统都需要所有层。一个简单的 RAG 聊天机器人可能只需要 LLM Runtime + Memory Layer。一个自动化运维 Agent 可能需要 Control Loop + Tool + Planner。架构图是上界，不是下界。</li>
</ul>
<hr>
<h2>5. 14 篇文章导航地图</h2>
<p>以下是整个系列的文章列表，以及每篇文章对应全景图中的位置：</p>
<h3>Phase 1: What Is an Agent?</h3>
<table>
<thead>
<tr>
<th>#</th>
<th>文章</th>
<th>聚焦层</th>
</tr>
</thead>
<tbody><tr>
<td><strong>01</strong></td>
<td><strong>From LLM to Agent: Agentic 系统的知识地图</strong> ← 本文</td>
<td>全景总览</td>
</tr>
<tr>
<td>02</td>
<td>From Prompt to Agent: 为什么 LLM 本身不是 Agent</td>
<td>LLM Runtime → Control Loop</td>
</tr>
<tr>
<td>03</td>
<td>Agent vs Workflow vs Automation: 选对抽象才是关键</td>
<td>架构决策</td>
</tr>
</tbody></table>
<h3>Phase 2: How to Program an Agent?</h3>
<table>
<thead>
<tr>
<th>#</th>
<th>文章</th>
<th>聚焦层</th>
</tr>
</thead>
<tbody><tr>
<td>04</td>
<td>The Agent Control Loop: Agent 运行时的核心抽象</td>
<td>Control Loop Layer</td>
</tr>
<tr>
<td>05</td>
<td>Tool Calling Deep Dive: 让 LLM 成为可编程接口</td>
<td>Tool Layer</td>
</tr>
<tr>
<td>06</td>
<td>Prompt Engineering for Agents: 面向 Agent 的提示词工程</td>
<td>LLM Runtime + Planner</td>
</tr>
<tr>
<td>07</td>
<td>Agent Runtime from Scratch: 不依赖框架构建 Agent</td>
<td>Control Loop + Tool + Memory</td>
</tr>
</tbody></table>
<h3>Phase 3: How to Scale Agent Intelligence?</h3>
<table>
<thead>
<tr>
<th>#</th>
<th>文章</th>
<th>聚焦层</th>
</tr>
</thead>
<tbody><tr>
<td>08</td>
<td>Memory Architecture: Agent 的状态与记忆体系</td>
<td>Memory Layer</td>
</tr>
<tr>
<td>09</td>
<td>RAG as Cognitive Memory: 检索增强生成的工程实践</td>
<td>Memory Layer (RAG)</td>
</tr>
<tr>
<td>10</td>
<td>Planning and Reflection: 从 ReAct 到分层规划</td>
<td>Planner Layer</td>
</tr>
<tr>
<td>11</td>
<td>Multi-Agent Collaboration: 多 Agent 协作模式</td>
<td>Multi-Agent Layer</td>
</tr>
</tbody></table>
<h3>Phase 4: How to Ship Agents to Production?</h3>
<table>
<thead>
<tr>
<th>#</th>
<th>文章</th>
<th>聚焦层</th>
</tr>
</thead>
<tbody><tr>
<td>12</td>
<td>LangChain vs LangGraph: 框架的价值与边界</td>
<td>Control Loop + Tool (框架视角)</td>
</tr>
<tr>
<td>13</td>
<td>MCP and Tool Protocol: Agent 工具的协议化未来</td>
<td>Protocol Layer</td>
</tr>
<tr>
<td>14</td>
<td>Production-Grade Agent Systems: 评估、成本与安全</td>
<td>Production Layer</td>
</tr>
</tbody></table>
<p>每篇文章都可以独立阅读，但按顺序阅读可以获得最连贯的知识构建过程。</p>
<hr>
<h2>6. 从 ChatCompletion 到 Agent 的演进路径</h2>
<p>下面通过代码展示从最简单的 API 调用到完整 Agent 的逐步演进。每一级都在前一级的基础上增加一个关键能力。理解这个演进过程，就理解了 Agent 的设计逻辑。</p>
<h3>Level 0: 单次 ChatCompletion</h3>
<p>最基础的用法——一问一答，无状态，无工具。</p>
<pre><code class="language-python">import openai

def chat(user_message: str) -&gt; str:
    &quot;&quot;&quot;Level 0: 纯粹的 LLM 调用，Text In → Text Out&quot;&quot;&quot;
    response = openai.chat.completions.create(
        model=&quot;gpt-4o&quot;,
        messages=[
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_message},
        ],
    )
    return response.choices[0].message.content

# 能力边界：只能回答训练数据内的问题，无法查实时数据，无法执行动作
</code></pre>
<p><strong>局限</strong>：这就是一个函数调用。它不知道今天是星期几，不能帮你查天气，不记得你上一句说了什么。</p>
<h3>Level 1: + Tool Calling</h3>
<p>让 LLM 能够调用外部函数，从&quot;能说&quot;进化到&quot;能做&quot;。</p>
<pre><code class="language-python">import json

# 定义工具：用 JSON Schema 描述函数签名
tools = [
    {
        &quot;type&quot;: &quot;function&quot;,
        &quot;function&quot;: {
            &quot;name&quot;: &quot;get_weather&quot;,
            &quot;description&quot;: &quot;获取指定城市的当前天气&quot;,
            &quot;parameters&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;city&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;城市名称&quot;}
                },
                &quot;required&quot;: [&quot;city&quot;],
            },
        },
    }
]

# 工具实现
def get_weather(city: str) -&gt; str:
    # 实际场景中调用天气 API
    return json.dumps({&quot;city&quot;: city, &quot;temp&quot;: &quot;22°C&quot;, &quot;condition&quot;: &quot;晴&quot;})

# 工具注册表：名称 → 函数的映射
tool_registry = {&quot;get_weather&quot;: get_weather}

def chat_with_tools(user_message: str) -&gt; str:
    &quot;&quot;&quot;Level 1: LLM + Tool Calling&quot;&quot;&quot;
    messages = [
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_message},
    ]

    response = openai.chat.completions.create(
        model=&quot;gpt-4o&quot;,
        messages=messages,
        tools=tools,
    )

    msg = response.choices[0].message

    # 如果 LLM 决定调用工具
    if msg.tool_calls:
        # 执行工具调用
        for tool_call in msg.tool_calls:
            fn_name = tool_call.function.name
            fn_args = json.loads(tool_call.function.arguments)
            result = tool_registry[fn_name](**fn_args)

            # 将工具结果反馈给 LLM
            messages.append(msg)
            messages.append({
                &quot;role&quot;: &quot;tool&quot;,
                &quot;tool_call_id&quot;: tool_call.id,
                &quot;content&quot;: result,
            })

        # LLM 根据工具结果生成最终回答
        final = openai.chat.completions.create(
            model=&quot;gpt-4o&quot;, messages=messages
        )
        return final.choices[0].message.content

    return msg.content
</code></pre>
<p><strong>进步</strong>：LLM 现在能&quot;做事&quot;了——但只能做一步。如果任务需要先查天气、再查航班、最后订酒店，这个结构无法处理。</p>
<h3>Level 2: + Control Loop</h3>
<p>引入循环，让 Agent 能够多步执行、迭代推进。</p>
<pre><code class="language-python">MAX_ITERATIONS = 10

def agent_loop(user_message: str) -&gt; str:
    &quot;&quot;&quot;Level 2: LLM + Tools + Control Loop&quot;&quot;&quot;
    messages = [
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant with tools.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_message},
    ]

    for i in range(MAX_ITERATIONS):
        response = openai.chat.completions.create(
            model=&quot;gpt-4o&quot;, messages=messages, tools=tools
        )
        msg = response.choices[0].message
        messages.append(msg)

        # 退出条件：LLM 不再请求工具调用，认为任务完成
        if not msg.tool_calls:
            return msg.content

        # 执行所有工具调用
        for tool_call in msg.tool_calls:
            fn_name = tool_call.function.name
            fn_args = json.loads(tool_call.function.arguments)

            try:
                result = tool_registry[fn_name](**fn_args)
            except Exception as e:
                result = json.dumps({&quot;error&quot;: str(e)})

            messages.append({
                &quot;role&quot;: &quot;tool&quot;,
                &quot;tool_call_id&quot;: tool_call.id,
                &quot;content&quot;: result,
            })

    return &quot;达到最大迭代次数，任务未完成。&quot;
</code></pre>
<p><strong>进步</strong>：Agent 现在能连续执行多步操作。但它没有记忆——每次对话从零开始，也没有规划能力——走一步看一步。</p>
<h3>Level 3: + Memory</h3>
<p>加入记忆系统，让 Agent 能跨步骤、甚至跨会话地积累信息。</p>
<pre><code class="language-python">from dataclasses import dataclass, field
from typing import Any

@dataclass
class AgentMemory:
    &quot;&quot;&quot;Agent 的记忆系统&quot;&quot;&quot;
    # 短期记忆：当前会话的消息历史
    conversation: list[dict] = field(default_factory=list)
    # 工作记忆：当前任务的中间状态
    working: dict[str, Any] = field(default_factory=dict)
    # 长期记忆：跨会话持久化（简化版，生产中用向量数据库）
    long_term: list[dict] = field(default_factory=list)

    def add_message(self, message: dict):
        self.conversation.append(message)

    def store_fact(self, key: str, value: Any):
        &quot;&quot;&quot;存入工作记忆&quot;&quot;&quot;
        self.working[key] = value

    def commit_to_long_term(self, summary: str):
        &quot;&quot;&quot;将重要信息提交到长期记忆&quot;&quot;&quot;
        self.long_term.append({
            &quot;summary&quot;: summary,
            &quot;timestamp&quot;: __import__(&quot;time&quot;).time(),
        })

    def get_context_window(self, max_messages: int = 20) -&gt; list[dict]:
        &quot;&quot;&quot;获取上下文窗口：最近的消息 + 长期记忆摘要&quot;&quot;&quot;
        context = []
        # 注入长期记忆摘要
        if self.long_term:
            memory_text = &quot;\n&quot;.join(m[&quot;summary&quot;] for m in self.long_term[-5:])
            context.append({
                &quot;role&quot;: &quot;system&quot;,
                &quot;content&quot;: f&quot;你的长期记忆：\n{memory_text}&quot;,
            })
        # 最近的对话消息
        context.extend(self.conversation[-max_messages:])
        return context


def agent_with_memory(user_message: str, memory: AgentMemory) -&gt; str:
    &quot;&quot;&quot;Level 3: LLM + Tools + Control Loop + Memory&quot;&quot;&quot;
    memory.add_message({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_message})

    system_prompt = {
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: &quot;You are a helpful assistant. Use your memory and tools.&quot;,
    }
    messages = [system_prompt] + memory.get_context_window()

    for i in range(MAX_ITERATIONS):
        response = openai.chat.completions.create(
            model=&quot;gpt-4o&quot;, messages=messages, tools=tools
        )
        msg = response.choices[0].message
        memory.add_message(msg.model_dump())

        if not msg.tool_calls:
            # 任务完成，考虑是否需要存入长期记忆
            return msg.content

        for tool_call in msg.tool_calls:
            fn_name = tool_call.function.name
            fn_args = json.loads(tool_call.function.arguments)
            try:
                result = tool_registry[fn_name](**fn_args)
                # 将关键结果存入工作记忆
                memory.store_fact(f&quot;{fn_name}_result&quot;, result)
            except Exception as e:
                result = json.dumps({&quot;error&quot;: str(e)})

            tool_msg = {
                &quot;role&quot;: &quot;tool&quot;,
                &quot;tool_call_id&quot;: tool_call.id,
                &quot;content&quot;: result,
            }
            memory.add_message(tool_msg)

        messages = [system_prompt] + memory.get_context_window()

    return &quot;达到最大迭代次数。&quot;
</code></pre>
<p><strong>进步</strong>：Agent 有了&quot;记性&quot;。但它仍然是 reactive 的——一步一步地响应，没有全局计划。</p>
<h3>Level 4: + Planner</h3>
<p>加入规划能力，让 Agent 先思考再行动。这是 ReAct 模式的核心思想。</p>
<pre><code class="language-python">PLANNER_PROMPT = &quot;&quot;&quot;你是一个任务规划器。给定用户的目标，你需要：
1. 将目标分解为具体的子步骤
2. 为每个步骤指定需要的工具
3. 标明步骤间的依赖关系
4. 输出 JSON 格式的计划

输出格式：
{
  &quot;goal&quot;: &quot;用户目标&quot;,
  &quot;steps&quot;: [
    {&quot;id&quot;: 1, &quot;action&quot;: &quot;描述&quot;, &quot;tool&quot;: &quot;工具名或null&quot;, &quot;depends_on&quot;: []},
    ...
  ]
}
&quot;&quot;&quot;

def plan_task(goal: str) -&gt; dict:
    &quot;&quot;&quot;使用 LLM 生成执行计划&quot;&quot;&quot;
    response = openai.chat.completions.create(
        model=&quot;gpt-4o&quot;,
        messages=[
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: PLANNER_PROMPT},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: goal},
        ],
        response_format={&quot;type&quot;: &quot;json_object&quot;},
    )
    return json.loads(response.choices[0].message.content)


REFLECT_PROMPT = &quot;&quot;&quot;你是一个任务审查器。根据以下信息判断：
- 原始目标：{goal}
- 已执行步骤：{executed_steps}
- 当前结果：{current_result}

请回答：
1. 任务是否已完成？(yes/no)
2. 如果未完成，下一步应该做什么？
3. 是否需要修改原计划？
&quot;&quot;&quot;

def agent_with_planner(user_message: str, memory: AgentMemory) -&gt; str:
    &quot;&quot;&quot;Level 4: LLM + Tools + Loop + Memory + Planner&quot;&quot;&quot;
    # Phase 1: Plan
    plan = plan_task(user_message)
    memory.store_fact(&quot;plan&quot;, plan)

    executed = []

    # Phase 2: Execute plan step by step
    for step in plan.get(&quot;steps&quot;, []):
        # 检查依赖是否满足
        deps = step.get(&quot;depends_on&quot;, [])
        if not all(d in [s[&quot;id&quot;] for s in executed] for d in deps):
            continue

        if step.get(&quot;tool&quot;):
            # 通过 agent_loop 执行工具调用
            result = agent_loop(
                f&quot;执行以下步骤：{step[&#39;action&#39;]}。只使用 {step[&#39;tool&#39;]} 工具。&quot;
            )
        else:
            result = agent_loop(step[&quot;action&quot;])

        executed.append({&quot;id&quot;: step[&quot;id&quot;], &quot;result&quot;: result})

    # Phase 3: Reflect
    reflection_prompt = REFLECT_PROMPT.format(
        goal=user_message,
        executed_steps=json.dumps(executed, ensure_ascii=False),
        current_result=executed[-1][&quot;result&quot;] if executed else &quot;无结果&quot;,
    )

    final = openai.chat.completions.create(
        model=&quot;gpt-4o&quot;,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: reflection_prompt}],
    )

    return final.choices[0].message.content
</code></pre>
<p><strong>进步</strong>：Agent 现在会&quot;想了再做&quot;。但这还不是终态。</p>
<h3>Level 5: Full Agent System</h3>
<p>完整的 Agent 系统不只是上述组件的堆叠，还需要生产级的工程保障：</p>
<pre><code class="language-python">@dataclass
class AgentConfig:
    &quot;&quot;&quot;Agent 系统配置&quot;&quot;&quot;
    model: str = &quot;gpt-4o&quot;
    max_iterations: int = 10
    max_tokens_budget: int = 50000       # token 预算上限
    tool_timeout_seconds: int = 30       # 工具调用超时
    enable_reflection: bool = True       # 是否启用反思
    enable_planning: bool = True         # 是否启用规划
    fallback_model: str = &quot;gpt-4o-mini&quot;  # 降级模型


class Agent:
    &quot;&quot;&quot;Level 5: 完整的 Agent 系统骨架&quot;&quot;&quot;

    def __init__(self, config: AgentConfig):
        self.config = config
        self.memory = AgentMemory()
        self.tools = ToolRegistry()       # 工具注册中心
        self.planner = Planner(config)    # 规划器
        self.observer = Observer()        # 可观测性（trace/log/metrics）
        self.token_usage = 0             # token 消耗追踪

    def run(self, user_input: str) -&gt; str:
        &quot;&quot;&quot;Agent 主入口：完整的控制循环&quot;&quot;&quot;
        self.observer.trace_start(user_input)

        try:
            # 1. Observe: 接收输入，组装上下文
            context = self._observe(user_input)

            # 2. Plan: 如果启用规划，先生成执行计划
            plan = None
            if self.config.enable_planning:
                plan = self.planner.create_plan(context)
                self.observer.log_plan(plan)

            # 3. Execute: 控制循环
            result = self._execute_loop(context, plan)

            # 4. Reflect: 如果启用反思，评估结果质量
            if self.config.enable_reflection:
                result = self._reflect_and_refine(context, result)

            # 5. Update: 更新记忆
            self.memory.commit_to_long_term(
                f&quot;用户问: {user_input[:100]}... → 结果: {result[:100]}...&quot;
            )

            self.observer.trace_end(result, self.token_usage)
            return result

        except Exception as e:
            self.observer.trace_error(e)
            return f&quot;Agent 执行出错: {str(e)}&quot;

    def _observe(self, user_input: str) -&gt; dict:
        &quot;&quot;&quot;感知阶段：组装完整上下文&quot;&quot;&quot;
        return {
            &quot;user_input&quot;: user_input,
            &quot;conversation&quot;: self.memory.get_context_window(),
            &quot;working_memory&quot;: self.memory.working,
            &quot;available_tools&quot;: self.tools.list_schemas(),
        }

    def _execute_loop(self, context: dict, plan: dict | None) -&gt; str:
        &quot;&quot;&quot;核心执行循环&quot;&quot;&quot;
        steps = plan[&quot;steps&quot;] if plan else [{&quot;action&quot;: context[&quot;user_input&quot;]}]

        results = []
        for step in steps:
            for i in range(self.config.max_iterations):
                # 预算检查
                if self.token_usage &gt; self.config.max_tokens_budget:
                    return &quot;Token 预算耗尽，任务中断。&quot;

                # LLM 推理（含自动降级）
                response = self._call_llm(context, step)

                if response.tool_calls:
                    self._execute_tools(response.tool_calls)
                else:
                    results.append(response.content)
                    break

        return &quot;\n&quot;.join(results)

    def _call_llm(self, context, step):
        &quot;&quot;&quot;LLM 调用，含降级逻辑&quot;&quot;&quot;
        try:
            return self._invoke(self.config.model, context, step)
        except Exception:
            # 降级到备用模型
            return self._invoke(self.config.fallback_model, context, step)

    # ... 省略 _execute_tools, _reflect_and_refine 等实现细节
</code></pre>
<p><strong>这不是最终代码，而是架构骨架。</strong> 生产系统还需要：并发控制、幂等性保证、结构化日志、指标采集、灰度发布、A/B 测试、成本告警等。这些内容将在系列后续文章中逐一展开。</p>
<h3>演进路径总结</h3>
<pre><code>Level 0   Level 1     Level 2        Level 3         Level 4         Level 5
 LLM ───→ +Tools ───→ +Loop ───→ +Memory ───→ +Planner ───→ +Production
  │          │           │           │             │              │
  │          │           │           │             │              │
单次调用   一步行动    多步执行    有记忆的      有规划的      生产级
无状态     无循环     有迭代       迭代执行      智能执行      完整系统
</code></pre>
<p>每一级都引入一个<strong>新的能力维度</strong>，也同时引入<strong>新的复杂度和 trade-off</strong>。不是所有场景都需要 Level 5。选择哪个级别，取决于你的任务复杂度和工程约束。</p>
<hr>
<h2>7. Agent 不是银弹</h2>
<h3>7.1 适用场景</h3>
<p>Agent 擅长处理以下类型的任务：</p>
<ul>
<li><strong>探索性任务</strong>：不确定最终需要几步、用什么工具才能完成。例：研究某个技术方案的可行性。</li>
<li><strong>多工具协作</strong>：需要组合多个 API/数据源的信息。例：跨平台数据聚合分析。</li>
<li><strong>需要迭代优化</strong>：初版结果不够好，需要反思和改进。例：代码生成 + 自动测试 + 修复。</li>
<li><strong>半结构化流程</strong>：有大致方向但细节灵活。例：客户支持中的问题诊断。</li>
</ul>
<h3>7.2 不适用场景</h3>
<p>Agent 在以下场景中可能是错误的选择：</p>
<ul>
<li><strong>确定性流程</strong>：如果你能用 DAG 或状态机画出完整流程，用 Workflow 引擎比 Agent 更可靠、更可预测、更便宜。Agent 的价值在于处理&quot;不确定性&quot;——如果没有不确定性，你不需要 Agent。</li>
<li><strong>低延迟要求</strong>：Agent 的控制循环意味着多次 LLM 调用，延迟以秒计。对于需要毫秒级响应的场景，Agent 不合适。</li>
<li><strong>高精度要求 + 零容错</strong>：金融交易、医疗诊断等场景。LLM 的概率性本质意味着 Agent 不能保证 100% 正确。它可以辅助决策，但不应成为最终决策者。</li>
<li><strong>简单的问答</strong>：如果用户只是问&quot;1+1等于几&quot;，一次 ChatCompletion 足矣，不需要 Agent 的全部架构。</li>
</ul>
<h3>7.3 关键 Trade-off</h3>
<table>
<thead>
<tr>
<th>维度</th>
<th>更多 Agent 能力</th>
<th>代价</th>
</tr>
</thead>
<tbody><tr>
<td>自主性</td>
<td>Agent 自主决策，减少人工干预</td>
<td>不可预测行为，调试困难</td>
</tr>
<tr>
<td>复杂度</td>
<td>能处理更复杂的任务</td>
<td>系统复杂度指数增长</td>
</tr>
<tr>
<td>成本</td>
<td>每个任务消耗更多 token</td>
<td>月度 API 账单可能惊人</td>
</tr>
<tr>
<td>延迟</td>
<td>多步推理产出更好结果</td>
<td>用户等待时间更长</td>
</tr>
<tr>
<td>可靠性</td>
<td>有反思和重试机制</td>
<td>但每一步都可能出错，错误会累积</td>
</tr>
</tbody></table>
<p><strong>核心决策原则</strong>：</p>
<blockquote>
<p>用最简单的抽象解决问题。如果 prompt engineering 够用，不要上 Agent。如果 Agent 够用，不要上 Multi-Agent。每增加一层抽象，都要问自己：这层抽象带来的能力提升，是否值得它引入的复杂度？</p>
</blockquote>
<hr>
<h2>8. 结语与后续预告</h2>
<p>本文作为系列开篇，建立了三个关键认知：</p>
<ol>
<li><strong>LLM 是函数，Agent 是系统</strong>。从函数到系统，需要补齐 Memory、Tools、Planner、Runtime 四个维度。</li>
<li><strong>Agent 的核心是控制循环</strong>。Observe → Think → Plan → Act → Reflect → Update。循环赋予了 Agent 迭代解决问题的能力。</li>
<li><strong>Agent 不是银弹</strong>。选择 Agent 是一个架构决策，需要在能力与复杂度之间做出权衡。</li>
</ol>
<p>在接下来的文章中，我们将逐层深入：</p>
<ul>
<li><strong>下一篇（02）</strong>：From Prompt to Agent —— 我们将用更严格的方式论证&quot;为什么 LLM 本身不是 Agent&quot;，并深入讨论从 Prompt Engineering 到 Agent Engineering 的思维转换。</li>
<li><strong>第 03 篇</strong>：Agent vs Workflow vs Automation —— 你的场景到底该用 Agent、DAG 还是规则引擎？我们会给出一个清晰的决策框架。</li>
<li><strong>第 04 篇</strong>：The Agent Control Loop —— 深入控制循环的每一个环节，讨论状态管理、中断恢复、错误处理的工程细节。</li>
</ul>
<p>整个系列的目标不是教你使用某个框架的 API，而是帮你建立<strong>从第一性原理理解 Agentic 系统</strong>的能力。框架会变，API 会变，但系统设计的基本原理不会变。</p>
<hr>
<blockquote>
<p><strong>系列导航</strong>：本文是 Agentic 系列的第 01 篇。</p>
<ul>
<li>下一篇：<a href="/blog/engineering/agentic/02-From%20Prompt%20to%20Agent">02 | From Prompt to Agent</a></li>
<li>完整目录见第 5 节</li>
</ul>
</blockquote>
17:T509c,<blockquote>
<p>本文面向 DevOps 架构师与云原生工程师，介绍如何基于 <strong>AWS CodePipeline + CloudFormation</strong> 构建一套支持多泳道（Multi-Lane）并行部署的<strong>ECS 持续交付体系</strong>。<br>该方案不仅解决并发部署的资源锁冲突问题，还实现模板集中治理与业务仓库完全解耦。</p>
</blockquote>
<h2>一、背景与痛点：当 DevOps 模板失控</h2>
<p>在多数微服务项目中，随着服务数量增加、环境层次复杂化，CI/CD 模板往往会失控：</p>
<ul>
<li>各服务仓库内各自维护一份 buildspec、pipeline、CFN 模板；</li>
<li>模板更新无法统一发布；</li>
<li>资源命名与导出不一致；</li>
<li>多泳道部署（如灰度、蓝绿）存在栈级锁冲突；</li>
<li>模板合规性无法集中审计。</li>
</ul>
<p><strong>问题本质：</strong> DevOps 模板分散，难以统一演进与治理。</p>
<p>在这种背景下，我们设计了一个具备“集中模板治理 + 并发部署能力”的体系：<br><strong>双仓 + 三层 Pipeline + Lane 栈隔离</strong>，下图展示了多泳道 CI/CD 的分层架构设计。</p>
<pre><code class="language-mermaid">flowchart TB
  subgraph InfraRepo[&quot;Infra Repo（DevOps 模板仓）&quot;]
    A1[buildspec.yaml]
    A2[pipeline.yaml]
    A3[service-stack.yaml]
  end

  subgraph AppRepo[&quot;App Repo（业务代码仓）&quot;]
    B1[&quot;src/&quot;]
    B2[Dockerfile]
  end

  A1 --&gt;|双源输入| P1[&quot;AWS CodePipeline&quot;]
  B1 --&gt;|双源输入| P1
  B2 --&gt; P1

  subgraph PipelineLayer[&quot;Pipeline 层&quot;]
    direction TB
    P2[&quot;Infra Pipeline (infra-{env})&quot;]
    P3[&quot;Bootstrap Pipeline (bootstrap-{env})&quot;]
    P4[&quot;App Pipeline ({service}-{env}-{lane})&quot;]
  end

  P1 --&gt; P2 --&gt; P3 --&gt; P4

  subgraph ResourceLayer[&quot;CloudFormation 栈层&quot;]
    direction LR
    C1[&quot;Infra Stack\n(VPC, Subnets, Namespace)&quot;]
    C2[&quot;Boot Stack\n(ALB, LogGroup, Cloud Map Service)&quot;]
    C3[&quot;App Lane Stack\n(TaskDef, ECS Service, TG, ListenerRule)&quot;]
  end

  P4 --&gt;|ImportValue| C3
  P3 --&gt;|导出共享资源| C2
  P2 --&gt;|导出共享资源| C1

  subgraph Traffic[&quot;智能流量路由&quot;]
    direction TB
    T1[&quot;ALB ListenerRule&quot;]
    T2[&quot;TargetGroup (lane=gray)&quot;]
    T3[&quot;TargetGroup (lane=blue)&quot;]
    T4[&quot;TargetGroup (default)&quot;]
  end
  C3 --&gt; T1 --&gt; T2 &amp; T3 &amp; T4

  classDef repo fill:#E6F0FF,stroke:#6D8FFF;
  classDef pipe fill:#FFF6E1,stroke:#FFB200;
  classDef res fill:#E8FFE8,stroke:#40C057;
  classDef traf fill:#FBE9E7,stroke:#E57373;

  class InfraRepo,AppRepo repo;
  class P1,P2,P3,P4 pipe;
  class C1,C2,C3 res;
  class T1,T2,T3,T4 traf;
</code></pre>
<h2>二、核心理念：双仓 + 三层 + Lane 栈</h2>
<p>整个体系的设计核心是三个关键词：<strong>双仓、分层、泳道（Lane）</strong>。</p>
<h3>双仓架构：逻辑分治</h3>
<table>
<thead>
<tr>
<th>仓库类型</th>
<th>内容职责</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>Infra Repo</td>
<td>统一的 DevOps 模板、buildspec、CFN 栈模板、脚本工具</td>
<td>ci/buildspec.yaml, ci/app/templates/service-stack.yaml</td>
</tr>
<tr>
<td>App Repo</td>
<td>业务代码与配置、Dockerfile、服务逻辑</td>
<td>src/, Dockerfile</td>
</tr>
</tbody></table>
<p>实现机制：<strong>双源输入（Dual-Source Inputs）</strong></p>
<p>在 Pipeline 的 Source 阶段输出两个 Artifact：</p>
<ul>
<li>Name: InfraSource → OutputArtifacts: [InfraOut]</li>
<li>Name: AppSource → OutputArtifacts: [AppOut]</li>
</ul>
<p>Build 阶段以 InfraOut 为主输入（含统一 buildspec），AppOut 为副输入（含业务代码）。<br>CodeBuild 会自动挂载环境变量：</p>
<ul>
<li><code>$CODEBUILD_SRC_DIR</code> → InfraOut</li>
<li><code>$CODEBUILD_SRC_DIR_AppOut</code> → AppOut</li>
</ul>
<p>这样，所有服务共用一套 CI/CD 模板，DevOps 团队统一维护，App 团队只关注业务逻辑。</p>
<h3>三层 Pipeline 架构：职责分层 + 无锁部署</h3>
<p>整个系统通过 <strong>三层 Pipeline 架构</strong> 实现部署解耦与并行化：</p>
<ul>
<li><strong>infra 层</strong>：负责环境通用基础设施（VPC、子网、ECS Cluster、Cloud Map 命名空间）。</li>
<li><strong>boot 层</strong>：统一管理负载均衡、日志、注册发现等<strong>服务接入设施</strong>。</li>
<li><strong>app 层</strong>：负责具体服务的泳道级部署（TaskDefinition、ECS Service、ListenerRule）。</li>
</ul>
<table>
<thead>
<tr>
<th>层级</th>
<th>Pipeline 命名</th>
<th>管理资源</th>
<th>Pipeline 变量</th>
<th>更新频率</th>
<th>并发特性</th>
</tr>
</thead>
<tbody><tr>
<td>环境级</td>
<td>infra-{env}</td>
<td>VPC、Subnets、ECS Cluster、Cloud Map Namespace</td>
<td><code>ENV=dev</code></td>
<td>几乎不变</td>
<td>独立运行</td>
</tr>
<tr>
<td>服务级</td>
<td>boot-{env}</td>
<td>ALB、LogGroup、Cloud Map Service</td>
<td><code>ENV=dev,SERVICE=user-api</code></td>
<td>新服务接入</td>
<td>按服务并行</td>
</tr>
<tr>
<td>应用级</td>
<td>{service}-{env}</td>
<td>TaskDefinition、ECS Service、TG、ListenerRule</td>
<td><code>ENV=dev,SERVICE=user-api,LANE=gray</code></td>
<td>高频发布</td>
<td>按泳道并行</td>
</tr>
</tbody></table>
<p>其中，<code>bootstrap-{env}</code> 是<strong>按环境聚合的通用服务层</strong>，而非按服务拆分。它本身不绑定单一服务，而是通过 **Pipeline 变量 <code>SERVICE</code>**动态生成服务相关资源。</p>
<p>系统分层设计的最大优势在于：<strong>部署互不加锁、并发天然安全。</strong></p>
<h3>栈级并行与 Lane 架构：高并发部署的核心</h3>
<h4>1. 栈级并行的核心逻辑</h4>
<p>CloudFormation 的锁粒度是 <strong>Stack 级别</strong>。<br>系统通过“<strong>分层 + 多栈 + 命名隔离</strong>”实现了既能并行部署、又无资源冲突的持续交付能力。</p>
<ul>
<li><p><strong>同层可并行</strong><br>每个环境（infra）、服务（boot）、泳道（app-lane）都对应独立 Stack，资源命名与写集完全隔离，可同时执行更新、互不加锁。<br>例如多个泳道（gray、blue、default）可在同一服务下并行部署。</p>
</li>
<li><p><strong>跨层有序</strong><br>上层 Pipeline 仅读取下层导出值（Outputs/ImportValue），不修改下层资源。<br><code>infra</code> 栈创建网络 → <code>boot</code> 栈创建接入资源 → <code>app</code> 栈完成版本发布。<br>依赖有序但无写冲突，下层更新完即可被上层安全引用。</p>
</li>
<li><p><strong>整体效果：并行 + 无锁 + 可控依赖</strong><br>同层可并发，跨层有序执行，形成从网络到业务的高并发、零锁冲突交付体系。</p>
</li>
</ul>
<blockquote>
<p><strong>简而言之：</strong> 同层多栈并行，跨层只读依赖。<br>这是实现高并发、零冲突持续交付的核心机制。</p>
</blockquote>
<h4>2. Lane 栈：多版本共存的关键</h4>
<p>在传统 ECS 模型中，一个服务通常只对应一个 <strong>ECS Service</strong>，意味着任意时刻只能存在一个活动版本。这种设计的局限是显而易见的：</p>
<ul>
<li>无法同时维护多个版本（灰度 / 蓝绿 / A/B 测试不具备原生支持）；</li>
<li>每次更新都需锁定整个 Service，阻塞并发发布；</li>
<li>流量切换、回滚、实验策略往往依赖外部网关或人工操作。</li>
</ul>
<p>为解决这些痛点，系统引入了 <strong>Lane（泳道）栈模型</strong>，其设计核心：Lane = 独立生命周期的版本栈。</p>
<p><strong>Lane（泳道）栈模型</strong> 为每个版本创建独立 Stack，每个 Lane 拥有自己的 ECS Service、TargetGroup、ListenerRule，并通过请求 Header（如 <code>tracestate=ctx=lane:gray</code>）实现智能路由与流量隔离。</p>
<p>Lane 栈具有四大特性：</p>
<ol>
<li><strong>完全隔离</strong>：每个 Lane 拥有独立资源，更新与回滚互不影响。</li>
<li><strong>天然并发</strong>：栈级锁粒度允许多个 Lane 同时部署，无互斥冲突。</li>
<li><strong>动态扩展</strong>：新增泳道无需改动主栈，删除 Lane 自动清理资源。</li>
<li><strong>架构原生灰度</strong>：灰度、蓝绿、A/B 测试由架构层原生支持，无需业务侵入。</li>
</ol>
<h4>3. Lane 驱动的交付模式</h4>
<table>
<thead>
<tr>
<th>模式</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><strong>灰度发布（Gray Release）</strong></td>
<td>在新版本泳道 gray 中发布小流量验证稳定性</td>
</tr>
<tr>
<td><strong>蓝绿发布（Blue/Green）</strong></td>
<td>两个版本并行，流量平滑切换</td>
</tr>
<tr>
<td><strong>A/B 测试（Traffic Split）</strong></td>
<td>按 Header、Cookie 或用户维度分流</td>
</tr>
</tbody></table>
<p>Lane 机制让<strong>部署、流量与回滚逻辑全部架构化</strong>，实现：</p>
<ul>
<li>高并发发布（无锁冲突）</li>
<li>多版本共存（灰度、蓝绿、A/B）</li>
<li>一键清理与回滚</li>
<li>模板级治理与可审计性</li>
</ul>
<blockquote>
<p><strong>一句话概括：</strong><br>Lane 栈通过“多栈并行 + 独立路由 + 参数化部署”，实现真正意义上的高并发、零冲突持续交付体系。</p>
</blockquote>
<h2>三、技术实现：从模板到执行</h2>
<h3>BuildSpec：统一入口，逻辑外移</h3>
<p>所有服务共用统一构建描述文件 <code>ci/buildspec.yaml</code>：</p>
<pre><code class="language-yaml">version: 0.2
env:
  shell: bash
  variables:
    MODULE_PATH: &quot;.&quot;                  # 相对&quot;应用仓根目录&quot;（AppOut）
  # 跨 phase 变量传递
  exported-variables:
    - ECR_REPO_URI
    - IMAGE_TAG_URI

phases:
  install:
    runtime-versions:
      java: corretto21
    commands:
      - chmod +x ci/*.sh
  pre_build:
    commands:
      - &#39;. ci/build.sh; prebuild&#39;
  build:
    commands:
      - &#39;. ci/build.sh; build&#39;
  post_build:
    commands:
      - &#39;. ci/build.sh; postbuild&#39;
artifacts:
  files:
    - cfn-params.json   # 从主输入根目录打包
</code></pre>
<p>实际逻辑集中在 <code>ci/build.sh</code>：</p>
<pre><code class="language-bash">prebuild() {
  aws ecr get-login-password | docker login ...
}
build() {
  docker build -t $SERVICE_NAME .
  docker push $ECR_URI/$SERVICE_NAME:$IMAGE_TAG
}
postbuild() {
  echo &quot;{&quot;Parameters&quot;:{&quot;ImageUri&quot;:&quot;$ECR_URI/$SERVICE_NAME:$IMAGE_TAG&quot;}}&quot; &gt; cfn-params.json
}
</code></pre>
<p>这种“轻 buildspec + 重脚本”的结构极大增强了模板复用性与可审计性。</p>
<h3>栈设计：Infra → Boot → App</h3>
<h4>Infra 栈（环境级共享）</h4>
<pre><code class="language-yaml">Parameters:
  CreateNetwork:
    Type: String
    Default: &#39;true&#39;

Conditions:
  CreateNetworkCond: !Equals [ !Ref CreateNetwork, &#39;true&#39; ]

Resources:
  VPC:
    Type: AWS::EC2::VPC
    Condition: CreateNetworkCond

  Namespace:
    Type: AWS::ServiceDiscovery::PrivateDnsNamespace

Outputs:
  VpcId:
    Value: !Ref VPC
    Export:
      Name: !Sub &#39;infra-environment-${Env}-VpcId&#39;
</code></pre>
<p>若已存在网络，可设置 <code>CreateNetwork=false</code> 进入 Wrap 模式：仅包装已有 VPC/Subnets 并导出 ID。</p>
<h4>Boot 栈（服务级）</h4>
<p>负责创建：</p>
<ul>
<li>ALB + 默认 TargetGroup + Listener；</li>
<li>LogGroup；</li>
<li>Cloud Map Service。</li>
</ul>
<p>导出值：</p>
<pre><code>boot-user-api-dev-LoadBalancerArn
boot-user-api-dev-HttpListenerArn
boot-user-api-dev-LogGroupName
boot-user-api-dev-user-api-service-arn
</code></pre>
<h4>App 栈（泳道级）</h4>
<p>创建：</p>
<ul>
<li>TaskDefinition；</li>
<li>ECS Service；</li>
<li>TargetGroup；</li>
<li>ListenerRule（Header 匹配 lane）。</li>
</ul>
<pre><code class="language-yaml">Conditions:
  IsGray: !Equals [ !Ref Lane, &#39;gray&#39; ]
LaneRule:
  Type: AWS::ElasticLoadBalancingV2::ListenerRule
  Properties:
    ListenerArn: !ImportValue boot-${ServiceName}-${Env}-HttpListenerArn
    Priority: 1000
    Conditions:
      - Field: http-header
        HttpHeaderConfig:
          HttpHeaderName: tracestate
          Values: [ !Sub &#39;ctx=lane:${Lane}&#39; ]
    Actions:
      - Type: forward
        TargetGroupArn: !Ref LaneTargetGroup
</code></pre>
<h2>四、参数与权限：闭环与最小授权</h2>
<h3>参数闭环</h3>
<pre><code class="language-bash"># Pipeline 触发变量
LANE=gray BRANCH=release/1.2.3

# CodeBuild 环境变量
SERVICE_NAME=user-api APP_ENV=dev

# 输出参数文件
{
  &quot;Parameters&quot;: {
    &quot;ServiceName&quot;: &quot;user-api&quot;,
    &quot;Env&quot;: &quot;dev&quot;,
    &quot;Lane&quot;: &quot;gray&quot;,
    &quot;ImageUri&quot;: &quot;xxx.dkr.ecr.ap-southeast-2.amazonaws.com/user-api:sha-abc123&quot;
  }
}
</code></pre>
<h3>权限边界</h3>
<p>App Pipeline 的 IAM 策略：</p>
<pre><code class="language-json">[
  {
    &quot;Effect&quot;: &quot;Allow&quot;,
    &quot;Action&quot;: &quot;cloudformation:*&quot;,
    &quot;Resource&quot;: &quot;arn:aws:cloudformation:*:*:stack/app-*/*&quot;
  },
  {
    &quot;Effect&quot;: &quot;Deny&quot;,
    &quot;Action&quot;: &quot;cloudformation:*&quot;,
    &quot;Resource&quot;: [
      &quot;arn:aws:cloudformation:*:*:stack/boot-*/*&quot;,
      &quot;arn:aws:cloudformation:*:*:stack/infra-environment-*/*&quot;
    ]
  }
]
</code></pre>
<p>Stack Policy 保护：</p>
<ul>
<li>禁止修改 Boot 栈 Listener、证书；</li>
<li>禁止删除 Infra 栈网络资源。</li>
</ul>
<h2>五、流量路由与灰度策略</h2>
<h3>Trace Context 驱动的智能路由</h3>
<p>系统遵循 W3C Trace Context 标准，在 tracestate 中注入 lane 信息：</p>
<pre><code>tracestate: ctx=lane:gray
</code></pre>
<p>ALB 按 Header 匹配：</p>
<ul>
<li>命中 → 转发到对应 TG；</li>
<li>未命中 → 回退至 default TG。</li>
</ul>
<h3>典型灰度流程</h3>
<ol>
<li>触发新 Lane：<code>LANE=gray</code></li>
<li>发布 <code>app-user-api-dev-gray</code></li>
<li>小流量 Header 导入 gray；</li>
<li>验证稳定后，将 gray 升级为 default；</li>
<li>删除旧 Lane 栈。</li>
</ol>
<p>整个流程无须改 ALB 或共享层，完全自动化。</p>
<h2>六、可观测性与回滚机制</h2>
<h3>日志聚合</h3>
<p>每个服务在 Boot 栈创建 <code>/ecs/{env}/{service}</code> LogGroup；<br>每 Lane 使用独立 <code>stream-prefix={lane}</code>，实现多维检索。</p>
<h3>自动回滚</h3>
<p>ECS Deployment Circuit Breaker 自动检测：</p>
<ul>
<li>部署失败时回滚至上个 TaskRevision；</li>
<li>发布脚本支持一键重发上个镜像标签。</li>
</ul>
<h3>监控指标</h3>
<table>
<thead>
<tr>
<th>类别</th>
<th>指标</th>
<th>告警条件</th>
</tr>
</thead>
<tbody><tr>
<td>ALB</td>
<td>HTTPCode_Target_5XX_Count</td>
<td>&gt; 1%</td>
</tr>
<tr>
<td>ECS</td>
<td>RunningCount &lt; DesiredCount</td>
<td>连续 3 次</td>
</tr>
<tr>
<td>TG</td>
<td>HealthyHostCount</td>
<td>&lt; 1</td>
</tr>
</tbody></table>
<h2>七、实施与价值</h2>
<p>下面展示如何基于 AWS CloudFormation 和 CodePipeline 部署多层持续交付体系， 并通过 JSON 文件定义模板参数，实现模板集中治理与参数可审计。</p>
<h3>部署 pipeline（一次性）</h3>
<pre><code class="language-bash"># 环境级（一次性部署）
aws cloudformation deploy \
  --template-file ci/infra/pipeline.yaml \
  --stack-name infra-dev \
  --parameter-overrides file://params/infra-dev.json

# 服务接入层 boot（一次性部署，通用 pipeline）
aws cloudformation deploy \
  --template-file ci/boot/pipeline.yaml \
  --stack-name bootstrap-dev \
  --parameter-overrides file://params/bootstrap-dev.json

# 应用层 app（每个服务独立一条 pipeline）
aws cloudformation deploy \
  --template-file ci/app/pipeline.yaml \
  --stack-name user-api-dev \
  --parameter-overrides file://params/user-api-dev.json
</code></pre>
<h3>参数文件</h3>
<p>每个阶段都在 params/ 目录下定义独立 JSON 参数文件，按规范区分环境、服务与泳道：</p>
<table>
<thead>
<tr>
<th>层级</th>
<th>参数文件</th>
<th>示例</th>
<th>用途</th>
</tr>
</thead>
<tbody><tr>
<td>环境级</td>
<td><code>infra-{env}.json</code></td>
<td><code>infra-dev.json</code></td>
<td>基础设施参数，定义基础网络、VPC、Subnet、Cluster、Namespace 等通用资源。</td>
</tr>
<tr>
<td>服务级</td>
<td><code>boot-{env}.json</code></td>
<td><code>boot-dev.json</code></td>
<td>服务引导参数，通过运行时变量 <code>SERVICE</code> 来动态创建各服务的 ALB、LogGroup、Cloud Map</td>
</tr>
<tr>
<td>应用级</td>
<td><code>{service}-{env}.json</code></td>
<td><code>user-api-dev.json</code></td>
<td>应用层参数，每个服务一份独立参数文件，支持通过SERVICE、LANE、BRANCH 变量控制泳道部署与镜像版本。</td>
</tr>
</tbody></table>
<blockquote>
<p>这种命名约定便于版本化与审计，也可在 CodePipeline 中动态选择。所有参数文件统一存放在 <code>params/</code> 目录中，并纳入 Git 版本管理，<br>便于在不同环境间复用、审计、回滚与自动化生成。</p>
</blockquote>
<h3>服务引导（服务级共享资源）</h3>
<p>在部署 <strong>应用层 pipeline</strong>（如 <code>user-api-dev</code>）之前，必须先触发一次<strong>boot 层通用 pipeline（boot-{env}）</strong>，以创建该服务的共享接入资源：</p>
<ul>
<li>ALB TargetGroup</li>
<li>Cloud Map Service</li>
<li>LogGroup</li>
<li>默认 ListenerRule</li>
</ul>
<p>这些资源由 boot 层集中管理，所有应用层泳道（如 gray、blue、default）都会复用，因此必须保证该阶段先于 <strong>app pipeline</strong> 执行。</p>
<pre><code class="language-bash"># 使用 bootstrap-dev pipeline，通过 SERVICE 参数创建服务接入资源
aws codepipeline start-pipeline-execution \
  --name boot-dev \
  --variables name=SERVICE,value=user-api
</code></pre>
<h3>发布与泳道管理（app 层）</h3>
<pre><code class="language-bash"># 发布到 gray 泳道
aws codepipeline start-pipeline-execution \
  --name user-api-dev \
  --variables name=SERVICE,value=user-api \
              name=LANE,value=gray \
              name=BRANCH,value=release/1.2.3

# 删除 gray 泳道（自动回收 TG/ListenerRule/ECS Service）
aws cloudformation delete-stack \
  --stack-name app-user-api-dev-gray
</code></pre>
<h3>价值总结</h3>
<ul>
<li>使用 <code>params/</code> 目录集中存放模板参数，配合 Git 版本管理。</li>
<li>参数文件与模板解耦，方便在不同环境间复用相同模板。</li>
<li>通过 CodePipeline 的变量参数（如 <code>SERVICE</code>、<code>LANE</code>、<code>BRANCH</code>）控制发布粒度。</li>
<li>删除泳道时只需删除对应 Stack，系统会自动回收资源。</li>
<li>在多泳道部署中保持命名一致性与参数规范，确保各层之间可审计、可追溯。</li>
</ul>
<table>
<thead>
<tr>
<th>维度</th>
<th>成果</th>
</tr>
</thead>
<tbody><tr>
<td><strong>技术</strong></td>
<td>无锁并发部署、模板集中治理、智能流量路由</td>
</tr>
<tr>
<td><strong>运维</strong></td>
<td>零人工泳道切换、标准化监控与自动回滚</td>
</tr>
<tr>
<td><strong>业务</strong></td>
<td>快速灰度 / 蓝绿 / A/B 测试，显著缩短发布周期</td>
</tr>
<tr>
<td><strong>治理</strong></td>
<td>模板合规集中、权限最小化、栈保护机制，支持统一审计</td>
</tr>
</tbody></table>
<blockquote>
<p>✅ 通过以上实践，整个 CI/CD 体系实现了模板化、参数化、自动化、可治理化，<br>让“多泳道高并发交付”成为一种工程标准，而非复杂特例。</p>
</blockquote>
<h2>结语：从流程到体系</h2>
<p>该架构的核心思想是“让 CI/CD 自治，而非依赖人治”，通过：</p>
<ul>
<li>模板集中治理（Infra Repo）</li>
<li>业务仓独立演进（App Repo）</li>
<li>Pipeline 分层解耦</li>
<li>Lane 栈级并发隔离</li>
</ul>
<p>我们不仅在工程上解决了并发冲突和灰度复杂度， 更在组织层面建立了 DevOps 模板的统一“基建层”。<br><strong>DevOps 模板不再是脚本集合，而是服务化的基础设施。</strong></p>
18:T8130,<h1>From Prompt to Agent: 为什么 LLM 本身不是 Agent</h1>
<blockquote>
<p>当我们说&quot;让 AI 帮我完成这件事&quot;时，我们期望的不是一次文本生成，而是一次<strong>有目标、有规划、有执行、有反馈</strong>的任务完成过程。这正是 LLM 和 Agent 的根本区别。</p>
<p>本文是 Agentic 系列第 02 篇。上一篇我们绘制了全景地图，这一篇我们回到原点：为什么一个再强大的 LLM，本身也不是 Agent？从&quot;不是什么&quot;出发，才能精确定义&quot;是什么&quot;。</p>
</blockquote>
<hr>
<h2>1. LLM 的本质：一个文本到文本的函数</h2>
<p>把所有复杂性剥离，LLM 的数学本质极其简洁：</p>
<pre><code>f(prompt) → response
</code></pre>
<p>给定一段输入文本（prompt），经过前向推理，输出一段文本（response）。就这样。</p>
<p>更严格地说，LLM 做的是<strong>条件概率采样</strong>：给定已有 token 序列 <code>[t₁, t₂, ..., tₙ]</code>，逐个预测下一个 token 的概率分布 <code>P(tₙ₊₁ | t₁, ..., tₙ)</code>，然后按某种策略（greedy、top-k、top-p）从分布中采样。</p>
<p>这意味着三个关键性质：</p>
<ul>
<li><strong>无状态（Stateless）</strong>：模型权重在推理时不变，两次相同输入产生的概率分布相同（忽略采样随机性）。模型本身不存储任何关于&quot;之前发生了什么&quot;的信息。</li>
<li><strong>无副作用（Side-effect Free）</strong>：模型不会改变外部世界的任何状态——不会写文件、不会调 API、不会修改数据库。它只输出文本。</li>
<li><strong>无记忆（Memoryless）</strong>：每次调用都是独立的函数调用。上一次对话的内容，除非你手动拼接进 prompt，否则模型完全不知道。</li>
</ul>
<p>用一个 Python 类比，LLM 就是一个纯函数：</p>
<pre><code class="language-python">def llm(prompt: str) -&gt; str:
    &quot;&quot;&quot;
    纯函数：相同输入 → 相同输出分布
    无副作用：不修改任何外部状态
    无记忆：不保留任何调用历史
    &quot;&quot;&quot;
    tokens = tokenize(prompt)
    output_tokens = []
    for _ in range(max_tokens):
        next_token_probs = model.forward(tokens + output_tokens)
        next_token = sample(next_token_probs, temperature=0.7)
        output_tokens.append(next_token)
        if next_token == EOS:
            break
    return detokenize(output_tokens)
</code></pre>
<p>这是一个非常优雅的抽象。但正是这个抽象的简洁性，决定了它的局限性。</p>
<hr>
<h2>2. LLM 的五大局限</h2>
<h3>2.1 无记忆：每次对话都是独立宇宙</h3>
<p><strong>场景</strong>：你让 LLM 帮你写一个项目方案。第一轮你说了需求，第二轮你补充了约束，第三轮你修改了目标。LLM 怎么&quot;记住&quot;前两轮？</p>
<p>答案是：它不记。所谓的&quot;多轮对话&quot;，本质上是<strong>客户端把历史消息全部拼接进 prompt</strong> 重新发送。每一轮调用，LLM 都在从零开始阅读整个对话历史。</p>
<pre><code class="language-python"># 所谓&quot;多轮对话&quot;的真相
messages = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;帮我写个项目方案&quot;},          # 第一轮
    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;好的，请问项目目标是什么？&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;做一个推荐系统&quot;},            # 第二轮
    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;了解，技术栈偏好？&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;用 Python，预算 50 万&quot;},     # 第三轮
]
# 每次都是把 *全部* messages 发给 LLM，它并不&quot;记得&quot;前两轮
response = llm.chat(messages)
</code></pre>
<p>这带来两个工程问题：一是 <strong>context window 有限</strong>，对话太长会被截断，早期关键信息丢失；二是 <strong>token 成本线性增长</strong>，每一轮都在为重复传输历史对话付费。</p>
<h3>2.2 无工具：只能生成文本，不能执行操作</h3>
<p><strong>场景</strong>：你问 LLM&quot;现在北京气温多少度？&quot;它会给你一个看起来很自信的答案——但这个答案是从训练数据中&quot;编&quot;出来的，不是实时查询的结果。</p>
<p>LLM 不能发 HTTP 请求，不能查数据库，不能读文件系统，不能调用任何外部服务。它唯一的&quot;输出通道&quot;就是文本。</p>
<pre><code>用户：帮我创建一个 GitHub 仓库叫 my-project
LLM：好的，已经为您创建了 GitHub 仓库 my-project！  ← 这是幻觉，什么都没发生
</code></pre>
<p>LLM 的&quot;执行&quot;是一种语言层面的模拟——它可以生成看起来像执行结果的文本，但实际上没有任何副作用发生。这是 hallucination 问题在工具层面的体现。</p>
<h3>2.3 无规划：只有 next-token prediction，没有 multi-step reasoning</h3>
<p><strong>场景</strong>：你让 LLM&quot;规划一次三天的日本旅行&quot;。它会一口气输出一个看起来完整的方案。但这不是&quot;规划&quot;——这是&quot;自回归生成&quot;。它不会先列出约束（预算、时间、兴趣），再枚举可能的方案，再比较 trade-off，再做决策。它只是在逐 token 地预测&quot;下一个最可能的词&quot;。</p>
<p>真正的规划需要：</p>
<ol>
<li><strong>目标分解</strong>：把大目标拆成子目标</li>
<li><strong>约束满足</strong>：在多个维度上满足约束条件</li>
<li><strong>方案评估</strong>：对多个候选方案进行比较</li>
<li><strong>回溯修正</strong>：发现某条路不通时能回退</li>
</ol>
<p>LLM 的自回归生成是单向的、线性的，没有回溯机制。它无法在生成第 50 个 token 时&quot;回头修改&quot;第 10 个 token。所有看起来像&quot;规划&quot;的输出，都是语言模式匹配的结果，不是搜索与优化的结果。</p>
<h3>2.4 无状态：不知道自己之前做了什么</h3>
<p><strong>场景</strong>：你让 LLM 执行一个多步骤任务——先查数据，再分析，再写报告。即使它能生成每一步的文本描述，它也不知道&quot;第一步的结果是什么&quot;，因为它没有一个持久化的状态空间来记录执行进度。</p>
<p>无状态和无记忆不同：</p>
<ul>
<li><strong>无记忆</strong>强调的是跨调用的信息丢失</li>
<li><strong>无状态</strong>强调的是在一次任务中，没有结构化的执行状态追踪</li>
</ul>
<p>一个 Agent 需要知道：&quot;我已经完成了步骤 1 和 2，步骤 3 失败了，我需要重试步骤 3&quot;。LLM 没有这个能力。</p>
<h3>2.5 无反思：无法评估自己的输出质量</h3>
<p><strong>场景</strong>：你让 LLM 写一段代码。它写完了。这段代码是否正确？LLM 不知道。它不会自动运行代码验证，不会检查边界条件，不会评估时间复杂度是否满足要求。</p>
<p>更深层的问题是：LLM 无法区分&quot;我确信这是对的&quot;和&quot;我在瞎猜&quot;。它的 confidence 不等于 correctness。一个 softmax 输出 0.95 的概率，并不意味着答案有 95% 的概率是正确的。</p>
<pre><code>                    LLM 的五大局限

    +----------+----------+----------+----------+----------+
    |          |          |          |          |          |
    | 无记忆    | 无工具    | 无规划    | 无状态    | 无反思    |
    | Memoryless| Toolless | Planless | Stateless| Reflectless|
    |          |          |          |          |          |
    | 跨调用    | 只输出    | 单向生成  | 无执行    | 无法自我  |
    | 信息丢失  | 文本     | 无回溯    | 进度追踪  | 评估质量  |
    |          |          |          |          |          |
    +----------+----------+----------+----------+----------+
                          |
                          v
              LLM 需要一个&quot;外壳&quot;来弥补这些局限
              这个外壳，就是 Agent Runtime
</code></pre>
<hr>
<h2>3. Agent 的精确定义</h2>
<h3>3.1 定义</h3>
<p><strong>Agent = LLM + Memory + Tools + Planner + Runtime</strong></p>
<p>这不是一个松散的隐喻，而是一个精确的组件模型。每个组件有明确的职责边界：</p>
<table>
<thead>
<tr>
<th>组件</th>
<th>职责</th>
<th>类比</th>
</tr>
</thead>
<tbody><tr>
<td><strong>LLM</strong></td>
<td>语义理解、推理、生成</td>
<td>大脑的语言区</td>
</tr>
<tr>
<td><strong>Memory</strong></td>
<td>存储对话历史、任务状态、长期知识</td>
<td>海马体 + 笔记本</td>
</tr>
<tr>
<td><strong>Tools</strong></td>
<td>与外部世界交互的能力集合</td>
<td>双手 + 工具箱</td>
</tr>
<tr>
<td><strong>Planner</strong></td>
<td>目标分解、行动排序、策略选择</td>
<td>前额叶皮层</td>
</tr>
<tr>
<td><strong>Runtime</strong></td>
<td>控制循环、状态管理、错误处理、生命周期</td>
<td>自主神经系统</td>
</tr>
</tbody></table>
<h3>3.2 组件交互模型</h3>
<pre><code>    +---------------------------------------------------------+
    |                     Agent Runtime                        |
    |                                                         |
    |   +----------+     +----------+     +----------+        |
    |   |          |     |          |     |          |        |
    |   |  Memory  |&lt;---&gt;|   LLM    |&lt;---&gt;| Planner  |        |
    |   |          |     |          |     |          |        |
    |   +----+-----+     +----+-----+     +----+-----+        |
    |        |                |                 |              |
    |        |           +----v-----+           |              |
    |        +----------&gt;|          |&lt;----------+              |
    |                    |  Tools   |                          |
    |                    |          |                          |
    |                    +----+-----+                          |
    |                         |                                |
    +---------------------------------------------------------+
                              |
                              v
                     External World
                  (APIs, DBs, Files, Users)
</code></pre>
<p><strong>交互流程</strong>：</p>
<ol>
<li><strong>Runtime</strong> 接收外部输入（用户消息、系统事件）</li>
<li><strong>Runtime</strong> 从 <strong>Memory</strong> 加载相关上下文</li>
<li><strong>LLM</strong> 基于输入 + 上下文进行推理</li>
<li><strong>Planner</strong>（通常由 LLM 驱动）决定下一步行动</li>
<li>如果需要执行操作，<strong>Runtime</strong> 调度 <strong>Tools</strong> 执行</li>
<li>工具执行结果写回 <strong>Memory</strong>，进入下一轮循环</li>
</ol>
<p>关键设计决策：<strong>Planner 是一个独立组件，还是 LLM 的一部分？</strong> 这取决于你对确定性的需求。如果 Planner 由 LLM 驱动（如 ReAct 模式），灵活但不可控；如果 Planner 是硬编码的状态机，可控但不灵活。这个 trade-off 贯穿整个 Agent 架构设计，我们会在第 03 篇深入讨论。</p>
<hr>
<h2>4. Agent 的核心循环详解</h2>
<p>Agent 的运行可以抽象为六个阶段的循环：</p>
<pre><code>    +-------+     +-------+     +------+
    |       |     |       |     |      |
    |Observe+----&gt;| Think +----&gt;| Plan |
    |       |     |       |     |      |
    +---^---+     +-------+     +--+---+
        |                          |
        |                          v
    +---+----+                 +---+---+
    |        |                 |       |
    | Update |&lt;----+ Reflect  &lt;+ Act   |
    |        |     |          ||       |
    +--------+     +----------++-------+
</code></pre>
<h3>4.1 各阶段详解</h3>
<p><strong>Observe（感知）</strong>：收集当前环境信息。这包括用户的最新输入、上一步工具的返回结果、系统级事件（如超时、异常）、从 Memory 中检索的相关上下文。感知阶段的核心问题是<strong>信息筛选</strong>——不是所有信息都应该进入 LLM 的上下文，context window 是稀缺资源。</p>
<p><strong>Think（推理）</strong>：基于感知到的信息，理解当前处境。这是 LLM 最擅长的部分——语义理解、意图识别、情境分析。Think 阶段的输出是对当前状态的结构化理解，而不是最终答案。</p>
<p><strong>Plan（规划）</strong>：基于对当前状态的理解，决定下一步做什么。Plan 可以是单步的（&quot;调用天气 API&quot;），也可以是多步的（&quot;先查天气，再根据天气决定穿什么，再创建提醒&quot;）。规划的粒度直接影响系统的可控性和灵活性。</p>
<p><strong>Act（执行）</strong>：执行规划中的动作。可能是调用工具（Tool Calling）、生成文本回复、更新内部状态，或者向用户提问以获取更多信息。执行是唯一产生副作用的阶段。</p>
<p><strong>Reflect（反思）</strong>：评估执行结果。工具调用成功了吗？返回的数据符合预期吗？是否需要重试或换一个方案？反思是 Agent 与简单 Chain 的关键区别——它引入了<strong>自我纠错</strong>的能力。</p>
<p><strong>Update（更新）</strong>：将本轮循环中产生的信息写入 Memory。包括更新对话历史、记录执行结果、修改任务状态。Update 确保下一轮循环有最新的上下文可用。</p>
<h3>4.2 最简实现</h3>
<p>下面是这个控制循环的 Python 伪代码实现。注意，这不是生产代码，而是用于精确表达架构意图的最简抽象：</p>
<pre><code class="language-python">from dataclasses import dataclass, field
from typing import Any

@dataclass
class AgentState:
    &quot;&quot;&quot;Agent 的可序列化状态&quot;&quot;&quot;
    messages: list[dict] = field(default_factory=list)       # 对话历史
    task_status: str = &quot;pending&quot;                              # 任务状态
    plan: list[str] = field(default_factory=list)             # 当前计划
    step_index: int = 0                                       # 执行进度
    observations: list[Any] = field(default_factory=list)     # 感知缓冲

class Agent:
    def __init__(self, llm, tools: dict, max_iterations: int = 10):
        self.llm = llm
        self.tools = tools           # {&quot;tool_name&quot;: callable}
        self.max_iterations = max_iterations
        self.state = AgentState()

    def run(self, user_input: str) -&gt; str:
        self.state.messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input})

        for i in range(self.max_iterations):
            # --- Observe ---
            context = self._observe()

            # --- Think ---
            thought = self.llm.generate(
                system_prompt=THINK_PROMPT,
                messages=context,
            )

            # --- Plan ---
            plan = self.llm.generate(
                system_prompt=PLAN_PROMPT,
                messages=context + [{&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: thought}],
                response_format={&quot;type&quot;: &quot;json&quot;, &quot;schema&quot;: PlanSchema},
            )

            if plan.action == &quot;finish&quot;:
                return plan.final_answer

            # --- Act ---
            tool_name = plan.tool_name
            tool_args = plan.tool_args
            try:
                result = self.tools[tool_name](**tool_args)
            except Exception as e:
                result = f&quot;Error: {e}&quot;

            # --- Reflect ---
            reflection = self.llm.generate(
                system_prompt=REFLECT_PROMPT,
                messages=context + [
                    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: f&quot;Action: {tool_name}({tool_args})&quot;},
                    {&quot;role&quot;: &quot;tool&quot;, &quot;content&quot;: str(result)},
                ],
            )

            # --- Update ---
            self.state.messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: thought})
            self.state.messages.append({&quot;role&quot;: &quot;tool&quot;, &quot;content&quot;: str(result)})
            self.state.observations.append({
                &quot;step&quot;: i,
                &quot;action&quot;: tool_name,
                &quot;result&quot;: result,
                &quot;reflection&quot;: reflection,
            })

            if reflection.should_retry:
                continue  # 重试当前步骤
            self.state.step_index += 1

        return &quot;达到最大迭代次数，任务未完成。&quot;

    def _observe(self) -&gt; list[dict]:
        &quot;&quot;&quot;从 Memory 中组装当前上下文&quot;&quot;&quot;
        # 实际系统中这里会有复杂的上下文压缩、检索等逻辑
        return self.state.messages[-20:]  # 简化：取最近 20 条消息
</code></pre>
<p>这段代码中有几个值得关注的设计决策：</p>
<ol>
<li><strong>Think 和 Plan 分两次 LLM 调用</strong>：可以使用不同的 system prompt 引导不同的思维模式，也便于独立观测和调试。代价是额外的 latency 和 token 成本。</li>
<li><strong>Plan 使用 Structured Output</strong>：规划结果以 JSON Schema 约束，确保输出可解析、可校验。这是将 LLM 的非确定性输出转化为确定性执行的关键桥梁。</li>
<li><strong>Reflect 独立成阶段</strong>：而不是合并到下一轮的 Think 中。这使得反思的 prompt 可以专注于&quot;评估&quot;而不是&quot;理解+评估&quot;，通常能得到更准确的自我评价。</li>
<li><strong>max_iterations 作为安全阀</strong>：防止 Agent 陷入无限循环。这是生产系统中必须有的机制，没有它，一个错误的 Reflect 判断就可能导致无限重试。</li>
</ol>
<hr>
<h2>5. 从 Chatbot 到 Agent 的光谱</h2>
<p>Agent 不是一个二元概念——&quot;是 Agent&quot;或&quot;不是 Agent&quot;。从最简单的 LLM 调用到完整的 Agent 系统，中间存在一个连续的光谱，每向右移动一步，都在引入新的复杂性来换取新的能力。</p>
<pre><code>确定性 ←─────────────────────────────────────────────────→ 自主性

Pure LLM    System     RAG       Tool       ReAct       Full
            Prompt               Calling    Agent       Agent

  f(x)→y   定制化     知识增强   函数调用   推理+执行    完整系统
            对话                            循环
</code></pre>
<p>下表从六个维度对比这个光谱的各个阶段：</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>记忆</th>
<th>工具</th>
<th>规划</th>
<th>状态</th>
<th>反思</th>
<th>典型产品/模式</th>
</tr>
</thead>
<tbody><tr>
<td>Pure LLM</td>
<td>无</td>
<td>无</td>
<td>无</td>
<td>无</td>
<td>无</td>
<td>单次 API 调用</td>
</tr>
<tr>
<td>+ System Prompt</td>
<td>无</td>
<td>无</td>
<td>无</td>
<td>无</td>
<td>无</td>
<td>定制化 Chatbot</td>
</tr>
<tr>
<td>+ RAG</td>
<td>外部知识</td>
<td>检索</td>
<td>无</td>
<td>无</td>
<td>无</td>
<td>知识问答系统</td>
</tr>
<tr>
<td>+ Tool Calling</td>
<td>会话级</td>
<td>有</td>
<td>单步</td>
<td>无</td>
<td>无</td>
<td>Function Calling</td>
</tr>
<tr>
<td>+ Loop（ReAct）</td>
<td>会话级</td>
<td>有</td>
<td>多步</td>
<td>运行时</td>
<td>隐式</td>
<td>ReAct Agent</td>
</tr>
<tr>
<td>Full Agent</td>
<td>长期</td>
<td>有</td>
<td>多步</td>
<td>持久化</td>
<td>显式</td>
<td>自主 Agent 系统</td>
</tr>
</tbody></table>
<p>每个阶段的跃迁都有明确的 trade-off：</p>
<ul>
<li><strong>Pure LLM → + System Prompt</strong>：几乎零成本，但能显著改变模型的行为风格和专业度。Trade-off：prompt 越长，留给用户输入的 context window 越少。</li>
<li><strong>+ System Prompt → + RAG</strong>：引入外部知识源，解决知识时效性和专业性问题。Trade-off：检索质量直接决定回答质量（garbage in, garbage out），且增加了 latency 和基础设施成本。</li>
<li><strong>+ RAG → + Tool Calling</strong>：从&quot;只读&quot;变成&quot;可写&quot;，LLM 可以触发外部操作。Trade-off：引入了安全风险（LLM 可能调用不该调用的工具）和确定性问题（工具调用可能失败）。</li>
<li><strong>+ Tool Calling → + Loop</strong>：从单次推理变成多步推理-执行循环。这是质变。Trade-off：循环次数不可预测，token 成本不可预测，调试复杂度指数级上升。</li>
<li><strong>+ Loop → Full Agent</strong>：引入持久化记忆和显式反思。Trade-off：系统复杂度大幅提升，需要处理记忆一致性、状态持久化、长时间运行等问题。</li>
</ul>
<hr>
<h2>6. 一个完整的例子</h2>
<p>用同一个任务——&quot;帮我查看明天北京的天气并创建日程提醒&quot;——展示不同阶段的实现差异。</p>
<h3>6.1 Pure LLM</h3>
<pre><code class="language-python">response = llm.generate(&quot;帮我查看明天北京的天气并创建日程提醒&quot;)
# 输出：好的，明天北京的天气大约是 25°C，晴转多云...（纯幻觉，没有真实数据）
# 日程提醒也不会真的被创建
</code></pre>
<p>问题：没有真实数据，没有真实执行，一切都是生成的&quot;假&quot;内容。</p>
<h3>6.2 LLM + RAG</h3>
<pre><code class="language-python"># 预先检索天气相关知识
weather_docs = retriever.search(&quot;北京天气预报&quot;)
context = format_docs(weather_docs)

response = llm.generate(
    f&quot;根据以下信息回答用户问题：\n{context}\n\n用户：帮我查看明天北京的天气并创建日程提醒&quot;
)
# 输出基于检索到的文档，但如果文档不包含明天的天气（高概率），仍然无法回答
# 日程提醒依然无法创建
</code></pre>
<p>问题：RAG 提供了知识，但无法获取实时数据，更无法执行&quot;创建日程&quot;这个写操作。</p>
<h3>6.3 LLM + Tool Calling（单步）</h3>
<pre><code class="language-python">tools = [
    {
        &quot;name&quot;: &quot;get_weather&quot;,
        &quot;description&quot;: &quot;获取指定城市的天气预报&quot;,
        &quot;parameters&quot;: {&quot;city&quot;: &quot;string&quot;, &quot;date&quot;: &quot;string&quot;}
    },
    {
        &quot;name&quot;: &quot;create_reminder&quot;,
        &quot;description&quot;: &quot;创建日程提醒&quot;,
        &quot;parameters&quot;: {&quot;title&quot;: &quot;string&quot;, &quot;time&quot;: &quot;string&quot;, &quot;note&quot;: &quot;string&quot;}
    }
]

response = llm.generate(
    &quot;帮我查看明天北京的天气并创建日程提醒&quot;,
    tools=tools,
)
# LLM 返回一个 tool_call：get_weather(city=&quot;北京&quot;, date=&quot;2025-08-04&quot;)
# 但只能调用一个工具——它选了查天气，日程提醒怎么办？
# 需要第二轮调用，但谁来发起？没有循环机制。
</code></pre>
<p>问题：单步 Tool Calling 只能执行一个动作。多步任务需要外部编排。</p>
<h3>6.4 LLM + Tools + Loop（ReAct Agent）</h3>
<pre><code class="language-python">agent = Agent(llm=llm, tools={&quot;get_weather&quot;: get_weather, &quot;create_reminder&quot;: create_reminder})
result = agent.run(&quot;帮我查看明天北京的天气并创建日程提醒&quot;)

# Agent 内部执行过程：
#
# [Iteration 1]
# Think:  用户想查天气并创建提醒，我需要先查天气，再用天气信息创建提醒。
# Plan:   调用 get_weather(city=&quot;北京&quot;, date=&quot;2025-08-04&quot;)
# Act:    → {&quot;temp&quot;: 31, &quot;condition&quot;: &quot;多云转雷阵雨&quot;, &quot;humidity&quot;: 78}
# Reflect: 成功获取天气数据，接下来需要创建日程提醒。
# Update:  记录天气数据到 state。
#
# [Iteration 2]
# Think:  已获取天气信息（31°C，多云转雷阵雨），需要创建提醒。
# Plan:   调用 create_reminder(
#             title=&quot;明天北京天气提醒&quot;,
#             time=&quot;2025-08-04T07:00:00&quot;,
#             note=&quot;31°C，多云转雷阵雨，湿度 78%，建议带伞&quot;
#         )
# Act:    → {&quot;status&quot;: &quot;created&quot;, &quot;id&quot;: &quot;rem_abc123&quot;}
# Reflect: 提醒创建成功。两个子任务都已完成，可以返回最终结果。
# Plan:   finish
#
# 最终输出：
# &quot;明天北京天气：31°C，多云转雷阵雨，湿度 78%。
#  已为您创建早上 7:00 的天气提醒，建议带伞。&quot;
</code></pre>
<p>这才是我们期望的行为：<strong>理解意图 → 分解任务 → 逐步执行 → 组合结果</strong>。注意 Agent 做了几件 Pure LLM 做不到的事：</p>
<ol>
<li><strong>任务分解</strong>：识别出&quot;查天气&quot;和&quot;创建提醒&quot;是两个子任务，且有依赖关系</li>
<li><strong>信息传递</strong>：把第一步的天气数据作为第二步的输入（note 字段）</li>
<li><strong>智能补全</strong>：用户没说提醒时间，Agent 推断了一个合理的时间（早上 7 点）</li>
<li><strong>结果整合</strong>：把多步执行的结果组合成一个连贯的自然语言回复</li>
</ol>
<h3>6.5 Full Agent（增加长期记忆与反思）</h3>
<pre><code class="language-python"># Full Agent 在 ReAct 基础上增加：

# 1. 长期记忆：记住用户偏好
user_profile = memory.recall(user_id=&quot;u_001&quot;)
# → {&quot;preferred_reminder_time&quot;: &quot;06:30&quot;, &quot;weather_sensitivity&quot;: &quot;rain&quot;}

# 2. 个性化决策：基于用户历史偏好
# Agent 不再推断 7:00，而是使用用户偏好的 6:30
# Agent 知道用户对雨天敏感，会强调带伞建议

# 3. 显式反思：执行后回顾
reflection = agent.reflect(
    task=&quot;查天气并创建提醒&quot;,
    result=result,
    criteria=[&quot;信息完整性&quot;, &quot;时间合理性&quot;, &quot;个性化程度&quot;]
)
# → &quot;时间使用了用户偏好，天气包含了降雨提醒。但缺少穿衣建议，下次可以补充。&quot;

# 4. 记忆更新：学习本次交互
memory.store(
    user_id=&quot;u_001&quot;,
    fact=&quot;用户关注北京天气，可能是北京居民或近期有出行计划&quot;,
    source=&quot;interaction_20250803&quot;
)
</code></pre>
<p>Full Agent 的核心区别在于：<strong>它在跨会话的时间尺度上持续学习和个性化</strong>。这需要一个完整的 Memory 架构来支撑——短期会话记忆、长期用户画像、事实知识库——我们将在第 08 篇详细展开。</p>
<hr>
<h2>7. Agent 的设计哲学</h2>
<h3>7.1 LLM as the Reasoning Engine, Not the Entire System</h3>
<p>这是 Agent 架构最核心的设计原则。LLM 是推理引擎，不是整个系统。就像汽车的发动机不是汽车本身——你还需要变速箱（Planner）、方向盘（Tools）、仪表盘（Memory）和底盘（Runtime）。</p>
<p>这个原则的工程含义是：<strong>不要让 LLM 做所有事情。</strong> 让它做它擅长的——语义理解、推理、决策——然后用确定性代码处理其余部分。</p>
<h3>7.2 确定性 vs 非确定性的边界</h3>
<p>Agent 系统的核心设计问题之一是：<strong>哪些部分让 LLM 做（非确定性），哪些部分用代码做（确定性）？</strong></p>
<pre><code>    确定性 (代码)                        非确定性 (LLM)
    +-----------------------+          +-----------------------+
    | 输入校验              |          | 意图理解              |
    | 工具调度              |          | 工具选择              |
    | 参数类型检查          |          | 参数填充              |
    | 权限控制              |          | 上下文摘要            |
    | 错误重试逻辑          |          | 结果解释              |
    | 速率限制              |          | 对话策略              |
    | 日志记录              |          | 异常情况判断          |
    | 状态持久化            |          | 任务分解              |
    +-----------------------+          +-----------------------+
            |                                    |
            v                                    v
    可预测、可审计、可测试            灵活、自适应、但不可控
</code></pre>
<p>决策原则：</p>
<ol>
<li><strong>如果逻辑可以穷举，用代码</strong>。比如&quot;用户必须先登录才能创建日程&quot;——这是业务规则，不需要 LLM 判断。</li>
<li><strong>如果需要理解自然语言语义，用 LLM</strong>。比如&quot;用户说&#39;帮我约个会&#39;是什么意思&quot;——这需要语义理解。</li>
<li><strong>如果错误的代价很高，用代码兜底</strong>。比如转账操作的金额校验，无论 LLM 怎么说，都必须用代码做最终确认。</li>
<li><strong>如果需要处理开放域输入，用 LLM</strong>。比如用户可能用任何方式描述他们的需求，只有 LLM 能处理这种多样性。</li>
</ol>
<h3>7.3 何时不需要 Agent</h3>
<p>并非所有问题都需要 Agent。以下场景用更简单的方案更好：</p>
<ul>
<li><strong>固定流程的自动化</strong>：发票处理、数据同步——用 Workflow（DAG）更可靠</li>
<li><strong>单轮问答</strong>：FAQ、知识检索——LLM + RAG 就够了</li>
<li><strong>确定性决策</strong>：基于规则的审批——规则引擎更合适</li>
<li><strong>高吞吐低延迟</strong>：实时推荐——Agent 的多轮调用延迟太高</li>
</ul>
<p>Agent 的最佳应用场景是：<strong>任务需要多步推理、工具组合使用、且执行路径在运行时才能确定</strong>。如果执行路径在编译时就能确定，你需要的是 Workflow，不是 Agent。这正是我们下一篇要深入讨论的主题。</p>
<hr>
<h2>8. 总结与思考</h2>
<p>本文从 LLM 的本质出发，论证了为什么 <code>f(prompt) → response</code> 不等于 Agent。核心论点可以压缩为一句话：</p>
<blockquote>
<p><strong>LLM 是推理能力的来源，Agent 是将推理能力转化为行动能力的系统。</strong></p>
</blockquote>
<p>我们建立了三个关键的心智模型：</p>
<ol>
<li><strong>组件模型</strong>：Agent = LLM + Memory + Tools + Planner + Runtime，五个组件各有职责，协作运行。</li>
<li><strong>循环模型</strong>：Observe → Think → Plan → Act → Reflect → Update，Agent 通过控制循环将单次推理扩展为多步执行。</li>
<li><strong>光谱模型</strong>：从 Pure LLM 到 Full Agent 是一个连续光谱，每一步都有明确的能力增益和复杂性代价。</li>
</ol>
<h3>进一步思考</h3>
<p>在进入下一篇之前，留几个值得深入思考的问题：</p>
<p><strong>关于 Agent 的边界</strong>：如果 Planner 是硬编码的（比如一个固定的 DAG），这还算 Agent 吗？如果所有工具都是预定义的、参数是模板化的，LLM 只负责填参数，这算 Agent 还是 Workflow？这个边界在哪里，决定了你在工程实践中应该选择什么样的架构。</p>
<p><strong>关于 LLM 的演进</strong>：随着模型能力的增强（更长的 context window、更强的 reasoning、内置的 tool use），LLM 和 Agent 之间的边界是否会逐渐模糊？OpenAI 的 o1/o3 系列通过 chain-of-thought 在模型内部实现了某种程度的&quot;规划&quot;，这是否意味着 Agent Runtime 的部分功能会被吸收进模型本身？</p>
<p><strong>关于成本和延迟</strong>：Agent 的每一轮循环都包含至少一次 LLM 调用。如果一个任务需要 5 轮循环，每轮 3 次 LLM 调用（Think + Plan + Reflect），就是 15 次调用。这个成本和延迟在生产环境中是否可接受？如何在 Agent 的灵活性和系统的性能之间找到平衡点？</p>
<p>这些问题没有标准答案，但它们定义了 Agentic 系统设计的核心张力。</p>
<hr>
<blockquote>
<p><strong>系列导航</strong>：本文是 Agentic 系列的第 02 篇。</p>
<ul>
<li>上一篇：<a href="/blog/engineering/agentic/01-From%20LLM%20to%20Agent">01 | From LLM to Agent</a></li>
<li>下一篇：<a href="/blog/engineering/agentic/03-Agent%20vs%20Workflow%20vs%20Automation">03 | Agent vs Workflow vs Automation</a></li>
<li>完整目录：<a href="/blog/engineering/agentic/01-From%20LLM%20to%20Agent">01 | From LLM to Agent</a></li>
</ul>
</blockquote>
19:T498b,<h2>一、引言</h2>
<p>人工智能正处于一次范式迁移的节点：从“能说”的大语言模型（LLM）走向“能做”的智能体（Agent）。LLM 带来了通用的语言理解和生成能力，但它仍然是一个<br><strong>封闭、被动、短期记忆</strong>的系统：知识停留在训练时刻，无法直接访问实时世界；只能在用户输入后响应；上下文窗口限制使得记忆易失；输出不含可执行语义，更谈不上与外界系统协作。</p>
<p><strong>Agent</strong> 的提出，正是为 LLM 补齐“行动力”：通过<strong>工具调用</strong>连入 API/数据库/计算环境，通过<strong>记忆</strong>维持跨会话状态，通过<strong>编排<br><strong>将复杂任务拆解为可控的工作流，必要时引入</strong>多 Agent 协作</strong>。当这四个维度协同起来，语言就不再是终点，而是驱动系统执行任务的接口。</p>
<h2>二、Agent 是什么</h2>
<p>我们将 Agent 抽象为：<strong>大脑（LLM） + 工具（Tools/Functions） + 记忆（Memory） + 编排（Orchestration）</strong>。</p>
<ul>
<li><strong>大脑</strong>：理解意图、推理计划、生成结构化中间表示（思考链/计划/工具参数）。</li>
<li><strong>工具</strong>：把自然语言转化为<strong>外部动作</strong>：HTTP API、数据库查询、代码执行、文件读写，甚至机器人控制。</li>
<li><strong>记忆</strong>：短期记忆承载对话上下文与临时事实；长期记忆借助向量数据库/关系库沉淀用户偏好、文档知识与任务状态。</li>
<li><strong>编排</strong>：以<strong>状态机/DAG</strong>表达任务流程，处理条件分支、并行、重试回退、超时与配额，提供可观测性与审计。</li>
</ul>
<blockquote>
<p>换句话说：Agent 是“会说话的操作系统进程”。它既遵循自然语言接口，又遵守工程系统的边界与约束。</p>
</blockquote>
<h2>三、Agent 能做什么</h2>
<ol>
<li><strong>检索增强生成（RAG）</strong>：在回答前检索企业知识库或互联网，降低幻觉，确保时效与可追溯引用。</li>
<li><strong>工具化操作</strong>：把“帮我预定会议室/查 Jira/跑报表”翻译为真实 API 调用与数据落库。</li>
<li><strong>任务分解与计划执行</strong>：从“调研—起草—审稿—发布”的完整管道，到“数据提取—转换—加载（ETL）”的数据工程链路。</li>
<li><strong>多 Agent 协作</strong>：研究员、撰稿员、质检员、执行官等角色并行或串行协同。</li>
<li><strong>持续记忆与个性化</strong>：长期学习用户偏好与业务上下文，形成“专属助理”。</li>
</ol>
<p>这些能力已在<strong>客服、法务审查、财务报表、运维巡检、投研分析、政企知识库</strong>等场景落地。</p>
<h2>四、为什么需要编排</h2>
<p>单一 LLM + 工具调用可以跑出 demo，但难以支撑生产。<strong>编排</strong>让 Agent 系统具备：</p>
<ul>
<li><strong>任务有序性</strong>：复杂流程的前后置依赖、并行合并、条件分支。</li>
<li><strong>可靠性</strong>：失败重试、幂等、回退策略、超时与熔断、降级链路。</li>
<li><strong>安全性</strong>：提示注入防护、工具白名单、参数校验、沙箱执行、RBAC 与审计。</li>
<li><strong>可观测性</strong>：结构化日志、链路追踪（OTEL）、成本与延迟指标、交互回放。</li>
</ul>
<blockquote>
<p>没有编排，就没有“可运营”的 Agent。</p>
</blockquote>
<h2>五、主流框架详解</h2>
<p>当前最具代表性的范式与框架：<br><strong>ReAct、Plan-and-Execute、LLMCompiler、LangChain、LangGraph、LlamaIndex、CrewAI/AutoGen</strong>。</p>
<h3>5.1 ReAct（Reason + Act）</h3>
<p><strong>背景</strong><br>2022 年提出，动机是让 LLM 的行为<em>可解释</em>：将“思考过程”与“实际动作”分离，便于调试与审计。</p>
<p><strong>要解决的问题</strong></p>
<ul>
<li>让模型在调用工具前给出<strong>思考链（Thought）</strong>，避免“黑箱行动”。</li>
<li>在“思考—行动—观察”循环中逐步逼近目标。</li>
</ul>
<p><strong>核心机制</strong><br><code>Thought → Action(tool, params) → Observation → Thought → ...</code></p>
<ul>
<li><strong>Thought</strong>：输出中间推理（可省略给用户，但用于系统决策）。</li>
<li><strong>Action</strong>：按 JSON/函数签名触发工具调用。</li>
<li><strong>Observation</strong>：工具/环境返回，再进入下一轮推理。</li>
</ul>
<p><strong>现状与生态</strong><br>ReAct 已成为各框架默认参考范式，LangChain/AutoGen 等均内置。</p>
<p><strong>典型应用</strong></p>
<ul>
<li>RAG 问答（先思考应检索哪些关键字→检索→解读→回答）。</li>
<li>金融/运维查询（先枚举数据源→调用行情/监控 API→计算→结论）。</li>
</ul>
<p><strong>优缺点</strong></p>
<ul>
<li><strong>优点</strong>：透明、易调试、适合逐步探索。</li>
<li><strong>缺点</strong>：每步都要调 LLM，延迟与成本上升；需要控制泄露 Thought。</li>
</ul>
<p><strong>示例（LangChain 简化）</strong></p>
<pre><code class="language-python">from langchain.agents import initialize_agent, load_tools
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model=&quot;gpt-4o-mini&quot;)
tools = load_tools([&quot;serpapi&quot;, &quot;llm-math&quot;], llm=llm)

agent = initialize_agent(tools, llm, agent=&quot;zero-shot-react-description&quot;, verbose=True)
agent.run(&quot;美元兑日元的即期汇率是多少？100 美元大约换多少日元？&quot;)
</code></pre>
<p><strong>学习建议</strong><br>先学 ReAct，再看其他模式；理解“中间思考—外部行动”的边界与安全性。</p>
<h3>5.2 Plan-and-Execute</h3>
<p><strong>背景</strong><br>为缓解 ReAct 调用频繁、成本高的问题，提出“先规划再执行”，把 LLM 调用集中到<strong>规划阶段</strong>。</p>
<p><strong>要解决的问题</strong></p>
<ul>
<li>降低长任务的 LLM 调用次数与延迟。</li>
<li>提高执行阶段的确定性与可回放性。</li>
</ul>
<p><strong>核心机制</strong></p>
<ul>
<li><strong>Planning</strong>：LLM 产出任务分解（步骤、依赖、所需工具）。</li>
<li><strong>Execution</strong>：流程引擎按计划逐步执行，必要时少量“再规划”。</li>
</ul>
<p><strong>现状与生态</strong><br>LangChain 等框架提供内置链路；在复杂长任务中广泛使用。</p>
<p><strong>典型应用</strong></p>
<ul>
<li>报告/白皮书生成（规划章节→检索资料→写作→审稿）。</li>
<li>数据工程（ETL）与指标计算。</li>
</ul>
<p><strong>优缺点</strong></p>
<ul>
<li><strong>优点</strong>：成本可控；对工程侧友好。</li>
<li><strong>缺点</strong>：对“初始计划质量”依赖高；需要良好的失败恢复策略。</li>
</ul>
<p><strong>示例（伪代码）</strong></p>
<pre><code class="language-python">plan = llm(&quot;把‘新能源车行业研究’分解为可执行步骤&quot;)
for step in plan.steps:
    execute(step)  # 工具/代码/SQL
final = llm(f&quot;根据执行产物撰写摘要：{collect_outputs()}&quot;)
</code></pre>
<p><strong>学习建议</strong><br>结合任务编排引擎（如 LangGraph）使用；关注“计划修正”的闭环设计。</p>
<h3>5.3 LLMCompiler</h3>
<p><strong>背景</strong><br>源自微软研究，借鉴编译器思想：把自然语言任务<strong>编译</strong>为可并行执行的<strong>DAG</strong>，以获得高吞吐。</p>
<p><strong>要解决的问题</strong></p>
<ul>
<li>将多工具/多数据源任务并行化，避免串行瓶颈。</li>
<li>把“任务—执行图”的关系结构化，便于优化。</li>
</ul>
<p><strong>核心机制</strong></p>
<ul>
<li><strong>编译</strong>：LLM 将任务语义转成节点与依赖（DAG）。</li>
<li><strong>执行</strong>：节点并行运行，统一汇总。</li>
</ul>
<p><strong>现状与生态</strong><br>学术与实验为主，工程落地探索中。</p>
<p><strong>典型应用</strong></p>
<ul>
<li>多网站并行爬取与聚合分析。</li>
<li>多 API 并行获取数据后统一建模。</li>
</ul>
<p><strong>优缺点</strong></p>
<ul>
<li><strong>优点</strong>：吞吐高、结构清晰。</li>
<li><strong>缺点</strong>：实现复杂；缺少成熟的标准化工具链。</li>
</ul>
<p><strong>示例（伪代码）</strong></p>
<pre><code class="language-python">dag = compile_to_dag(&quot;对‘政策/销量/技术’三方面做新能源车行业分析&quot;)
dag.execute_parallel()
summary = llm(&quot;汇总 DAG 结果并给出结论&quot;)
</code></pre>
<p><strong>学习建议</strong><br>理解 DAG/并行执行与幂等性；适合系统工程背景的团队。</p>
<h3>5.4 LangChain</h3>
<p><strong>背景</strong><br>2022 年开源，首个“把 LLM 嵌入应用”的<strong>通用开发框架</strong>。</p>
<p><strong>要解决的问题</strong></p>
<ul>
<li>统一抽象 Prompt/LLM/Memory/Tools/Chains/Agents。</li>
<li>快速搭建原型与 PoC，降低入门门槛。</li>
</ul>
<p><strong>核心特征/架构</strong></p>
<ul>
<li><strong>LLM Wrappers</strong>：适配主流云模型与本地模型。</li>
<li><strong>PromptTemplates</strong>：可参数化提示词。</li>
<li><strong>Memory</strong>：会话/长期记忆，支持自定义后端。</li>
<li><strong>Tools</strong>：声明式工具定义与参数校验。</li>
<li><strong>Chains/Agents</strong>：组装工作流或启用工具化智能体。</li>
</ul>
<p><strong>现状与生态</strong></p>
<ul>
<li>社区最大、教程与示例最全；大量第三方集成。</li>
<li>复杂生产系统往往与<strong>LangGraph</strong>/自研编排结合使用。</li>
</ul>
<p><strong>典型应用</strong></p>
<ul>
<li>文档问答（RAG Agent）。</li>
<li>智能客服/助手。</li>
<li>代码/数据处理助手。</li>
</ul>
<p><strong>优缺点</strong></p>
<ul>
<li><strong>优点</strong>：生态全、迭代快、原型成本低。</li>
<li><strong>缺点</strong>：组件众多、耦合度易升高；需谨慎裁剪。</li>
</ul>
<p><strong>示例（RAG QA 极简）</strong></p>
<pre><code class="language-python">from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model=&quot;gpt-4o-mini&quot;)
qa = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever())
print(qa.run(&quot;总结这份合同的关键风险&quot;))
</code></pre>
<p><strong>学习建议</strong><br>用它“站起来”，但不要把它当全部；与观测/编排/缓存协同设计。</p>
<h3>5.5 LangGraph（含 LangGraph Platform）</h3>
<p><strong>背景</strong><br>LangChain 的链式范式难以表达<strong>循环、回退、并行</strong>与<strong>长时状态</strong>。LangGraph 将 Agent 视为<strong>显式状态机</strong>/DAG，并与观测平台集成。</p>
<p><strong>要解决的问题</strong></p>
<ul>
<li>复杂工作流的<strong>可控性</strong>与<strong>可观测性</strong>。</li>
<li>长运行任务的<strong>状态持久化</strong>与<strong>弹性伸缩</strong>。</li>
</ul>
<p><strong>核心特征/架构</strong></p>
<ul>
<li><strong>状态图（StateGraph）</strong>：定义节点（函数/Agent）与边（条件/并行/回路）。</li>
<li><strong>人机协作</strong>：在关键节点注入“人工审核/纠偏”。</li>
<li><strong>与 LangSmith/OTEL</strong> 联动：日志、追踪、成本面板。</li>
<li><strong>Platform</strong>：受管端点、持久队列、版本化与回放。</li>
</ul>
<p><strong>现状与生态</strong><br>企业采用度上升；Platform 侧提供“从开发到部署”的一体化体验。</p>
<p><strong>典型应用</strong></p>
<ul>
<li>合规审查流水线：抽取 → 规则/LLM 检查 → 复核 → 报告。</li>
<li>企业知识库问答：检索 → 生成 → 评估不合格回退。</li>
</ul>
<p><strong>优缺点</strong></p>
<ul>
<li><strong>优点</strong>：工程化最佳平衡点；对复杂任务友好。</li>
<li><strong>缺点</strong>：学习成本较高；图的演进需要治理。</li>
</ul>
<p><strong>示例（检索→生成→评估→回退）</strong></p>
<pre><code class="language-python">from langgraph.graph import StateGraph


def retrieve(state): ...


def generate(state): ...


def evaluate(state): ...  # 返回 pass/fail


g = StateGraph()
g.add_node(&quot;retrieve&quot;, retrieve)
g.add_node(&quot;generate&quot;, generate)
g.add_node(&quot;evaluate&quot;, evaluate)

g.set_entry_point(&quot;retrieve&quot;)
g.add_edge(&quot;retrieve&quot;, &quot;generate&quot;)
g.add_edge(&quot;generate&quot;, &quot;evaluate&quot;)
g.add_conditional_edges(&quot;evaluate&quot;, {&quot;pass&quot;: &quot;END&quot;, &quot;fail&quot;: &quot;generate&quot;})
</code></pre>
<p><strong>学习建议</strong><br>把“业务流程图”翻译成“状态图”，自下而上替换节点：先用伪实现跑通，再替换为真实工具/服务。</p>
<h3>5.6 LlamaIndex</h3>
<p><strong>背景</strong><br>（原 GPT Index）从“让 LLM 使用外部数据”出发，沉淀为<strong>数据接入与检索增强平台</strong>。</p>
<p><strong>要解决的问题</strong></p>
<ul>
<li>把文档/表格/数据库接入到 LLM。</li>
<li>提供<strong>多索引</strong>与<strong>混合检索</strong>以提高召回与可控性。</li>
</ul>
<p><strong>核心特征/架构</strong></p>
<ul>
<li><strong>数据连接器</strong>：FS、S3、GDrive、Notion、数据库等。</li>
<li><strong>索引</strong>：向量索引、关键词索引、图索引等。</li>
<li><strong>检索</strong>：BM25 + 向量 + 重排（可插拔）。</li>
<li><strong>与 LangChain/LangGraph 兼容</strong>，可作为检索层。</li>
</ul>
<p><strong>现状与生态</strong><br>在知识库/文档问答领域最常用；正扩展到多模态。</p>
<p><strong>典型应用</strong></p>
<ul>
<li>合同与政策问答；内部 Wiki 助手；会议纪要问答。</li>
</ul>
<p><strong>优缺点</strong></p>
<ul>
<li><strong>优点</strong>：数据侧强、接入快、检索策略丰富。</li>
<li><strong>缺点</strong>：编排弱；需要配合工作流框架。</li>
</ul>
<p><strong>示例（向量索引）</strong></p>
<pre><code class="language-python">from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader

docs = SimpleDirectoryReader(&quot;docs&quot;).load_data()
index = GPTVectorStoreIndex.from_documents(docs)
query_engine = index.as_query_engine()
print(query_engine.query(&quot;列出这份合同的终止条款&quot;))
</code></pre>
<p><strong>学习建议</strong><br>作为“数据/RAG 层”的强力搭档，与 LangGraph 共同组成“检索 + 编排”的主干。</p>
<h3>5.7 CrewAI / AutoGen（多 Agent 协作）</h3>
<p><strong>背景</strong><br>开源社区探索“虚拟团队”形态：通过多个角色化 Agent 的协作完成复杂任务。</p>
<p><strong>要解决的问题</strong></p>
<ul>
<li>单 Agent 能力边界：需要专家分工与相互制衡。</li>
<li>让“研究—写作—审稿—发布”自然映射到多 Agent。</li>
</ul>
<p><strong>核心特征/架构</strong></p>
<ul>
<li><strong>角色与职责</strong>：researcher、writer、reviewer 等。</li>
<li><strong>消息编排</strong>：对话驱动的协同；可插人类审核。</li>
<li><strong>任务路由</strong>：不同子任务交由不同角色处理。</li>
</ul>
<p><strong>现状与生态</strong><br>科研/实验社区活跃；企业落地需要补齐观测、安全与治理。</p>
<p><strong>典型应用</strong></p>
<ul>
<li>行业研报与竞品分析；内容生产流水线。</li>
</ul>
<p><strong>优缺点</strong></p>
<ul>
<li><strong>优点</strong>：贴近人的协作心智模型，易扩展角色库。</li>
<li><strong>缺点</strong>：生产治理薄弱；复杂度随角色数上升。</li>
</ul>
<p><strong>示例（AutoGen 极简）</strong></p>
<pre><code class="language-python">from autogen import AssistantAgent, UserProxyAgent

assistant = AssistantAgent(&quot;researcher&quot;, llm_config={&quot;model&quot;: &quot;gpt-4o-mini&quot;})
user_proxy = UserProxyAgent(&quot;writer&quot;, human_input_mode=&quot;NEVER&quot;)
user_proxy.initiate_chat(assistant, message=&quot;写一份新能源车行业调研大纲&quot;)
</code></pre>
<p><strong>学习建议</strong><br>以“小团队”起步（2–3 角色），收敛职责边界；引入编排框架承接生产治理。</p>
<h2>六、学习路径（技术依赖关系）</h2>
<blockquote>
<p>只给“依赖链”，便于立刻开工：</p>
</blockquote>
<ol>
<li><strong>语言与接口</strong> → Python/JS 基础；HTTP/JSON；异步与并发。</li>
<li><strong>LLM 能力</strong> → Prompt Engineering；<strong>Function Calling/Tool Use</strong>；结构化输出（JSON Schema）。</li>
<li><strong>RAG 能力</strong> → 文档分块与清洗；嵌入模型；<strong>向量数据库（pgvector/Milvus/Weaviate）</strong>；混合检索与重排。</li>
<li><strong>编排能力</strong> → <strong>状态机/DAG（LangGraph）</strong>；重试回退；超时熔断；人机协作。</li>
<li><strong>运维能力</strong> → 日志/追踪（OpenTelemetry）；指标（Prometheus/Grafana）；安全（提示注入防护、RBAC、审计）；部署（Docker/K8s/Cloud<br>Run）。</li>
</ol>
<p>沿这条路径递进，你可以从“能调模型与工具”，稳步走到“能搭生产可运维的 Agent 系统”。</p>
<h2>七、未来展望</h2>
<p><strong>多模态 Agent</strong> 将同时处理文本、图像、语音与视频，统一在一个任务图里协同；<strong>模型路由与降级</strong>会让系统自动在质量、成本、延迟之间折中；<br><strong>Agent OS/编排平台</strong>将成为企业的“智能内核”，承载权限、任务、审计与经济计量；而 <strong>LLMOps 标准化</strong><br>则会把“可观测、安全治理、回放评测”固化为工程必修课。</p>
<h2>八、结语</h2>
<p>从 LLM 到 Agent，不只是“接口变了”，而是<strong>软件工程边界</strong>的扩大：语言成了新的“应用协议”，编排成了“智能内核”，数据与工具成了“外设”。掌握本文的框架图谱与依赖链，意味着你可以按需组装：以<br>LlamaIndex 做数据底座，以 LangGraph 管编排，以 LangChain/AutoGen/CrewAI 做场景拼装，再用监控与安全把它变成真正<strong>可运营</strong><br>的系统。愿你从 demo 出发，驶向生产。</p>
5:["$","article",null,{"className":"min-h-screen","children":["$","div",null,{"className":"mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12","children":[["$","header",null,{"className":"mb-8","children":[["$","nav",null,{"className":"flex items-center gap-1 text-sm mb-4","children":[["$","$L13",null,{"href":"/blog/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"博客"}],["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"Engineering"}],[["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/agentic/page/1","className":"text-blue-600 hover:text-blue-700 transition-colors","children":"Agentic 系统"}]]]}],["$","div",null,{"className":"flex items-center mb-6","children":["$","div",null,{"className":"inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal","children":[["$","svg",null,{"className":"w-4 h-4 mr-2 text-gray-400","fill":"none","stroke":"currentColor","viewBox":"0 0 24 24","children":["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","strokeWidth":2,"d":"M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z"}]}],["$","time",null,{"dateTime":"2025-12-01","children":"2025年12月01日"}]]}]}],["$","h1",null,{"className":"text-4xl font-bold text-gray-900 mb-6 text-center","children":"From LLM to Agent: Agentic 系统的知识地图"}],["$","div",null,{"className":"flex flex-wrap gap-2 mb-6 justify-center","children":[["$","$L13","Agentic",{"href":"/blog/tag/Agentic/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"Agentic"}],["$","$L13","AI Engineering",{"href":"/blog/tag/AI%20Engineering/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"AI Engineering"}],["$","$L13","LLM",{"href":"/blog/tag/LLM/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"LLM"}]]}]]}],["$","div",null,{"className":"max-w-5xl mx-auto","children":["$","$L14",null,{"content":"$15"}]}],["$","$10",null,{"fallback":["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":"加载导航中..."}],"children":["$","$L16",null,{"globalNav":{"prev":{"slug":"engineering/practice/AWS多泳道自动化持续交付实践","title":"AWS多泳道自动化持续交付实践","description":"本文面向 DevOps 架构师与云原生工程师，介绍如何基于 AWS CodePipeline + CloudFormation 构建一套支持多泳道（Multi-Lane）并行部署的 ECS 持续交付体系。该方案不仅解决并发部署的资源锁冲突问题，还实现模板集中治理与业务仓库完全解耦。","pubDate":"2025-10-29","tags":["AWS","DevOps","泳道部署"],"heroImage":"$undefined","content":"$17"},"next":{"slug":"engineering/agentic/02-From Prompt to Agent","title":"From Prompt to Agent: 为什么 LLM 本身不是 Agent","description":"LLM 是一个无状态的文本函数，Agent 是一个有状态的推理系统。本文从 LLM 的五大局限出发，精确定义 Agent 的组件模型与控制循环，并沿 Chatbot → Agent 的光谱逐级拆解，帮助你建立从 Prompt 到 Agent 的完整认知框架。","pubDate":"2025-12-05","tags":["Agentic","AI Engineering","LLM"],"heroImage":"$undefined","content":"$18"}},"tagNav":{"Agentic":{"prev":null,"next":"$5:props:children:props:children:props:children:2:props:children:props:globalNav:next"},"AI Engineering":{"prev":null,"next":"$5:props:children:props:children:props:children:2:props:children:props:globalNav:next"},"LLM":{"prev":{"slug":"insights/technology/AI-Agent技术科普","title":"Agent 技术科普：开启智能体的新时代","description":"本文面向工程与产品落地，采用“概述长文 + 框架细化 + 技术依赖链”的结构：前半部分回答*为什么与是什么*，中段把*主流框架逐一讲透*（背景、要解决的问题、核心机制、现状与生态、典型应用、优缺点、示例、学习建议），最后给出*最小依赖链*以便快速动手。","pubDate":"2025-09-25","tags":["AI Agent","LLM","智能体"],"heroImage":"$undefined","content":"$19"},"next":"$5:props:children:props:children:props:children:2:props:children:props:globalNav:next"}}}]}],["$","$L1a",null,{}]]}]}]}]
8:null
c:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
7:null
a:{"metadata":[["$","title","0",{"children":"From LLM to Agent: Agentic 系统的知识地图 - Skyfalling Blog"}],["$","meta","1",{"name":"description","content":"Agentic 系列开篇。从 LLM 的局限出发，定义 Agent 的核心组成，绘制 Agentic 系统全景架构图，并通过代码演示从 ChatCompletion 到完整 Agent 的演进路径。本文是整个系列 14 篇文章的精神锚点与导航地图。"}],["$","meta","2",{"property":"og:title","content":"From LLM to Agent: Agentic 系统的知识地图"}],["$","meta","3",{"property":"og:description","content":"Agentic 系列开篇。从 LLM 的局限出发，定义 Agent 的核心组成，绘制 Agentic 系统全景架构图，并通过代码演示从 ChatCompletion 到完整 Agent 的演进路径。本文是整个系列 14 篇文章的精神锚点与导航地图。"}],["$","meta","4",{"property":"og:type","content":"article"}],["$","meta","5",{"property":"article:published_time","content":"2025-12-01"}],["$","meta","6",{"property":"article:author","content":"Skyfalling"}],["$","meta","7",{"name":"twitter:card","content":"summary"}],["$","meta","8",{"name":"twitter:title","content":"From LLM to Agent: Agentic 系统的知识地图"}],["$","meta","9",{"name":"twitter:description","content":"Agentic 系列开篇。从 LLM 的局限出发，定义 Agent 的核心组成，绘制 Agentic 系统全景架构图，并通过代码演示从 ChatCompletion 到完整 Agent 的演进路径。本文是整个系列 14 篇文章的精神锚点与导航地图。"}],["$","link","10",{"rel":"shortcut icon","href":"/favicon.png"}],["$","link","11",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","link","12",{"rel":"icon","href":"/favicon.png"}],["$","link","13",{"rel":"apple-touch-icon","href":"/favicon.png"}]],"error":null,"digest":"$undefined"}
12:{"metadata":"$a:metadata","error":null,"digest":"$undefined"}
