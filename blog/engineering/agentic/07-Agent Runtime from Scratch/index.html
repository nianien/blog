<!DOCTYPE html><html lang="zh-CN"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/fffdcdb4fb651185.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-42d55485b4428e47.js"/><script src="/_next/static/chunks/4bd1b696-8ec333fca6b38e39.js" async=""></script><script src="/_next/static/chunks/1684-a2aac8a674e5d38c.js" async=""></script><script src="/_next/static/chunks/main-app-2791dc86ed05573e.js" async=""></script><script src="/_next/static/chunks/6874-7791217feaf05c17.js" async=""></script><script src="/_next/static/chunks/app/layout-142e67ac4336647c.js" async=""></script><script src="/_next/static/chunks/968-d7155a2506e36f1d.js" async=""></script><script src="/_next/static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js" async=""></script><meta name="next-size-adjust" content=""/><title>Agent Runtime from Scratch: 不依赖框架构建 Agent - Skyfalling Blog</title><meta name="description" content="不依赖 LangChain 等框架，从零实现一个功能完整的 Agent Runtime。逐模块构建 LLMClient、ToolRegistry、ToolExecutor、MessageManager 和核心控制循环，包含并行工具调用、Streaming、超时控制、死循环检测等高级特性，附完整可运行代码。"/><meta property="og:title" content="Agent Runtime from Scratch: 不依赖框架构建 Agent"/><meta property="og:description" content="不依赖 LangChain 等框架，从零实现一个功能完整的 Agent Runtime。逐模块构建 LLMClient、ToolRegistry、ToolExecutor、MessageManager 和核心控制循环，包含并行工具调用、Streaming、超时控制、死循环检测等高级特性，附完整可运行代码。"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-12-28"/><meta property="article:author" content="Skyfalling"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Agent Runtime from Scratch: 不依赖框架构建 Agent"/><meta name="twitter:description" content="不依赖 LangChain 等框架，从零实现一个功能完整的 Agent Runtime。逐模块构建 LLMClient、ToolRegistry、ToolExecutor、MessageManager 和核心控制循环，包含并行工具调用、Streaming、超时控制、死循环检测等高级特性，附完整可运行代码。"/><link rel="shortcut icon" href="/favicon.png"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><link rel="icon" href="/favicon.png"/><link rel="apple-touch-icon" href="/favicon.png"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_f367f3"><div hidden=""><!--$--><!--/$--></div><div class="min-h-screen flex flex-col"><header class="bg-[var(--background)]"><nav class="mx-auto flex max-w-7xl items-center justify-between p-6 lg:px-8" aria-label="Global"><div class="flex lg:flex-1"><a class="-m-1.5 p-1.5" href="/"><span class="sr-only">Skyfalling Blog</span><span class="text-2xl font-bold text-gray-900">Skyfalling</span></a></div><div class="flex lg:hidden"><button type="button" class="-m-2.5 inline-flex items-center justify-center rounded-md p-2.5 text-gray-700"><span class="sr-only">打开主菜单</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></button></div><div class="hidden lg:flex lg:gap-x-12"><a class="text-base font-semibold leading-6 transition-colors text-gray-900 hover:text-blue-600" href="/">首页</a><a class="text-base font-semibold leading-6 transition-colors text-blue-600 border-b-2 border-blue-600 pb-1" href="/blog/">博客</a><a class="text-base font-semibold leading-6 transition-colors text-gray-900 hover:text-blue-600" href="/about/">关于</a></div><div class="hidden lg:flex lg:flex-1 lg:justify-end"></div></nav></header><main class="flex-1"><article class="min-h-screen"><div class="mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8"><div class="rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12"><header class="mb-8"><nav class="flex items-center gap-1 text-sm mb-4"><a class="text-gray-500 hover:text-blue-600 transition-colors" href="/blog/page/1/">博客</a><span class="text-gray-300">/</span><a class="text-gray-500 hover:text-blue-600 transition-colors" href="/blog/category/engineering/page/1/">Engineering</a><span class="text-gray-300">/</span><a class="text-blue-600 hover:text-blue-700 transition-colors" href="/blog/category/engineering/agentic/page/1/">Agentic 系统</a></nav><div class="flex items-center mb-6"><div class="inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal"><svg class="w-4 h-4 mr-2 text-gray-400" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z"></path></svg><time dateTime="2025-12-28">2025年12月28日</time></div></div><h1 class="text-4xl font-bold text-gray-900 mb-6 text-center">Agent Runtime from Scratch: 不依赖框架构建 Agent</h1><div class="flex flex-wrap gap-2 mb-6 justify-center"><a class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors" href="/blog/tag/Agentic/page/1/">Agentic</a><a class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors" href="/blog/tag/AI%20Engineering/page/1/">AI Engineering</a><a class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors" href="/blog/tag/Runtime/page/1/">Runtime</a></div></header><div class="max-w-5xl mx-auto"><div class="prose prose-lg prose-gray mx-auto max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:text-blue-700 prose-strong:text-gray-900 prose-strong:font-semibold prose-li:text-gray-700 prose-hr:border-gray-300"><h1>Agent Runtime from Scratch: 不依赖框架构建 Agent</h1>
<blockquote>
<p>框架是加速器，不是知识的替代品。</p>
<p>本文是 Agentic 系列第 07 篇，也是 Phase 2 的收官之作。我们将抛开所有框架，用纯 Python 从零构建一个功能完整的 Agent Runtime。这是系列中代码量最大的一篇——每一行代码都指向同一个目标：让你彻底理解 Agent 的运行本质。</p>
</blockquote>
<hr>
<h2>1. 为什么要自己写 Agent Runtime？</h2>
<p>前几篇我们理解了控制循环（第 04 篇）、Tool Calling（第 05 篇）、Prompt 工程（第 06 篇）。但这些还停留在概念层面。现在的问题是：<strong>不用 LangChain、不用 LangGraph——你能写出一个 Agent 吗？</strong></p>
<p>自建 Runtime 的价值：</p>
<ul>
<li><strong>透明性</strong>：每一行代码你都清楚，出了问题知道往哪里看</li>
<li><strong>可控性</strong>：精确控制重试策略、超时机制、消息压缩、工具调度，而不被框架的默认行为绑架</li>
<li><strong>本质理解</strong>：理解了 Runtime 本质，用任何框架时都能一眼看出它在做什么、哪里做得不好</li>
</ul>
<p>更现实的原因：<strong>生产环境中很多 Agent 系统最终都走向了自研</strong>。框架在 PoC 阶段很方便，但到了需要精细控制 Token 成本、自定义 Observability、与内部基础设施深度集成时，框架往往成为障碍。</p>
<hr>
<h2>2. 架构设计</h2>
<pre><code>┌───────────────────────────────────────────────────┐
│                   AgentRuntime                     │
│                (Core Control Loop)                 │
│                                                    │
│  ┌────────────┐  ┌──────────────┐  ┌───────────┐ │
│  │ LLMClient  │  │MessageManager│  │ StateStore │ │
│  │ chat()     │  │ append()     │  │ save()     │ │
│  │ stream()   │  │ compress()   │  │ load()     │ │
│  │ retry()    │  │ count_tokens │  │ clear()    │ │
│  └─────┬──────┘  └──────┬───────┘  └───────────┘ │
│        │                │                          │
│        ▼                ▼                          │
│  ┌────────────────────────────────────┐            │
│  │          Runtime Loop              │            │
│  │  while not done and turns &lt; max:   │            │
│  │    response = llm.chat(messages)   │            │
│  │    if tool_calls:                  │            │
│  │      results = executor.run()      │            │
│  │    else: done = True               │            │
│  └──────────┬─────────────────────────┘            │
│       ┌─────┴──────┐                               │
│       ▼            ▼                                │
│  ┌──────────┐ ┌────────────┐                       │
│  │ToolRegist│ │ToolExecutor│                       │
│  │ register │ │ execute()  │                       │
│  │ schema() │ │ parallel() │                       │
│  └──────────┘ └────────────┘                       │
└───────────────────────────────────────────────────┘
</code></pre>
<p><strong>核心设计原则——职责分离</strong>：</p>
<table>
<thead>
<tr>
<th>模块</th>
<th>职责</th>
<th>边界</th>
</tr>
</thead>
<tbody><tr>
<td><code>LLMClient</code></td>
<td>封装模型调用，处理重试</td>
<td>只管&quot;调 API&quot;，不管消息历史</td>
</tr>
<tr>
<td><code>ToolRegistry</code></td>
<td>注册工具，生成 JSON Schema</td>
<td>只管&quot;有哪些工具&quot;，不管怎么调</td>
</tr>
<tr>
<td><code>ToolExecutor</code></td>
<td>解析 tool_calls，分发执行</td>
<td>只管&quot;执行工具&quot;，不管谁触发的</td>
</tr>
<tr>
<td><code>MessageManager</code></td>
<td>管理消息列表，Token 计数和压缩</td>
<td>只管&quot;消息&quot;，不管消息从哪来</td>
</tr>
<tr>
<td><code>AgentRuntime</code></td>
<td>组装一切，驱动控制循环</td>
<td>只管&quot;编排&quot;，不自己做具体事</td>
</tr>
</tbody></table>
<p>任何模块可独立替换。换 Anthropic API？只改 <code>LLMClient</code>。状态存 Redis？只改 <code>StateStore</code>。Runtime 本身不需要变动。</p>
<hr>
<h2>3. 逐步构建</h2>
<h3>Step 1: LLMClient — 封装模型调用</h3>
<p>封装 OpenAI 兼容接口，支持 <code>tools</code> / <code>tool_choice</code>，处理流式/非流式，实现指数退避重试。</p>
<pre><code class="language-python"># llm_client.py
import time, json, logging
from dataclasses import dataclass, field
from typing import Optional, Generator
from openai import OpenAI, APIError, RateLimitError, APITimeoutError

logger = logging.getLogger(__name__)

@dataclass
class ToolCall:
    id: str
    name: str
    arguments: dict

@dataclass
class LLMResponse:
    content: Optional[str] = None
    tool_calls: list[ToolCall] = field(default_factory=list)
    usage: dict = field(default_factory=dict)
    finish_reason: str = &quot;&quot;

    @property
    def has_tool_calls(self) -&gt; bool:
        return len(self.tool_calls) &gt; 0

class LLMClient:
    RETRYABLE_ERRORS = (RateLimitError, APITimeoutError, APIError)

    def __init__(self, model=&quot;gpt-4o&quot;, base_url=None, api_key=None,
                 max_retries=3, retry_base_delay=1.0, timeout=60.0):
        self.model = model
        self.max_retries = max_retries
        self.retry_base_delay = retry_base_delay
        self.client = OpenAI(base_url=base_url, api_key=api_key, timeout=timeout)

    def chat(self, messages, tools=None, tool_choice=&quot;auto&quot;, temperature=0.0):
        kwargs = {&quot;model&quot;: self.model, &quot;messages&quot;: messages,
                  &quot;temperature&quot;: temperature}
        if tools:
            kwargs[&quot;tools&quot;] = tools
            kwargs[&quot;tool_choice&quot;] = tool_choice
        raw = self._call_with_retry(**kwargs)
        return self._parse_response(raw)

    def stream(self, messages, tools=None, tool_choice=&quot;auto&quot;,
               temperature=0.0) -&gt; Generator[LLMResponse, None, None]:
        kwargs = {&quot;model&quot;: self.model, &quot;messages&quot;: messages,
                  &quot;temperature&quot;: temperature, &quot;stream&quot;: True}
        if tools:
            kwargs[&quot;tools&quot;] = tools
            kwargs[&quot;tool_choice&quot;] = tool_choice

        accumulated_tool_calls: dict[int, dict] = {}
        for chunk in self._call_with_retry(**kwargs):
            delta = chunk.choices[0].delta if chunk.choices else None
            if not delta:
                continue
            if delta.content:
                yield LLMResponse(content=delta.content)
            # 流式下 tool_calls 分片到达，需要累积拼装
            if delta.tool_calls:
                for tc in delta.tool_calls:
                    idx = tc.index
                    if idx not in accumulated_tool_calls:
                        accumulated_tool_calls[idx] = {
                            &quot;id&quot;: &quot;&quot;, &quot;name&quot;: &quot;&quot;, &quot;arguments&quot;: &quot;&quot;}
                    if tc.id: accumulated_tool_calls[idx][&quot;id&quot;] = tc.id
                    if tc.function.name:
                        accumulated_tool_calls[idx][&quot;name&quot;] = tc.function.name
                    if tc.function.arguments:
                        accumulated_tool_calls[idx][&quot;arguments&quot;] += \
                            tc.function.arguments

        if accumulated_tool_calls:
            tool_calls = []
            for d in accumulated_tool_calls.values():
                args = json.loads(d[&quot;arguments&quot;]) if d[&quot;arguments&quot;] else {}
                tool_calls.append(ToolCall(d[&quot;id&quot;], d[&quot;name&quot;], args))
            yield LLMResponse(tool_calls=tool_calls)

    def _call_with_retry(self, **kwargs):
        last_error = None
        for attempt in range(self.max_retries + 1):
            try:
                return self.client.chat.completions.create(**kwargs)
            except self.RETRYABLE_ERRORS as e:
                last_error = e
                if attempt &lt; self.max_retries:
                    delay = self.retry_base_delay * (2 ** attempt)
                    logger.warning(f&quot;Retry {attempt+1} in {delay}s: {e}&quot;)
                    time.sleep(delay)
        raise last_error

    def _parse_response(self, raw) -&gt; LLMResponse:
        choice = raw.choices[0]
        msg = choice.message
        tool_calls = []
        if msg.tool_calls:
            for tc in msg.tool_calls:
                args = json.loads(tc.function.arguments) \
                    if tc.function.arguments else {}
                tool_calls.append(ToolCall(tc.id, tc.function.name, args))
        return LLMResponse(
            content=msg.content, tool_calls=tool_calls,
            usage={&quot;prompt_tokens&quot;: raw.usage.prompt_tokens,
                   &quot;completion_tokens&quot;: raw.usage.completion_tokens,
                   &quot;total_tokens&quot;: raw.usage.total_tokens},
            finish_reason=choice.finish_reason)
</code></pre>
<p><strong>关键设计决策</strong>：</p>
<ol>
<li><strong>统一 <code>LLMResponse</code></strong>：无论底层用什么模型，Runtime 只看到同一结构——适配器模式。</li>
<li><strong>重试只针对可恢复错误</strong>：<code>RateLimitError</code> 值得重试，<code>AuthenticationError</code> 重试一万次也没用。</li>
<li><strong>流式 tool_calls 累积拼装</strong>：OpenAI 把 tool_calls 拆成多个 chunk（先发 name，再逐步发 arguments），必须在客户端拼装。这是容易踩的坑。</li>
</ol>
<hr>
<h3>Step 2: ToolRegistry — 工具注册与发现</h3>
<p>用装饰器注册函数，通过 type hints 和 docstring 自动生成 OpenAI 格式的 JSON Schema。</p>
<pre><code class="language-python"># tool_registry.py
import inspect, json
from typing import Any, Callable, Optional, get_type_hints

TYPE_MAP = {str: &quot;string&quot;, int: &quot;integer&quot;, float: &quot;number&quot;,
            bool: &quot;boolean&quot;, list: &quot;array&quot;, dict: &quot;object&quot;}

class ToolRegistry:
    def __init__(self):
        self._tools: dict[str, Callable] = {}
        self._schemas: dict[str, dict] = {}

    def tool(self, name=None, description=None):
        &quot;&quot;&quot;装饰器注册工具&quot;&quot;&quot;
        def decorator(func):
            n = name or func.__name__
            d = description or (func.__doc__ or &quot;&quot;).strip().split(&quot;\n&quot;)[0]
            self._tools[n] = func
            self._schemas[n] = self._gen_schema(func, n, d)
            return func
        return decorator

    def register(self, func, name=None, description=None):
        &quot;&quot;&quot;命令式注册（适用于无法加装饰器的场景）&quot;&quot;&quot;
        n = name or func.__name__
        d = description or (func.__doc__ or &quot;&quot;).strip().split(&quot;\n&quot;)[0]
        self._tools[n] = func
        self._schemas[n] = self._gen_schema(func, n, d)

    def get_function(self, name): return self._tools.get(name)
    def get_all_schemas(self): return list(self._schemas.values())
    def list_tools(self): return list(self._tools.keys())

    def _gen_schema(self, func, name, description):
        sig = inspect.signature(func)
        hints = get_type_hints(func)
        properties, required = {}, []
        for pname, param in sig.parameters.items():
            if pname in (&quot;self&quot;, &quot;cls&quot;): continue
            ptype = hints.get(pname, str)
            prop = {&quot;type&quot;: TYPE_MAP.get(ptype, &quot;string&quot;)}
            # 从 Google 风格 docstring 提取参数描述
            pdesc = self._param_desc(func, pname)
            if pdesc: prop[&quot;description&quot;] = pdesc
            properties[pname] = prop
            if param.default is inspect.Parameter.empty:
                required.append(pname)
        return {&quot;type&quot;: &quot;function&quot;, &quot;function&quot;: {
            &quot;name&quot;: name, &quot;description&quot;: description,
            &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;,
                           &quot;properties&quot;: properties, &quot;required&quot;: required}}}

    @staticmethod
    def _param_desc(func, param_name):
        doc = func.__doc__ or &quot;&quot;
        in_args = False
        for line in doc.split(&quot;\n&quot;):
            s = line.strip()
            if s.lower().startswith(&quot;args:&quot;): in_args = True; continue
            if in_args and param_name + &quot;:&quot; in s:
                return s.split(&quot;:&quot;, 1)[1].strip()
        return &quot;&quot;
</code></pre>
<p>验证效果：</p>
<pre><code class="language-python">registry = ToolRegistry()

@registry.tool()
def web_search(query: str, max_results: int = 5) -&gt; str:
    &quot;&quot;&quot;搜索网页内容
    Args:
        query: 搜索关键词
        max_results: 最大返回结果数量
    &quot;&quot;&quot;
    return f&quot;Results for: {query}&quot;

# 输出 OpenAI 格式的 tool schema
# {&quot;type&quot;:&quot;function&quot;,&quot;function&quot;:{&quot;name&quot;:&quot;web_search&quot;,&quot;description&quot;:&quot;搜索网页内容&quot;,
#  &quot;parameters&quot;:{&quot;type&quot;:&quot;object&quot;,&quot;properties&quot;:{&quot;query&quot;:{&quot;type&quot;:&quot;string&quot;,
#  &quot;description&quot;:&quot;搜索关键词&quot;},&quot;max_results&quot;:{&quot;type&quot;:&quot;integer&quot;,
#  &quot;description&quot;:&quot;最大返回结果数量&quot;}},&quot;required&quot;:[&quot;query&quot;]}}}
</code></pre>
<hr>
<h3>Step 3: ToolExecutor — 工具执行与结果处理</h3>
<p>接收 LLM 返回的 <code>tool_calls</code>，分发执行，收集结果，处理异常。支持串行和并行两种模式。</p>
<pre><code class="language-python"># tool_executor.py
import json, time, logging, traceback
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FTE
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class ToolResult:
    tool_call_id: str
    name: str
    result: str
    success: bool
    duration_ms: float = 0.0

class ToolExecutor:
    def __init__(self, registry, default_timeout=30.0, max_workers=4):
        self.registry = registry
        self.default_timeout = default_timeout
        self.max_workers = max_workers

    def execute(self, tool_calls) -&gt; list[ToolResult]:
        &quot;&quot;&quot;串行执行&quot;&quot;&quot;
        return [self._run_one(tc) for tc in tool_calls]

    def execute_parallel(self, tool_calls) -&gt; list[ToolResult]:
        &quot;&quot;&quot;并行执行（LLM 一次返回多个 tool_calls 时使用）&quot;&quot;&quot;
        if len(tool_calls) &lt;= 1:
            return self.execute(tool_calls)
        results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as pool:
            futures = {pool.submit(self._run_one, tc): tc for tc in tool_calls}
            for fut in futures:
                try:
                    results.append(fut.result(timeout=self.default_timeout))
                except FTE:
                    tc = futures[fut]
                    results.append(ToolResult(
                        tc.id, tc.name,
                        f&quot;Error: &#39;{tc.name}&#39; timed out after &quot;
                        f&quot;{self.default_timeout}s&quot;, False))
        return results

    def _run_one(self, tool_call) -&gt; ToolResult:
        start = time.monotonic()
        func = self.registry.get_function(tool_call.name)
        if not func:
            return ToolResult(tool_call.id, tool_call.name,
                f&quot;Error: Unknown tool &#39;{tool_call.name}&#39;. &quot;
                f&quot;Available: {self.registry.list_tools()}&quot;, False)
        try:
            result = func(**tool_call.arguments)
            if not isinstance(result, str):
                result = json.dumps(result, ensure_ascii=False, default=str)
            ms = (time.monotonic() - start) * 1000
            logger.info(f&quot;Tool &#39;{tool_call.name}&#39; OK in {ms:.0f}ms&quot;)
            return ToolResult(tool_call.id, tool_call.name, result, True, ms)
        except Exception as e:
            ms = (time.monotonic() - start) * 1000
            msg = f&quot;Error: {type(e).__name__}: {e}&quot;
            logger.error(f&quot;{msg}\n{traceback.format_exc()}&quot;)
            return ToolResult(tool_call.id, tool_call.name, msg, False, ms)

    @staticmethod
    def results_to_messages(results):
        return [{&quot;role&quot;: &quot;tool&quot;, &quot;tool_call_id&quot;: r.tool_call_id,
                 &quot;content&quot;: r.result} for r in results]
</code></pre>
<p><strong>串行 vs 并行的 Trade-off</strong>：串行简单可调试；并行在 LLM 同时返回多个独立 tool_calls 时显著降低延迟。LLM 在一次响应中返回多个 tool_calls 本身就隐含了&quot;它们之间无依赖&quot;——否则它会分成多轮调用。</p>
<hr>
<h3>Step 4: MessageManager — 消息历史管理与压缩</h3>
<p>解决 Agent 长对话中最常遇到的问题：<strong>消息越来越多，Context Window 不够用了</strong>。</p>
<pre><code class="language-python"># message_manager.py
import json, logging, tiktoken
from typing import Optional
from copy import deepcopy

logger = logging.getLogger(__name__)

class MessageManager:
    def __init__(self, system_prompt=&quot;&quot;, model=&quot;gpt-4o&quot;,
                 max_tokens=120000, compression_threshold=0.75):
        self.system_prompt = system_prompt
        self.max_tokens = max_tokens
        self.compression_threshold = compression_threshold
        try: self.enc = tiktoken.encoding_for_model(model)
        except KeyError: self.enc = tiktoken.get_encoding(&quot;cl100k_base&quot;)
        self._messages: list[dict] = []

    def append(self, msg):
        self._messages.append(msg)
        self._maybe_compress()

    def extend(self, msgs):
        self._messages.extend(msgs)
        self._maybe_compress()

    def get_messages(self):
        out = []
        if self.system_prompt:
            out.append({&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: self.system_prompt})
        out.extend(deepcopy(self._messages))
        return out

    def count_tokens(self, msgs=None):
        msgs = msgs or self.get_messages()
        total = 2  # priming tokens
        for m in msgs:
            total += 4  # per-message overhead
            for v in m.values():
                if isinstance(v, str): total += len(self.enc.encode(v))
                elif isinstance(v, list):
                    total += len(self.enc.encode(json.dumps(v)))
        return total

    def _maybe_compress(self):
        threshold = int(self.max_tokens * self.compression_threshold)
        if self.count_tokens() &lt;= threshold: return
        logger.info(&quot;Token threshold exceeded, compressing...&quot;)
        self._sliding_window_compress(threshold)

    def _sliding_window_compress(self, target):
        &quot;&quot;&quot;从最早的消息移除，保持 tool_call 对完整性。

        关键约束：assistant(tool_calls) 后面的 tool(result) 消息必须
        一起移除，否则 OpenAI API 会报错。
        &quot;&quot;&quot;
        msgs, i = self._messages, 0
        while i &lt; len(msgs):
            remaining = msgs[i:]
            sys_msgs = ([{&quot;role&quot;:&quot;system&quot;,&quot;content&quot;:self.system_prompt}]
                        if self.system_prompt else [])
            if self.count_tokens(sys_msgs + remaining) &lt;= target: break
            i += 1
            # 如果刚移除的是含 tool_calls 的 assistant，连续移除后续 tool 消息
            if (i &gt; 0 and msgs[i-1].get(&quot;role&quot;) == &quot;assistant&quot;
                    and msgs[i-1].get(&quot;tool_calls&quot;)):
                while i &lt; len(msgs) and msgs[i].get(&quot;role&quot;) == &quot;tool&quot;:
                    i += 1
        if i &gt; 0:
            summary = {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;:
                f&quot;[{i} earlier messages removed to fit context window.]&quot;}
            self._messages = [summary] + msgs[i:]
            logger.info(f&quot;Removed {i} msgs, tokens: {self.count_tokens()}&quot;)
</code></pre>
<p><strong>三个关键点</strong>：System Prompt 始终保留不参与压缩；tool_call 对必须保持完整（<code>assistant</code> + 后续 <code>tool</code> 消息一起删或一起留）；在 75% 时就触发压缩，给回复留够空间。</p>
<hr>
<h3>Step 5: StateStore — 状态持久化</h3>
<p>简单的键值存储，生产中替换为 Redis 或数据库即可。</p>
<pre><code class="language-python"># state_store.py
import json
from typing import Any, Optional
from pathlib import Path

class StateStore:
    def __init__(self, store_dir=&quot;.agent_state&quot;):
        self.dir = Path(store_dir)
        self.dir.mkdir(parents=True, exist_ok=True)
        self._cache: dict[str, Any] = {}

    def save(self, key, value):
        self._cache[key] = value
        (self.dir / f&quot;{key}.json&quot;).write_text(
            json.dumps(value, ensure_ascii=False, indent=2, default=str))

    def load(self, key, default=None):
        if key in self._cache: return self._cache[key]
        f = self.dir / f&quot;{key}.json&quot;
        if f.exists():
            v = json.loads(f.read_text())
            self._cache[key] = v
            return v
        return default

    def clear(self, key=None):
        if key:
            self._cache.pop(key, None)
            (self.dir / f&quot;{key}.json&quot;).unlink(missing_ok=True)
        else:
            self._cache.clear()
            for f in self.dir.glob(&quot;*.json&quot;): f.unlink()
</code></pre>
<hr>
<h2>4. 核心 Runtime Loop</h2>
<p>所有模块就绪，组装成完整的 <code>AgentRuntime</code>。这是整篇文章的核心。</p>
<pre><code class="language-python"># agent_runtime.py
import json, time, logging
from dataclasses import dataclass, field
from typing import Optional, Callable
from collections import Counter

from llm_client import LLMClient, LLMResponse
from tool_registry import ToolRegistry
from tool_executor import ToolExecutor
from message_manager import MessageManager
from state_store import StateStore

logger = logging.getLogger(__name__)

@dataclass
class RuntimeConfig:
    max_turns: int = 20               # 最大循环轮次
    max_total_time: float = 300.0     # 最大总执行时间（秒）
    parallel_tool_calls: bool = True  # 是否并行执行工具
    loop_detection_window: int = 4    # 死循环检测窗口
    loop_detection_threshold: int = 3 # 相同调用出现次数阈值

@dataclass
class AgentResult:
    content: str
    turns: int = 0
    total_tokens: int = 0
    tool_calls_made: list[dict] = field(default_factory=list)
    duration_ms: float = 0.0
    stopped_reason: str = &quot;&quot;

class AgentRuntime:
    def __init__(self, llm: LLMClient, registry: ToolRegistry,
                 system_prompt=&quot;You are a helpful assistant.&quot;,
                 config: Optional[RuntimeConfig] = None):
        self.llm = llm
        self.registry = registry
        self.executor = ToolExecutor(registry)
        self.config = config or RuntimeConfig()
        self.messages = MessageManager(system_prompt=system_prompt,
                                       model=llm.model)
        self.state = StateStore()
        self.on_tool_start: Optional[Callable] = None
        self.on_tool_end: Optional[Callable] = None

    def run(self, user_input: str) -&gt; AgentResult:
        start_time = time.monotonic()
        self.messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input})
        tools = self.registry.get_all_schemas() or None

        turns, total_tokens, all_tc = 0, 0, []
        tc_history: list[str] = []
        final_content, stopped = &quot;&quot;, &quot;completed&quot;

        while turns &lt; self.config.max_turns:
            turns += 1

            # ── 全局超时检查 ─────────────────────────────
            if time.monotonic() - start_time &gt; self.config.max_total_time:
                stopped = f&quot;timeout ({self.config.max_total_time}s)&quot;
                break

            # ── 调用 LLM ────────────────────────────────
            logger.info(f&quot;Turn {turns}: calling LLM...&quot;)
            resp = self.llm.chat(self.messages.get_messages(), tools=tools)
            total_tokens += resp.usage.get(&quot;total_tokens&quot;, 0)

            # ── 情况 1: 有 tool_calls → 执行工具 ────────
            if resp.has_tool_calls:
                # 构建 assistant 消息（必须包含 tool_calls 字段）
                asst = {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: resp.content,
                        &quot;tool_calls&quot;: [
                    {&quot;id&quot;: tc.id, &quot;type&quot;: &quot;function&quot;,
                     &quot;function&quot;: {&quot;name&quot;: tc.name,
                                  &quot;arguments&quot;: json.dumps(tc.arguments)}}
                    for tc in resp.tool_calls]}
                self.messages.append(asst)

                # 死循环检测
                sig = json.dumps([(tc.name, tc.arguments)
                                  for tc in resp.tool_calls], sort_keys=True)
                tc_history.append(sig)
                if self._detect_loop(tc_history):
                    stopped = &quot;loop_detected&quot;
                    final_content = (&quot;I&#39;m repeating the same actions. &quot;
                                     &quot;Stopping to summarize findings.&quot;)
                    break

                # 执行
                if self.on_tool_start: self.on_tool_start(resp.tool_calls)
                if self.config.parallel_tool_calls and len(resp.tool_calls) &gt; 1:
                    results = self.executor.execute_parallel(resp.tool_calls)
                else:
                    results = self.executor.execute(resp.tool_calls)
                if self.on_tool_end: self.on_tool_end(results)

                for tc, r in zip(resp.tool_calls, results):
                    all_tc.append({&quot;turn&quot;: turns, &quot;name&quot;: tc.name,
                        &quot;arguments&quot;: tc.arguments,
                        &quot;success&quot;: r.success, &quot;duration_ms&quot;: r.duration_ms})

                self.messages.extend(ToolExecutor.results_to_messages(results))

            # ── 情况 2: 纯文本 → 任务完成 ───────────────
            else:
                final_content = resp.content or &quot;&quot;
                self.messages.append(
                    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: final_content})
                break
        else:
            stopped = f&quot;max_turns ({self.config.max_turns})&quot;

        return AgentResult(
            content=final_content, turns=turns, total_tokens=total_tokens,
            tool_calls_made=all_tc,
            duration_ms=(time.monotonic() - start_time) * 1000,
            stopped_reason=stopped)

    def _detect_loop(self, history):
        &quot;&quot;&quot;滑动窗口 + 频次统计，同时捕获连续重复和交替重复&quot;&quot;&quot;
        w = self.config.loop_detection_window
        t = self.config.loop_detection_threshold
        if len(history) &lt; t: return False
        return any(c &gt;= t for c in Counter(history[-w:]).values())
</code></pre>
<h3>核心循环解读</h3>
<p><strong>两种退出路径</strong>——这是 Agent 与 Workflow 的本质区别：</p>
<pre><code>resp.has_tool_calls == True   → 继续（还有事要做）
resp.has_tool_calls == False  → break（LLM 认为任务完成了）
</code></pre>
<p><strong>为什么 assistant 消息必须包含 tool_calls 字段？</strong> 这是 OpenAI API 的协议约束。消息流必须是：<code>user</code> → <code>assistant(tool_calls)</code> → <code>tool(result)</code> → <code>assistant(final)</code>。打破这个顺序会报错。</p>
<p><strong>死循环检测</strong>用滑动窗口而非简单的&quot;连续 N 次相同&quot;，因为 LLM 有时会在两个工具间交替调用（A→B→A→B→...），这也是死循环，但不是&quot;连续相同&quot;。</p>
<hr>
<h2>5. 高级特性</h2>
<h3>5.1 Streaming 支持</h3>
<p>流式模式下需要边输出文本、边判断是否有 tool_calls：</p>
<pre><code class="language-python"># 添加到 AgentRuntime
def run_stream(self, user_input: str):
    self.messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input})
    tools = self.registry.get_all_schemas() or None
    turns = 0

    while turns &lt; self.config.max_turns:
        turns += 1
        content, final_tc = &quot;&quot;, None

        for chunk in self.llm.stream(self.messages.get_messages(), tools=tools):
            if chunk.content:
                content += chunk.content
                yield {&quot;type&quot;: &quot;text&quot;, &quot;content&quot;: chunk.content}
            if chunk.tool_calls:
                final_tc = chunk.tool_calls

        if final_tc:
            yield {&quot;type&quot;: &quot;tool_start&quot;,
                   &quot;calls&quot;: [{&quot;name&quot;:tc.name} for tc in final_tc]}
            asst = {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: content,
                    &quot;tool_calls&quot;: [
                {&quot;id&quot;:tc.id, &quot;type&quot;:&quot;function&quot;,
                 &quot;function&quot;:{&quot;name&quot;:tc.name,
                             &quot;arguments&quot;:json.dumps(tc.arguments)}}
                for tc in final_tc]}
            self.messages.append(asst)
            results = self.executor.execute(final_tc)
            self.messages.extend(ToolExecutor.results_to_messages(results))
            yield {&quot;type&quot;: &quot;tool_end&quot;,
                   &quot;results&quot;: [{&quot;name&quot;:r.name, &quot;ok&quot;:r.success} for r in results]}
        else:
            self.messages.append({&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:content})
            yield {&quot;type&quot;: &quot;done&quot;, &quot;content&quot;: content}
            break
</code></pre>
<h3>5.2 超时控制的两层设计</h3>
<pre><code>┌──────────────────────────────────────┐
│ 全局超时 (max_total_time = 300s)     │
│  ┌──────┐ ┌──────┐ ┌──────┐        │
│  │Tool 1│ │Tool 2│ │Tool 3│        │
│  │30s   │ │30s   │ │30s   │        │
│  └──────┘ └──────┘ └──────┘        │
│ 单工具超时 (default_timeout = 30s)   │
└──────────────────────────────────────┘
</code></pre>
<p>单工具超时在 <code>ToolExecutor</code> 中通过 <code>ThreadPoolExecutor.result(timeout=30)</code> 控制；全局超时在 Runtime 每轮循环开始时检查 elapsed time。</p>
<hr>
<h2>6. 完整示例：研究助手 Agent</h2>
<pre><code class="language-python"># research_agent.py
import json, os, logging
from agent_runtime import AgentRuntime, RuntimeConfig
from llm_client import LLMClient
from tool_registry import ToolRegistry

logging.basicConfig(level=logging.INFO,
    format=&quot;%(asctime)s [%(levelname)s] %(name)s: %(message)s&quot;)

registry = ToolRegistry()

@registry.tool()
def web_search(query: str, max_results: int = 5) -&gt; str:
    &quot;&quot;&quot;搜索网页内容
    Args:
        query: 搜索关键词
        max_results: 最大返回数量
    &quot;&quot;&quot;
    # 生产环境替换为 SerpAPI / Bing API
    return json.dumps([{&quot;title&quot;: f&quot;Result {i+1} for &#39;{query}&#39;&quot;,
        &quot;url&quot;: f&quot;https://example.com/article-{i+1}&quot;,
        &quot;snippet&quot;: f&quot;Detailed article about {query}, section {i+1}...&quot;}
        for i in range(min(max_results, 3))], ensure_ascii=False)

@registry.tool()
def read_url(url: str) -&gt; str:
    &quot;&quot;&quot;读取网页内容
    Args:
        url: 网页地址
    &quot;&quot;&quot;
    # 生产环境替换为 requests + BeautifulSoup
    return (f&quot;[Content from {url}]\n&quot;
            f&quot;Key points: 1) Fundamental concepts 2) Best practices &quot;
            f&quot;3) Common pitfalls 4) Case studies and benchmarks&quot;)

@registry.tool()
def write_file(filename: str, content: str) -&gt; str:
    &quot;&quot;&quot;写入文件
    Args:
        filename: 文件名
        content: 文本内容
    &quot;&quot;&quot;
    os.makedirs(&quot;output&quot;, exist_ok=True)
    path = os.path.join(&quot;output&quot;, os.path.basename(filename))
    with open(path, &quot;w&quot;) as f: f.write(content)
    return f&quot;Wrote {len(content)} chars to {path}&quot;

@registry.tool()
def ask_user(question: str) -&gt; str:
    &quot;&quot;&quot;向用户提问
    Args:
        question: 问题
    &quot;&quot;&quot;
    print(f&quot;\nAgent asks: {question}&quot;)
    return input(&quot;Your answer: &quot;)

SYSTEM_PROMPT = &quot;&quot;&quot;You are a research assistant. Workflow:
1. Search for information using web_search
2. Read promising articles using read_url (at least 2 sources)
3. Synthesize into a report and save with write_file
4. Present a summary. Use ask_user if the topic is unclear.&quot;&quot;&quot;

agent = AgentRuntime(
    llm=LLMClient(model=&quot;gpt-4o&quot;, api_key=os.environ.get(&quot;OPENAI_API_KEY&quot;)),
    registry=registry,
    system_prompt=SYSTEM_PROMPT,
    config=RuntimeConfig(max_turns=15, max_total_time=120.0))

if __name__ == &quot;__main__&quot;:
    result = agent.run(&quot;研究 Python asyncio 最佳实践，整理成技术报告并保存。&quot;)
    print(f&quot;\n{&#39;=&#39;*50}\nTurns: {result.turns} | Tokens: {result.total_tokens} &quot;
          f&quot;| {result.duration_ms:.0f}ms | {result.stopped_reason}&quot;)
    for tc in result.tool_calls_made:
        print(f&quot;  Turn {tc[&#39;turn&#39;]}: {tc[&#39;name&#39;]}() &quot;
              f&quot;{&#39;OK&#39; if tc[&#39;success&#39;] else &#39;FAIL&#39;} {tc[&#39;duration_ms&#39;]:.0f}ms&quot;)
    print(f&quot;\n{result.content[:300]}&quot;)
</code></pre>
<h3>执行 Trace</h3>
<pre><code>Turn 1: calling LLM...  → web_search(&quot;Python asyncio best practices&quot;)
Turn 2: calling LLM...  → read_url(url1) + read_url(url2)  [parallel]
Turn 3: calling LLM...  → web_search(&quot;asyncio common pitfalls&quot;)
Turn 4: calling LLM...  → read_url(url3)
Turn 5: calling LLM...  → write_file(&quot;asyncio-report.md&quot;, ...)
Turn 6: calling LLM...  → [no tool_calls] → Done

==================================================
Turns: 6 | Tokens: 8432 | 13245ms | completed
  Turn 1: web_search() OK 45ms
  Turn 2: read_url() OK 120ms
  Turn 2: read_url() OK 135ms
  Turn 3: web_search() OK 38ms
  Turn 4: read_url() OK 110ms
  Turn 5: write_file() OK 5ms
</code></pre>
<p>注意 Turn 2：LLM 返回了两个 <code>read_url</code>，Runtime 自动并行执行。</p>
<hr>
<h2>7. 与框架对比</h2>
<h3>自建 vs 框架</h3>
<table>
<thead>
<tr>
<th>维度</th>
<th>自建 Runtime</th>
<th>框架（LangChain 等）</th>
</tr>
</thead>
<tbody><tr>
<td><strong>透明性</strong></td>
<td>完全透明</td>
<td>需要读框架源码</td>
</tr>
<tr>
<td><strong>调试</strong></td>
<td>直接 breakpoint</td>
<td>需要理解框架抽象层</td>
</tr>
<tr>
<td><strong>定制</strong></td>
<td>任何行为可改</td>
<td>受 API 设计约束</td>
</tr>
<tr>
<td><strong>依赖</strong></td>
<td><code>openai</code> + <code>tiktoken</code></td>
<td>几十个传递依赖</td>
</tr>
<tr>
<td><strong>边界情况</strong></td>
<td>自己发现和处理</td>
<td>社区帮你踩过坑</td>
</tr>
<tr>
<td><strong>生态集成</strong></td>
<td>每个都要自己写</td>
<td>现成的 VectorStore/Retriever</td>
</tr>
<tr>
<td><strong>开发速度</strong></td>
<td>初期更慢</td>
<td>有模板更快</td>
</tr>
</tbody></table>
<h3>决策建议</h3>
<ul>
<li><strong>学习阶段</strong>：一定要自建一次。不理解原理就用框架，永远无法判断框架是否在坑你。</li>
<li><strong>PoC / Hackathon</strong>：用框架，速度第一。</li>
<li><strong>生产系统</strong>：自建核心 Runtime + 选择性使用框架组件（如只用 LangChain 的 Retriever）。</li>
<li><strong>基础设施团队</strong>：自建。你们的需求框架大概率满足不了。</li>
</ul>
<hr>
<h2>8. 结语：Phase 2 完成</h2>
<p>到这里，Phase 2 四篇文章全部完成：</p>
<ul>
<li><strong>第 04 篇</strong>：理解控制循环 — Observe → Think → Act → Reflect</li>
<li><strong>第 05 篇</strong>：深入 Tool Calling — JSON Schema、Function Calling、Structured Output</li>
<li><strong>第 06 篇</strong>：Prompt Engineering — System Prompt 设计、工具选择引导、Reflection Prompt</li>
<li><strong>第 07 篇（本篇）</strong>：把以上所有知识组装成可运行的 Agent Runtime</li>
</ul>
<p>此刻你有能力<strong>不依赖任何框架，从零构建功能完整的 Agent 系统</strong>。</p>
<p>但如果你运行过这个 Agent，会很快发现几个问题：</p>
<ol>
<li><strong>没有记忆</strong>：每次启动都是白纸，不记得上次的对话</li>
<li><strong>不会计划</strong>：面对复杂任务只是一步步试，没有全局规划</li>
<li><strong>一个不够用</strong>：有些任务需要不同角色的 Agent 协作</li>
</ol>
<p>这就是 Phase 3 要解决的问题：</p>
<ul>
<li><strong>第 08 篇</strong>：Memory Architecture — Agent 的状态与记忆体系</li>
<li><strong>第 09 篇</strong>：RAG as Cognitive Memory — 检索增强生成的工程实践</li>
<li><strong>第 10 篇</strong>：Planning and Reflection — 从 ReAct 到分层规划</li>
<li><strong>第 11 篇</strong>：Multi-Agent Collaboration — 多 Agent 协作</li>
</ul>
<p>Phase 2 给了你造一把锤子的能力。Phase 3 将教你如何造一个工具箱。</p>
<hr>
<blockquote>
<p><strong>系列导航</strong>：本文是 Agentic 系列的第 07 篇。</p>
<ul>
<li>上一篇：<a href="/blog/engineering/agentic/06-Prompt%20Engineering%20for%20Agents">06 | Prompt Engineering for Agents</a></li>
<li>下一篇：<a href="/blog/engineering/agentic/08-Memory%20Architecture">08 | Memory Architecture</a></li>
<li>完整目录：<a href="/blog/engineering/agentic/01-From%20LLM%20to%20Agent">01 | From LLM to Agent</a></li>
</ul>
</blockquote>
</div></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><div class="mt-12 pt-8 border-t border-gray-200">加载导航中...</div><!--/$--><div class="mt-16 border-t border-gray-200 pt-8"><div class="mx-auto max-w-3xl"><h3 class="text-2xl font-bold text-gray-900 mb-8">评论</h3></div></div></div></div></article><!--$--><!--/$--></main><footer class="bg-[var(--background)]"><div class="mx-auto max-w-7xl px-6 py-12 lg:px-8"><p class="text-center text-xs leading-5 text-gray-400">© <!-- -->2026<!-- --> Skyfalling</p></div></footer></div><script src="/_next/static/chunks/webpack-42d55485b4428e47.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[10616,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"7177\",\"static/chunks/app/layout-142e67ac4336647c.js\"],\"default\"]\n3:I[87555,[],\"\"]\n4:I[31295,[],\"\"]\n6:I[59665,[],\"OutletBoundary\"]\n9:I[74911,[],\"AsyncMetadataOutlet\"]\nb:I[59665,[],\"ViewportBoundary\"]\nd:I[59665,[],\"MetadataBoundary\"]\nf:I[26614,[],\"\"]\n:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/fffdcdb4fb651185.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"bJbIT2Kmv3sjcEJSOV0Wp\",\"p\":\"\",\"c\":[\"\",\"blog\",\"engineering\",\"agentic\",\"07-Agent%20Runtime%20from%20Scratch\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"engineering/agentic/07-Agent%20Runtime%20from%20Scratch\",\"c\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/fffdcdb4fb651185.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"zh-CN\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_f367f3\",\"children\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex flex-col\",\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"main\",null,{\"className\":\"flex-1\",\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"footer\",null,{\"className\":\"bg-[var(--background)]\",\"children\":[\"$\",\"div\",null,{\"className\":\"mx-auto max-w-7xl px-6 py-12 lg:px-8\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-center text-xs leading-5 text-gray-400\",\"children\":[\"© \",2026,\" Skyfalling\"]}]}]}]]}]}]}]]}],{\"children\":[\"blog\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"engineering/agentic/07-Agent%20Runtime%20from%20Scratch\",\"c\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",null,[\"$\",\"$L6\",null,{\"children\":[\"$L7\",\"$L8\",[\"$\",\"$L9\",null,{\"promise\":\"$@a\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"UdXMkSJghQ15vncsG_NM-v\",{\"children\":[[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],[\"$\",\"$Ld\",null,{\"children\":\"$Le\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$f\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"10:\"$Sreact.suspense\"\n11:I[74911,[],\"AsyncMetadata\"]\n13:I[6874,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"\"]\n14:I[32923,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"default\"]\n16:I[40780,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"default\"]\n1b:I[85300,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"default\"]\ne:[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$10\",null,{\"fallback\":null,\"children\":[\"$\",\"$L11\",null,{\"promise\":\"$@12\"}]}]}]\n15:T9c80,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eAgent Runtime from Scratch: 不依赖框架构建 Agent\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e框架是加速器，不是知识的替代品。\u003c/p\u003e\n\u003cp\u003e本文是 Agentic 系列第 07 篇，也是 Phase 2 的收官之作。我们将抛开所有框架，用纯 Python 从零构建一个功能完整的 Agent Runtime。这是系列中代码量最大的一篇——每一行代码都指向同一个目标：让你彻底理解 Agent 的运行本质。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch2\u003e1. 为什么要自己写 Agent Runtime？\u003c/h2\u003e\n\u003cp\u003e前几篇我们理解了控制循环（第 04 篇）、Tool Calling（第 05 篇）、Prompt 工程（第 06 篇）。但这些还停留在概念层面。现在的问题是：\u003cstrong\u003e不用 LangChain、不用 LangGraph——你能写出一个 Agent 吗？\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e自建 Runtime 的价值：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e透明性\u003c/strong\u003e：每一行代码你都清楚，出了问题知道往哪里看\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e可控性\u003c/strong\u003e：精确控制重试策略、超时机制、消息压缩、工具调度，而不被框架的默认行为绑架\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e本质理解\u003c/strong\u003e：理解了 Runtime 本质，用任何框架时都能一眼看出它在做什么、哪里做得不好\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e更现实的原因：\u003cstrong\u003e生产环境中很多 Agent 系统最终都走向了自研\u003c/strong\u003e。框架在 PoC 阶段很方便，但到了需要精细控制 Token 成本、自定义 Observability、与内部基础设施深度集成时，框架往往成为障碍。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e2. 架构设计\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e┌───────────────────────────────────────────────────┐\n│                   AgentRuntime                     │\n│                (Core Control Loop)                 │\n│                                                    │\n│  ┌────────────┐  ┌──────────────┐  ┌───────────┐ │\n│  │ LLMClient  │  │MessageManager│  │ StateStore │ │\n│  │ chat()     │  │ append()     │  │ save()     │ │\n│  │ stream()   │  │ compress()   │  │ load()     │ │\n│  │ retry()    │  │ count_tokens │  │ clear()    │ │\n│  └─────┬──────┘  └──────┬───────┘  └───────────┘ │\n│        │                │                          │\n│        ▼                ▼                          │\n│  ┌────────────────────────────────────┐            │\n│  │          Runtime Loop              │            │\n│  │  while not done and turns \u0026lt; max:   │            │\n│  │    response = llm.chat(messages)   │            │\n│  │    if tool_calls:                  │            │\n│  │      results = executor.run()      │            │\n│  │    else: done = True               │            │\n│  └──────────┬─────────────────────────┘            │\n│       ┌─────┴──────┐                               │\n│       ▼            ▼                                │\n│  ┌──────────┐ ┌────────────┐                       │\n│  │ToolRegist│ │ToolExecutor│                       │\n│  │ register │ │ execute()  │                       │\n│  │ schema() │ │ parallel() │                       │\n│  └──────────┘ └────────────┘                       │\n└───────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e核心设计原则——职责分离\u003c/strong\u003e：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e模块\u003c/th\u003e\n\u003cth\u003e职责\u003c/th\u003e\n\u003cth\u003e边界\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eLLMClient\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e封装模型调用，处理重试\u003c/td\u003e\n\u003ctd\u003e只管\u0026quot;调 API\u0026quot;，不管消息历史\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eToolRegistry\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e注册工具，生成 JSON Schema\u003c/td\u003e\n\u003ctd\u003e只管\u0026quot;有哪些工具\u0026quot;，不管怎么调\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eToolExecutor\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e解析 tool_calls，分发执行\u003c/td\u003e\n\u003ctd\u003e只管\u0026quot;执行工具\u0026quot;，不管谁触发的\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eMessageManager\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e管理消息列表，Token 计数和压缩\u003c/td\u003e\n\u003ctd\u003e只管\u0026quot;消息\u0026quot;，不管消息从哪来\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eAgentRuntime\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e组装一切，驱动控制循环\u003c/td\u003e\n\u003ctd\u003e只管\u0026quot;编排\u0026quot;，不自己做具体事\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e任何模块可独立替换。换 Anthropic API？只改 \u003ccode\u003eLLMClient\u003c/code\u003e。状态存 Redis？只改 \u003ccode\u003eStateStore\u003c/code\u003e。Runtime 本身不需要变动。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e3. 逐步构建\u003c/h2\u003e\n\u003ch3\u003eStep 1: LLMClient — 封装模型调用\u003c/h3\u003e\n\u003cp\u003e封装 OpenAI 兼容接口，支持 \u003ccode\u003etools\u003c/code\u003e / \u003ccode\u003etool_choice\u003c/code\u003e，处理流式/非流式，实现指数退避重试。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# llm_client.py\nimport time, json, logging\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Generator\nfrom openai import OpenAI, APIError, RateLimitError, APITimeoutError\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ToolCall:\n    id: str\n    name: str\n    arguments: dict\n\n@dataclass\nclass LLMResponse:\n    content: Optional[str] = None\n    tool_calls: list[ToolCall] = field(default_factory=list)\n    usage: dict = field(default_factory=dict)\n    finish_reason: str = \u0026quot;\u0026quot;\n\n    @property\n    def has_tool_calls(self) -\u0026gt; bool:\n        return len(self.tool_calls) \u0026gt; 0\n\nclass LLMClient:\n    RETRYABLE_ERRORS = (RateLimitError, APITimeoutError, APIError)\n\n    def __init__(self, model=\u0026quot;gpt-4o\u0026quot;, base_url=None, api_key=None,\n                 max_retries=3, retry_base_delay=1.0, timeout=60.0):\n        self.model = model\n        self.max_retries = max_retries\n        self.retry_base_delay = retry_base_delay\n        self.client = OpenAI(base_url=base_url, api_key=api_key, timeout=timeout)\n\n    def chat(self, messages, tools=None, tool_choice=\u0026quot;auto\u0026quot;, temperature=0.0):\n        kwargs = {\u0026quot;model\u0026quot;: self.model, \u0026quot;messages\u0026quot;: messages,\n                  \u0026quot;temperature\u0026quot;: temperature}\n        if tools:\n            kwargs[\u0026quot;tools\u0026quot;] = tools\n            kwargs[\u0026quot;tool_choice\u0026quot;] = tool_choice\n        raw = self._call_with_retry(**kwargs)\n        return self._parse_response(raw)\n\n    def stream(self, messages, tools=None, tool_choice=\u0026quot;auto\u0026quot;,\n               temperature=0.0) -\u0026gt; Generator[LLMResponse, None, None]:\n        kwargs = {\u0026quot;model\u0026quot;: self.model, \u0026quot;messages\u0026quot;: messages,\n                  \u0026quot;temperature\u0026quot;: temperature, \u0026quot;stream\u0026quot;: True}\n        if tools:\n            kwargs[\u0026quot;tools\u0026quot;] = tools\n            kwargs[\u0026quot;tool_choice\u0026quot;] = tool_choice\n\n        accumulated_tool_calls: dict[int, dict] = {}\n        for chunk in self._call_with_retry(**kwargs):\n            delta = chunk.choices[0].delta if chunk.choices else None\n            if not delta:\n                continue\n            if delta.content:\n                yield LLMResponse(content=delta.content)\n            # 流式下 tool_calls 分片到达，需要累积拼装\n            if delta.tool_calls:\n                for tc in delta.tool_calls:\n                    idx = tc.index\n                    if idx not in accumulated_tool_calls:\n                        accumulated_tool_calls[idx] = {\n                            \u0026quot;id\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;arguments\u0026quot;: \u0026quot;\u0026quot;}\n                    if tc.id: accumulated_tool_calls[idx][\u0026quot;id\u0026quot;] = tc.id\n                    if tc.function.name:\n                        accumulated_tool_calls[idx][\u0026quot;name\u0026quot;] = tc.function.name\n                    if tc.function.arguments:\n                        accumulated_tool_calls[idx][\u0026quot;arguments\u0026quot;] += \\\n                            tc.function.arguments\n\n        if accumulated_tool_calls:\n            tool_calls = []\n            for d in accumulated_tool_calls.values():\n                args = json.loads(d[\u0026quot;arguments\u0026quot;]) if d[\u0026quot;arguments\u0026quot;] else {}\n                tool_calls.append(ToolCall(d[\u0026quot;id\u0026quot;], d[\u0026quot;name\u0026quot;], args))\n            yield LLMResponse(tool_calls=tool_calls)\n\n    def _call_with_retry(self, **kwargs):\n        last_error = None\n        for attempt in range(self.max_retries + 1):\n            try:\n                return self.client.chat.completions.create(**kwargs)\n            except self.RETRYABLE_ERRORS as e:\n                last_error = e\n                if attempt \u0026lt; self.max_retries:\n                    delay = self.retry_base_delay * (2 ** attempt)\n                    logger.warning(f\u0026quot;Retry {attempt+1} in {delay}s: {e}\u0026quot;)\n                    time.sleep(delay)\n        raise last_error\n\n    def _parse_response(self, raw) -\u0026gt; LLMResponse:\n        choice = raw.choices[0]\n        msg = choice.message\n        tool_calls = []\n        if msg.tool_calls:\n            for tc in msg.tool_calls:\n                args = json.loads(tc.function.arguments) \\\n                    if tc.function.arguments else {}\n                tool_calls.append(ToolCall(tc.id, tc.function.name, args))\n        return LLMResponse(\n            content=msg.content, tool_calls=tool_calls,\n            usage={\u0026quot;prompt_tokens\u0026quot;: raw.usage.prompt_tokens,\n                   \u0026quot;completion_tokens\u0026quot;: raw.usage.completion_tokens,\n                   \u0026quot;total_tokens\u0026quot;: raw.usage.total_tokens},\n            finish_reason=choice.finish_reason)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e关键设计决策\u003c/strong\u003e：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e统一 \u003ccode\u003eLLMResponse\u003c/code\u003e\u003c/strong\u003e：无论底层用什么模型，Runtime 只看到同一结构——适配器模式。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e重试只针对可恢复错误\u003c/strong\u003e：\u003ccode\u003eRateLimitError\u003c/code\u003e 值得重试，\u003ccode\u003eAuthenticationError\u003c/code\u003e 重试一万次也没用。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e流式 tool_calls 累积拼装\u003c/strong\u003e：OpenAI 把 tool_calls 拆成多个 chunk（先发 name，再逐步发 arguments），必须在客户端拼装。这是容易踩的坑。\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3\u003eStep 2: ToolRegistry — 工具注册与发现\u003c/h3\u003e\n\u003cp\u003e用装饰器注册函数，通过 type hints 和 docstring 自动生成 OpenAI 格式的 JSON Schema。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# tool_registry.py\nimport inspect, json\nfrom typing import Any, Callable, Optional, get_type_hints\n\nTYPE_MAP = {str: \u0026quot;string\u0026quot;, int: \u0026quot;integer\u0026quot;, float: \u0026quot;number\u0026quot;,\n            bool: \u0026quot;boolean\u0026quot;, list: \u0026quot;array\u0026quot;, dict: \u0026quot;object\u0026quot;}\n\nclass ToolRegistry:\n    def __init__(self):\n        self._tools: dict[str, Callable] = {}\n        self._schemas: dict[str, dict] = {}\n\n    def tool(self, name=None, description=None):\n        \u0026quot;\u0026quot;\u0026quot;装饰器注册工具\u0026quot;\u0026quot;\u0026quot;\n        def decorator(func):\n            n = name or func.__name__\n            d = description or (func.__doc__ or \u0026quot;\u0026quot;).strip().split(\u0026quot;\\n\u0026quot;)[0]\n            self._tools[n] = func\n            self._schemas[n] = self._gen_schema(func, n, d)\n            return func\n        return decorator\n\n    def register(self, func, name=None, description=None):\n        \u0026quot;\u0026quot;\u0026quot;命令式注册（适用于无法加装饰器的场景）\u0026quot;\u0026quot;\u0026quot;\n        n = name or func.__name__\n        d = description or (func.__doc__ or \u0026quot;\u0026quot;).strip().split(\u0026quot;\\n\u0026quot;)[0]\n        self._tools[n] = func\n        self._schemas[n] = self._gen_schema(func, n, d)\n\n    def get_function(self, name): return self._tools.get(name)\n    def get_all_schemas(self): return list(self._schemas.values())\n    def list_tools(self): return list(self._tools.keys())\n\n    def _gen_schema(self, func, name, description):\n        sig = inspect.signature(func)\n        hints = get_type_hints(func)\n        properties, required = {}, []\n        for pname, param in sig.parameters.items():\n            if pname in (\u0026quot;self\u0026quot;, \u0026quot;cls\u0026quot;): continue\n            ptype = hints.get(pname, str)\n            prop = {\u0026quot;type\u0026quot;: TYPE_MAP.get(ptype, \u0026quot;string\u0026quot;)}\n            # 从 Google 风格 docstring 提取参数描述\n            pdesc = self._param_desc(func, pname)\n            if pdesc: prop[\u0026quot;description\u0026quot;] = pdesc\n            properties[pname] = prop\n            if param.default is inspect.Parameter.empty:\n                required.append(pname)\n        return {\u0026quot;type\u0026quot;: \u0026quot;function\u0026quot;, \u0026quot;function\u0026quot;: {\n            \u0026quot;name\u0026quot;: name, \u0026quot;description\u0026quot;: description,\n            \u0026quot;parameters\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;object\u0026quot;,\n                           \u0026quot;properties\u0026quot;: properties, \u0026quot;required\u0026quot;: required}}}\n\n    @staticmethod\n    def _param_desc(func, param_name):\n        doc = func.__doc__ or \u0026quot;\u0026quot;\n        in_args = False\n        for line in doc.split(\u0026quot;\\n\u0026quot;):\n            s = line.strip()\n            if s.lower().startswith(\u0026quot;args:\u0026quot;): in_args = True; continue\n            if in_args and param_name + \u0026quot;:\u0026quot; in s:\n                return s.split(\u0026quot;:\u0026quot;, 1)[1].strip()\n        return \u0026quot;\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e验证效果：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eregistry = ToolRegistry()\n\n@registry.tool()\ndef web_search(query: str, max_results: int = 5) -\u0026gt; str:\n    \u0026quot;\u0026quot;\u0026quot;搜索网页内容\n    Args:\n        query: 搜索关键词\n        max_results: 最大返回结果数量\n    \u0026quot;\u0026quot;\u0026quot;\n    return f\u0026quot;Results for: {query}\u0026quot;\n\n# 输出 OpenAI 格式的 tool schema\n# {\u0026quot;type\u0026quot;:\u0026quot;function\u0026quot;,\u0026quot;function\u0026quot;:{\u0026quot;name\u0026quot;:\u0026quot;web_search\u0026quot;,\u0026quot;description\u0026quot;:\u0026quot;搜索网页内容\u0026quot;,\n#  \u0026quot;parameters\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;object\u0026quot;,\u0026quot;properties\u0026quot;:{\u0026quot;query\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;string\u0026quot;,\n#  \u0026quot;description\u0026quot;:\u0026quot;搜索关键词\u0026quot;},\u0026quot;max_results\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;integer\u0026quot;,\n#  \u0026quot;description\u0026quot;:\u0026quot;最大返回结果数量\u0026quot;}},\u0026quot;required\u0026quot;:[\u0026quot;query\u0026quot;]}}}\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch3\u003eStep 3: ToolExecutor — 工具执行与结果处理\u003c/h3\u003e\n\u003cp\u003e接收 LLM 返回的 \u003ccode\u003etool_calls\u003c/code\u003e，分发执行，收集结果，处理异常。支持串行和并行两种模式。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# tool_executor.py\nimport json, time, logging, traceback\nfrom concurrent.futures import ThreadPoolExecutor, TimeoutError as FTE\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ToolResult:\n    tool_call_id: str\n    name: str\n    result: str\n    success: bool\n    duration_ms: float = 0.0\n\nclass ToolExecutor:\n    def __init__(self, registry, default_timeout=30.0, max_workers=4):\n        self.registry = registry\n        self.default_timeout = default_timeout\n        self.max_workers = max_workers\n\n    def execute(self, tool_calls) -\u0026gt; list[ToolResult]:\n        \u0026quot;\u0026quot;\u0026quot;串行执行\u0026quot;\u0026quot;\u0026quot;\n        return [self._run_one(tc) for tc in tool_calls]\n\n    def execute_parallel(self, tool_calls) -\u0026gt; list[ToolResult]:\n        \u0026quot;\u0026quot;\u0026quot;并行执行（LLM 一次返回多个 tool_calls 时使用）\u0026quot;\u0026quot;\u0026quot;\n        if len(tool_calls) \u0026lt;= 1:\n            return self.execute(tool_calls)\n        results = []\n        with ThreadPoolExecutor(max_workers=self.max_workers) as pool:\n            futures = {pool.submit(self._run_one, tc): tc for tc in tool_calls}\n            for fut in futures:\n                try:\n                    results.append(fut.result(timeout=self.default_timeout))\n                except FTE:\n                    tc = futures[fut]\n                    results.append(ToolResult(\n                        tc.id, tc.name,\n                        f\u0026quot;Error: \u0026#39;{tc.name}\u0026#39; timed out after \u0026quot;\n                        f\u0026quot;{self.default_timeout}s\u0026quot;, False))\n        return results\n\n    def _run_one(self, tool_call) -\u0026gt; ToolResult:\n        start = time.monotonic()\n        func = self.registry.get_function(tool_call.name)\n        if not func:\n            return ToolResult(tool_call.id, tool_call.name,\n                f\u0026quot;Error: Unknown tool \u0026#39;{tool_call.name}\u0026#39;. \u0026quot;\n                f\u0026quot;Available: {self.registry.list_tools()}\u0026quot;, False)\n        try:\n            result = func(**tool_call.arguments)\n            if not isinstance(result, str):\n                result = json.dumps(result, ensure_ascii=False, default=str)\n            ms = (time.monotonic() - start) * 1000\n            logger.info(f\u0026quot;Tool \u0026#39;{tool_call.name}\u0026#39; OK in {ms:.0f}ms\u0026quot;)\n            return ToolResult(tool_call.id, tool_call.name, result, True, ms)\n        except Exception as e:\n            ms = (time.monotonic() - start) * 1000\n            msg = f\u0026quot;Error: {type(e).__name__}: {e}\u0026quot;\n            logger.error(f\u0026quot;{msg}\\n{traceback.format_exc()}\u0026quot;)\n            return ToolResult(tool_call.id, tool_call.name, msg, False, ms)\n\n    @staticmethod\n    def results_to_messages(results):\n        return [{\u0026quot;role\u0026quot;: \u0026quot;tool\u0026quot;, \u0026quot;tool_call_id\u0026quot;: r.tool_call_id,\n                 \u0026quot;content\u0026quot;: r.result} for r in results]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e串行 vs 并行的 Trade-off\u003c/strong\u003e：串行简单可调试；并行在 LLM 同时返回多个独立 tool_calls 时显著降低延迟。LLM 在一次响应中返回多个 tool_calls 本身就隐含了\u0026quot;它们之间无依赖\u0026quot;——否则它会分成多轮调用。\u003c/p\u003e\n\u003chr\u003e\n\u003ch3\u003eStep 4: MessageManager — 消息历史管理与压缩\u003c/h3\u003e\n\u003cp\u003e解决 Agent 长对话中最常遇到的问题：\u003cstrong\u003e消息越来越多，Context Window 不够用了\u003c/strong\u003e。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# message_manager.py\nimport json, logging, tiktoken\nfrom typing import Optional\nfrom copy import deepcopy\n\nlogger = logging.getLogger(__name__)\n\nclass MessageManager:\n    def __init__(self, system_prompt=\u0026quot;\u0026quot;, model=\u0026quot;gpt-4o\u0026quot;,\n                 max_tokens=120000, compression_threshold=0.75):\n        self.system_prompt = system_prompt\n        self.max_tokens = max_tokens\n        self.compression_threshold = compression_threshold\n        try: self.enc = tiktoken.encoding_for_model(model)\n        except KeyError: self.enc = tiktoken.get_encoding(\u0026quot;cl100k_base\u0026quot;)\n        self._messages: list[dict] = []\n\n    def append(self, msg):\n        self._messages.append(msg)\n        self._maybe_compress()\n\n    def extend(self, msgs):\n        self._messages.extend(msgs)\n        self._maybe_compress()\n\n    def get_messages(self):\n        out = []\n        if self.system_prompt:\n            out.append({\u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;, \u0026quot;content\u0026quot;: self.system_prompt})\n        out.extend(deepcopy(self._messages))\n        return out\n\n    def count_tokens(self, msgs=None):\n        msgs = msgs or self.get_messages()\n        total = 2  # priming tokens\n        for m in msgs:\n            total += 4  # per-message overhead\n            for v in m.values():\n                if isinstance(v, str): total += len(self.enc.encode(v))\n                elif isinstance(v, list):\n                    total += len(self.enc.encode(json.dumps(v)))\n        return total\n\n    def _maybe_compress(self):\n        threshold = int(self.max_tokens * self.compression_threshold)\n        if self.count_tokens() \u0026lt;= threshold: return\n        logger.info(\u0026quot;Token threshold exceeded, compressing...\u0026quot;)\n        self._sliding_window_compress(threshold)\n\n    def _sliding_window_compress(self, target):\n        \u0026quot;\u0026quot;\u0026quot;从最早的消息移除，保持 tool_call 对完整性。\n\n        关键约束：assistant(tool_calls) 后面的 tool(result) 消息必须\n        一起移除，否则 OpenAI API 会报错。\n        \u0026quot;\u0026quot;\u0026quot;\n        msgs, i = self._messages, 0\n        while i \u0026lt; len(msgs):\n            remaining = msgs[i:]\n            sys_msgs = ([{\u0026quot;role\u0026quot;:\u0026quot;system\u0026quot;,\u0026quot;content\u0026quot;:self.system_prompt}]\n                        if self.system_prompt else [])\n            if self.count_tokens(sys_msgs + remaining) \u0026lt;= target: break\n            i += 1\n            # 如果刚移除的是含 tool_calls 的 assistant，连续移除后续 tool 消息\n            if (i \u0026gt; 0 and msgs[i-1].get(\u0026quot;role\u0026quot;) == \u0026quot;assistant\u0026quot;\n                    and msgs[i-1].get(\u0026quot;tool_calls\u0026quot;)):\n                while i \u0026lt; len(msgs) and msgs[i].get(\u0026quot;role\u0026quot;) == \u0026quot;tool\u0026quot;:\n                    i += 1\n        if i \u0026gt; 0:\n            summary = {\u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;, \u0026quot;content\u0026quot;:\n                f\u0026quot;[{i} earlier messages removed to fit context window.]\u0026quot;}\n            self._messages = [summary] + msgs[i:]\n            logger.info(f\u0026quot;Removed {i} msgs, tokens: {self.count_tokens()}\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e三个关键点\u003c/strong\u003e：System Prompt 始终保留不参与压缩；tool_call 对必须保持完整（\u003ccode\u003eassistant\u003c/code\u003e + 后续 \u003ccode\u003etool\u003c/code\u003e 消息一起删或一起留）；在 75% 时就触发压缩，给回复留够空间。\u003c/p\u003e\n\u003chr\u003e\n\u003ch3\u003eStep 5: StateStore — 状态持久化\u003c/h3\u003e\n\u003cp\u003e简单的键值存储，生产中替换为 Redis 或数据库即可。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# state_store.py\nimport json\nfrom typing import Any, Optional\nfrom pathlib import Path\n\nclass StateStore:\n    def __init__(self, store_dir=\u0026quot;.agent_state\u0026quot;):\n        self.dir = Path(store_dir)\n        self.dir.mkdir(parents=True, exist_ok=True)\n        self._cache: dict[str, Any] = {}\n\n    def save(self, key, value):\n        self._cache[key] = value\n        (self.dir / f\u0026quot;{key}.json\u0026quot;).write_text(\n            json.dumps(value, ensure_ascii=False, indent=2, default=str))\n\n    def load(self, key, default=None):\n        if key in self._cache: return self._cache[key]\n        f = self.dir / f\u0026quot;{key}.json\u0026quot;\n        if f.exists():\n            v = json.loads(f.read_text())\n            self._cache[key] = v\n            return v\n        return default\n\n    def clear(self, key=None):\n        if key:\n            self._cache.pop(key, None)\n            (self.dir / f\u0026quot;{key}.json\u0026quot;).unlink(missing_ok=True)\n        else:\n            self._cache.clear()\n            for f in self.dir.glob(\u0026quot;*.json\u0026quot;): f.unlink()\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e4. 核心 Runtime Loop\u003c/h2\u003e\n\u003cp\u003e所有模块就绪，组装成完整的 \u003ccode\u003eAgentRuntime\u003c/code\u003e。这是整篇文章的核心。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# agent_runtime.py\nimport json, time, logging\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Callable\nfrom collections import Counter\n\nfrom llm_client import LLMClient, LLMResponse\nfrom tool_registry import ToolRegistry\nfrom tool_executor import ToolExecutor\nfrom message_manager import MessageManager\nfrom state_store import StateStore\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RuntimeConfig:\n    max_turns: int = 20               # 最大循环轮次\n    max_total_time: float = 300.0     # 最大总执行时间（秒）\n    parallel_tool_calls: bool = True  # 是否并行执行工具\n    loop_detection_window: int = 4    # 死循环检测窗口\n    loop_detection_threshold: int = 3 # 相同调用出现次数阈值\n\n@dataclass\nclass AgentResult:\n    content: str\n    turns: int = 0\n    total_tokens: int = 0\n    tool_calls_made: list[dict] = field(default_factory=list)\n    duration_ms: float = 0.0\n    stopped_reason: str = \u0026quot;\u0026quot;\n\nclass AgentRuntime:\n    def __init__(self, llm: LLMClient, registry: ToolRegistry,\n                 system_prompt=\u0026quot;You are a helpful assistant.\u0026quot;,\n                 config: Optional[RuntimeConfig] = None):\n        self.llm = llm\n        self.registry = registry\n        self.executor = ToolExecutor(registry)\n        self.config = config or RuntimeConfig()\n        self.messages = MessageManager(system_prompt=system_prompt,\n                                       model=llm.model)\n        self.state = StateStore()\n        self.on_tool_start: Optional[Callable] = None\n        self.on_tool_end: Optional[Callable] = None\n\n    def run(self, user_input: str) -\u0026gt; AgentResult:\n        start_time = time.monotonic()\n        self.messages.append({\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: user_input})\n        tools = self.registry.get_all_schemas() or None\n\n        turns, total_tokens, all_tc = 0, 0, []\n        tc_history: list[str] = []\n        final_content, stopped = \u0026quot;\u0026quot;, \u0026quot;completed\u0026quot;\n\n        while turns \u0026lt; self.config.max_turns:\n            turns += 1\n\n            # ── 全局超时检查 ─────────────────────────────\n            if time.monotonic() - start_time \u0026gt; self.config.max_total_time:\n                stopped = f\u0026quot;timeout ({self.config.max_total_time}s)\u0026quot;\n                break\n\n            # ── 调用 LLM ────────────────────────────────\n            logger.info(f\u0026quot;Turn {turns}: calling LLM...\u0026quot;)\n            resp = self.llm.chat(self.messages.get_messages(), tools=tools)\n            total_tokens += resp.usage.get(\u0026quot;total_tokens\u0026quot;, 0)\n\n            # ── 情况 1: 有 tool_calls → 执行工具 ────────\n            if resp.has_tool_calls:\n                # 构建 assistant 消息（必须包含 tool_calls 字段）\n                asst = {\u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;, \u0026quot;content\u0026quot;: resp.content,\n                        \u0026quot;tool_calls\u0026quot;: [\n                    {\u0026quot;id\u0026quot;: tc.id, \u0026quot;type\u0026quot;: \u0026quot;function\u0026quot;,\n                     \u0026quot;function\u0026quot;: {\u0026quot;name\u0026quot;: tc.name,\n                                  \u0026quot;arguments\u0026quot;: json.dumps(tc.arguments)}}\n                    for tc in resp.tool_calls]}\n                self.messages.append(asst)\n\n                # 死循环检测\n                sig = json.dumps([(tc.name, tc.arguments)\n                                  for tc in resp.tool_calls], sort_keys=True)\n                tc_history.append(sig)\n                if self._detect_loop(tc_history):\n                    stopped = \u0026quot;loop_detected\u0026quot;\n                    final_content = (\u0026quot;I\u0026#39;m repeating the same actions. \u0026quot;\n                                     \u0026quot;Stopping to summarize findings.\u0026quot;)\n                    break\n\n                # 执行\n                if self.on_tool_start: self.on_tool_start(resp.tool_calls)\n                if self.config.parallel_tool_calls and len(resp.tool_calls) \u0026gt; 1:\n                    results = self.executor.execute_parallel(resp.tool_calls)\n                else:\n                    results = self.executor.execute(resp.tool_calls)\n                if self.on_tool_end: self.on_tool_end(results)\n\n                for tc, r in zip(resp.tool_calls, results):\n                    all_tc.append({\u0026quot;turn\u0026quot;: turns, \u0026quot;name\u0026quot;: tc.name,\n                        \u0026quot;arguments\u0026quot;: tc.arguments,\n                        \u0026quot;success\u0026quot;: r.success, \u0026quot;duration_ms\u0026quot;: r.duration_ms})\n\n                self.messages.extend(ToolExecutor.results_to_messages(results))\n\n            # ── 情况 2: 纯文本 → 任务完成 ───────────────\n            else:\n                final_content = resp.content or \u0026quot;\u0026quot;\n                self.messages.append(\n                    {\u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;, \u0026quot;content\u0026quot;: final_content})\n                break\n        else:\n            stopped = f\u0026quot;max_turns ({self.config.max_turns})\u0026quot;\n\n        return AgentResult(\n            content=final_content, turns=turns, total_tokens=total_tokens,\n            tool_calls_made=all_tc,\n            duration_ms=(time.monotonic() - start_time) * 1000,\n            stopped_reason=stopped)\n\n    def _detect_loop(self, history):\n        \u0026quot;\u0026quot;\u0026quot;滑动窗口 + 频次统计，同时捕获连续重复和交替重复\u0026quot;\u0026quot;\u0026quot;\n        w = self.config.loop_detection_window\n        t = self.config.loop_detection_threshold\n        if len(history) \u0026lt; t: return False\n        return any(c \u0026gt;= t for c in Counter(history[-w:]).values())\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e核心循环解读\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e两种退出路径\u003c/strong\u003e——这是 Agent 与 Workflow 的本质区别：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eresp.has_tool_calls == True   → 继续（还有事要做）\nresp.has_tool_calls == False  → break（LLM 认为任务完成了）\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e为什么 assistant 消息必须包含 tool_calls 字段？\u003c/strong\u003e 这是 OpenAI API 的协议约束。消息流必须是：\u003ccode\u003euser\u003c/code\u003e → \u003ccode\u003eassistant(tool_calls)\u003c/code\u003e → \u003ccode\u003etool(result)\u003c/code\u003e → \u003ccode\u003eassistant(final)\u003c/code\u003e。打破这个顺序会报错。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e死循环检测\u003c/strong\u003e用滑动窗口而非简单的\u0026quot;连续 N 次相同\u0026quot;，因为 LLM 有时会在两个工具间交替调用（A→B→A→B→...），这也是死循环，但不是\u0026quot;连续相同\u0026quot;。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e5. 高级特性\u003c/h2\u003e\n\u003ch3\u003e5.1 Streaming 支持\u003c/h3\u003e\n\u003cp\u003e流式模式下需要边输出文本、边判断是否有 tool_calls：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 添加到 AgentRuntime\ndef run_stream(self, user_input: str):\n    self.messages.append({\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: user_input})\n    tools = self.registry.get_all_schemas() or None\n    turns = 0\n\n    while turns \u0026lt; self.config.max_turns:\n        turns += 1\n        content, final_tc = \u0026quot;\u0026quot;, None\n\n        for chunk in self.llm.stream(self.messages.get_messages(), tools=tools):\n            if chunk.content:\n                content += chunk.content\n                yield {\u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;content\u0026quot;: chunk.content}\n            if chunk.tool_calls:\n                final_tc = chunk.tool_calls\n\n        if final_tc:\n            yield {\u0026quot;type\u0026quot;: \u0026quot;tool_start\u0026quot;,\n                   \u0026quot;calls\u0026quot;: [{\u0026quot;name\u0026quot;:tc.name} for tc in final_tc]}\n            asst = {\u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;, \u0026quot;content\u0026quot;: content,\n                    \u0026quot;tool_calls\u0026quot;: [\n                {\u0026quot;id\u0026quot;:tc.id, \u0026quot;type\u0026quot;:\u0026quot;function\u0026quot;,\n                 \u0026quot;function\u0026quot;:{\u0026quot;name\u0026quot;:tc.name,\n                             \u0026quot;arguments\u0026quot;:json.dumps(tc.arguments)}}\n                for tc in final_tc]}\n            self.messages.append(asst)\n            results = self.executor.execute(final_tc)\n            self.messages.extend(ToolExecutor.results_to_messages(results))\n            yield {\u0026quot;type\u0026quot;: \u0026quot;tool_end\u0026quot;,\n                   \u0026quot;results\u0026quot;: [{\u0026quot;name\u0026quot;:r.name, \u0026quot;ok\u0026quot;:r.success} for r in results]}\n        else:\n            self.messages.append({\u0026quot;role\u0026quot;:\u0026quot;assistant\u0026quot;,\u0026quot;content\u0026quot;:content})\n            yield {\u0026quot;type\u0026quot;: \u0026quot;done\u0026quot;, \u0026quot;content\u0026quot;: content}\n            break\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e5.2 超时控制的两层设计\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e┌──────────────────────────────────────┐\n│ 全局超时 (max_total_time = 300s)     │\n│  ┌──────┐ ┌──────┐ ┌──────┐        │\n│  │Tool 1│ │Tool 2│ │Tool 3│        │\n│  │30s   │ │30s   │ │30s   │        │\n│  └──────┘ └──────┘ └──────┘        │\n│ 单工具超时 (default_timeout = 30s)   │\n└──────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e单工具超时在 \u003ccode\u003eToolExecutor\u003c/code\u003e 中通过 \u003ccode\u003eThreadPoolExecutor.result(timeout=30)\u003c/code\u003e 控制；全局超时在 Runtime 每轮循环开始时检查 elapsed time。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e6. 完整示例：研究助手 Agent\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# research_agent.py\nimport json, os, logging\nfrom agent_runtime import AgentRuntime, RuntimeConfig\nfrom llm_client import LLMClient\nfrom tool_registry import ToolRegistry\n\nlogging.basicConfig(level=logging.INFO,\n    format=\u0026quot;%(asctime)s [%(levelname)s] %(name)s: %(message)s\u0026quot;)\n\nregistry = ToolRegistry()\n\n@registry.tool()\ndef web_search(query: str, max_results: int = 5) -\u0026gt; str:\n    \u0026quot;\u0026quot;\u0026quot;搜索网页内容\n    Args:\n        query: 搜索关键词\n        max_results: 最大返回数量\n    \u0026quot;\u0026quot;\u0026quot;\n    # 生产环境替换为 SerpAPI / Bing API\n    return json.dumps([{\u0026quot;title\u0026quot;: f\u0026quot;Result {i+1} for \u0026#39;{query}\u0026#39;\u0026quot;,\n        \u0026quot;url\u0026quot;: f\u0026quot;https://example.com/article-{i+1}\u0026quot;,\n        \u0026quot;snippet\u0026quot;: f\u0026quot;Detailed article about {query}, section {i+1}...\u0026quot;}\n        for i in range(min(max_results, 3))], ensure_ascii=False)\n\n@registry.tool()\ndef read_url(url: str) -\u0026gt; str:\n    \u0026quot;\u0026quot;\u0026quot;读取网页内容\n    Args:\n        url: 网页地址\n    \u0026quot;\u0026quot;\u0026quot;\n    # 生产环境替换为 requests + BeautifulSoup\n    return (f\u0026quot;[Content from {url}]\\n\u0026quot;\n            f\u0026quot;Key points: 1) Fundamental concepts 2) Best practices \u0026quot;\n            f\u0026quot;3) Common pitfalls 4) Case studies and benchmarks\u0026quot;)\n\n@registry.tool()\ndef write_file(filename: str, content: str) -\u0026gt; str:\n    \u0026quot;\u0026quot;\u0026quot;写入文件\n    Args:\n        filename: 文件名\n        content: 文本内容\n    \u0026quot;\u0026quot;\u0026quot;\n    os.makedirs(\u0026quot;output\u0026quot;, exist_ok=True)\n    path = os.path.join(\u0026quot;output\u0026quot;, os.path.basename(filename))\n    with open(path, \u0026quot;w\u0026quot;) as f: f.write(content)\n    return f\u0026quot;Wrote {len(content)} chars to {path}\u0026quot;\n\n@registry.tool()\ndef ask_user(question: str) -\u0026gt; str:\n    \u0026quot;\u0026quot;\u0026quot;向用户提问\n    Args:\n        question: 问题\n    \u0026quot;\u0026quot;\u0026quot;\n    print(f\u0026quot;\\nAgent asks: {question}\u0026quot;)\n    return input(\u0026quot;Your answer: \u0026quot;)\n\nSYSTEM_PROMPT = \u0026quot;\u0026quot;\u0026quot;You are a research assistant. Workflow:\n1. Search for information using web_search\n2. Read promising articles using read_url (at least 2 sources)\n3. Synthesize into a report and save with write_file\n4. Present a summary. Use ask_user if the topic is unclear.\u0026quot;\u0026quot;\u0026quot;\n\nagent = AgentRuntime(\n    llm=LLMClient(model=\u0026quot;gpt-4o\u0026quot;, api_key=os.environ.get(\u0026quot;OPENAI_API_KEY\u0026quot;)),\n    registry=registry,\n    system_prompt=SYSTEM_PROMPT,\n    config=RuntimeConfig(max_turns=15, max_total_time=120.0))\n\nif __name__ == \u0026quot;__main__\u0026quot;:\n    result = agent.run(\u0026quot;研究 Python asyncio 最佳实践，整理成技术报告并保存。\u0026quot;)\n    print(f\u0026quot;\\n{\u0026#39;=\u0026#39;*50}\\nTurns: {result.turns} | Tokens: {result.total_tokens} \u0026quot;\n          f\u0026quot;| {result.duration_ms:.0f}ms | {result.stopped_reason}\u0026quot;)\n    for tc in result.tool_calls_made:\n        print(f\u0026quot;  Turn {tc[\u0026#39;turn\u0026#39;]}: {tc[\u0026#39;name\u0026#39;]}() \u0026quot;\n              f\u0026quot;{\u0026#39;OK\u0026#39; if tc[\u0026#39;success\u0026#39;] else \u0026#39;FAIL\u0026#39;} {tc[\u0026#39;duration_ms\u0026#39;]:.0f}ms\u0026quot;)\n    print(f\u0026quot;\\n{result.content[:300]}\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e执行 Trace\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eTurn 1: calling LLM...  → web_search(\u0026quot;Python asyncio best practices\u0026quot;)\nTurn 2: calling LLM...  → read_url(url1) + read_url(url2)  [parallel]\nTurn 3: calling LLM...  → web_search(\u0026quot;asyncio common pitfalls\u0026quot;)\nTurn 4: calling LLM...  → read_url(url3)\nTurn 5: calling LLM...  → write_file(\u0026quot;asyncio-report.md\u0026quot;, ...)\nTurn 6: calling LLM...  → [no tool_calls] → Done\n\n==================================================\nTurns: 6 | Tokens: 8432 | 13245ms | completed\n  Turn 1: web_search() OK 45ms\n  Turn 2: read_url() OK 120ms\n  Turn 2: read_url() OK 135ms\n  Turn 3: web_search() OK 38ms\n  Turn 4: read_url() OK 110ms\n  Turn 5: write_file() OK 5ms\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e注意 Turn 2：LLM 返回了两个 \u003ccode\u003eread_url\u003c/code\u003e，Runtime 自动并行执行。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e7. 与框架对比\u003c/h2\u003e\n\u003ch3\u003e自建 vs 框架\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e维度\u003c/th\u003e\n\u003cth\u003e自建 Runtime\u003c/th\u003e\n\u003cth\u003e框架（LangChain 等）\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e透明性\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e完全透明\u003c/td\u003e\n\u003ctd\u003e需要读框架源码\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e调试\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e直接 breakpoint\u003c/td\u003e\n\u003ctd\u003e需要理解框架抽象层\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e定制\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e任何行为可改\u003c/td\u003e\n\u003ctd\u003e受 API 设计约束\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e依赖\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eopenai\u003c/code\u003e + \u003ccode\u003etiktoken\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e几十个传递依赖\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e边界情况\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e自己发现和处理\u003c/td\u003e\n\u003ctd\u003e社区帮你踩过坑\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e生态集成\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e每个都要自己写\u003c/td\u003e\n\u003ctd\u003e现成的 VectorStore/Retriever\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e开发速度\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e初期更慢\u003c/td\u003e\n\u003ctd\u003e有模板更快\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch3\u003e决策建议\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e学习阶段\u003c/strong\u003e：一定要自建一次。不理解原理就用框架，永远无法判断框架是否在坑你。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoC / Hackathon\u003c/strong\u003e：用框架，速度第一。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e生产系统\u003c/strong\u003e：自建核心 Runtime + 选择性使用框架组件（如只用 LangChain 的 Retriever）。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e基础设施团队\u003c/strong\u003e：自建。你们的需求框架大概率满足不了。\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e8. 结语：Phase 2 完成\u003c/h2\u003e\n\u003cp\u003e到这里，Phase 2 四篇文章全部完成：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e第 04 篇\u003c/strong\u003e：理解控制循环 — Observe → Think → Act → Reflect\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e第 05 篇\u003c/strong\u003e：深入 Tool Calling — JSON Schema、Function Calling、Structured Output\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e第 06 篇\u003c/strong\u003e：Prompt Engineering — System Prompt 设计、工具选择引导、Reflection Prompt\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e第 07 篇（本篇）\u003c/strong\u003e：把以上所有知识组装成可运行的 Agent Runtime\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e此刻你有能力\u003cstrong\u003e不依赖任何框架，从零构建功能完整的 Agent 系统\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e但如果你运行过这个 Agent，会很快发现几个问题：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e没有记忆\u003c/strong\u003e：每次启动都是白纸，不记得上次的对话\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e不会计划\u003c/strong\u003e：面对复杂任务只是一步步试，没有全局规划\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e一个不够用\u003c/strong\u003e：有些任务需要不同角色的 Agent 协作\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e这就是 Phase 3 要解决的问题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e第 08 篇\u003c/strong\u003e：Memory Architecture — Agent 的状态与记忆体系\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e第 09 篇\u003c/strong\u003e：RAG as Cognitive Memory — 检索增强生成的工程实践\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e第 10 篇\u003c/strong\u003e：Planning and Reflection — 从 ReAct 到分层规划\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e第 11 篇\u003c/strong\u003e：Multi-Agent Collaboration — 多 Agent 协作\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePhase 2 给了你造一把锤子的能力。Phase 3 将教你如何造一个工具箱。\u003c/p\u003e\n\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e系列导航\u003c/strong\u003e：本文是 Agentic 系列的第 07 篇。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e上一篇：\u003ca href=\"/blog/engineering/agentic/06-Prompt%20Engineering%20for%20Agents\"\u003e06 | Prompt Engineering for Agents\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下一篇：\u003ca href=\"/blog/engineering/agentic/08-Memory%20Architecture\"\u003e08 | Memory Architecture\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e完整目录：\u003ca href=\"/blog/engineering/agentic/01-From%20LLM%20to%20Agent\"\u003e01 | From LLM to Agent\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"17:T5870,"])</script><script>self.__next_f.push([1,"\u003cblockquote\u003e\n\u003cp\u003eJava 中的大部分同步工具（ReentrantLock、Semaphore、CountDownLatch、ReentrantReadWriteLock 等）都基于 AbstractQueuedSynchronizer（AQS）实现。理解 AQS，就等于掌握了 Java 并发编程的底层脉络。本文从设计思想出发，逐层深入 AQS 的数据结构、核心流程和源码实现，并通过 ReentrantLock 串联全局，最后梳理 AQS 在 JUC 中的应用全景。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eAQS 是什么？\u003c/h2\u003e\n\u003cp\u003eAQS（AbstractQueuedSynchronizer）是 \u003ccode\u003ejava.util.concurrent.locks\u003c/code\u003e 包中的一个\u003cstrong\u003e抽象类\u003c/strong\u003e，是构建锁和同步器的基础框架。Doug Lea 设计 AQS 的核心目标是：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e降低构建锁和同步器的工作量\u003c/li\u003e\n\u003cli\u003e避免在多个位置处理竞争问题\u003c/li\u003e\n\u003cli\u003e在基于 AQS 的同步器中，阻塞只可能在一个时刻发生，降低上下文切换开销，提高吞吐量\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAQS 支持两种工作模式：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e模式\u003c/th\u003e\n\u003cth\u003e含义\u003c/th\u003e\n\u003cth\u003e典型实现\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e独占模式（Exclusive）\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e同一时刻只能有一个线程获取到锁\u003c/td\u003e\n\u003ctd\u003eReentrantLock\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e共享模式（Shared）\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e同一时刻可以有多个线程同时获取\u003c/td\u003e\n\u003ctd\u003eCountDownLatch、ReadWriteLock、Semaphore\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e无论哪种模式，本质上都是对 AQS 内部一个 \u003cstrong\u003e\u003ccode\u003estate\u003c/code\u003e 变量\u003c/strong\u003e的获取和释放。\u003c/p\u003e\n\u003ch2\u003eAQS 的整体架构\u003c/h2\u003e\n\u003cp\u003eAQS 框架共分为\u003cstrong\u003e五层\u003c/strong\u003e，自上而下由浅入深：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e层次\u003c/th\u003e\n\u003cth\u003e内容\u003c/th\u003e\n\u003cth\u003e说明\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e第一层\u003c/td\u003e\n\u003ctd\u003eAPI 层\u003c/td\u003e\n\u003ctd\u003e自定义同步器需重写的方法（tryAcquire、tryRelease 等）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e第二层\u003c/td\u003e\n\u003ctd\u003e获取/释放方法\u003c/td\u003e\n\u003ctd\u003eacquire、release、acquireShared、releaseShared\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e第三层\u003c/td\u003e\n\u003ctd\u003e队列操作\u003c/td\u003e\n\u003ctd\u003eaddWaiter、acquireQueued、shouldParkAfterFailedAcquire\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e第四层\u003c/td\u003e\n\u003ctd\u003e线程阻塞/唤醒\u003c/td\u003e\n\u003ctd\u003eLockSupport.park / unpark\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e第五层\u003c/td\u003e\n\u003ctd\u003e基础数据\u003c/td\u003e\n\u003ctd\u003estate、Node、CLH 变体队列\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e当接入自定义同步器时，\u003cstrong\u003e只需重写第一层的部分方法即可\u003c/strong\u003e，不需要关注底层实现。当加锁或解锁操作触发时，沿着第一层到第五层逐层深入。\u003c/p\u003e\n\u003ch2\u003e核心数据结构\u003c/h2\u003e\n\u003ch3\u003e同步状态 State\u003c/h3\u003e\n\u003cp\u003eAQS 使用一个 \u003ccode\u003evolatile int\u003c/code\u003e 类型的成员变量 \u003ccode\u003estate\u003c/code\u003e 来表示同步状态：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003eprivate volatile int state;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eState 的含义由具体的同步器定义，例如：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReentrantLock\u003c/strong\u003e：state 表示锁被重入的次数，0 表示未被持有\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSemaphore\u003c/strong\u003e：state 表示可用许可的数量\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCountDownLatch\u003c/strong\u003e：state 表示计数器的值\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAQS 提供三个方法操作 state，均为 \u003ccode\u003efinal\u003c/code\u003e 修饰，子类不可重写：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e方法\u003c/th\u003e\n\u003cth\u003e说明\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003egetState()\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e获取当前 state 值\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003esetState(int)\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e设置 state 值\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003ecompareAndSetState(int, int)\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eCAS 方式更新 state\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch3\u003eCLH 变体队列与 Node 节点\u003c/h3\u003e\n\u003cp\u003eAQS 的核心思想是：如果请求的共享资源空闲，就将当前线程设置为有效的工作线程，并将资源设置为锁定状态；\u003cstrong\u003e如果资源被占用，就通过一个 CLH 变体的 FIFO 双向队列来管理等待线程\u003c/strong\u003e。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCLH 队列以其发明者 Craig、Landin 和 Hagersten 命名，原始 CLH 是单向链表。AQS 中的变体是虚拟双向队列，通过将每条请求线程封装成 Node 节点来实现锁的分配。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNode 节点的关键属性：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e属性\u003c/th\u003e\n\u003cth\u003e含义\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003ethread\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e该节点代表的线程\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003ewaitStatus\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e当前节点在队列中的等待状态\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eprev\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e前驱指针\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003enext\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e后继指针\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003enextWaiter\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e指向下一个处于 CONDITION 状态的节点\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e\u003ccode\u003ewaitStatus\u003c/code\u003e 的枚举值：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e值\u003c/th\u003e\n\u003cth\u003e名称\u003c/th\u003e\n\u003cth\u003e含义\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e默认值\u003c/td\u003e\n\u003ctd\u003eNode 初始化时的状态\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003eCANCELLED\u003c/td\u003e\n\u003ctd\u003e线程获取锁的请求已取消\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-1\u003c/td\u003e\n\u003ctd\u003eSIGNAL\u003c/td\u003e\n\u003ctd\u003e后继节点的线程需要被唤醒\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-2\u003c/td\u003e\n\u003ctd\u003eCONDITION\u003c/td\u003e\n\u003ctd\u003e节点在条件队列中，等待 Condition 唤醒\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-3\u003c/td\u003e\n\u003ctd\u003ePROPAGATE\u003c/td\u003e\n\u003ctd\u003e共享模式下，释放操作需要向后传播\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eAQS 内部还维护了\u003cstrong\u003e两种队列\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e同步队列（Sync Queue）\u003c/strong\u003e：获取资源失败的线程进入此队列自旋等待，当前驱节点是头节点时尝试获取资源\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e条件队列（Condition Queue）\u003c/strong\u003e：基于 \u003ccode\u003eCondition\u003c/code\u003e 实现，调用 \u003ccode\u003eawait()\u003c/code\u003e 时线程进入条件队列，调用 \u003ccode\u003esignal()\u003c/code\u003e 时转移到同步队列\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003e注意：双向链表的\u003cstrong\u003e头节点是一个虚节点\u003c/strong\u003e（不存储实际线程信息），真正的第一个有效节点从第二个开始。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e自定义同步器需要重写的方法\u003c/h2\u003e\n\u003cp\u003eAQS 采用\u003cstrong\u003e模板方法模式\u003c/strong\u003e，自定义同步器只需根据需要重写以下方法：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e方法\u003c/th\u003e\n\u003cth\u003e模式\u003c/th\u003e\n\u003cth\u003e说明\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003etryAcquire(int)\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e独占\u003c/td\u003e\n\u003ctd\u003e尝试获取资源，成功返回 true\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003etryRelease(int)\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e独占\u003c/td\u003e\n\u003ctd\u003e尝试释放资源，成功返回 true\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003etryAcquireShared(int)\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e共享\u003c/td\u003e\n\u003ctd\u003e尝试获取资源，负数=失败，0=成功但无剩余，正数=成功且有剩余\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003etryReleaseShared(int)\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e共享\u003c/td\u003e\n\u003ctd\u003e尝试释放资源，如果释放后允许唤醒后续节点返回 true\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eisHeldExclusively()\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e独占\u003c/td\u003e\n\u003ctd\u003e当前线程是否独占资源，用到 Condition 时需实现\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e独占模式实现 \u003ccode\u003etryAcquire-tryRelease\u003c/code\u003e，共享模式实现 \u003ccode\u003etryAcquireShared-tryReleaseShared\u003c/code\u003e。AQS 也支持同时实现两种模式，如 \u003ccode\u003eReentrantReadWriteLock\u003c/code\u003e。\u003c/p\u003e\n\u003ch2\u003e通过 ReentrantLock 理解加锁流程\u003c/h2\u003e\n\u003cp\u003eReentrantLock 是 AQS 独占模式最典型的实现。我们以\u003cstrong\u003e非公平锁\u003c/strong\u003e为例，完整追踪加锁流程。\u003c/p\u003e\n\u003ch3\u003e第一步：lock()\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// ReentrantLock.NonfairSync\nfinal void lock() {\n    if (compareAndSetState(0, 1))           // 直接 CAS 尝试获取锁\n        setExclusiveOwnerThread(Thread.currentThread());\n    else\n        acquire(1);                          // 失败则进入 AQS 框架流程\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e非公平锁上来就尝试 CAS 抢锁（不管队列中有没有等待线程），这是它\u0026quot;非公平\u0026quot;的体现。\u003c/p\u003e\n\u003ch3\u003e第二步：acquire()\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// AbstractQueuedSynchronizer\npublic final void acquire(int arg) {\n    if (!tryAcquire(arg) \u0026amp;\u0026amp;\n        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))\n        selfInterrupt();\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这一行代码浓缩了整个加锁流程的四个步骤：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etryAcquire → addWaiter → acquireQueued → selfInterrupt\n\u003c/code\u003e\u003c/pre\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003etryAcquire\u003c/strong\u003e：尝试获取锁（由子类实现）\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eaddWaiter\u003c/strong\u003e：获取失败，将当前线程封装为 Node 加入队列尾部\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eacquireQueued\u003c/strong\u003e：在队列中自旋等待，直到获取到锁\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eselfInterrupt\u003c/strong\u003e：如果等待过程中被中断过，补上中断\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e第三步：tryAcquire（公平 vs 非公平）\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e非公平锁\u003c/strong\u003e的实现：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003efinal boolean nonfairTryAcquire(int acquires) {\n    final Thread current = Thread.currentThread();\n    int c = getState();\n    if (c == 0) {\n        if (compareAndSetState(0, acquires)) {   // 直接 CAS，不检查队列\n            setExclusiveOwnerThread(current);\n            return true;\n        }\n    }\n    else if (current == getExclusiveOwnerThread()) {  // 可重入逻辑\n        int nextc = c + acquires;\n        if (nextc \u0026lt; 0) throw new Error(\u0026quot;Maximum lock count exceeded\u0026quot;);\n        setState(nextc);\n        return true;\n    }\n    return false;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e公平锁\u003c/strong\u003e的区别仅在于多了一个 \u003ccode\u003ehasQueuedPredecessors()\u003c/code\u003e 检查：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003eif (c == 0) {\n    if (!hasQueuedPredecessors() \u0026amp;\u0026amp;   // 公平锁：先检查队列中是否有等待线程\n        compareAndSetState(0, acquires)) {\n        setExclusiveOwnerThread(current);\n        return true;\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e锁类型\u003c/th\u003e\n\u003cth\u003estate == 0 时的行为\u003c/th\u003e\n\u003cth\u003e可重入逻辑\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e非公平锁\u003c/td\u003e\n\u003ctd\u003e直接 CAS 抢锁\u003c/td\u003e\n\u003ctd\u003e相同：state + 1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e公平锁\u003c/td\u003e\n\u003ctd\u003e先检查队列再 CAS\u003c/td\u003e\n\u003ctd\u003e相同：state + 1\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch3\u003e第四步：addWaiter — 入队\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003eprivate Node addWaiter(Node mode) {\n    Node node = new Node(Thread.currentThread(), mode);\n    Node pred = tail;\n    if (pred != null) {            // 队列已初始化，尝试快速入队\n        node.prev = pred;\n        if (compareAndSetTail(pred, node)) {\n            pred.next = node;\n            return node;\n        }\n    }\n    enq(node);                     // 快速入队失败或队列未初始化\n    return node;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003eenq()\u003c/code\u003e 方法通过\u003cstrong\u003e自旋 + CAS\u003c/strong\u003e 确保入队成功：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003eprivate Node enq(final Node node) {\n    for (;;) {\n        Node t = tail;\n        if (t == null) {                         // 队列为空，初始化\n            if (compareAndSetHead(new Node()))    // 创建虚拟头节点\n                tail = head;\n        } else {\n            node.prev = t;\n            if (compareAndSetTail(t, node)) {\n                t.next = node;\n                return t;\n            }\n        }\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e线程获取锁的过程可以形象理解为：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e线程1获取锁成功 → 线程2申请锁失败 → 线程2入队等待 → 线程3申请失败 → 线程3排在线程2后面 → ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e第五步：acquireQueued — 自旋获取锁\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003efinal boolean acquireQueued(final Node node, int arg) {\n    boolean failed = true;\n    try {\n        boolean interrupted = false;\n        for (;;) {\n            final Node p = node.predecessor();\n            if (p == head \u0026amp;\u0026amp; tryAcquire(arg)) {   // 前驱是头节点，尝试获取锁\n                setHead(node);                     // 获取成功，当前节点成为新的头节点\n                p.next = null;                     // help GC\n                failed = false;\n                return interrupted;\n            }\n            if (shouldParkAfterFailedAcquire(p, node) \u0026amp;\u0026amp;\n                parkAndCheckInterrupt())           // 获取失败，判断是否需要挂起\n                interrupted = true;\n        }\n    } finally {\n        if (failed)\n            cancelAcquire(node);\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e核心逻辑：\u003cstrong\u003e只有前驱节点是头节点的线程才有资格尝试获取锁\u003c/strong\u003e。获取失败后，通过 \u003ccode\u003eshouldParkAfterFailedAcquire\u003c/code\u003e 判断是否需要挂起（将前驱节点的 waitStatus 设为 SIGNAL），然后通过 \u003ccode\u003eLockSupport.park()\u003c/code\u003e 挂起线程，避免空转浪费 CPU。\u003c/p\u003e\n\u003ch3\u003eshouldParkAfterFailedAcquire 的三种情况\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003eprivate static boolean shouldParkAfterFailedAcquire(Node pred, Node node) {\n    int ws = pred.waitStatus;\n    if (ws == Node.SIGNAL)        // 前驱已经是 SIGNAL，可以安全挂起\n        return true;\n    if (ws \u0026gt; 0) {                 // 前驱已取消，向前找到有效节点\n        do {\n            node.prev = pred = pred.prev;\n        } while (pred.waitStatus \u0026gt; 0);\n        pred.next = node;\n    } else {                      // 前驱状态为 0 或 PROPAGATE，设为 SIGNAL\n        compareAndSetWaitStatus(pred, ws, Node.SIGNAL);\n    }\n    return false;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e前驱 waitStatus\u003c/th\u003e\n\u003cth\u003e处理\u003c/th\u003e\n\u003cth\u003e是否挂起\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003eSIGNAL (-1)\u003c/td\u003e\n\u003ctd\u003e直接返回 true\u003c/td\u003e\n\u003ctd\u003e是\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCANCELLED (\u0026gt;0)\u003c/td\u003e\n\u003ctd\u003e跳过所有取消节点，重新链接\u003c/td\u003e\n\u003ctd\u003e否，下次循环再判断\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e0 或 PROPAGATE\u003c/td\u003e\n\u003ctd\u003eCAS 设为 SIGNAL\u003c/td\u003e\n\u003ctd\u003e否，下次循环再判断\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch2\u003e解锁流程\u003c/h2\u003e\n\u003cp\u003eReentrantLock 解锁时\u003cstrong\u003e不区分公平和非公平\u003c/strong\u003e：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// ReentrantLock\npublic void unlock() {\n    sync.release(1);\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// AbstractQueuedSynchronizer\npublic final boolean release(int arg) {\n    if (tryRelease(arg)) {\n        Node h = head;\n        if (h != null \u0026amp;\u0026amp; h.waitStatus != 0)\n            unparkSuccessor(h);          // 唤醒后继节点\n        return true;\n    }\n    return false;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003etryRelease — 可重入锁的释放\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// ReentrantLock.Sync\nprotected final boolean tryRelease(int releases) {\n    int c = getState() - releases;       // state 减 1\n    if (Thread.currentThread() != getExclusiveOwnerThread())\n        throw new IllegalMonitorStateException();\n    boolean free = false;\n    if (c == 0) {                         // 只有 state 减到 0，锁才真正释放\n        free = true;\n        setExclusiveOwnerThread(null);\n    }\n    setState(c);\n    return free;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eunparkSuccessor — 唤醒后继线程\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003eprivate void unparkSuccessor(Node node) {\n    int ws = node.waitStatus;\n    if (ws \u0026lt; 0)\n        compareAndSetWaitStatus(node, ws, 0);\n\n    Node s = node.next;\n    if (s == null || s.waitStatus \u0026gt; 0) {\n        s = null;\n        // 从尾部向前遍历，找到第一个非取消状态的节点\n        for (Node t = tail; t != null \u0026amp;\u0026amp; t != node; t = t.prev)\n            if (t.waitStatus \u0026lt;= 0)\n                s = t;\n    }\n    if (s != null)\n        LockSupport.unpark(s.thread);    // 唤醒线程\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e为什么要从后向前遍历？\u003c/strong\u003e 两个原因：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eaddWaiter\u003c/code\u003e 中节点入队不是原子操作——\u003ccode\u003enode.prev = pred\u003c/code\u003e 和 \u003ccode\u003ecompareAndSetTail\u003c/code\u003e 完成后，\u003ccode\u003epred.next = node\u003c/code\u003e 可能还未执行。此时从前向后遍历会断链。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecancelAcquire\u003c/code\u003e 产生 CANCELLED 节点时，先断开的是 next 指针，prev 指针未断开。因此从后向前遍历才能保证遍历完整。\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eCANCELLED 节点的处理\u003c/h2\u003e\n\u003cp\u003e当 \u003ccode\u003eacquireQueued\u003c/code\u003e 中发生异常时，会执行 \u003ccode\u003ecancelAcquire(node)\u003c/code\u003e 将节点标记为 CANCELLED。处理逻辑根据节点位置分为三种情况：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e节点位置\u003c/th\u003e\n\u003cth\u003e处理方式\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e尾节点\u003c/td\u003e\n\u003ctd\u003e将前驱设为新的 tail，其 next 置为 null\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e头节点的后继\u003c/td\u003e\n\u003ctd\u003e唤醒当前节点的后继线程（unparkSuccessor）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e中间节点\u003c/td\u003e\n\u003ctd\u003e将前驱的 next 指向当前节点的后继，跳过当前节点\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ccode\u003ecancelAcquire\u003c/code\u003e 只操作 next 指针，不操作 prev 指针。因为执行 cancel 时前驱可能已经出队，修改 prev 不安全。prev 指针的清理留给 \u003ccode\u003eshouldParkAfterFailedAcquire\u003c/code\u003e——此方法在获取锁失败时执行，此时共享资源已被占用，前方节点不会变化，修改 prev 是安全的。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e中断处理机制\u003c/h2\u003e\n\u003cp\u003eAQS 的 \u003ccode\u003eacquire\u003c/code\u003e 方法是\u003cstrong\u003e不可中断\u003c/strong\u003e的——线程在等待过程中不会响应中断，而是记录中断状态，等获取到锁后再\u0026quot;补上\u0026quot;中断：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic final void acquire(int arg) {\n    if (!tryAcquire(arg) \u0026amp;\u0026amp;\n        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))  // 返回 true 说明被中断过\n        selfInterrupt();                                  // 补上中断\n}\n\nstatic void selfInterrupt() {\n    Thread.currentThread().interrupt();\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这种设计的考量是：线程被唤醒时并不知道原因（可能是前驱释放了锁，也可能是被中断），所以通过 \u003ccode\u003eThread.interrupted()\u003c/code\u003e 检查并清除中断标记，记录下来，最后在获取锁成功后统一补上。\u003c/p\u003e\n\u003ch2\u003epark / unpark 机制\u003c/h2\u003e\n\u003cp\u003eAQS 中线程的阻塞和唤醒通过 \u003ccode\u003eLockSupport\u003c/code\u003e 实现：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e方法\u003c/th\u003e\n\u003cth\u003e作用\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eLockSupport.park(this)\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e阻塞当前线程\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eLockSupport.unpark(thread)\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e唤醒指定线程\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e它们的底层实现是通过 \u003ccode\u003eUnsafe\u003c/code\u003e 类调用 CPU 原语。相比 \u003ccode\u003eObject.wait/notify\u003c/code\u003e，park/unpark 的优势在于：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e不需要在同步块中使用\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eunpark\u003c/code\u003e 可以先于 \u003ccode\u003epark\u003c/code\u003e 调用（基于许可机制）\u003c/li\u003e\n\u003cli\u003e可以精确唤醒指定线程\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在 AQS 中使用 park 的主要目的是：\u003cstrong\u003e让排队等待的线程挂起，停止自旋以避免浪费 CPU 资源\u003c/strong\u003e，并在需要时通过 unpark 精确唤醒。\u003c/p\u003e\n\u003ch2\u003eAQS 在 JUC 中的应用场景\u003c/h2\u003e\n\u003cp\u003eAQS 是 JUC 包的基石，几乎所有同步工具都构建在它之上：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e同步工具\u003c/th\u003e\n\u003cth\u003e如何使用 AQS\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eReentrantLock\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003estate 表示锁的重入次数。获取锁时 state+1，释放时 state-1。state 为 0 表示锁空闲。同时记录持有锁的线程用于重入检测。\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eSemaphore\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003estate 表示可用许可数。\u003ccode\u003eacquireShared\u003c/code\u003e 减少计数，\u003ccode\u003etryReleaseShared\u003c/code\u003e 增加计数。\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eCountDownLatch\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003estate 表示计数器。每次 \u003ccode\u003ecountDown()\u003c/code\u003e 减 1，\u003ccode\u003eawait()\u003c/code\u003e 等待 state 变为 0 后所有线程被唤醒。\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eReentrantReadWriteLock\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003estate 的高 16 位保存读锁持有次数，低 16 位保存写锁持有次数。读锁用共享模式，写锁用独占模式。\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eThreadPoolExecutor\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eWorker 内部类继承 AQS，利用独占模式实现对工作线程的状态管理。\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch3\u003eState 在不同同步器中的语义\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eReentrantLock:       state = 重入次数 (0 = 空闲)\nSemaphore:           state = 可用许可数\nCountDownLatch:      state = 剩余计数 (0 = 所有线程放行)\nReadWriteLock:       state = [高16位:读锁次数][低16位:写锁次数]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e自定义同步器示例\u003c/h2\u003e\n\u003cp\u003e理解 AQS 后，我们可以用极少的代码实现一个简单的互斥锁：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic class SimpleLock {\n\n    private static class Sync extends AbstractQueuedSynchronizer {\n        @Override\n        protected boolean tryAcquire(int arg) {\n            return compareAndSetState(0, 1);\n        }\n\n        @Override\n        protected boolean tryRelease(int arg) {\n            setState(0);\n            return true;\n        }\n\n        @Override\n        protected boolean isHeldExclusively() {\n            return getState() == 1;\n        }\n    }\n\n    private final Sync sync = new Sync();\n\n    public void lock()   { sync.acquire(1); }\n    public void unlock() { sync.release(1); }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e使用：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic static void main(String[] args) throws InterruptedException {\n    SimpleLock lock = new SimpleLock();\n    int[] count = {0};\n\n    Runnable task = () -\u0026gt; {\n        lock.lock();\n        try {\n            for (int i = 0; i \u0026lt; 10000; i++) count[0]++;\n        } finally {\n            lock.unlock();\n        }\n    };\n\n    Thread t1 = new Thread(task);\n    Thread t2 = new Thread(task);\n    t1.start(); t2.start();\n    t1.join();  t2.join();\n    System.out.println(count[0]);  // 始终输出 20000\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e只需重写 \u003ccode\u003etryAcquire\u003c/code\u003e 和 \u003ccode\u003etryRelease\u003c/code\u003e，AQS 就接管了排队、阻塞、唤醒、中断处理等全部复杂逻辑。\u003c/p\u003e\n\u003ch2\u003e总结\u003c/h2\u003e\n\u003cp\u003eAQS 的设计精髓可以归纳为以下几点：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e一个 state 变量统一抽象\u003c/strong\u003e：不同的同步器通过赋予 state 不同的语义（重入次数、许可数、计数器等），复用同一套框架\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCLH 变体双向队列管理等待线程\u003c/strong\u003e：通过 FIFO 队列保证公平性，通过 CAS + 自旋保证入队的线程安全\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e模板方法模式降低接入成本\u003c/strong\u003e：自定义同步器只需实现 tryAcquire/tryRelease 等少量方法，框架处理全部排队和唤醒逻辑\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003epark/unpark 精确控制线程状态\u003c/strong\u003e：避免自旋空转浪费 CPU，同时支持精确唤醒\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e从后向前遍历保证正确性\u003c/strong\u003e：在非原子入队操作和 CANCELLED 节点处理中，始终保证能遍历到所有有效节点\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAQS 是 Doug Lea 在并发编程领域的杰作。理解了 AQS，就理解了 JUC 包中绝大部分同步工具的底层运作方式。它不仅是面试的高频考点，更是我们在实际工程中设计自定义同步器时可以直接借鉴的框架。\u003c/p\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"18:Tfd22,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eMemory Architecture: Agent 的状态与记忆体系\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003eLLM 是一个纯函数：给定相同的 prompt，产生相同的输出。它没有\u0026quot;昨天\u0026quot;，没有\u0026quot;上次\u0026quot;，没有\u0026quot;你之前说过\u0026quot;。\u003c/p\u003e\n\u003cp\u003e但一个合格的 Agent 必须记得：用户的偏好、上一步的结果、三天前那个失败的任务、以及从知识库中检索到的关键事实。\u003c/p\u003e\n\u003cp\u003e记忆，是 Agent 从\u0026quot;单轮工具\u0026quot;变成\u0026quot;持续助手\u0026quot;的分水岭。本文是 Agentic 系列第 08 篇，将系统拆解 Agent 记忆的四层架构，从认知科学类比到工程实现，给出完整的设计方案。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch2\u003e1. 为什么 Agent 需要记忆\u003c/h2\u003e\n\u003cp\u003eLLM 的本质是一个 \u003cstrong\u003estateless function\u003c/strong\u003e：\u003ccode\u003eresponse = llm(prompt)\u003c/code\u003e。每次调用都是一个全新的开始，模型不知道上一次调用发生了什么。\u003c/p\u003e\n\u003cp\u003e这在单轮问答场景下没有问题。但当我们把 LLM 嵌入 Agent 系统后，\u003cstrong\u003e无状态\u003c/strong\u003e就成了致命缺陷：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e多轮对话\u003c/strong\u003e：用户说\u0026quot;把上面那个改成蓝色\u0026quot;——\u0026quot;上面那个\u0026quot;在哪里？\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e长任务执行\u003c/strong\u003e：Agent 执行到第 5 步，需要回顾第 2 步的输出来做决策\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e跨会话连续性\u003c/strong\u003e：用户昨天让 Agent 分析了一份报告，今天问\u0026quot;上次那份报告的结论是什么？\u0026quot;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e个性化服务\u003c/strong\u003e：Agent 需要记住用户偏好（\u0026quot;我喜欢简洁的回答\u0026quot;、\u0026quot;输出用 Markdown 表格\u0026quot;）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e没有记忆的 Agent，每次对话都是一个\u0026quot;失忆症患者\u0026quot;——它可能很聪明，但永远无法建立连续的工作关系。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e核心命题：如何为一个 stateless 的 LLM 构建一套 stateful 的记忆体系，使 Agent 在有限的 Context Window 内，获得\u0026quot;无限\u0026quot;的记忆能力？\u003c/strong\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e2. 从认知科学看 Agent 记忆\u003c/h2\u003e\n\u003cp\u003e在设计 Agent 记忆架构之前，先看看人类大脑是怎么处理记忆的。认知心理学中 Atkinson-Shiffrin 模型把人类记忆分为多个层级，这个分层对 Agent 设计有极强的指导意义。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌─────────────────────────────────────────────────────────────────────┐\n│                    人类记忆 vs Agent 记忆                            │\n├─────────────────┬──────────────────┬────────────────────────────────┤\n│   人类记忆层级    │   Agent 对应       │   特征                        │\n├─────────────────┼──────────────────┼────────────────────────────────┤\n│ 感觉记忆         │ 当前输入           │ 极短暂，未经处理的原始信息        │\n│ (Sensory)       │ (User msg/Tool)  │ 持续 \u0026lt; 1秒 / 单次请求           │\n├─────────────────┼──────────────────┼────────────────────────────────┤\n│ 工作记忆         │ Context Window   │ 容量有限，正在处理的信息           │\n│ (Working)       │ (~128K tokens)   │ 持续几秒 / 单次 LLM 调用         │\n├─────────────────┼──────────────────┼────────────────────────────────┤\n│ 短期记忆         │ 会话状态           │ 当前任务上下文，可被覆写          │\n│ (Short-term)    │ (Session state)  │ 持续分钟~小时 / 单次会话          │\n├─────────────────┼──────────────────┼────────────────────────────────┤\n│ 长期记忆-情景     │ 历史交互记录        │ 过去的经验，可被检索              │\n│ (Episodic)      │ (Task history)   │ 持续天~月 / 跨会话               │\n├─────────────────┼──────────────────┼────────────────────────────────┤\n│ 长期记忆-语义     │ 知识库             │ 结构化知识，相对稳定              │\n│ (Semantic)      │ (Knowledge/RAG)  │ 持续月~年 / 持久化               │\n└─────────────────┴──────────────────┴────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这个类比的价值在于：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e分层处理\u003c/strong\u003e：不是所有信息都需要\u0026quot;记住\u0026quot;，大部分感觉输入会被丢弃\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e容量约束\u003c/strong\u003e：工作记忆（Context Window）的容量是硬性限制，必须在这个限制内做信息的取舍\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e编码与检索\u003c/strong\u003e：信息从短期记忆进入长期记忆需要\u0026quot;编码\u0026quot;（写入），使用时需要\u0026quot;检索\u0026quot;（读取）\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e遗忘是特性\u003c/strong\u003e：遗忘不是 bug，而是一种必要的信息过滤机制\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e基于这个认知框架，我们设计 Agent 的四层记忆架构。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e3. Agent 记忆的四层架构\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e                         ┌──────────────────────┐\n                         │     LLM Context       │\n                         │      Window           │\n                         │  ┌────────────────┐   │\n                         │  │ System Prompt  │   │\n                         │  ├────────────────┤   │\n    ┌─────────────┐      │  │ Memory Inject  │◄──┼──── Layer 3: Episodic Memory\n    │  User Input  │─────►│  ├────────────────┤   │     (向量数据库 / 关系数据库)\n    │  Tool Output │      │  │ Working Memory │◄──┼──── Layer 2: Working Memory\n    └─────────────┘      │  ├────────────────┤   │     (任务状态 / Scratchpad)\n                         │  │ Conv. History  │◄──┼──── Layer 1: Conversation Buffer\n                         │  ├────────────────┤   │     (消息历史 / 滑动窗口)\n                         │  │ Tool Schemas   │   │\n                         │  └────────────────┘   │        Layer 4: Semantic Memory\n                         └──────────┬───────────┘        (知识库 / RAG)\n                                    │                          │\n                                    │    ┌────────────────┐    │\n                                    └───►│  LLM Response   │◄───┘\n                                         └────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eLayer 1: Conversation Buffer — 对话历史\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e本质\u003c/strong\u003e：保存完整的 message history，让 LLM 能\u0026quot;看到\u0026quot;之前的对话。\u003c/p\u003e\n\u003cp\u003e这是最直觉的记忆形式：把所有 \u003ccode\u003euser\u003c/code\u003e 和 \u003ccode\u003eassistant\u003c/code\u003e 消息按顺序存起来，每次调用 LLM 时全量传入。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass ConversationBuffer:\n    \u0026quot;\u0026quot;\u0026quot;最简单的对话记忆：完整保存消息历史\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, max_tokens: int = 8000):\n        self.messages: list[dict] = []\n        self.max_tokens = max_tokens\n\n    def add(self, role: str, content: str):\n        self.messages.append({\u0026quot;role\u0026quot;: role, \u0026quot;content\u0026quot;: content})\n        self._enforce_limit()\n\n    def get_messages(self) -\u0026gt; list[dict]:\n        return list(self.messages)\n\n    def _enforce_limit(self):\n        \u0026quot;\u0026quot;\u0026quot;当超出 token 限制时，从最旧的消息开始裁剪\u0026quot;\u0026quot;\u0026quot;\n        while self._estimate_tokens() \u0026gt; self.max_tokens and len(self.messages) \u0026gt; 2:\n            # 保留第一条（通常包含重要上下文）和最后一条\n            self.messages.pop(1)\n\n    def _estimate_tokens(self) -\u0026gt; int:\n        # 粗略估算：1 token ≈ 4 chars (英文) 或 1.5 chars (中文)\n        return sum(len(m[\u0026quot;content\u0026quot;]) // 3 for m in self.messages)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e问题与解决方案\u003c/strong\u003e：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e问题\u003c/th\u003e\n\u003cth\u003e影响\u003c/th\u003e\n\u003cth\u003e解决方案\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003eContext Window 有限\u003c/td\u003e\n\u003ctd\u003e消息多了装不下\u003c/td\u003e\n\u003ctd\u003e滑动窗口：只保留最近 N 条\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e旧消息价值不均\u003c/td\u003e\n\u003ctd\u003e早期关键信息被丢弃\u003c/td\u003e\n\u003ctd\u003e消息摘要：用 LLM 压缩旧消息\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eToken 成本线性增长\u003c/td\u003e\n\u003ctd\u003e每轮调用的 token 越来越多\u003c/td\u003e\n\u003ctd\u003e选择性保留：只保留有工具调用或关键决策的消息\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e滑动窗口 + 摘要\u003c/strong\u003e是最常见的策略：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass SummarizingBuffer:\n    \u0026quot;\u0026quot;\u0026quot;带摘要能力的对话缓冲区\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, llm_client, window_size: int = 20, max_tokens: int = 8000):\n        self.llm_client = llm_client\n        self.window_size = window_size\n        self.max_tokens = max_tokens\n        self.messages: list[dict] = []\n        self.summary: str = \u0026quot;\u0026quot;  # 旧消息的压缩摘要\n\n    def add(self, role: str, content: str):\n        self.messages.append({\u0026quot;role\u0026quot;: role, \u0026quot;content\u0026quot;: content})\n        if len(self.messages) \u0026gt; self.window_size:\n            self._compress()\n\n    def get_messages(self) -\u0026gt; list[dict]:\n        result = []\n        if self.summary:\n            result.append({\n                \u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;,\n                \u0026quot;content\u0026quot;: f\u0026quot;[Previous conversation summary]\\n{self.summary}\u0026quot;\n            })\n        result.extend(self.messages)\n        return result\n\n    def _compress(self):\n        \u0026quot;\u0026quot;\u0026quot;将窗口外的消息压缩为摘要\u0026quot;\u0026quot;\u0026quot;\n        # 取出要压缩的消息（保留最近 window_size 条）\n        to_compress = self.messages[:-self.window_size]\n        self.messages = self.messages[-self.window_size:]\n\n        # 用 LLM 生成摘要\n        old_context = \u0026quot;\\n\u0026quot;.join(\n            f\u0026quot;{m[\u0026#39;role\u0026#39;]}: {m[\u0026#39;content\u0026#39;]}\u0026quot; for m in to_compress\n        )\n        prompt = (\n            f\u0026quot;Summarize this conversation history concisely, \u0026quot;\n            f\u0026quot;preserving key decisions, facts, and user preferences:\\n\\n\u0026quot;\n            f\u0026quot;Previous summary: {self.summary}\\n\\n\u0026quot;\n            f\u0026quot;New messages:\\n{old_context}\u0026quot;\n        )\n        self.summary = self.llm_client.complete(prompt)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e关键决策点\u003c/strong\u003e：摘要的质量直接决定 Agent 的\u0026quot;记忆保真度\u0026quot;。摘要太短会丢失关键信息，太长又失去压缩的意义。实践中，摘要长度控制在原文的 20%-30% 是比较好的平衡点。\u003c/p\u003e\n\u003chr\u003e\n\u003ch3\u003eLayer 2: Working Memory — 任务执行状态\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e本质\u003c/strong\u003e：当前任务的\u0026quot;草稿纸\u0026quot;，记录正在进行的工作的结构化状态。\u003c/p\u003e\n\u003cp\u003eConversation Buffer 保存的是\u0026quot;说了什么\u0026quot;，Working Memory 保存的是\u0026quot;正在做什么\u0026quot;。两者的核心区别：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eConversation Buffer:                Working Memory:\n┌─────────────────────┐            ┌─────────────────────────────┐\n│ user: 帮我分析这份数据 │            │ current_goal: 分析销售数据      │\n│ assistant: 好的...   │            │ completed_steps:               │\n│ user: 用柱状图展示    │            │   - 读取 CSV ✓                │\n│ assistant: ...       │            │   - 清洗缺失值 ✓               │\n│ tool: [read_csv...]  │            │ next_step: 生成柱状图           │\n│ ...                  │            │ scratchpad:                    │\n│ (线性的消息流)         │            │   - 数据有 1000 行, 15 列       │\n└─────────────────────┘            │   - 销售额列有 3% 空值          │\n                                   │   - 日期范围: 2024-01 ~ 2024-12 │\n                                   └─────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWorking Memory 的价值在长任务中尤为明显。当 Agent 执行一个需要 10+ 步的任务时，把所有中间结果都塞在对话历史里是低效的。Working Memory 提供了一个结构化的\u0026quot;任务视图\u0026quot;。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom dataclasses import dataclass, field\nfrom typing import Any\nfrom enum import Enum\n\nclass StepStatus(Enum):\n    PENDING = \u0026quot;pending\u0026quot;\n    IN_PROGRESS = \u0026quot;in_progress\u0026quot;\n    COMPLETED = \u0026quot;completed\u0026quot;\n    FAILED = \u0026quot;failed\u0026quot;\n\n@dataclass\nclass TaskStep:\n    description: str\n    status: StepStatus = StepStatus.PENDING\n    result: Any = None\n    error: str | None = None\n\n@dataclass\nclass WorkingMemory:\n    \u0026quot;\u0026quot;\u0026quot;当前任务的执行状态\u0026quot;\u0026quot;\u0026quot;\n\n    goal: str = \u0026quot;\u0026quot;\n    plan: list[TaskStep] = field(default_factory=list)\n    scratchpad: dict[str, Any] = field(default_factory=dict)\n    iteration: int = 0\n    max_iterations: int = 20\n\n    def set_goal(self, goal: str):\n        self.goal = goal\n        self.plan = []\n        self.scratchpad = {}\n        self.iteration = 0\n\n    def add_step(self, description: str) -\u0026gt; int:\n        self.plan.append(TaskStep(description=description))\n        return len(self.plan) - 1\n\n    def complete_step(self, index: int, result: Any):\n        self.plan[index].status = StepStatus.COMPLETED\n        self.plan[index].result = result\n\n    def fail_step(self, index: int, error: str):\n        self.plan[index].status = StepStatus.FAILED\n        self.plan[index].error = error\n\n    def note(self, key: str, value: Any):\n        \u0026quot;\u0026quot;\u0026quot;在 scratchpad 上记录中间发现\u0026quot;\u0026quot;\u0026quot;\n        self.scratchpad[key] = value\n\n    def to_context_string(self) -\u0026gt; str:\n        \u0026quot;\u0026quot;\u0026quot;序列化为可注入 prompt 的文本\u0026quot;\u0026quot;\u0026quot;\n        lines = [f\u0026quot;## Current Task State\u0026quot;]\n        lines.append(f\u0026quot;**Goal**: {self.goal}\u0026quot;)\n        lines.append(f\u0026quot;**Progress**: Step {self.iteration}/{self.max_iterations}\u0026quot;)\n        lines.append(\u0026quot;\u0026quot;)\n        lines.append(\u0026quot;### Plan:\u0026quot;)\n        for i, step in enumerate(self.plan):\n            status_icon = {\n                StepStatus.PENDING: \u0026quot;[ ]\u0026quot;,\n                StepStatus.IN_PROGRESS: \u0026quot;[\u0026gt;]\u0026quot;,\n                StepStatus.COMPLETED: \u0026quot;[x]\u0026quot;,\n                StepStatus.FAILED: \u0026quot;[!]\u0026quot;,\n            }[step.status]\n            lines.append(f\u0026quot;  {status_icon} {i+1}. {step.description}\u0026quot;)\n            if step.result:\n                lines.append(f\u0026quot;       Result: {str(step.result)[:200]}\u0026quot;)\n            if step.error:\n                lines.append(f\u0026quot;       Error: {step.error}\u0026quot;)\n\n        if self.scratchpad:\n            lines.append(\u0026quot;\u0026quot;)\n            lines.append(\u0026quot;### Scratchpad:\u0026quot;)\n            for k, v in self.scratchpad.items():\n                lines.append(f\u0026quot;  - {k}: {str(v)[:300]}\u0026quot;)\n\n        return \u0026quot;\\n\u0026quot;.join(lines)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eWorking Memory 什么时候更新？\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePlan 阶段\u003c/strong\u003e：Agent 制定计划后，写入 \u003ccode\u003eplan\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e每步执行后\u003c/strong\u003e：更新 step 状态和结果\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e发现新信息时\u003c/strong\u003e：写入 \u003ccode\u003escratchpad\u003c/code\u003e（例如发现数据有异常值）\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e任务完成/失败时\u003c/strong\u003e：清空或归档到 Episodic Memory\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3\u003eLayer 3: Episodic Memory — 历史经验\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e本质\u003c/strong\u003e：过去交互的结构化记录，用于跨会话的经验积累。\u003c/p\u003e\n\u003cp\u003e如果说 Working Memory 是\u0026quot;今天的笔记\u0026quot;，Episodic Memory 就是\u0026quot;过去的日记\u0026quot;。它回答的问题是：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u0026quot;上次用户让我处理类似的任务，我是怎么做的？\u0026quot;\u003c/li\u003e\n\u003cli\u003e\u0026quot;用户偏好什么样的输出格式？\u0026quot;\u003c/li\u003e\n\u003cli\u003e\u0026quot;上次这个工具调用失败了，原因是什么？\u0026quot;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport time\nimport json\nimport hashlib\nfrom dataclasses import dataclass, asdict\n\n@dataclass\nclass Episode:\n    \u0026quot;\u0026quot;\u0026quot;一次交互的结构化记录\u0026quot;\u0026quot;\u0026quot;\n\n    episode_id: str\n    timestamp: float\n    task_description: str\n    approach: str              # Agent 采用的方法\n    outcome: str               # 成功/失败/部分成功\n    key_decisions: list[str]   # 关键决策点\n    user_feedback: str | None  # 用户反馈（如果有）\n    tools_used: list[str]      # 使用了哪些工具\n    lessons: list[str]         # 经验教训\n    importance: float          # 重要性评分 0-1\n    embedding: list[float] | None = None  # 向量表示\n\n    def to_context_string(self) -\u0026gt; str:\n        return (\n            f\u0026quot;[Past experience - {self.task_description}]\\n\u0026quot;\n            f\u0026quot;Approach: {self.approach}\\n\u0026quot;\n            f\u0026quot;Outcome: {self.outcome}\\n\u0026quot;\n            f\u0026quot;Lessons: {\u0026#39;; \u0026#39;.join(self.lessons)}\u0026quot;\n        )\n\n\nclass EpisodicMemory:\n    \u0026quot;\u0026quot;\u0026quot;基于向量检索的情景记忆\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, embedding_fn, vector_store):\n        self.embedding_fn = embedding_fn   # text -\u0026gt; vector\n        self.vector_store = vector_store   # 向量数据库客户端\n        self.decay_factor = 0.95           # 时间衰减因子\n\n    def store(self, episode: Episode):\n        \u0026quot;\u0026quot;\u0026quot;写入一条记忆\u0026quot;\u0026quot;\u0026quot;\n        # 生成向量表示\n        text_for_embedding = (\n            f\u0026quot;{episode.task_description} {episode.approach} \u0026quot;\n            f\u0026quot;{\u0026#39; \u0026#39;.join(episode.lessons)}\u0026quot;\n        )\n        episode.embedding = self.embedding_fn(text_for_embedding)\n\n        # 写入向量数据库\n        self.vector_store.upsert(\n            id=episode.episode_id,\n            vector=episode.embedding,\n            metadata=asdict(episode)\n        )\n\n    def recall(self, query: str, top_k: int = 5) -\u0026gt; list[Episode]:\n        \u0026quot;\u0026quot;\u0026quot;根据当前任务检索相关记忆\u0026quot;\u0026quot;\u0026quot;\n        query_embedding = self.embedding_fn(query)\n\n        # 向量相似度检索\n        results = self.vector_store.query(\n            vector=query_embedding,\n            top_k=top_k * 2  # 多检索一些，后面再过滤\n        )\n\n        # 综合评分：相似度 × 时间衰减 × 重要性\n        scored_episodes = []\n        now = time.time()\n        for result in results:\n            episode = Episode(**result.metadata)\n            age_days = (now - episode.timestamp) / 86400\n\n            # 综合评分公式\n            time_decay = self.decay_factor ** age_days\n            final_score = (\n                result.similarity * 0.5 +    # 语义相似度\n                time_decay * 0.3 +            # 时间新鲜度\n                episode.importance * 0.2      # 重要性\n            )\n            scored_episodes.append((episode, final_score))\n\n        # 按综合分排序，取 top_k\n        scored_episodes.sort(key=lambda x: x[1], reverse=True)\n        return [ep for ep, _ in scored_episodes[:top_k]]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eEpisodic Memory 的检索策略\u003c/strong\u003e：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e                    Query: \u0026quot;用户要分析 Q3 销售数据\u0026quot;\n                              │\n                    ┌─────────▼──────────┐\n                    │   Embedding Model   │\n                    └─────────┬──────────┘\n                              │ query_vector\n                    ┌─────────▼──────────┐\n                    │   Vector Search     │──── 语义相似度 (0.5)\n                    │   (Top 10)          │\n                    └─────────┬──────────┘\n                              │ candidates\n                    ┌─────────▼──────────┐\n                    │   Scoring \u0026amp; Rank    │\n                    │  ├─ time_decay (0.3)│──── 新消息 \u0026gt; 旧消息\n                    │  └─ importance (0.2)│──── 成功经验 \u0026gt; 普通记录\n                    └─────────┬──────────┘\n                              │ top_k\n                    ┌─────────▼──────────┐\n                    │ Format \u0026amp; Inject     │──── 注入 Context Window\n                    │ into Prompt         │\n                    └────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e关键决策：什么时候写入 Episodic Memory？\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e不是每轮对话都值得记住。过度记忆会导致检索噪声。实践中推荐以下策略：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e触发条件\u003c/th\u003e\n\u003cth\u003e写入内容\u003c/th\u003e\n\u003cth\u003e重要性\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e任务成功完成\u003c/td\u003e\n\u003ctd\u003e完整的任务描述、方法、结果\u003c/td\u003e\n\u003ctd\u003e0.7-0.9\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e任务失败\u003c/td\u003e\n\u003ctd\u003e失败原因、错误信息、教训\u003c/td\u003e\n\u003ctd\u003e0.8-1.0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e用户显式反馈\u003c/td\u003e\n\u003ctd\u003e用户的表扬/批评/修正\u003c/td\u003e\n\u003ctd\u003e0.9-1.0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e发现新的用户偏好\u003c/td\u003e\n\u003ctd\u003e偏好描述\u003c/td\u003e\n\u003ctd\u003e0.8\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e使用了新的工具/方法\u003c/td\u003e\n\u003ctd\u003e工具使用经验\u003c/td\u003e\n\u003ctd\u003e0.5-0.7\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003chr\u003e\n\u003ch3\u003eLayer 4: Semantic Memory — 知识库\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e本质\u003c/strong\u003e：相对稳定的事实性知识，通常通过 RAG (Retrieval-Augmented Generation) 接入。\u003c/p\u003e\n\u003cp\u003eSemantic Memory 与 Episodic Memory 的区别：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e维度\u003c/th\u003e\n\u003cth\u003eEpisodic Memory\u003c/th\u003e\n\u003cth\u003eSemantic Memory\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e存储内容\u003c/td\u003e\n\u003ctd\u003eAgent 的经验（做过什么）\u003c/td\u003e\n\u003ctd\u003e外部知识（世界是什么样）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e更新频率\u003c/td\u003e\n\u003ctd\u003e每次任务后可能更新\u003c/td\u003e\n\u003ctd\u003e相对稳定，定期更新\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e来源\u003c/td\u003e\n\u003ctd\u003eAgent 自身的交互历史\u003c/td\u003e\n\u003ctd\u003e文档、数据库、API\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e检索触发\u003c/td\u003e\n\u003ctd\u003e遇到类似任务时\u003c/td\u003e\n\u003ctd\u003e需要事实性知识时\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e在本篇中，我们只关注 Agent 如何\u0026quot;使用\u0026quot;Semantic Memory。知识如何构建、如何切分、如何检索——这些 RAG 工程问题留给下一篇文章。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass SemanticMemory:\n    \u0026quot;\u0026quot;\u0026quot;知识库接口（RAG 的消费侧）\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, retriever):\n        self.retriever = retriever  # RAG 检索器\n\n    def query(self, question: str, top_k: int = 3) -\u0026gt; list[dict]:\n        \u0026quot;\u0026quot;\u0026quot;检索相关知识片段\u0026quot;\u0026quot;\u0026quot;\n        results = self.retriever.search(question, top_k=top_k)\n        return [\n            {\n                \u0026quot;content\u0026quot;: r.text,\n                \u0026quot;source\u0026quot;: r.metadata.get(\u0026quot;source\u0026quot;, \u0026quot;unknown\u0026quot;),\n                \u0026quot;relevance\u0026quot;: r.score,\n            }\n            for r in results\n        ]\n\n    def format_for_context(self, results: list[dict]) -\u0026gt; str:\n        \u0026quot;\u0026quot;\u0026quot;格式化为可注入 prompt 的文本\u0026quot;\u0026quot;\u0026quot;\n        if not results:\n            return \u0026quot;\u0026quot;\n        lines = [\u0026quot;## Relevant Knowledge:\u0026quot;]\n        for i, r in enumerate(results, 1):\n            lines.append(f\u0026quot;\\n### [{i}] (source: {r[\u0026#39;source\u0026#39;]})\u0026quot;)\n            lines.append(r[\u0026quot;content\u0026quot;])\n        return \u0026quot;\\n\u0026quot;.join(lines)\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e4. 记忆的读写操作\u003c/h2\u003e\n\u003cp\u003e记忆系统的核心操作可以概括为四个：\u003cstrong\u003eWrite、Read、Update、Forget\u003c/strong\u003e。每个操作都有其触发时机和策略选择。\u003c/p\u003e\n\u003ch3\u003eWrite：写入记忆\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass MemoryWriter:\n    \u0026quot;\u0026quot;\u0026quot;决定什么信息、在什么时候写入哪层记忆\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, working_memory, episodic_memory, llm_client):\n        self.working = working_memory\n        self.episodic = episodic_memory\n        self.llm = llm_client\n\n    def on_step_complete(self, step_index: int, result: Any):\n        \u0026quot;\u0026quot;\u0026quot;每步执行完成后的写入\u0026quot;\u0026quot;\u0026quot;\n        # 更新 Working Memory\n        self.working.complete_step(step_index, result)\n\n        # 重要发现写入 scratchpad\n        if self._is_notable(result):\n            key = f\u0026quot;step_{step_index}_finding\u0026quot;\n            self.working.note(key, self._extract_key_info(result))\n\n    def on_task_complete(self, task_description: str, success: bool):\n        \u0026quot;\u0026quot;\u0026quot;任务完成后的写入\u0026quot;\u0026quot;\u0026quot;\n        # 用 LLM 提取经验教训\n        reflection_prompt = (\n            f\u0026quot;Task: {task_description}\\n\u0026quot;\n            f\u0026quot;Working Memory:\\n{self.working.to_context_string()}\\n\\n\u0026quot;\n            f\u0026quot;Extract key lessons learned from this task. \u0026quot;\n            f\u0026quot;Output as JSON with keys: approach, lessons, importance (0-1)\u0026quot;\n        )\n        reflection = self.llm.complete(reflection_prompt, json_mode=True)\n        parsed = json.loads(reflection)\n\n        # 写入 Episodic Memory\n        episode = Episode(\n            episode_id=hashlib.md5(\n                f\u0026quot;{task_description}{time.time()}\u0026quot;.encode()\n            ).hexdigest(),\n            timestamp=time.time(),\n            task_description=task_description,\n            approach=parsed.get(\u0026quot;approach\u0026quot;, \u0026quot;\u0026quot;),\n            outcome=\u0026quot;success\u0026quot; if success else \u0026quot;failure\u0026quot;,\n            key_decisions=[],\n            user_feedback=None,\n            tools_used=[],\n            lessons=parsed.get(\u0026quot;lessons\u0026quot;, []),\n            importance=parsed.get(\u0026quot;importance\u0026quot;, 0.5),\n        )\n        self.episodic.store(episode)\n\n    def on_user_feedback(self, feedback: str, task_description: str):\n        \u0026quot;\u0026quot;\u0026quot;用户反馈时的写入——高优先级\u0026quot;\u0026quot;\u0026quot;\n        episode = Episode(\n            episode_id=hashlib.md5(\n                f\u0026quot;feedback_{time.time()}\u0026quot;.encode()\n            ).hexdigest(),\n            timestamp=time.time(),\n            task_description=task_description,\n            approach=\u0026quot;\u0026quot;,\n            outcome=\u0026quot;user_feedback\u0026quot;,\n            key_decisions=[],\n            user_feedback=feedback,\n            tools_used=[],\n            lessons=[f\u0026quot;User feedback: {feedback}\u0026quot;],\n            importance=0.9,  # 用户反馈总是高重要性\n        )\n        self.episodic.store(episode)\n\n    def _is_notable(self, result: Any) -\u0026gt; bool:\n        \u0026quot;\u0026quot;\u0026quot;判断结果是否值得特别记录\u0026quot;\u0026quot;\u0026quot;\n        # 简单启发式：结果较长或包含数字时可能重要\n        text = str(result)\n        return len(text) \u0026gt; 200 or any(c.isdigit() for c in text)\n\n    def _extract_key_info(self, result: Any) -\u0026gt; str:\n        \u0026quot;\u0026quot;\u0026quot;提取关键信息（可以用 LLM，也可以用规则）\u0026quot;\u0026quot;\u0026quot;\n        text = str(result)\n        if len(text) \u0026lt;= 300:\n            return text\n        return text[:300] + \u0026quot;...\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eRead：读取记忆\u003c/h3\u003e\n\u003cp\u003e读取操作发生在每次 LLM 调用之前——我们需要从各层记忆中组装 context。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass MemoryReader:\n    \u0026quot;\u0026quot;\u0026quot;从各层记忆中组装 LLM 调用的上下文\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(\n        self,\n        conversation_buffer,\n        working_memory,\n        episodic_memory,\n        semantic_memory,\n    ):\n        self.conversation = conversation_buffer\n        self.working = working_memory\n        self.episodic = episodic_memory\n        self.semantic = semantic_memory\n\n    def assemble_context(\n        self,\n        user_query: str,\n        system_prompt: str,\n        token_budget: int = 16000,\n    ) -\u0026gt; list[dict]:\n        \u0026quot;\u0026quot;\u0026quot;组装完整的消息列表\u0026quot;\u0026quot;\u0026quot;\n        messages = []\n\n        # 1. System Prompt（固定分配）\n        messages.append({\u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;, \u0026quot;content\u0026quot;: system_prompt})\n\n        # 2. 检索 Episodic Memory（相关历史经验）\n        relevant_episodes = self.episodic.recall(user_query, top_k=3)\n        if relevant_episodes:\n            episode_text = \u0026quot;\\n\\n\u0026quot;.join(\n                ep.to_context_string() for ep in relevant_episodes\n            )\n            messages.append({\n                \u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;,\n                \u0026quot;content\u0026quot;: f\u0026quot;## Relevant Past Experience:\\n{episode_text}\u0026quot;\n            })\n\n        # 3. 检索 Semantic Memory（相关知识）\n        knowledge_results = self.semantic.query(user_query, top_k=3)\n        if knowledge_results:\n            knowledge_text = self.semantic.format_for_context(knowledge_results)\n            messages.append({\n                \u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;,\n                \u0026quot;content\u0026quot;: knowledge_text\n            })\n\n        # 4. Working Memory（当前任务状态）\n        if self.working.goal:\n            messages.append({\n                \u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;,\n                \u0026quot;content\u0026quot;: self.working.to_context_string()\n            })\n\n        # 5. Conversation History（对话历史）\n        messages.extend(self.conversation.get_messages())\n\n        # 6. Token 预算检查与裁剪\n        messages = self._fit_to_budget(messages, token_budget)\n\n        return messages\n\n    def _fit_to_budget(\n        self, messages: list[dict], budget: int\n    ) -\u0026gt; list[dict]:\n        \u0026quot;\u0026quot;\u0026quot;确保总 token 数不超过预算\u0026quot;\u0026quot;\u0026quot;\n        total = sum(len(m[\u0026quot;content\u0026quot;]) // 3 for m in messages)\n        if total \u0026lt;= budget:\n            return messages\n\n        # 裁剪策略：优先裁减对话历史中间部分\n        # 保留: system prompts + 最早2条 + 最近5条\n        system_msgs = [m for m in messages if m[\u0026quot;role\u0026quot;] == \u0026quot;system\u0026quot;]\n        non_system = [m for m in messages if m[\u0026quot;role\u0026quot;] != \u0026quot;system\u0026quot;]\n\n        if len(non_system) \u0026gt; 7:\n            kept = non_system[:2] + non_system[-5:]\n            messages = system_msgs + kept\n\n        return messages\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eUpdate：记忆更新\u003c/h3\u003e\n\u003cp\u003e记忆更新有三种模式，适用于不同场景：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass MemoryUpdateStrategy:\n    \u0026quot;\u0026quot;\u0026quot;记忆更新策略\u0026quot;\u0026quot;\u0026quot;\n\n    @staticmethod\n    def overwrite(store: dict, key: str, value: Any):\n        \u0026quot;\u0026quot;\u0026quot;覆盖：新值完全替换旧值\n        适用于：用户偏好（用户说\u0026quot;我改主意了，用英文回复\u0026quot;）\n        \u0026quot;\u0026quot;\u0026quot;\n        store[key] = value\n\n    @staticmethod\n    def append(store: dict, key: str, value: Any):\n        \u0026quot;\u0026quot;\u0026quot;追加：保留历史，添加新记录\n        适用于：任务历史（每次任务都是新记录）\n        \u0026quot;\u0026quot;\u0026quot;\n        if key not in store:\n            store[key] = []\n        store[key].append(value)\n\n    @staticmethod\n    def merge(store: dict, key: str, value: dict, llm_client=None):\n        \u0026quot;\u0026quot;\u0026quot;合并：智能融合旧信息和新信息\n        适用于：用户画像（逐步积累，可能有矛盾需要解决）\n        \u0026quot;\u0026quot;\u0026quot;\n        if key not in store:\n            store[key] = value\n            return\n\n        old = store[key]\n        if llm_client:\n            # 用 LLM 智能合并\n            prompt = (\n                f\u0026quot;Merge these two user profiles, resolving conflicts \u0026quot;\n                f\u0026quot;by preferring newer information:\\n\u0026quot;\n                f\u0026quot;Old: {json.dumps(old)}\\nNew: {json.dumps(value)}\u0026quot;\n            )\n            merged = json.loads(llm_client.complete(prompt, json_mode=True))\n            store[key] = merged\n        else:\n            # 简单合并：新值覆盖旧值中的同名字段\n            if isinstance(old, dict) and isinstance(value, dict):\n                store[key] = {**old, **value}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eForget：记忆遗忘\u003c/h3\u003e\n\u003cp\u003e遗忘是记忆系统的必要组成部分。没有遗忘，记忆库会无限膨胀，检索质量会持续下降。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass MemoryForgetting:\n    \u0026quot;\u0026quot;\u0026quot;记忆遗忘策略\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, episodic_memory, decay_rate: float = 0.01):\n        self.episodic = episodic_memory\n        self.decay_rate = decay_rate\n\n    def time_based_decay(self, max_age_days: int = 90):\n        \u0026quot;\u0026quot;\u0026quot;基于时间的遗忘：超过 N 天且重要性低的记忆被清除\u0026quot;\u0026quot;\u0026quot;\n        cutoff = time.time() - (max_age_days * 86400)\n        all_episodes = self.episodic.vector_store.list_all()\n\n        for episode_data in all_episodes:\n            if (\n                episode_data[\u0026quot;timestamp\u0026quot;] \u0026lt; cutoff\n                and episode_data[\u0026quot;importance\u0026quot;] \u0026lt; 0.7\n            ):\n                self.episodic.vector_store.delete(episode_data[\u0026quot;episode_id\u0026quot;])\n\n    def capacity_based_eviction(self, max_episodes: int = 1000):\n        \u0026quot;\u0026quot;\u0026quot;基于容量的驱逐：保留最重要的 N 条记忆\u0026quot;\u0026quot;\u0026quot;\n        all_episodes = self.episodic.vector_store.list_all()\n\n        if len(all_episodes) \u0026lt;= max_episodes:\n            return\n\n        # 按综合分排序（重要性 × 时间衰减）\n        now = time.time()\n        scored = []\n        for ep in all_episodes:\n            age_days = (now - ep[\u0026quot;timestamp\u0026quot;]) / 86400\n            score = ep[\u0026quot;importance\u0026quot;] * (0.95 ** age_days)\n            scored.append((ep[\u0026quot;episode_id\u0026quot;], score))\n\n        scored.sort(key=lambda x: x[1])\n\n        # 删除分数最低的\n        to_remove = len(all_episodes) - max_episodes\n        for episode_id, _ in scored[:to_remove]:\n            self.episodic.vector_store.delete(episode_id)\n\n    def explicit_forget(self, episode_id: str):\n        \u0026quot;\u0026quot;\u0026quot;主动遗忘：用户要求或隐私合规\u0026quot;\u0026quot;\u0026quot;\n        self.episodic.vector_store.delete(episode_id)\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e5. 记忆存储方案对比\u003c/h2\u003e\n\u003cp\u003e不同的记忆层适合不同的存储后端。选择存储方案时需要考虑：数据结构、访问模式、持久化需求和查询能力。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌──────────────┬──────────────────┬──────────────────┬──────────────────┐\n│   存储方案     │   适用记忆层        │   优点             │   缺点            │\n├──────────────┼──────────────────┼──────────────────┼──────────────────┤\n│ 内存          │ Conversation     │ 零延迟            │ 重启丢失          │\n│ (dict/list)  │ Buffer,          │ 实现简单           │ 不可跨进程        │\n│              │ Working Memory   │ 无外部依赖         │ 容量受限          │\n├──────────────┼──────────────────┼──────────────────┼──────────────────┤\n│ Redis        │ 会话状态,          │ 亚毫秒读写         │ 无语义检索        │\n│              │ Working Memory,  │ 支持 TTL 自动过期   │ 数据结构较简单     │\n│              │ 短期缓存          │ 可跨进程           │ 需要额外运维       │\n├──────────────┼──────────────────┼──────────────────┼──────────────────┤\n│ 向量数据库     │ Episodic Memory, │ 语义相似度检索      │ 写入有延迟        │\n│ (Chroma /    │ Semantic Memory  │ 适合非结构化数据     │ 精确查询弱        │\n│  Pinecone)   │                  │ 可扩展             │ 需要 Embedding    │\n├──────────────┼──────────────────┼──────────────────┼──────────────────┤\n│ 关系数据库     │ 用户偏好,          │ 结构化查询强        │ 无语义检索        │\n│ (PostgreSQL) │ 任务历史,         │ 事务保证            │ Schema 需设计     │\n│              │ 审计日志          │ 成熟稳定            │ 向量支持有限       │\n├──────────────┼──────────────────┼──────────────────┼──────────────────┤\n│ 混合方案       │ 生产环境          │ 各取所长            │ 复杂度高          │\n│ PG + Vector  │ 全层级            │ 一个系统解决多需求    │ 需要编排层        │\n│ + Redis      │                  │                   │                  │\n└──────────────┴──────────────────┴──────────────────┴──────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e实践建议\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e原型阶段\u003c/strong\u003e：全部用内存（dict + list），快速验证\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e单用户产品\u003c/strong\u003e：SQLite + ChromaDB（本地向量库），零运维\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e多用户产品\u003c/strong\u003e：PostgreSQL（结构化数据 + pgvector 扩展）+ Redis（会话缓存）\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e大规模系统\u003c/strong\u003e：PostgreSQL + 专用向量数据库（Pinecone/Qdrant）+ Redis Cluster\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e6. 完整实现：MemoryManager\u003c/h2\u003e\n\u003cp\u003e将四层记忆整合到一个统一的管理器中，在 Agent Loop 中使用。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport time\nimport json\nimport hashlib\nfrom dataclasses import dataclass, field, asdict\nfrom typing import Any, Protocol\n\n\nclass LLMClient(Protocol):\n    \u0026quot;\u0026quot;\u0026quot;LLM 客户端接口\u0026quot;\u0026quot;\u0026quot;\n    def complete(self, prompt: str, json_mode: bool = False) -\u0026gt; str: ...\n\n\nclass VectorStore(Protocol):\n    \u0026quot;\u0026quot;\u0026quot;向量存储接口\u0026quot;\u0026quot;\u0026quot;\n    def upsert(self, id: str, vector: list[float], metadata: dict): ...\n    def query(self, vector: list[float], top_k: int) -\u0026gt; list: ...\n    def delete(self, id: str): ...\n    def list_all(self) -\u0026gt; list[dict]: ...\n\n\nclass Retriever(Protocol):\n    \u0026quot;\u0026quot;\u0026quot;RAG 检索器接口\u0026quot;\u0026quot;\u0026quot;\n    def search(self, query: str, top_k: int) -\u0026gt; list: ...\n\n\nclass MemoryManager:\n    \u0026quot;\u0026quot;\u0026quot;\n    统一记忆管理器，整合四层记忆架构。\n\n    职责：\n    1. 管理四层记忆的生命周期\n    2. 在 Agent Loop 中提供 read/write 接口\n    3. 处理 Context Window 的 token 预算分配\n    \u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(\n        self,\n        llm_client: LLMClient,\n        embedding_fn,\n        vector_store: VectorStore,\n        retriever: Retriever,\n        config: dict | None = None,\n    ):\n        self.llm = llm_client\n        self.config = config or {}\n\n        # Layer 1: Conversation Buffer\n        self.conversation = SummarizingBuffer(\n            llm_client=llm_client,\n            window_size=self.config.get(\u0026quot;conversation_window\u0026quot;, 20),\n            max_tokens=self.config.get(\u0026quot;conversation_max_tokens\u0026quot;, 8000),\n        )\n\n        # Layer 2: Working Memory\n        self.working = WorkingMemory(\n            max_iterations=self.config.get(\u0026quot;max_iterations\u0026quot;, 20)\n        )\n\n        # Layer 3: Episodic Memory\n        self.episodic = EpisodicMemory(\n            embedding_fn=embedding_fn,\n            vector_store=vector_store,\n        )\n\n        # Layer 4: Semantic Memory\n        self.semantic = SemanticMemory(retriever=retriever)\n\n        # Token budget config\n        self.total_budget = self.config.get(\u0026quot;total_token_budget\u0026quot;, 16000)\n        self.budget_allocation = self.config.get(\u0026quot;budget_allocation\u0026quot;, {\n            \u0026quot;system_prompt\u0026quot;: 0.20,  # 20% for system prompt\n            \u0026quot;memory\u0026quot;: 0.30,         # 30% for episodic + semantic memory\n            \u0026quot;history\u0026quot;: 0.30,        # 30% for conversation history\n            \u0026quot;reserve\u0026quot;: 0.20,        # 20% for tool schemas + response\n        })\n\n    # ── Read: 组装 LLM 上下文 ──────────────────────────────────\n\n    def build_context(\n        self, user_query: str, system_prompt: str\n    ) -\u0026gt; list[dict]:\n        \u0026quot;\u0026quot;\u0026quot;在每次 LLM 调用前，组装完整的消息列表\u0026quot;\u0026quot;\u0026quot;\n        messages = []\n        budget = self.total_budget\n\n        # 1. System Prompt\n        sp_budget = int(budget * self.budget_allocation[\u0026quot;system_prompt\u0026quot;])\n        system_content = self._truncate(system_prompt, sp_budget)\n        messages.append({\u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;, \u0026quot;content\u0026quot;: system_content})\n\n        # 2. Memory injection (Episodic + Semantic + Working)\n        mem_budget = int(budget * self.budget_allocation[\u0026quot;memory\u0026quot;])\n        memory_parts = []\n\n        # 2a. Working Memory\n        if self.working.goal:\n            memory_parts.append(self.working.to_context_string())\n\n        # 2b. Episodic Memory\n        episodes = self.episodic.recall(user_query, top_k=3)\n        if episodes:\n            ep_text = \u0026quot;\\n\\n\u0026quot;.join(ep.to_context_string() for ep in episodes)\n            memory_parts.append(f\u0026quot;## Past Experience:\\n{ep_text}\u0026quot;)\n\n        # 2c. Semantic Memory\n        knowledge = self.semantic.query(user_query, top_k=3)\n        if knowledge:\n            memory_parts.append(self.semantic.format_for_context(knowledge))\n\n        if memory_parts:\n            combined = \u0026quot;\\n\\n---\\n\\n\u0026quot;.join(memory_parts)\n            combined = self._truncate(combined, mem_budget)\n            messages.append({\u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;, \u0026quot;content\u0026quot;: combined})\n\n        # 3. Conversation History\n        hist_budget = int(budget * self.budget_allocation[\u0026quot;history\u0026quot;])\n        history = self.conversation.get_messages()\n        history = self._truncate_messages(history, hist_budget)\n        messages.extend(history)\n\n        return messages\n\n    # ── Write: 记忆写入 ─────────────────────────────────────\n\n    def on_user_message(self, content: str):\n        \u0026quot;\u0026quot;\u0026quot;用户消息到来时\u0026quot;\u0026quot;\u0026quot;\n        self.conversation.add(\u0026quot;user\u0026quot;, content)\n\n    def on_assistant_message(self, content: str):\n        \u0026quot;\u0026quot;\u0026quot;Agent 回复时\u0026quot;\u0026quot;\u0026quot;\n        self.conversation.add(\u0026quot;assistant\u0026quot;, content)\n\n    def on_tool_result(self, tool_name: str, result: str):\n        \u0026quot;\u0026quot;\u0026quot;工具返回结果时\u0026quot;\u0026quot;\u0026quot;\n        self.conversation.add(\n            \u0026quot;tool\u0026quot;, f\u0026quot;[{tool_name}] {result}\u0026quot;\n        )\n\n    def on_step_complete(self, step_index: int, result: Any):\n        \u0026quot;\u0026quot;\u0026quot;单步完成时更新 Working Memory\u0026quot;\u0026quot;\u0026quot;\n        self.working.complete_step(step_index, result)\n\n    def on_task_start(self, goal: str, plan: list[str]):\n        \u0026quot;\u0026quot;\u0026quot;任务开始时初始化 Working Memory\u0026quot;\u0026quot;\u0026quot;\n        self.working.set_goal(goal)\n        for step_desc in plan:\n            self.working.add_step(step_desc)\n\n    def on_task_complete(self, task_description: str, success: bool):\n        \u0026quot;\u0026quot;\u0026quot;任务完成时归档到 Episodic Memory\u0026quot;\u0026quot;\u0026quot;\n        # 用 LLM 从 Working Memory 中提取经验\n        reflection_prompt = (\n            f\u0026quot;Reflect on this completed task.\\n\u0026quot;\n            f\u0026quot;Task: {task_description}\\n\u0026quot;\n            f\u0026quot;State:\\n{self.working.to_context_string()}\\n\\n\u0026quot;\n            f\u0026quot;Extract: approach (string), lessons (list of strings), \u0026quot;\n            f\u0026quot;importance (float 0-1). Output JSON.\u0026quot;\n        )\n        try:\n            raw = self.llm.complete(reflection_prompt, json_mode=True)\n            parsed = json.loads(raw)\n        except (json.JSONDecodeError, Exception):\n            parsed = {\n                \u0026quot;approach\u0026quot;: \u0026quot;unknown\u0026quot;,\n                \u0026quot;lessons\u0026quot;: [],\n                \u0026quot;importance\u0026quot;: 0.5,\n            }\n\n        episode = Episode(\n            episode_id=hashlib.md5(\n                f\u0026quot;{task_description}{time.time()}\u0026quot;.encode()\n            ).hexdigest(),\n            timestamp=time.time(),\n            task_description=task_description,\n            approach=parsed.get(\u0026quot;approach\u0026quot;, \u0026quot;\u0026quot;),\n            outcome=\u0026quot;success\u0026quot; if success else \u0026quot;failure\u0026quot;,\n            key_decisions=[],\n            user_feedback=None,\n            tools_used=[],\n            lessons=parsed.get(\u0026quot;lessons\u0026quot;, []),\n            importance=parsed.get(\u0026quot;importance\u0026quot;, 0.5),\n        )\n        self.episodic.store(episode)\n\n        # 清空 Working Memory\n        self.working = WorkingMemory(\n            max_iterations=self.config.get(\u0026quot;max_iterations\u0026quot;, 20)\n        )\n\n    # ── Forget: 定期维护 ────────────────────────────────────\n\n    def maintenance(self, max_age_days: int = 90, max_episodes: int = 1000):\n        \u0026quot;\u0026quot;\u0026quot;定期执行的记忆维护\u0026quot;\u0026quot;\u0026quot;\n        forgetting = MemoryForgetting(self.episodic)\n        forgetting.time_based_decay(max_age_days)\n        forgetting.capacity_based_eviction(max_episodes)\n\n    # ── 辅助方法 ─────────────────────────────────────────\n\n    def _truncate(self, text: str, max_tokens: int) -\u0026gt; str:\n        max_chars = max_tokens * 3  # 粗略估算\n        if len(text) \u0026lt;= max_chars:\n            return text\n        return text[:max_chars] + \u0026quot;\\n...[truncated]\u0026quot;\n\n    def _truncate_messages(\n        self, messages: list[dict], max_tokens: int\n    ) -\u0026gt; list[dict]:\n        total = sum(len(m[\u0026quot;content\u0026quot;]) // 3 for m in messages)\n        if total \u0026lt;= max_tokens:\n            return messages\n        # 保留最早 1 条 + 最近 N 条\n        if len(messages) \u0026gt; 6:\n            return messages[:1] + messages[-5:]\n        return messages[-5:]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e在 Agent Loop 中集成\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef agent_loop(\n    user_input: str,\n    memory: MemoryManager,\n    llm_client: LLMClient,\n    tools: dict,\n    system_prompt: str,\n    max_steps: int = 10,\n):\n    \u0026quot;\u0026quot;\u0026quot;带记忆管理的 Agent 主循环\u0026quot;\u0026quot;\u0026quot;\n\n    # 记录用户输入\n    memory.on_user_message(user_input)\n\n    for step in range(max_steps):\n        # ── Read: 从记忆中组装上下文 ──\n        messages = memory.build_context(user_input, system_prompt)\n\n        # ── Think: 调用 LLM ──\n        response = llm_client.chat(messages, tools=tools)\n\n        # ── 判断是否需要调用工具 ──\n        if response.tool_calls:\n            for tool_call in response.tool_calls:\n                tool_name = tool_call.function.name\n                tool_args = json.loads(tool_call.function.arguments)\n\n                # ── Act: 执行工具 ──\n                result = tools[tool_name](**tool_args)\n\n                # ── Write: 记录工具结果 ──\n                memory.on_tool_result(tool_name, str(result))\n                memory.on_step_complete(step, result)\n        else:\n            # 没有工具调用，Agent 给出了最终回答\n            final_answer = response.content\n            memory.on_assistant_message(final_answer)\n\n            # 任务完成，归档到 Episodic Memory\n            memory.on_task_complete(user_input, success=True)\n\n            return final_answer\n\n    # 超过最大步数\n    memory.on_task_complete(user_input, success=False)\n    return \u0026quot;Task exceeded maximum steps.\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e7. Context Window 管理策略\u003c/h2\u003e\n\u003cp\u003eContext Window 是 Agent 记忆系统中最关键的瓶颈。所有层级的记忆最终都要\u0026quot;挤进\u0026quot;这个有限的空间。\u003c/p\u003e\n\u003ch3\u003eToken 预算分配\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e┌─────────────────────────────────────────────────────────┐\n│                Context Window (128K tokens)               │\n│                                                          │\n│  ┌──────────────────┐  ← System Prompt: ~20%             │\n│  │  角色定义、指令集    │    稳定不变，每次都一样              │\n│  │  输出格式要求       │                                   │\n│  ├──────────────────┤  ← Memory Injection: ~30%          │\n│  │  Working Memory   │    动态变化，按相关性选取             │\n│  │  Episodic Recall  │                                   │\n│  │  Semantic Recall  │                                   │\n│  ├──────────────────┤  ← Conversation History: ~30%      │\n│  │  历史消息          │    滑动窗口 + 摘要                  │\n│  │  (含摘要)         │                                    │\n│  ├──────────────────┤  ← Tool Schemas + Reserve: ~20%    │\n│  │  工具定义          │    为 response 预留空间              │\n│  │  Response 空间     │                                   │\n│  └──────────────────┘                                    │\n└─────────────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这个比例不是固定的。关键在于\u003cstrong\u003e动态调整\u003c/strong\u003e：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass TokenBudgetAllocator:\n    \u0026quot;\u0026quot;\u0026quot;根据任务特征动态分配 token 预算\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, total_budget: int = 16000):\n        self.total = total_budget\n\n    def allocate(\n        self,\n        task_complexity: str = \u0026quot;medium\u0026quot;,\n        has_knowledge_need: bool = False,\n        conversation_length: int = 0,\n    ) -\u0026gt; dict[str, int]:\n        \u0026quot;\u0026quot;\u0026quot;\n        根据任务特征动态调整各部分预算。\n\n        - 简单任务：减少 memory，增加 history（对话上下文更重要）\n        - 复杂任务：增加 memory，减少 history（需要更多参考信息）\n        - 知识密集：增加 semantic memory 的比重\n        \u0026quot;\u0026quot;\u0026quot;\n        if task_complexity == \u0026quot;simple\u0026quot;:\n            allocation = {\n                \u0026quot;system_prompt\u0026quot;: 0.15,\n                \u0026quot;memory\u0026quot;: 0.15,\n                \u0026quot;history\u0026quot;: 0.45,\n                \u0026quot;reserve\u0026quot;: 0.25,\n            }\n        elif task_complexity == \u0026quot;complex\u0026quot;:\n            allocation = {\n                \u0026quot;system_prompt\u0026quot;: 0.15,\n                \u0026quot;memory\u0026quot;: 0.40,\n                \u0026quot;history\u0026quot;: 0.25,\n                \u0026quot;reserve\u0026quot;: 0.20,\n            }\n        else:  # medium\n            allocation = {\n                \u0026quot;system_prompt\u0026quot;: 0.20,\n                \u0026quot;memory\u0026quot;: 0.30,\n                \u0026quot;history\u0026quot;: 0.30,\n                \u0026quot;reserve\u0026quot;: 0.20,\n            }\n\n        # 如果需要知识检索，从 history 匀一些给 memory\n        if has_knowledge_need:\n            allocation[\u0026quot;memory\u0026quot;] += 0.10\n            allocation[\u0026quot;history\u0026quot;] -= 0.10\n\n        # 对话很长时，给 history 更多空间\n        if conversation_length \u0026gt; 30:\n            allocation[\u0026quot;history\u0026quot;] += 0.05\n            allocation[\u0026quot;reserve\u0026quot;] -= 0.05\n\n        return {\n            k: int(v * self.total) for k, v in allocation.items()\n        }\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e消息压缩策略\u003c/h3\u003e\n\u003cp\u003e当对话历史超出预算时，需要压缩。两种主要方案：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e方案 A：LLM 摘要压缩\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef llm_summarize(messages: list[dict], llm_client) -\u0026gt; str:\n    \u0026quot;\u0026quot;\u0026quot;用 LLM 压缩对话历史\u0026quot;\u0026quot;\u0026quot;\n    conversation_text = \u0026quot;\\n\u0026quot;.join(\n        f\u0026quot;{m[\u0026#39;role\u0026#39;]}: {m[\u0026#39;content\u0026#39;][:500]}\u0026quot; for m in messages\n    )\n    prompt = (\n        \u0026quot;Summarize this conversation, preserving:\\n\u0026quot;\n        \u0026quot;1. Key decisions and their rationale\\n\u0026quot;\n        \u0026quot;2. Important facts and data points\\n\u0026quot;\n        \u0026quot;3. User preferences and corrections\\n\u0026quot;\n        \u0026quot;4. Current task status\\n\\n\u0026quot;\n        \u0026quot;Be concise but complete. Do not lose critical information.\\n\\n\u0026quot;\n        f\u0026quot;Conversation:\\n{conversation_text}\u0026quot;\n    )\n    return llm_client.complete(prompt)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e方案 B：规则压缩（零成本）\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef rule_based_compress(messages: list[dict]) -\u0026gt; list[dict]:\n    \u0026quot;\u0026quot;\u0026quot;基于规则的消息压缩，不需要额外 LLM 调用\u0026quot;\u0026quot;\u0026quot;\n    compressed = []\n    for msg in messages:\n        content = msg[\u0026quot;content\u0026quot;]\n\n        # 规则 1: 截断超长的工具输出\n        if msg[\u0026quot;role\u0026quot;] == \u0026quot;tool\u0026quot; and len(content) \u0026gt; 500:\n            content = content[:500] + \u0026quot;\\n...[output truncated]\u0026quot;\n\n        # 规则 2: 移除纯确认消息（\u0026quot;好的\u0026quot;、\u0026quot;明白了\u0026quot;）\n        if msg[\u0026quot;role\u0026quot;] == \u0026quot;assistant\u0026quot; and len(content) \u0026lt; 20:\n            continue\n\n        # 规则 3: 移除重复的错误消息\n        if \u0026quot;error\u0026quot; in content.lower() and any(\n            content == m[\u0026quot;content\u0026quot;] for m in compressed\n        ):\n            continue\n\n        compressed.append({\u0026quot;role\u0026quot;: msg[\u0026quot;role\u0026quot;], \u0026quot;content\u0026quot;: content})\n    return compressed\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e方案对比\u003c/strong\u003e：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e维度\u003c/th\u003e\n\u003cth\u003eLLM 摘要\u003c/th\u003e\n\u003cth\u003e规则压缩\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e压缩质量\u003c/td\u003e\n\u003ctd\u003e高，能理解语义\u003c/td\u003e\n\u003ctd\u003e中，可能丢失隐含信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e额外成本\u003c/td\u003e\n\u003ctd\u003e需要一次 LLM 调用\u003c/td\u003e\n\u003ctd\u003e零\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e延迟\u003c/td\u003e\n\u003ctd\u003e增加 1-3 秒\u003c/td\u003e\n\u003ctd\u003e毫秒级\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e适用场景\u003c/td\u003e\n\u003ctd\u003e长对话、复杂任务\u003c/td\u003e\n\u003ctd\u003e短对话、实时场景\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e实践建议\u003c/strong\u003e：先用规则压缩兜底（保证不超 budget），当压缩比 \u0026gt; 50% 时再触发 LLM 摘要。两种方案可以组合使用：先规则裁剪，再 LLM 摘要。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e8. Trade-off 分析\u003c/h2\u003e\n\u003cp\u003e记忆系统的设计充满权衡。没有银弹，只有适合你场景的平衡点。\u003c/p\u003e\n\u003ch3\u003eTrade-off 1: 记忆丰富度 vs 成本\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e                    记忆注入量\n                        │\n     Token 成本 ────────┤──────────── 上下文质量\n     (线性增长)          │              (边际递减)\n                        │\n        $$$  ──────── ┐ │ ┌ ──────── 很好\n                      │ │ │\n         $$  ──────── ┤ │ ├ ──────── 好\n                      │ │ │\n          $  ──────── ┤ │ ├ ──────── 一般\n                      │ │ │\n          0  ──────── ┘ │ └ ──────── 差\n                        │\n                        └─── 最佳区间通常在中间偏左\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e记忆越多\u003c/strong\u003e → 上下文越丰富 → 回答质量越高 → \u003cstrong\u003e但 Token 成本线性增长，延迟线性增长\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e记忆太少\u003c/strong\u003e → Agent \u0026quot;健忘\u0026quot; → 重复劳动、答非所问 → \u003cstrong\u003e用户体验差\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e实践经验\u003c/strong\u003e：对于大多数 Agent，将 memory injection 控制在 Context Window 的 25-35% 是比较好的区间。超过 40% 时，边际收益急剧下降，但成本继续线性增长。\u003c/p\u003e\n\u003ch3\u003eTrade-off 2: 检索精度 vs 检索召回\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e  精确检索 (top_k=1)                模糊检索 (top_k=10)\n  ┌──────────────┐                 ┌──────────────┐\n  │ 命中：很准      │                 │ 命中：可能包含     │\n  │ 遗漏：可能大    │                 │ 遗漏：很少        │\n  │ Token 消耗：少  │                 │ Token 消耗：多    │\n  │ 噪声：几乎没有  │                 │ 噪声：可能较多     │\n  └──────────────┘                 └──────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e不同记忆层的最佳 top_k：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eEpisodic Memory\u003c/strong\u003e：\u003ccode\u003etop_k=3\u003c/code\u003e（过去经验不需要太多，2-3 条最相关的就够）\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSemantic Memory\u003c/strong\u003e：\u003ccode\u003etop_k=5\u003c/code\u003e（知识检索需要更全面，特别是当问题模糊时）\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eTrade-off 3: 实时性 vs 一致性\u003c/h3\u003e\n\u003cp\u003e写入记忆的时机也有权衡：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e写入时机\u003c/th\u003e\n\u003cth\u003e优点\u003c/th\u003e\n\u003cth\u003e缺点\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e同步写入\u003c/strong\u003e（每步结束立即写）\u003c/td\u003e\n\u003ctd\u003e记忆总是最新的\u003c/td\u003e\n\u003ctd\u003e增加每步延迟\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e异步写入\u003c/strong\u003e（后台批量写）\u003c/td\u003e\n\u003ctd\u003e不影响主循环延迟\u003c/td\u003e\n\u003ctd\u003e可能丢失最近的记忆\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e任务结束后写入\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e只写入\u0026quot;完整\u0026quot;的经验\u003c/td\u003e\n\u003ctd\u003e任务中途中断会丢失\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e建议\u003c/strong\u003e：Working Memory 同步更新（它在 Agent Loop 的关键路径上），Episodic Memory 任务结束后异步写入（不在关键路径上）。\u003c/p\u003e\n\u003ch3\u003eTrade-off 4: 通用记忆 vs 专用记忆\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e设计方向\u003c/th\u003e\n\u003cth\u003e场景\u003c/th\u003e\n\u003cth\u003e优点\u003c/th\u003e\n\u003cth\u003e缺点\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e通用记忆系统\u003c/td\u003e\n\u003ctd\u003e平台型 Agent\u003c/td\u003e\n\u003ctd\u003e一套代码支撑多场景\u003c/td\u003e\n\u003ctd\u003e每个场景都不够深入\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e专用记忆系统\u003c/td\u003e\n\u003ctd\u003e垂直领域 Agent\u003c/td\u003e\n\u003ctd\u003e为特定任务深度优化\u003c/td\u003e\n\u003ctd\u003e迁移成本高\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e建议\u003c/strong\u003e：先用通用方案（本文的四层架构），在验证了产品方向后，对核心场景做专用优化。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e9. 小结与下一步\u003c/h2\u003e\n\u003cp\u003e本文建立了 Agent 记忆的四层架构：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌─────────────────────────────────────────────────────┐\n│                  Agent Memory Stack                  │\n├─────────────┬──────────────┬────────────────────────┤\n│   Layer     │  存储        │  生命周期               │\n├─────────────┼──────────────┼────────────────────────┤\n│ L1 Conv.    │ 内存 / Redis │ 单次会话               │\n│ L2 Working  │ 内存 / Redis │ 单次任务               │\n│ L3 Episodic │ 向量数据库    │ 跨会话（天~月）         │\n│ L4 Semantic │ RAG 系统     │ 持久化（月~年）         │\n└─────────────┴──────────────┴────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e核心 takeaway：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e记忆是分层的\u003c/strong\u003e：不同信息有不同的生命周期和存储需求，不能\u0026quot;一刀切\u0026quot;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eContext Window 是硬约束\u003c/strong\u003e：所有记忆最终都要在有限的 token 预算内竞争，需要精细的预算分配\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e遗忘是特性\u003c/strong\u003e：没有遗忘机制的记忆系统最终会被噪声淹没\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e读写时机很关键\u003c/strong\u003e：什么时候写入、什么时候检索、检索多少条——这些决策直接影响 Agent 的表现\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e从简单开始\u003c/strong\u003e：先用内存 + 滑动窗口跑通，再逐步引入向量检索和 LLM 摘要\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e在四层记忆中，Layer 4 Semantic Memory 的\u0026quot;读取\u0026quot;操作——即如何从大规模知识库中高效检索相关信息——是一个足够深的话题。它涉及 Ingestion、Chunking、Embedding、Hybrid Retrieval、Reranking 等一系列工程决策。\u003c/p\u003e\n\u003cp\u003e这正是下一篇文章的主题：\u003cstrong\u003eRAG as Cognitive Memory: 检索增强生成的工程实践\u003c/strong\u003e。我们将深入 RAG 管线的每一个环节，探讨如何为 Agent 构建高质量的\u0026quot;外部大脑\u0026quot;。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e进一步思考\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eMemory Consolidation\u003c/strong\u003e：人类在睡眠中会将短期记忆\u0026quot;固化\u0026quot;为长期记忆。Agent 能否也有类似的机制——在空闲时对 Episodic Memory 做去重、聚合、抽象化？\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eShared Memory\u003c/strong\u003e：多 Agent 协作场景下，如何设计共享记忆？一个 Agent 的发现如何高效传递给另一个 Agent？\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMemory as Skill\u003c/strong\u003e：能否让 Agent 从记忆中\u0026quot;学会\u0026quot;新技能，而非仅仅\u0026quot;记住\u0026quot;过去的经验？比如从 10 次类似任务的记录中归纳出一个通用策略。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePrivacy-Aware Memory\u003c/strong\u003e：用户说\u0026quot;忘记我刚才说的\u0026quot;，记忆系统能否真正做到选择性遗忘？在向量数据库中，删除一条记录是否真的消除了它对其他向量的影响？\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMemory Hallucination\u003c/strong\u003e：当 Episodic Memory 中存储了不准确的信息（比如一次错误的推理结论），它会不会在后续检索中\u0026quot;污染\u0026quot;Agent 的决策？如何设计记忆的\u0026quot;自校正\u0026quot;机制？\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e系列导航\u003c/strong\u003e：本文是 Agentic 系列的第 08 篇。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e上一篇：\u003ca href=\"/blog/engineering/agentic/07-Agent%20Runtime%20from%20Scratch\"\u003e07 | Agent Runtime from Scratch\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下一篇：\u003ca href=\"/blog/engineering/agentic/09-RAG%20as%20Cognitive%20Memory\"\u003e09 | RAG as Cognitive Memory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e完整目录：\u003ca href=\"/blog/engineering/agentic/01-From%20LLM%20to%20Agent\"\u003e01 | From LLM to Agent\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"19:Td28a,"])</script><script>self.__next_f.push([1,"\u003ch1\u003ePrompt Engineering for Agents: 面向 Agent 的提示词工程\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAgentic 系列第 06 篇。前文我们讨论了 Tool Calling 的设计哲学与工程实践，LLM 已经具备了\u0026quot;使用工具\u0026quot;的能力。但工具只是 Agent 的四肢，Prompt 才是 Agent 的大脑皮层——它定义了 Agent 如何感知、如何推理、如何决策、如何行动。\u003c/p\u003e\n\u003cp\u003e本文的核心观点：\u003cstrong\u003eAgent 的 Prompt 不是\u0026quot;聊天提示词\u0026quot;，而是\u0026quot;系统接口规范\u0026quot;。\u003c/strong\u003e Chatbot 的 Prompt 追求对话自然，Agent 的 Prompt 追求行为可控。这两者的设计哲学截然不同。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch2\u003e1. 从\u0026quot;对话技巧\u0026quot;到\u0026quot;接口规范\u0026quot;\u003c/h2\u003e\n\u003cp\u003e大多数人对 Prompt Engineering 的印象停留在\u0026quot;写好提示词让 AI 回答更好\u0026quot;的阶段。这在 Chatbot 场景下基本成立——你调整措辞、给几个例子、加一句\u0026quot;请一步一步思考\u0026quot;，模型输出的质量就会改善。\u003c/p\u003e\n\u003cp\u003e但 Agent 场景完全不同。\u003c/p\u003e\n\u003cp\u003eAgent 的 Prompt 不是写给\u0026quot;一个聊天助手\u0026quot;的，而是写给\u0026quot;一个程序运行时\u0026quot;的。它的目的不是让输出\u0026quot;看起来更好\u0026quot;，而是让输出\u003cstrong\u003e可解析、可路由、可执行\u003c/strong\u003e。一个 Agent Prompt 的失败，不是\u0026quot;回答不够好\u0026quot;，而是\u003cstrong\u003e系统崩溃\u003c/strong\u003e——JSON 解析失败、工具调用参数错误、无限循环、状态机卡死。\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e维度\u003c/th\u003e\n\u003cth\u003eChatbot Prompt\u003c/th\u003e\n\u003cth\u003eAgent Prompt\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e目标\u003c/td\u003e\n\u003ctd\u003e自然、有帮助的回复\u003c/td\u003e\n\u003ctd\u003e可解析、可执行的结构化输出\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e消费者\u003c/td\u003e\n\u003ctd\u003e人类用户\u003c/td\u003e\n\u003ctd\u003e程序代码（Parser / Router / Executor）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e失败模式\u003c/td\u003e\n\u003ctd\u003e回答质量下降\u003c/td\u003e\n\u003ctd\u003e系统崩溃、无限循环、安全漏洞\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e格式要求\u003c/td\u003e\n\u003ctd\u003e宽松，Markdown 即可\u003c/td\u003e\n\u003ctd\u003e严格，JSON / XML / 特定 Schema\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e可测试性\u003c/td\u003e\n\u003ctd\u003e主观评估\u003c/td\u003e\n\u003ctd\u003e可自动化断言\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e版本管理\u003c/td\u003e\n\u003ctd\u003e通常不管理\u003c/td\u003e\n\u003ctd\u003e必须版本控制，等同于代码\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e这意味着，\u003cstrong\u003eAgent 的 Prompt Engineering 本质上是一种接口设计（Interface Design）\u003c/strong\u003e，而不是文案写作。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e2. Agent Prompt 的分层架构\u003c/h2\u003e\n\u003cp\u003e一个成熟的 Agent 系统，发送给 LLM 的 Prompt 不是一坨字符串，而是多个层次动态组装的结果。\u003c/p\u003e\n\u003ch3\u003e2.1 四层结构\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e┌─────────────────────────────────────────────────────┐\n│                   Final Prompt                       │\n│  ┌───────────────────────────────────────────────┐  │\n│  │  Layer 1: System Prompt (静态)                 │  │\n│  │  - 身份定义（你是谁，你的职责是什么）            │  │\n│  │  - 行为约束（必须做什么，禁止做什么）            │  │\n│  │  - 输出格式规范（JSON Schema / XML 模板）       │  │\n│  ├───────────────────────────────────────────────┤  │\n│  │  Layer 2: Context Injection (动态)             │  │\n│  │  - 可用工具列表及其描述                         │  │\n│  │  - 历史对话摘要 / 关键事实                      │  │\n│  │  - 当前系统状态（已完成步骤、中间结果）           │  │\n│  │  - 检索到的外部知识（RAG 结果）                  │  │\n│  ├───────────────────────────────────────────────┤  │\n│  │  Layer 3: User Input (外部)                    │  │\n│  │  - 用户的原始请求                               │  │\n│  │  - 或上一步 Agent 的输出（在 Multi-Agent 中）    │  │\n│  ├───────────────────────────────────────────────┤  │\n│  │  Layer 4: Constraints \u0026amp; Guardrails (静态+动态)  │  │\n│  │  - 安全边界（禁止调用的工具、禁止访问的数据）     │  │\n│  │  - 输出限制（最大步骤数、Token 预算）            │  │\n│  │  - 当前 Turn 的特殊指令                         │  │\n│  └───────────────────────────────────────────────┘  │\n└─────────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2.2 组装过程\u003c/h3\u003e\n\u003cp\u003ePrompt 组装不是简单的字符串拼接，而是一个有优先级、有裁剪策略的构建过程：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e                  Token Budget: 8000\n                       │\n      ┌────────────────┼────────────────┐\n      │                │                │\n      ▼                ▼                ▼\n System Prompt    Context Injection   User Input\n (固定预算:2000)  (弹性预算:4500)    (固定预算:1500)\n      │                │                │\n      │          ┌─────┴─────┐          │\n      │          │           │          │\n      │      Tool Descs   History       │\n      │      (1500 max)  (3000 max)     │\n      │          │           │          │\n      │          │     [若超预算]        │\n      │          │     → 压缩/截断      │\n      │          │           │          │\n      ▼          ▼           ▼          ▼\n     ┌──────────────────────────────────┐\n     │      Prompt Assembler            │\n     │  1. 拼装各层                      │\n     │  2. 计算总 Token                  │\n     │  3. 若超预算 → 压缩 Context 层    │\n     │  4. 注入 Constraints              │\n     └──────────────────────────────────┘\n                    │\n                    ▼\n              Final Prompt\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e关键设计决策：\u003cstrong\u003eSystem Prompt 和 User Input 的预算是固定的，Context Injection 的预算是弹性的。\u003c/strong\u003e 当总 Token 超出预算时，优先压缩 Context 层（截断历史、精简工具描述），而非删减 System Prompt 中的行为约束。因为行为约束一旦丢失，Agent 的行为就不可控了。\u003c/p\u003e\n\u003ch3\u003e2.3 Python 示例：Prompt 组装器\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom dataclasses import dataclass, field\n\n@dataclass\nclass PromptLayer:\n    content: str\n    priority: int        # 越高越不容易被裁剪\n    max_tokens: int\n    compressible: bool   # 是否允许被压缩\n\n@dataclass\nclass PromptAssembler:\n    total_budget: int = 8000\n    layers: list[PromptLayer] = field(default_factory=list)\n\n    def add_layer(self, layer: PromptLayer):\n        self.layers.append(layer)\n\n    def assemble(self) -\u0026gt; str:\n        # 按优先级排序：高优先级最后处理（最不容易被裁剪）\n        sorted_layers = sorted(self.layers, key=lambda l: l.priority)\n\n        total_used = sum(estimate_tokens(l.content) for l in self.layers)\n\n        if total_used \u0026gt; self.total_budget:\n            overflow = total_used - self.total_budget\n            # 从低优先级开始压缩\n            for layer in sorted_layers:\n                if not layer.compressible:\n                    continue\n                available_cut = estimate_tokens(layer.content) - 100  # 至少保留 100 token\n                cut = min(overflow, available_cut)\n                layer.content = truncate_to_tokens(layer.content,\n                                                    estimate_tokens(layer.content) - cut)\n                overflow -= cut\n                if overflow \u0026lt;= 0:\n                    break\n\n        # 按原始顺序拼装\n        return \u0026quot;\\n\\n\u0026quot;.join(l.content for l in self.layers)\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e3. 四种关键 Agent Prompt 设计模式\u003c/h2\u003e\n\u003cp\u003eAgent 系统中，不同角色的 Agent 需要不同风格的 Prompt。以下是四种最核心的设计模式，每种都给出完整可用的 Prompt 示例。\u003c/p\u003e\n\u003ch3\u003e3.1 Router Prompt：意图路由\u003c/h3\u003e\n\u003cp\u003eRouter 的职责是根据用户输入\u003cstrong\u003e选择正确的工具或子流程\u003c/strong\u003e，而不是自己去执行任务。它是 Agent 系统的\u0026quot;交通警察\u0026quot;。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eROUTER_PROMPT = \u0026quot;\u0026quot;\u0026quot;You are a request router. Your ONLY job is to analyze the user\u0026#39;s\nrequest and select the appropriate tool. Do NOT attempt to answer the question yourself.\n\n## Available Tools\n{tool_descriptions}\n\n## Routing Rules\n1. If the request involves real-time data (weather, stock prices, news) → use `web_search`\n2. If the request involves the user\u0026#39;s own data (files, emails, calendar) → use `data_query`\n3. If the request involves code generation or debugging → use `code_assistant`\n4. If the request involves image generation or editing → use `image_tool`\n5. If the request is ambiguous, ask a clarifying question instead of guessing.\n6. If NO tool matches, respond with tool_name: \u0026quot;none\u0026quot; and explain why.\n\n## Output Format (strict JSON, no markdown fence)\n{{\n  \u0026quot;reasoning\u0026quot;: \u0026quot;\u0026lt;one sentence explaining your routing decision\u0026gt;\u0026quot;,\n  \u0026quot;tool_name\u0026quot;: \u0026quot;\u0026lt;exact tool name from the list above, or \u0026#39;none\u0026#39;\u0026gt;\u0026quot;,\n  \u0026quot;tool_input\u0026quot;: {{\u0026lt;parameters to pass to the selected tool\u0026gt;}},\n  \u0026quot;confidence\u0026quot;: \u0026lt;float between 0.0 and 1.0\u0026gt;\n}}\n\n## Critical Constraints\n- NEVER fabricate a tool name not in the list.\n- NEVER return free-form text. ALWAYS return valid JSON.\n- If confidence \u0026lt; 0.6, set tool_name to \u0026quot;none\u0026quot; and ask for clarification.\n\u0026quot;\u0026quot;\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e设计要点：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e明确告诉 LLM \u0026quot;你不负责回答问题\u0026quot;，避免它自作主张直接回答\u003c/li\u003e\n\u003cli\u003e提供确定性的路由规则（if-then），减少 LLM 的自由裁量空间\u003c/li\u003e\n\u003cli\u003e要求输出 confidence 分数，让调用方可以做二次判断\u003c/li\u003e\n\u003cli\u003e兜底规则：没有匹配的工具时，显式输出 \u0026quot;none\u0026quot;\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e3.2 Planner Prompt：任务规划\u003c/h3\u003e\n\u003cp\u003ePlanner 的职责是将一个复杂请求\u003cstrong\u003e分解为可执行的子任务列表\u003c/strong\u003e。它是 Agent 的\u0026quot;项目经理\u0026quot;。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003ePLANNER_PROMPT = \u0026quot;\u0026quot;\u0026quot;You are a task planner. Given a complex user request, decompose it\ninto a sequence of concrete, executable sub-tasks.\n\n## Planning Principles\n1. Each sub-task must be independently executable by a single tool call.\n2. Sub-tasks should be ordered by dependency — a task can only depend on tasks before it.\n3. Minimize the number of steps. Do NOT over-decompose simple requests.\n4. If a request can be done in ONE tool call, return a plan with ONE step.\n\n## Available Tools\n{tool_descriptions}\n\n## Output Format (strict JSON)\n{{\n  \u0026quot;analysis\u0026quot;: \u0026quot;\u0026lt;brief analysis of the request\u0026#39;s complexity and required resources\u0026gt;\u0026quot;,\n  \u0026quot;plan\u0026quot;: [\n    {{\n      \u0026quot;step_id\u0026quot;: 1,\n      \u0026quot;description\u0026quot;: \u0026quot;\u0026lt;what this step does\u0026gt;\u0026quot;,\n      \u0026quot;tool_name\u0026quot;: \u0026quot;\u0026lt;tool to use\u0026gt;\u0026quot;,\n      \u0026quot;tool_input\u0026quot;: {{\u0026lt;parameters\u0026gt;}},\n      \u0026quot;depends_on\u0026quot;: []\n    }},\n    {{\n      \u0026quot;step_id\u0026quot;: 2,\n      \u0026quot;description\u0026quot;: \u0026quot;\u0026lt;what this step does\u0026gt;\u0026quot;,\n      \u0026quot;tool_name\u0026quot;: \u0026quot;\u0026lt;tool to use\u0026gt;\u0026quot;,\n      \u0026quot;tool_input\u0026quot;: {{\u0026lt;parameters, can reference $step_1_result\u0026gt;}},\n      \u0026quot;depends_on\u0026quot;: [1]\n    }}\n  ],\n  \u0026quot;estimated_steps\u0026quot;: \u0026lt;int\u0026gt;,\n  \u0026quot;can_parallelize\u0026quot;: [\u0026lt;list of step_id groups that can run concurrently\u0026gt;]\n}}\n\n## Constraints\n- Maximum 8 steps. If the task seems to need more, simplify or ask the user to narrow scope.\n- NEVER include steps like \u0026quot;verify result\u0026quot; or \u0026quot;report to user\u0026quot; — those are handled by the system.\n- Use $step_N_result to reference the output of a previous step.\n\u0026quot;\u0026quot;\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e设计要点：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u0026quot;最小化步骤数\u0026quot;原则防止 LLM 过度分解（这是规划器最常见的问题）\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edepends_on\u003c/code\u003e 字段使得执行引擎可以识别并行机会\u003c/li\u003e\n\u003cli\u003e明确设置步骤上限（8 步），避免 LLM 生成无休止的计划\u003c/li\u003e\n\u003cli\u003e禁止 LLM 添加\u0026quot;元步骤\u0026quot;（验证、汇报），这些是系统层的职责\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e3.3 Executor Prompt：执行操作\u003c/h3\u003e\n\u003cp\u003eExecutor 的职责是\u003cstrong\u003e执行单个具体操作\u003c/strong\u003e，并以严格的格式返回结果。它是 Agent 的\u0026quot;操作工\u0026quot;。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eEXECUTOR_PROMPT = \u0026quot;\u0026quot;\u0026quot;You are a task executor. You will receive a specific sub-task and\nmust execute it using the provided tool.\n\n## Current Task\n{task_description}\n\n## Tool to Use\nName: {tool_name}\nSchema: {tool_schema}\n\n## Context from Previous Steps\n{previous_results}\n\n## Execution Rules\n1. Call the tool EXACTLY ONCE with the correct parameters.\n2. Do NOT deviate from the task description.\n3. Do NOT call tools not specified for this task.\n4. If the tool call fails, report the error — do NOT retry or improvise.\n\n## Output Format (strict JSON)\n{{\n  \u0026quot;tool_call\u0026quot;: {{\n    \u0026quot;name\u0026quot;: \u0026quot;{tool_name}\u0026quot;,\n    \u0026quot;arguments\u0026quot;: {{\u0026lt;filled parameters\u0026gt;}}\n  }},\n  \u0026quot;explanation\u0026quot;: \u0026quot;\u0026lt;one sentence on why these parameters were chosen\u0026gt;\u0026quot;\n}}\n\u0026quot;\u0026quot;\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e设计要点：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExecutor 的设计哲学是\u0026quot;最小权限\u0026quot;——只做被告知的事\u003c/li\u003e\n\u003cli\u003e严禁 Executor 自主决策，发现错误只能上报，不能自行重试\u003c/li\u003e\n\u003cli\u003e这种设计让 Executor 成为一个确定性单元，便于测试和审计\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e3.4 Reflector Prompt：结果反思\u003c/h3\u003e\n\u003cp\u003eReflector 的职责是\u003cstrong\u003e评估执行结果\u003c/strong\u003e，判断是否达成目标，如果未达成则提出修正方案。它是 Agent 的\u0026quot;质量检查员\u0026quot;。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eREFLECTOR_PROMPT = \u0026quot;\u0026quot;\u0026quot;You are a result evaluator. Given the original user request and the\nexecution result, determine whether the task has been completed successfully.\n\n## Original Request\n{user_request}\n\n## Execution Plan\n{plan}\n\n## Execution Results\n{results}\n\n## Evaluation Criteria\n1. Completeness: Does the result fully address the user\u0026#39;s request?\n2. Correctness: Is the result factually and logically correct?\n3. Format: Is the result in the expected format?\n\n## Output Format (strict JSON)\n{{\n  \u0026quot;evaluation\u0026quot;: {{\n    \u0026quot;completeness\u0026quot;: {{\u0026quot;score\u0026quot;: \u0026lt;1-5\u0026gt;, \u0026quot;reason\u0026quot;: \u0026quot;\u0026lt;explanation\u0026gt;\u0026quot;}},\n    \u0026quot;correctness\u0026quot;: {{\u0026quot;score\u0026quot;: \u0026lt;1-5\u0026gt;, \u0026quot;reason\u0026quot;: \u0026quot;\u0026lt;explanation\u0026gt;\u0026quot;}},\n    \u0026quot;format\u0026quot;: {{\u0026quot;score\u0026quot;: \u0026lt;1-5\u0026gt;, \u0026quot;reason\u0026quot;: \u0026quot;\u0026lt;explanation\u0026gt;\u0026quot;}}\n  }},\n  \u0026quot;overall_pass\u0026quot;: \u0026lt;true|false\u0026gt;,\n  \u0026quot;action\u0026quot;: \u0026quot;\u0026lt;one of: \u0026#39;accept\u0026#39;, \u0026#39;retry_step\u0026#39;, \u0026#39;replan\u0026#39;, \u0026#39;escalate\u0026#39;\u0026gt;\u0026quot;,\n  \u0026quot;retry_details\u0026quot;: {{\n    \u0026quot;step_id\u0026quot;: \u0026lt;which step to retry, if applicable\u0026gt;,\n    \u0026quot;modification\u0026quot;: \u0026quot;\u0026lt;what to change in the retry\u0026gt;\u0026quot;\n  }}\n}}\n\n## Decision Rules\n- If all scores \u0026gt;= 4: action = \u0026quot;accept\u0026quot;\n- If any score \u0026lt;= 2 and retry_count \u0026lt; 3: action = \u0026quot;retry_step\u0026quot; or \u0026quot;replan\u0026quot;\n- If retry_count \u0026gt;= 3: action = \u0026quot;escalate\u0026quot; (ask user for help)\n- NEVER accept a result with correctness score \u0026lt;= 2.\n\u0026quot;\u0026quot;\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e设计要点：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e多维度评估（完整性、正确性、格式）而非简单的 pass/fail\u003c/li\u003e\n\u003cli\u003e明确的决策规则，减少 LLM 判断的主观性\u003c/li\u003e\n\u003cli\u003eretry_count 上限防止无限重试循环\u003c/li\u003e\n\u003cli\u003e\u0026quot;escalate\u0026quot; 作为最终兜底——承认失败比无限循环好得多\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e3.5 四种模式的协作\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eUser Request\n     │\n     ▼\n ┌────────┐     tool_name + input     ┌──────────┐\n │ Router │ ──── (简单请求直接执行) ───→│ Executor │──→ Result\n └────┬───┘                           └──────────┘\n      │ (复杂请求)                          ▲\n      ▼                                    │\n ┌─────────┐    plan[step_1..N]     ┌──────┴───┐\n │ Planner │ ─────────────────────→│ Executor  │\n └─────────┘                       │ (per step)│\n                                   └──────┬───┘\n                                          │ results\n                                          ▼\n                                   ┌───────────┐\n                                   │ Reflector  │\n                                   └─────┬─────┘\n                                         │\n                              ┌──────────┼──────────┐\n                              │          │          │\n                           accept    retry_step   replan\n                              │          │          │\n                              ▼          ▼          ▼\n                           Return    Executor    Planner\n                           to User  (重试该步)   (重新规划)\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e4. Chain-of-Thought 在 Agent 中的应用\u003c/h2\u003e\n\u003ch3\u003e4.1 标准 CoT vs Agent CoT\u003c/h3\u003e\n\u003cp\u003e标准的 Chain-of-Thought（CoT）是一种推理增强技术——\u0026quot;Let\u0026#39;s think step by step\u0026quot;。但在 Agent 中，CoT 的用途和形式有本质不同：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e维度\u003c/th\u003e\n\u003cth\u003e标准 CoT\u003c/th\u003e\n\u003cth\u003eAgent CoT\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e目的\u003c/td\u003e\n\u003ctd\u003e提高推理准确性\u003c/td\u003e\n\u003ctd\u003e让中间推理过程可审计、可路由\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e消费者\u003c/td\u003e\n\u003ctd\u003e最终输出的一部分\u003c/td\u003e\n\u003ctd\u003eAgent Runtime 的中间状态\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e格式\u003c/td\u003e\n\u003ctd\u003e自然语言\u003c/td\u003e\n\u003ctd\u003e结构化（通常嵌入 JSON 的某个字段）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e是否返回用户\u003c/td\u003e\n\u003ctd\u003e通常是\u003c/td\u003e\n\u003ctd\u003e通常不是（内部消费）\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eAgent 的 CoT 更像是一个\u003cstrong\u003e内部日志\u003c/strong\u003e，而非用户可见的推理过程。它的首要目标是让系统（而非人类）能够理解和利用中间推理。\u003c/p\u003e\n\u003ch3\u003e4.2 Scratchpad 模式\u003c/h3\u003e\n\u003cp\u003eScratchpad 模式是 Agent CoT 的典型实现——在 Prompt 中显式开辟一个\u0026quot;草稿区\u0026quot;，让 LLM 在其中进行中间推理，然后输出最终决策。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eSCRATCHPAD_PROMPT = \u0026quot;\u0026quot;\u0026quot;Analyze the user\u0026#39;s request and decide on an action.\n\n## User Request\n{user_request}\n\n## Available Tools\n{tools}\n\n## Instructions\nUse the \u0026lt;scratchpad\u0026gt; section to think through your decision. This section will NOT be\nshown to the user. Then provide your final action in the \u0026lt;action\u0026gt; section.\n\n\u0026lt;scratchpad\u0026gt;\nThink through:\n1. What is the user actually asking for?\n2. Which tools could help? What are the pros/cons of each?\n3. What information am I missing?\n4. What\u0026#39;s the simplest approach that works?\n\u0026lt;/scratchpad\u0026gt;\n\n\u0026lt;action\u0026gt;\nReturn strict JSON here:\n{{\u0026quot;tool_name\u0026quot;: \u0026quot;...\u0026quot;, \u0026quot;tool_input\u0026quot;: {{...}}, \u0026quot;reasoning_summary\u0026quot;: \u0026quot;...\u0026quot;}}\n\u0026lt;/action\u0026gt;\n\u0026quot;\u0026quot;\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRuntime 解析时，只提取 \u003ccode\u003e\u0026lt;action\u0026gt;\u003c/code\u003e 标签中的内容作为执行指令，\u003ccode\u003e\u0026lt;scratchpad\u0026gt;\u003c/code\u003e 的内容记录到日志中用于调试和审计。\u003c/p\u003e\n\u003ch3\u003e4.3 显式推理 vs 隐式推理的 Trade-off\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e显式推理（Explicit Reasoning）：\u003c/strong\u003e 在 Prompt 中要求 LLM 输出推理过程。\u003c/p\u003e\n\u003cp\u003e优势：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e可审计，出了问题能追溯\u0026quot;为什么做了这个决策\u0026quot;\u003c/li\u003e\n\u003cli\u003e推理质量通常更高（CoT 效应）\u003c/li\u003e\n\u003cli\u003e便于调试\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e劣势：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e消耗更多 Token（推理内容可能占输出的 50%+）\u003c/li\u003e\n\u003cli\u003e增加延迟\u003c/li\u003e\n\u003cli\u003e推理内容可能包含敏感的内部逻辑\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e隐式推理（Implicit Reasoning）：\u003c/strong\u003e 直接要求 LLM 输出最终决策，不要求中间过程。\u003c/p\u003e\n\u003cp\u003e优势：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eToken 用量更低，延迟更短\u003c/li\u003e\n\u003cli\u003e输出更简洁，解析更简单\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e劣势：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e黑盒，无法理解决策过程\u003c/li\u003e\n\u003cli\u003e在复杂场景下准确率下降明显\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e工程决策建议：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRouter 和 Executor（简单、确定性高）：倾向隐式推理，追求速度\u003c/li\u003e\n\u003cli\u003ePlanner 和 Reflector（复杂、需要判断）：必须显式推理，追求准确性和可审计性\u003c/li\u003e\n\u003cli\u003e在系统稳定后，可以通过 A/B 测试逐步将显式推理切换为隐式推理以降低成本\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e5. Few-shot vs Zero-shot 在 Agent 场景的选择\u003c/h2\u003e\n\u003cp\u003e这是 Agent Prompt 设计中一个重要但常被忽视的决策点。\u003c/p\u003e\n\u003ch3\u003e5.1 决策矩阵\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e                    输出结构化程度\n                 低 ◄──────────► 高\n                 │                │\n  任务复杂度  高  │  Few-shot      │  Zero-shot + Schema\n                 │  (复杂规划)     │  (结构化反思)\n                 │                │\n              低  │  Zero-shot     │  Zero-shot + Schema\n                 │  (简单对话)     │  (工具调用)\n                 │                │\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e5.2 工具调用：Zero-shot 优先\u003c/h3\u003e\n\u003cp\u003e工具调用场景天然适合 Zero-shot。原因是 \u003cstrong\u003eJSON Schema 本身就是最好的\u0026quot;示例\u0026quot;\u003c/strong\u003e——它精确定义了每个参数的名称、类型、描述和约束，比任何 Few-shot 示例都更完整。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 工具调用不需要 few-shot，Schema 就是最好的约束\ntool_schema = {\n    \u0026quot;name\u0026quot;: \u0026quot;search_database\u0026quot;,\n    \u0026quot;description\u0026quot;: \u0026quot;Search the product database with filters\u0026quot;,\n    \u0026quot;parameters\u0026quot;: {\n        \u0026quot;type\u0026quot;: \u0026quot;object\u0026quot;,\n        \u0026quot;properties\u0026quot;: {\n            \u0026quot;query\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;Search keywords\u0026quot;},\n            \u0026quot;category\u0026quot;: {\n                \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;,\n                \u0026quot;enum\u0026quot;: [\u0026quot;electronics\u0026quot;, \u0026quot;clothing\u0026quot;, \u0026quot;books\u0026quot;],\n                \u0026quot;description\u0026quot;: \u0026quot;Product category filter\u0026quot;\n            },\n            \u0026quot;max_results\u0026quot;: {\n                \u0026quot;type\u0026quot;: \u0026quot;integer\u0026quot;,\n                \u0026quot;default\u0026quot;: 10,\n                \u0026quot;minimum\u0026quot;: 1,\n                \u0026quot;maximum\u0026quot;: 100\n            }\n        },\n        \u0026quot;required\u0026quot;: [\u0026quot;query\u0026quot;]\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e加 Few-shot 反而可能引入问题：LLM 可能过度拟合示例中的具体值，而不是理解 Schema 的通用约束。\u003c/p\u003e\n\u003ch3\u003e5.3 复杂规划：Few-shot 有价值\u003c/h3\u003e\n\u003cp\u003e规划场景是 Few-shot 真正发挥价值的地方。因为\u0026quot;好的计划\u0026quot;是一个模糊的概念——仅凭输出格式定义不足以引导 LLM 产出高质量的计划。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003ePLANNER_WITH_EXAMPLES = \u0026quot;\u0026quot;\u0026quot;You are a task planner.\n\n## Example 1: Multi-step data analysis\nUser: \u0026quot;Compare last month\u0026#39;s sales with the same period last year and visualize the trend\u0026quot;\nPlan:\n[\n  {{\u0026quot;step_id\u0026quot;: 1, \u0026quot;tool\u0026quot;: \u0026quot;data_query\u0026quot;, \u0026quot;input\u0026quot;: \u0026quot;sales data for 2025-07\u0026quot;, \u0026quot;depends_on\u0026quot;: []}},\n  {{\u0026quot;step_id\u0026quot;: 2, \u0026quot;tool\u0026quot;: \u0026quot;data_query\u0026quot;, \u0026quot;input\u0026quot;: \u0026quot;sales data for 2024-07\u0026quot;, \u0026quot;depends_on\u0026quot;: []}},\n  {{\u0026quot;step_id\u0026quot;: 3, \u0026quot;tool\u0026quot;: \u0026quot;data_compare\u0026quot;, \u0026quot;input\u0026quot;: \u0026quot;$step_1_result, $step_2_result\u0026quot;, \u0026quot;depends_on\u0026quot;: [1, 2]}},\n  {{\u0026quot;step_id\u0026quot;: 4, \u0026quot;tool\u0026quot;: \u0026quot;chart_gen\u0026quot;, \u0026quot;input\u0026quot;: \u0026quot;$step_3_result, type=line\u0026quot;, \u0026quot;depends_on\u0026quot;: [3]}}\n]\nNote: Steps 1 and 2 can run in parallel since they have no dependencies.\n\n## Example 2: Simple single-step task\nUser: \u0026quot;What\u0026#39;s the weather in Tokyo?\u0026quot;\nPlan:\n[\n  {{\u0026quot;step_id\u0026quot;: 1, \u0026quot;tool\u0026quot;: \u0026quot;weather_api\u0026quot;, \u0026quot;input\u0026quot;: \u0026quot;Tokyo\u0026quot;, \u0026quot;depends_on\u0026quot;: []}}\n]\nNote: Simple requests should NOT be over-decomposed.\n\n## Now plan for:\nUser: \u0026quot;{user_request}\u0026quot;\n\u0026quot;\u0026quot;\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFew-shot 示例在这里传递了两个关键信息：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e粒度标准\u003c/strong\u003e——什么程度的分解是合适的\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e并行意识\u003c/strong\u003e——独立步骤应该标记为可并行\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e5.4 反思评估：Zero-shot + 结构化输出\u003c/h3\u003e\n\u003cp\u003e反思（Reflection）场景适合 Zero-shot + 结构化输出。原因是反思本质上是\u0026quot;评判\u0026quot;，而评判标准已经通过评估维度（completeness / correctness / format）和评分规则完整定义了。给出 Few-shot 示例反而可能让 LLM 锚定在示例的评分上，而不是独立评估当前结果。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e总结决策原则：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e格式约束充分（JSON Schema / 评分规则）→ Zero-shot\u003c/li\u003e\n\u003cli\u003e需要传递\u0026quot;风格\u0026quot;或\u0026quot;粒度标准\u0026quot; → Few-shot\u003c/li\u003e\n\u003cli\u003e两者都可以时 → 优先 Zero-shot（更省 Token，更不容易过拟合）\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e6. Prompt 工程化实践\u003c/h2\u003e\n\u003cp\u003e当 Agent 系统超过原型阶段，Prompt 管理就变成了一个严肃的工程问题。\u003c/p\u003e\n\u003ch3\u003e6.1 Prompt 模板化\u003c/h3\u003e\n\u003cp\u003e核心思想：\u003cstrong\u003e分离静态结构和动态内容\u003c/strong\u003e。静态部分（身份定义、行为规则、输出格式）是模板，动态部分（工具列表、历史消息、当前状态）通过变量注入。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom typing import Any\nfrom string import Template\nimport hashlib\nimport json\nfrom datetime import datetime\n\n\nclass PromptTemplate:\n    \u0026quot;\u0026quot;\u0026quot;可管理、可版本化、可测试的 Prompt 模板\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, name: str, template: str, version: str,\n                 required_vars: list[str], metadata: dict | None = None):\n        self.name = name\n        self.template = template\n        self.version = version\n        self.required_vars = required_vars\n        self.metadata = metadata or {}\n        self._hash = hashlib.sha256(template.encode()).hexdigest()[:12]\n\n    def render(self, **kwargs) -\u0026gt; str:\n        # 校验所有必需变量都已提供\n        missing = set(self.required_vars) - set(kwargs.keys())\n        if missing:\n            raise ValueError(f\u0026quot;Missing required variables: {missing}\u0026quot;)\n\n        # 渲染模板\n        rendered = self.template\n        for key, value in kwargs.items():\n            placeholder = \u0026quot;{\u0026quot; + key + \u0026quot;}\u0026quot;\n            if isinstance(value, (dict, list)):\n                value = json.dumps(value, indent=2, ensure_ascii=False)\n            rendered = rendered.replace(placeholder, str(value))\n\n        return rendered\n\n    def fingerprint(self) -\u0026gt; str:\n        \u0026quot;\u0026quot;\u0026quot;返回模板内容的哈希指纹，用于版本追踪\u0026quot;\u0026quot;\u0026quot;\n        return f\u0026quot;{self.name}@{self.version}#{self._hash}\u0026quot;\n\n\nclass PromptRegistry:\n    \u0026quot;\u0026quot;\u0026quot;Prompt 模板注册中心：集中管理所有 Agent 使用的 Prompt\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self):\n        self._templates: dict[str, dict[str, PromptTemplate]] = {}  # name -\u0026gt; {version -\u0026gt; template}\n\n    def register(self, template: PromptTemplate):\n        if template.name not in self._templates:\n            self._templates[template.name] = {}\n        self._templates[template.name][template.version] = template\n\n    def get(self, name: str, version: str = \u0026quot;latest\u0026quot;) -\u0026gt; PromptTemplate:\n        if name not in self._templates:\n            raise KeyError(f\u0026quot;Template \u0026#39;{name}\u0026#39; not found\u0026quot;)\n\n        versions = self._templates[name]\n        if version == \u0026quot;latest\u0026quot;:\n            latest_version = sorted(versions.keys())[-1]\n            return versions[latest_version]\n\n        if version not in versions:\n            raise KeyError(f\u0026quot;Version \u0026#39;{version}\u0026#39; not found for template \u0026#39;{name}\u0026#39;\u0026quot;)\n        return versions[version]\n\n    def list_all(self) -\u0026gt; dict[str, list[str]]:\n        return {name: sorted(vers.keys()) for name, vers in self._templates.items()}\n\n\n# ── 使用示例 ──\n\nregistry = PromptRegistry()\n\n# 注册 Router Prompt v1\nregistry.register(PromptTemplate(\n    name=\u0026quot;router\u0026quot;,\n    version=\u0026quot;1.0\u0026quot;,\n    template=\u0026quot;\u0026quot;\u0026quot;You are a request router.\nAvailable tools: {tool_descriptions}\nRoute the following request: {user_input}\nOutput JSON: {{\u0026quot;tool_name\u0026quot;: \u0026quot;...\u0026quot;, \u0026quot;tool_input\u0026quot;: {{...}}}}\u0026quot;\u0026quot;\u0026quot;,\n    required_vars=[\u0026quot;tool_descriptions\u0026quot;, \u0026quot;user_input\u0026quot;],\n    metadata={\u0026quot;author\u0026quot;: \u0026quot;agent-team\u0026quot;, \u0026quot;last_tested\u0026quot;: \u0026quot;2025-08-10\u0026quot;}\n))\n\n# 注册 Router Prompt v2（增加了 confidence 字段）\nregistry.register(PromptTemplate(\n    name=\u0026quot;router\u0026quot;,\n    version=\u0026quot;2.0\u0026quot;,\n    template=\u0026quot;\u0026quot;\u0026quot;You are a request router. Your ONLY job is to route, not to answer.\nAvailable tools: {tool_descriptions}\nRoute the following request: {user_input}\nOutput JSON: {{\u0026quot;tool_name\u0026quot;: \u0026quot;...\u0026quot;, \u0026quot;tool_input\u0026quot;: {{...}}, \u0026quot;confidence\u0026quot;: \u0026lt;0.0-1.0\u0026gt;}}\u0026quot;\u0026quot;\u0026quot;,\n    required_vars=[\u0026quot;tool_descriptions\u0026quot;, \u0026quot;user_input\u0026quot;],\n    metadata={\u0026quot;author\u0026quot;: \u0026quot;agent-team\u0026quot;, \u0026quot;last_tested\u0026quot;: \u0026quot;2025-08-13\u0026quot;}\n))\n\n# 获取并渲染\nrouter_prompt = registry.get(\u0026quot;router\u0026quot;, version=\u0026quot;2.0\u0026quot;)\nfinal_prompt = router_prompt.render(\n    tool_descriptions=\u0026quot;1. web_search: Search the web\\n2. calculator: Do math\u0026quot;,\n    user_input=\u0026quot;What is 42 * 17?\u0026quot;\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e6.2 Prompt 版本控制\u003c/h3\u003e\n\u003cp\u003e为什么 Prompt 需要版本控制？因为 \u003cstrong\u003ePrompt 是 Agent 行为的源代码\u003c/strong\u003e。改一个词可能导致 Agent 行为的巨大变化——从正确路由变成错误路由，从结构化输出变成自由文本。\u003c/p\u003e\n\u003cp\u003e版本控制策略：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eprompts/\n├── router/\n│   ├── v1.0.txt          # 初始版本\n│   ├── v1.1.txt          # 修复：低 confidence 时的行为\n│   ├── v2.0.txt          # 重大变更：新增 confidence 字段\n│   └── changelog.md      # 变更记录\n├── planner/\n│   ├── v1.0.txt\n│   └── v1.1.txt\n├── executor/\n│   └── v1.0.txt\n└── reflector/\n    └── v1.0.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e关键实践：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e每次 Prompt 变更都有对应的测试结果\u003c/strong\u003e（下面会详述）\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e生产环境使用固定版本号\u003c/strong\u003e，而非 \u0026quot;latest\u0026quot;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e支持灰度发布\u003c/strong\u003e：新版 Prompt 可以先对 10% 的流量生效\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e保留回滚能力\u003c/strong\u003e：发现新版 Prompt 导致问题时，立即切回旧版\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e6.3 Prompt 测试\u003c/h3\u003e\n\u003cp\u003ePrompt 测试的核心挑战是 LLM 输出的非确定性。我们不能像测试普通函数那样做精确断言，但可以做\u003cstrong\u003e结构化断言\u003c/strong\u003e和\u003cstrong\u003e统计性断言\u003c/strong\u003e。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom dataclasses import dataclass\n\n@dataclass\nclass PromptTestCase:\n    name: str\n    input_vars: dict[str, Any]       # 模板变量\n    assertions: list[dict]            # 断言列表\n\n    # 断言类型：\n    # {\u0026quot;type\u0026quot;: \u0026quot;json_valid\u0026quot;}                           → 输出是合法 JSON\n    # {\u0026quot;type\u0026quot;: \u0026quot;has_field\u0026quot;, \u0026quot;field\u0026quot;: \u0026quot;tool_name\u0026quot;}      → JSON 中包含指定字段\n    # {\u0026quot;type\u0026quot;: \u0026quot;field_in\u0026quot;, \u0026quot;field\u0026quot;: \u0026quot;tool_name\u0026quot;, \u0026quot;values\u0026quot;: [\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;]} → 字段值在范围内\n    # {\u0026quot;type\u0026quot;: \u0026quot;no_field\u0026quot;, \u0026quot;field\u0026quot;: \u0026quot;apology\u0026quot;}         → 不包含某字段（防止 LLM 废话）\n    # {\u0026quot;type\u0026quot;: \u0026quot;max_tokens\u0026quot;, \u0026quot;limit\u0026quot;: 200}             → 输出长度不超过限制\n\n\nclass PromptTestRunner:\n    def __init__(self, llm_client, template: PromptTemplate):\n        self.llm = llm_client\n        self.template = template\n\n    def run_test(self, test_case: PromptTestCase, n_runs: int = 5) -\u0026gt; dict:\n        \u0026quot;\u0026quot;\u0026quot;对同一个测试用例运行 N 次，统计通过率\u0026quot;\u0026quot;\u0026quot;\n        prompt = self.template.render(**test_case.input_vars)\n        results = []\n\n        for _ in range(n_runs):\n            output = self.llm.generate(prompt)\n            pass_all = True\n            details = []\n\n            for assertion in test_case.assertions:\n                passed = self._check_assertion(output, assertion)\n                details.append({\u0026quot;assertion\u0026quot;: assertion, \u0026quot;passed\u0026quot;: passed})\n                if not passed:\n                    pass_all = False\n\n            results.append({\u0026quot;output\u0026quot;: output, \u0026quot;passed\u0026quot;: pass_all, \u0026quot;details\u0026quot;: details})\n\n        pass_rate = sum(1 for r in results if r[\u0026quot;passed\u0026quot;]) / n_runs\n        return {\n            \u0026quot;test_case\u0026quot;: test_case.name,\n            \u0026quot;template\u0026quot;: self.template.fingerprint(),\n            \u0026quot;n_runs\u0026quot;: n_runs,\n            \u0026quot;pass_rate\u0026quot;: pass_rate,\n            \u0026quot;results\u0026quot;: results\n        }\n\n    def _check_assertion(self, output: str, assertion: dict) -\u0026gt; bool:\n        if assertion[\u0026quot;type\u0026quot;] == \u0026quot;json_valid\u0026quot;:\n            try:\n                json.loads(output)\n                return True\n            except json.JSONDecodeError:\n                return False\n\n        if assertion[\u0026quot;type\u0026quot;] == \u0026quot;has_field\u0026quot;:\n            try:\n                data = json.loads(output)\n                return assertion[\u0026quot;field\u0026quot;] in data\n            except (json.JSONDecodeError, TypeError):\n                return False\n\n        if assertion[\u0026quot;type\u0026quot;] == \u0026quot;field_in\u0026quot;:\n            try:\n                data = json.loads(output)\n                return data.get(assertion[\u0026quot;field\u0026quot;]) in assertion[\u0026quot;values\u0026quot;]\n            except (json.JSONDecodeError, TypeError):\n                return False\n\n        return False  # 未知断言类型\n\n\n# ── 测试用例示例 ──\n\ntest_cases = [\n    PromptTestCase(\n        name=\u0026quot;math_request_should_route_to_calculator\u0026quot;,\n        input_vars={\n            \u0026quot;tool_descriptions\u0026quot;: \u0026quot;1. web_search: Search the web\\n2. calculator: Do math\u0026quot;,\n            \u0026quot;user_input\u0026quot;: \u0026quot;What is 1024 * 768?\u0026quot;\n        },\n        assertions=[\n            {\u0026quot;type\u0026quot;: \u0026quot;json_valid\u0026quot;},\n            {\u0026quot;type\u0026quot;: \u0026quot;has_field\u0026quot;, \u0026quot;field\u0026quot;: \u0026quot;tool_name\u0026quot;},\n            {\u0026quot;type\u0026quot;: \u0026quot;field_in\u0026quot;, \u0026quot;field\u0026quot;: \u0026quot;tool_name\u0026quot;, \u0026quot;values\u0026quot;: [\u0026quot;calculator\u0026quot;]},\n        ]\n    ),\n    PromptTestCase(\n        name=\u0026quot;ambiguous_request_should_not_guess\u0026quot;,\n        input_vars={\n            \u0026quot;tool_descriptions\u0026quot;: \u0026quot;1. web_search: Search the web\\n2. calculator: Do math\u0026quot;,\n            \u0026quot;user_input\u0026quot;: \u0026quot;Help me with my project\u0026quot;\n        },\n        assertions=[\n            {\u0026quot;type\u0026quot;: \u0026quot;json_valid\u0026quot;},\n            {\u0026quot;type\u0026quot;: \u0026quot;has_field\u0026quot;, \u0026quot;field\u0026quot;: \u0026quot;tool_name\u0026quot;},\n            {\u0026quot;type\u0026quot;: \u0026quot;field_in\u0026quot;, \u0026quot;field\u0026quot;: \u0026quot;tool_name\u0026quot;, \u0026quot;values\u0026quot;: [\u0026quot;none\u0026quot;]},\n        ]\n    ),\n]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e测试策略建议：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e每个 Prompt 版本至少 10 个测试用例，覆盖正常路径、边界情况和对抗输入\u003c/li\u003e\n\u003cli\u003e每个测试用例运行 5-10 次，要求通过率 \u0026gt;= 90%（而非 100%，因为 LLM 输出非确定性）\u003c/li\u003e\n\u003cli\u003e将测试集纳入 CI，每次 Prompt 变更触发回归测试\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e6.4 Prompt 组合：模块化拼装\u003c/h3\u003e\n\u003cp\u003e复杂 Agent 的 Prompt 往往由多个模块组合而成。与其维护一个巨大的单体 Prompt，不如将其拆分为可复用的模块：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass PromptComposer:\n    \u0026quot;\u0026quot;\u0026quot;将多个 Prompt 模块按顺序组合\u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self):\n        self._modules: list[tuple[str, PromptTemplate]] = []\n\n    def add(self, section_name: str, template: PromptTemplate):\n        self._modules.append((section_name, template))\n        return self  # 支持链式调用\n\n    def compose(self, **all_vars) -\u0026gt; str:\n        sections = []\n        for section_name, template in self._modules:\n            # 每个模块只取自己需要的变量\n            relevant_vars = {k: v for k, v in all_vars.items()\n                           if k in template.required_vars}\n            rendered = template.render(**relevant_vars)\n            sections.append(f\u0026quot;## {section_name}\\n{rendered}\u0026quot;)\n        return \u0026quot;\\n\\n\u0026quot;.join(sections)\n\n\n# 使用方式\nidentity_module = PromptTemplate(\n    name=\u0026quot;identity\u0026quot;, version=\u0026quot;1.0\u0026quot;,\n    template=\u0026quot;You are {agent_role}. {agent_description}\u0026quot;,\n    required_vars=[\u0026quot;agent_role\u0026quot;, \u0026quot;agent_description\u0026quot;]\n)\n\ntools_module = PromptTemplate(\n    name=\u0026quot;tools\u0026quot;, version=\u0026quot;1.0\u0026quot;,\n    template=\u0026quot;Available tools:\\n{tool_descriptions}\u0026quot;,\n    required_vars=[\u0026quot;tool_descriptions\u0026quot;]\n)\n\noutput_format_module = PromptTemplate(\n    name=\u0026quot;output_format\u0026quot;, version=\u0026quot;1.0\u0026quot;,\n    template=\u0026quot;You MUST respond in the following JSON format:\\n{json_schema}\u0026quot;,\n    required_vars=[\u0026quot;json_schema\u0026quot;]\n)\n\nconstraints_module = PromptTemplate(\n    name=\u0026quot;constraints\u0026quot;, version=\u0026quot;1.0\u0026quot;,\n    template=\u0026quot;Constraints:\\n{constraint_list}\u0026quot;,\n    required_vars=[\u0026quot;constraint_list\u0026quot;]\n)\n\n# 组合\ncomposer = PromptComposer()\ncomposer.add(\u0026quot;Identity\u0026quot;, identity_module) \\\n        .add(\u0026quot;Tools\u0026quot;, tools_module) \\\n        .add(\u0026quot;Output Format\u0026quot;, output_format_module) \\\n        .add(\u0026quot;Constraints\u0026quot;, constraints_module)\n\nfinal_prompt = composer.compose(\n    agent_role=\u0026quot;a task router\u0026quot;,\n    agent_description=\u0026quot;You route user requests to the appropriate tool.\u0026quot;,\n    tool_descriptions=\u0026quot;1. search: web search\\n2. calc: calculator\u0026quot;,\n    json_schema=\u0026#39;{\u0026quot;tool_name\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;tool_input\u0026quot;: \u0026quot;object\u0026quot;}\u0026#39;,\n    constraint_list=\u0026quot;- Never fabricate tool names\\n- Always return valid JSON\u0026quot;\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e模块化的好处：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e同一个 \u003ccode\u003eoutput_format_module\u003c/code\u003e 可以被 Router、Planner、Executor 共享\u003c/li\u003e\n\u003cli\u003e修改 constraints 不需要触碰 identity 和 tools 部分\u003c/li\u003e\n\u003cli\u003e每个模块可以独立测试和版本控制\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e7. Context Window 管理\u003c/h2\u003e\n\u003cp\u003eAgent 的 Context Window 管理是一个独特且关键的工程挑战。与 Chatbot 的\u0026quot;对话越长体验越差\u0026quot;不同，Agent 的 context 膨胀会直接导致\u003cstrong\u003e系统性故障\u003c/strong\u003e。\u003c/p\u003e\n\u003ch3\u003e7.1 Agent 的 Context 膨胀问题\u003c/h3\u003e\n\u003cp\u003eAgent 的 context 会从三个维度快速膨胀：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eTurn 1:  System(2000) + User(200) + Response(500)              = 2,700 tokens\nTurn 2:  + Tool_Result(3000) + Response(800)                   = 6,500 tokens\nTurn 3:  + Tool_Result(5000) + Error_Msg(1000) + Response(600) = 13,100 tokens\nTurn 4:  + Tool_Result(2000) + Response(400)                   = 15,500 tokens\nTurn 5:  + RAG_Context(4000) + Response(1000)                  = 20,500 tokens\n  ...\nTurn 10: 很容易突破 50,000 tokens\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e三大膨胀源：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e工具返回值\u003c/strong\u003e：一次数据库查询可能返回几千 token 的 JSON，一次网页抓取可能返回上万 token\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e历史消息积累\u003c/strong\u003e：每一轮的 user message + assistant response + tool calls 都在累积\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e错误信息\u003c/strong\u003e：工具调用失败的 traceback、重试过程中的冗余信息\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e7.2 消息压缩策略\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e策略 1：摘要压缩（Summarization）\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e将早期的对话历史压缩为摘要，只保留关键事实和决策结果。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef compress_history(messages: list[dict], llm_client,\n                     keep_recent: int = 4) -\u0026gt; list[dict]:\n    \u0026quot;\u0026quot;\u0026quot;将早期历史压缩为摘要，保留最近 N 轮完整消息\u0026quot;\u0026quot;\u0026quot;\n    if len(messages) \u0026lt;= keep_recent:\n        return messages\n\n    old_messages = messages[:-keep_recent]\n    recent_messages = messages[-keep_recent:]\n\n    # 用 LLM 生成摘要\n    summary_prompt = f\u0026quot;\u0026quot;\u0026quot;Summarize the following conversation history into key facts\nand decisions. Keep only information that might be needed for future steps.\nBe concise — maximum 200 words.\n\n{format_messages(old_messages)}\u0026quot;\u0026quot;\u0026quot;\n\n    summary = llm_client.generate(summary_prompt)\n\n    # 将摘要作为一条 system message 注入\n    summary_message = {\n        \u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;,\n        \u0026quot;content\u0026quot;: f\u0026quot;[Conversation Summary]\\n{summary}\u0026quot;\n    }\n\n    return [summary_message] + recent_messages\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e策略 2：滑动窗口（Sliding Window）\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e更简单粗暴——只保留最近 N 条消息，丢弃更早的消息。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef sliding_window(messages: list[dict], max_messages: int = 10) -\u0026gt; list[dict]:\n    \u0026quot;\u0026quot;\u0026quot;保留 system message + 最近 N 条消息\u0026quot;\u0026quot;\u0026quot;\n    system_msgs = [m for m in messages if m[\u0026quot;role\u0026quot;] == \u0026quot;system\u0026quot;]\n    non_system = [m for m in messages if m[\u0026quot;role\u0026quot;] != \u0026quot;system\u0026quot;]\n    return system_msgs + non_system[-max_messages:]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e策略 3：选择性保留（Selective Retention）\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e根据消息的\u0026quot;重要性\u0026quot;决定保留还是丢弃。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef selective_retain(messages: list[dict], token_budget: int) -\u0026gt; list[dict]:\n    \u0026quot;\u0026quot;\u0026quot;按重要性保留消息，直到填满 token 预算\u0026quot;\u0026quot;\u0026quot;\n\n    def importance_score(msg: dict) -\u0026gt; int:\n        if msg[\u0026quot;role\u0026quot;] == \u0026quot;system\u0026quot;:\n            return 100  # 永远保留\n        if msg.get(\u0026quot;is_final_result\u0026quot;):\n            return 90   # 最终结果必须保留\n        if msg[\u0026quot;role\u0026quot;] == \u0026quot;user\u0026quot;:\n            return 80   # 用户输入高优先\n        if msg.get(\u0026quot;tool_error\u0026quot;):\n            return 20   # 错误信息低优先（已经被处理过了）\n        if msg.get(\u0026quot;tool_result\u0026quot;):\n            return 40   # 工具结果中等优先\n        return 50\n\n    scored = [(importance_score(m), i, m) for i, m in enumerate(messages)]\n    scored.sort(key=lambda x: (-x[0], x[1]))  # 按重要性降序，原始顺序升序\n\n    retained = []\n    used_tokens = 0\n    for score, idx, msg in scored:\n        msg_tokens = estimate_tokens(msg[\u0026quot;content\u0026quot;])\n        if used_tokens + msg_tokens \u0026lt;= token_budget:\n            retained.append((idx, msg))\n            used_tokens += msg_tokens\n\n    # 恢复原始顺序\n    retained.sort(key=lambda x: x[0])\n    return [msg for _, msg in retained]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e7.3 Token 预算分配\u003c/h3\u003e\n\u003cp\u003e一个经验性的 Token 预算分配方案（以 8K context window 为例）：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eTotal Context Window: 8,192 tokens\n                    │\n    ┌───────────────┼───────────────┐\n    │               │               │\nSystem Prompt    Working Area     Reserved for\n  ~25%            ~60%            Output ~15%\n (2,048)         (4,915)          (1,229)\n    │               │\n    │         ┌─────┴──────┐\n    │         │            │\n    │    Tool Descs    History + State\n    │     ~15%          ~45%\n    │    (1,229)       (3,686)\n    │\n    ├── Identity \u0026amp; Role: 500\n    ├── Behavior Rules: 800\n    ├── Output Format: 500\n    └── Constraints: 248\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e关键原则：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eOutput Reserved 不能省\u003c/strong\u003e：如果留给输出的空间不足，LLM 会输出截断的 JSON，导致解析失败\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSystem Prompt 预算固定\u003c/strong\u003e：行为约束不能因为 context 紧张而被裁剪\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHistory 是最大的压缩空间\u003c/strong\u003e：优先在这里节省 Token\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e工具描述可以按需加载\u003c/strong\u003e：如果 Router 已经选定了工具，后续 Executor 只需要注入被选中工具的描述，而非全部工具\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e7.4 工具返回值的处理\u003c/h3\u003e\n\u003cp\u003e工具返回值是 context 膨胀的最大单点源头。以下是几种处理策略：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef process_tool_result(result: str, max_tokens: int = 1500) -\u0026gt; str:\n    \u0026quot;\u0026quot;\u0026quot;处理工具返回值，防止 context 爆炸\u0026quot;\u0026quot;\u0026quot;\n\n    result_tokens = estimate_tokens(result)\n\n    if result_tokens \u0026lt;= max_tokens:\n        return result\n\n    # 策略 1：截断（适用于文本类结果）\n    if is_text(result):\n        return truncate_to_tokens(result, max_tokens) + \u0026quot;\\n[... truncated]\u0026quot;\n\n    # 策略 2：提取摘要（适用于 JSON 类结果）\n    if is_json(result):\n        data = json.loads(result)\n        if isinstance(data, list):\n            # 只保留前 N 条记录 + 总数信息\n            summary = {\n                \u0026quot;total_count\u0026quot;: len(data),\n                \u0026quot;showing_first\u0026quot;: 5,\n                \u0026quot;records\u0026quot;: data[:5],\n                \u0026quot;note\u0026quot;: f\u0026quot;Truncated from {len(data)} records. Request specific filters for more.\u0026quot;\n            }\n            return json.dumps(summary, ensure_ascii=False, indent=2)\n\n    # 策略 3：兜底截断\n    return truncate_to_tokens(result, max_tokens) + \u0026quot;\\n[... truncated]\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e8. 常见陷阱\u003c/h2\u003e\n\u003ch3\u003e8.1 Prompt 太长导致 LLM \u0026quot;忘记\u0026quot;关键指令\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e现象：\u003c/strong\u003e System Prompt 有 3000 token，其中包含 20 条行为规则。LLM 在前几轮严格遵守，但随着 context 变长，开始\u0026quot;遗忘\u0026quot;中间的规则——尤其是第 8-15 条。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e原因：\u003c/strong\u003e LLM 对 prompt 中不同位置内容的\u0026quot;注意力\u0026quot;不均匀。开头和结尾的内容通常被更好地遵循（primacy effect 和 recency effect），中间的内容最容易被忽略。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e应对：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将最关键的规则放在 System Prompt 的开头和结尾\u003c/li\u003e\n\u003cli\u003e将规则数量控制在 7 条以内（与人类工作记忆容量一致，也利于 LLM）\u003c/li\u003e\n\u003cli\u003e在消息末尾添加 reminder：\u0026quot;Remember: always output valid JSON. Never fabricate tool names.\u0026quot;\u003c/li\u003e\n\u003cli\u003e按当前 Turn 的需要动态注入最相关的规则子集，而非每次都注入全部规则\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e8.2 工具描述和 System Prompt 冲突\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e现象：\u003c/strong\u003e System Prompt 说\u0026quot;不要执行任何数据删除操作\u0026quot;，但某个工具的 description 中包含\u0026quot;Deletes records matching the query\u0026quot;。LLM 收到删除请求时，行为不确定——有时遵循 System Prompt 的禁令，有时遵循工具描述的能力。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e原因：\u003c/strong\u003e LLM 看到的是拼装后的完整 prompt，它不理解\u0026quot;System Prompt 优先级高于工具描述\u0026quot;这个层级关系。两段相互矛盾的文本让 LLM 陷入冲突。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e应对：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e在 Prompt 组装阶段做\u003cstrong\u003e一致性检查\u003c/strong\u003e：扫描工具描述中的关键词，与 System Prompt 的禁止列表做匹配\u003c/li\u003e\n\u003cli\u003e如果某个工具被禁用，\u003cstrong\u003e直接不注入它的描述\u003c/strong\u003e，而不是注入描述然后在 System Prompt 中禁止\u003c/li\u003e\n\u003cli\u003e在 System Prompt 中明确声明优先级：\u0026quot;If any tool description conflicts with these rules, these rules take priority.\u0026quot;\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e8.3 过度约束导致 LLM 无法灵活应对\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e现象：\u003c/strong\u003e 为了保证安全，System Prompt 中加了大量限制：\u0026quot;只能调用列表中的工具\u0026quot;、\u0026quot;只能输出 JSON\u0026quot;、\u0026quot;不能包含任何解释\u0026quot;、\u0026quot;不能问用户问题\u0026quot;、\u0026quot;必须在一次调用中完成\u0026quot;......结果 LLM 在遇到无法处理的请求时，输出空 JSON 或无意义的工具调用，而不是合理地拒绝或请求澄清。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e原因：\u003c/strong\u003e 过度约束堵死了 LLM 所有的\u0026quot;逃生通道\u0026quot;。它没有被允许说\u0026quot;我不知道\u0026quot;或\u0026quot;我需要更多信息\u0026quot;，所以只能在约束框架内硬凑一个输出。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e应对：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e永远为 LLM 保留一个\u0026quot;安全出口\u0026quot;：允许它输出 \u003ccode\u003e{\u0026quot;action\u0026quot;: \u0026quot;clarify\u0026quot;, \u0026quot;question\u0026quot;: \u0026quot;...\u0026quot;}\u003c/code\u003e 或 \u003ccode\u003e{\u0026quot;action\u0026quot;: \u0026quot;refuse\u0026quot;, \u0026quot;reason\u0026quot;: \u0026quot;...\u0026quot;}\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e区分\u0026quot;硬约束\u0026quot;和\u0026quot;软约束\u0026quot;：硬约束（安全规则）不可违反，软约束（输出偏好）在特殊情况下可以放松\u003c/li\u003e\n\u003cli\u003e将约束从\u0026quot;禁止列表\u0026quot;改为\u0026quot;优先级列表\u0026quot;：先尝试 X，如果不行可以 Y，最后可以 Z\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e8.4 Prompt Injection 在 Agent 中的放大效应\u003c/h3\u003e\n\u003cp\u003e在 Chatbot 中，Prompt Injection 最多让模型输出不当内容。但在 Agent 中，Prompt Injection 可能触发\u003cstrong\u003e真实的工具调用\u003c/strong\u003e——删除数据、发送邮件、调用 API。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e应对：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e用户输入和系统指令之间必须有明确的分隔标记\u003c/li\u003e\n\u003cli\u003e工具调用前做参数校验（schema validation），而非完全信任 LLM 输出\u003c/li\u003e\n\u003cli\u003e高危操作（删除、支付、发送）增加人工确认步骤\u003c/li\u003e\n\u003cli\u003e将用户输入视为\u0026quot;不可信数据\u0026quot;，在 Prompt 中明确标注：\u003ccode\u003e[USER INPUT - UNTRUSTED]: {user_message}\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e9. 结语：从 Prompt 到 Runtime\u003c/h2\u003e\n\u003cp\u003ePrompt Engineering for Agents 的本质是\u003cstrong\u003e为 LLM 定义一套可编程的行为接口\u003c/strong\u003e。我们在本文中讨论了分层架构、设计模式、推理策略、测试方法和 context 管理——这些都是让 Agent \u0026quot;可控\u0026quot;的基础设施。\u003c/p\u003e\n\u003cp\u003e但 Prompt 本身只是 Agent 系统的一个组件。再好的 Prompt 也需要一个可靠的 Runtime 来驱动——处理 LLM 的响应、管理状态机的转换、执行工具调用、处理错误和重试。\u003c/p\u003e\n\u003cp\u003e下一篇文章《Agent Runtime from Scratch: 不依赖框架构建 Agent》将从零开始实现一个完整的 Agent 运行时。我们会把本文设计的 Prompt 模式，放进一个真实可运行的控制循环中，展示 Prompt、工具、状态管理和错误处理如何在代码层面协同工作。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e进一步思考：\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePrompt 的自动优化\u003c/strong\u003e：如果我们有了 Prompt 测试框架和评估指标，是否可以用搜索算法（DSPy 的思路）自动优化 Prompt？这和手工调优的 trade-off 在哪里？\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eMulti-Model Prompt 策略\u003c/strong\u003e：Router 用小模型（快、便宜），Planner 用大模型（准、贵），Executor 用中等模型。不同模型对 Prompt 的响应特性不同，如何为不同模型定制 Prompt？\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePrompt 的可解释性\u003c/strong\u003e：当 Agent 做出错误决策时，我们如何从 Prompt 和输出中定位问题根因？这需要什么样的 observability 基础设施？\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e动态 Prompt 生成\u003c/strong\u003e：是否可以让一个 \u0026quot;Meta-Agent\u0026quot; 根据当前任务特征，动态生成最合适的 Prompt？这会引入什么样的复杂性和风险？\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e系列导航\u003c/strong\u003e：本文是 Agentic 系列的第 06 篇。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e上一篇：\u003ca href=\"/blog/engineering/agentic/05-Tool%20Calling%20Deep%20Dive\"\u003e05 | Tool Calling Deep Dive\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下一篇：\u003ca href=\"/blog/engineering/agentic/07-Agent%20Runtime%20from%20Scratch\"\u003e07 | Agent Runtime from Scratch\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e完整目录：\u003ca href=\"/blog/engineering/agentic/01-From%20LLM%20to%20Agent\"\u003e01 | From LLM to Agent\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"1a:Tc7d3,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eThe Agent Control Loop: Agent 运行时的核心抽象\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e如果说 LLM 是 Agent 的大脑，那么 Control Loop 就是 Agent 的心跳。\u003c/p\u003e\n\u003cp\u003e大多数教程在讲 Agent 时，上来就接框架、调 API、跑 demo。但如果你不理解 Agent 运行时的核心抽象——控制循环——你永远只是在用别人的黑盒。\u003c/p\u003e\n\u003cp\u003e本文是 Agentic 系列第 04 篇，整个系列的技术基石。我们会从状态机模型出发，逐层拆解 Agent Control Loop 的每一个阶段，给出完整的 Python 实现，并深入分析实际工程中的 trade-off。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch2\u003e1. Agent 的本质：可中断的控制循环\u003c/h2\u003e\n\u003cp\u003e一个常见的误解是把 Agent 等同于\u0026quot;一次 LLM 调用\u0026quot;。实际上，Agent 和 LLM 的关系，类似于操作系统和 CPU 的关系——LLM 是执行推理的计算单元，而 Agent 是管理整个执行生命周期的运行时系统。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLLM 是一个函数：\u003c/strong\u003e \u003ccode\u003ef(prompt) -\u0026gt; completion\u003c/code\u003e，输入文本，输出文本，调用一次就结束。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAgent 是一个循环：\u003c/strong\u003e 它持续运行，在每一轮中观察环境、调用 LLM 进行推理、执行动作、评估结果，然后决定是否继续。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eLLM:    Input ──→ Output            (一次调用)\n\nAgent:  Input ──→ [Observe → Think → Act → Reflect] ──→ ... ──→ Output\n                  └──────── 循环 N 次 ────────────┘     (多轮控制)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这个循环有几个关键特性：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e可中断\u003c/strong\u003e：循环可以在任何阶段暂停，等待外部输入（用户确认、异步工具返回）后恢复\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e有状态\u003c/strong\u003e：循环维护上下文信息，每一轮的输出影响下一轮的输入\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e有终止条件\u003c/strong\u003e：循环不会无限运行，它在满足特定条件时停止\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e可观测\u003c/strong\u003e：循环的每一步都应该是可追踪、可回溯的\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e理解了这一点，Agent 编程的核心问题就变成了：\u003cstrong\u003e如何设计和实现这个控制循环？\u003c/strong\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e2. 状态机模型：形式化定义\u003c/h2\u003e\n\u003cp\u003e要严谨地描述 Control Loop，最自然的方式是用\u003cstrong\u003e有限状态机（FSM）\u003c/strong\u003e。\u003c/p\u003e\n\u003ch3\u003e2.1 状态定义\u003c/h3\u003e\n\u003cp\u003e一个 Agent Control Loop 可以用以下状态集合描述：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom enum import Enum\n\nclass AgentState(Enum):\n    OBSERVE  = \u0026quot;observe\u0026quot;   # 接收并归一化输入\n    THINK    = \u0026quot;think\u0026quot;     # LLM 推理，决定下一步行动\n    ACT      = \u0026quot;act\u0026quot;       # 执行工具调用或产出结果\n    REFLECT  = \u0026quot;reflect\u0026quot;   # 评估执行结果，决定是否继续\n    DONE     = \u0026quot;done\u0026quot;      # 终止：任务完成\n    ERROR    = \u0026quot;error\u0026quot;     # 终止：不可恢复错误\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2.2 状态转移图\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e                    ┌─────────────────────────────────────────┐\n                    │                                         │\n                    ▼                                         │\n              ┌──────────┐                                    │\n   Input ───→│ OBSERVE  │                                    │\n              └────┬─────┘                                    │\n                   │                                         │\n                   ▼                                         │\n              ┌──────────┐    need_action    ┌──────────┐    │\n              │  THINK   │ ───────────────→ │   ACT    │    │\n              └────┬─────┘                   └────┬─────┘    │\n                   │                              │          │\n                   │ has_answer                   │          │\n                   │                              ▼          │\n                   │                        ┌──────────┐     │\n                   │                        │ REFLECT  │ ────┘\n                   │                        └────┬─────┘  continue\n                   │                             │\n                   ▼                             ▼\n              ┌──────────┐                  ┌──────────┐\n              │   DONE   │                  │  ERROR   │\n              └──────────┘                  └──────────┘\n                                       (max_retries exceeded\n                                        / unrecoverable)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e状态转移规则：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e当前状态\u003c/th\u003e\n\u003cth\u003e条件\u003c/th\u003e\n\u003cth\u003e下一状态\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003eOBSERVE\u003c/td\u003e\n\u003ctd\u003e输入就绪\u003c/td\u003e\n\u003ctd\u003eTHINK\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTHINK\u003c/td\u003e\n\u003ctd\u003eLLM 返回 tool_call\u003c/td\u003e\n\u003ctd\u003eACT\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTHINK\u003c/td\u003e\n\u003ctd\u003eLLM 返回最终回答\u003c/td\u003e\n\u003ctd\u003eDONE\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTHINK\u003c/td\u003e\n\u003ctd\u003eLLM 调用异常\u003c/td\u003e\n\u003ctd\u003eERROR\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eACT\u003c/td\u003e\n\u003ctd\u003e工具执行完成\u003c/td\u003e\n\u003ctd\u003eREFLECT\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eACT\u003c/td\u003e\n\u003ctd\u003e工具执行失败\u003c/td\u003e\n\u003ctd\u003eREFLECT (带错误信息)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eREFLECT\u003c/td\u003e\n\u003ctd\u003e需要继续\u003c/td\u003e\n\u003ctd\u003eOBSERVE (将结果作为新输入)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eREFLECT\u003c/td\u003e\n\u003ctd\u003e任务完成\u003c/td\u003e\n\u003ctd\u003eDONE\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eREFLECT\u003c/td\u003e\n\u003ctd\u003e超过重试上限\u003c/td\u003e\n\u003ctd\u003eERROR\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch3\u003e2.3 与 OODA Loop 的对比\u003c/h3\u003e\n\u003cp\u003eAgent Control Loop 并不是凭空发明的，它和军事决策理论中的 \u003cstrong\u003eOODA Loop（Observe-Orient-Decide-Act）\u003c/strong\u003e 有深层的结构对应：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eOODA Loop:          Agent Control Loop:\n┌─────────┐         ┌─────────┐\n│ Observe │ ──────→ │ OBSERVE │  感知环境\n├─────────┤         ├─────────┤\n│ Orient  │ ──────→ │ THINK   │  理解上下文，形成判断\n├─────────┤         │         │\n│ Decide  │ ──────→ │         │  (LLM 在 THINK 中同时完成 Orient+Decide)\n├─────────┤         ├─────────┤\n│  Act    │ ──────→ │  ACT    │  执行行动\n└─────────┘         ├─────────┤\n                    │ REFLECT │  OODA 中没有显式的反思阶段\n                    └─────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e关键区别在于 \u003cstrong\u003eREFLECT 阶段\u003c/strong\u003e。传统 OODA Loop 假设决策者能实时感知行动效果并自然融入下一轮 Observe。但 LLM Agent 不具备这种连续感知能力——它需要一个显式的反思步骤来评估工具返回值、判断是否需要修正。这是 Agent Control Loop 相对于经典决策循环的重要改进。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e3. 循环中每个阶段的深入分析\u003c/h2\u003e\n\u003ch3\u003e3.1 OBSERVE：输入归一化\u003c/h3\u003e\n\u003cp\u003eOBSERVE 阶段的职责是\u003cstrong\u003e收集并归一化各种来源的输入\u003c/strong\u003e，将它们统一为 LLM 可理解的格式。\u003c/p\u003e\n\u003cp\u003e输入来源远不止\u0026quot;用户消息\u0026quot;一种：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e输入来源                    归一化后\n┌─────────────────┐       ┌──────────────────────┐\n│ 用户消息         │ ────→ │ {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;,     │\n│ 工具返回值       │ ────→ │  \u0026quot;content\u0026quot;: \u0026quot;...\u0026quot;}   │\n│ 系统事件         │ ────→ │                      │\n│ 定时触发         │ ────→ │ {\u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;,   │\n│ 外部 Webhook    │ ────→ │  \u0026quot;content\u0026quot;: \u0026quot;...\u0026quot;}   │\n│ 上一轮反思结果   │ ────→ │                      │\n└─────────────────┘       └──────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e输入归一化的核心原则：\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e所有输入都必须序列化为 message 格式\u003c/strong\u003e。不管来源是什么，最终都要变成 \u003ccode\u003e{\u0026quot;role\u0026quot;: ..., \u0026quot;content\u0026quot;: ...}\u003c/code\u003e 的形式，因为 LLM 只理解 message 序列。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e工具返回值需要结构化包装\u003c/strong\u003e。不要直接把原始 JSON 甩给 LLM，要附上工具名称、执行状态和必要的摘要信息。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e输入需要截断和优先级排序\u003c/strong\u003e。当多个输入同时到达时，需要决定哪些放进当前轮次的 Context Window，哪些缓存到下一轮。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef observe(self, raw_inputs: list[dict]) -\u0026gt; list[dict]:\n    \u0026quot;\u0026quot;\u0026quot;将原始输入归一化为 LLM message 格式\u0026quot;\u0026quot;\u0026quot;\n    messages = []\n    for inp in raw_inputs:\n        match inp[\u0026quot;type\u0026quot;]:\n            case \u0026quot;user_message\u0026quot;:\n                messages.append({\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: inp[\u0026quot;text\u0026quot;]})\n            case \u0026quot;tool_result\u0026quot;:\n                messages.append({\n                    \u0026quot;role\u0026quot;: \u0026quot;tool\u0026quot;,\n                    \u0026quot;tool_call_id\u0026quot;: inp[\u0026quot;call_id\u0026quot;],\n                    \u0026quot;content\u0026quot;: self._format_tool_result(inp),\n                })\n            case \u0026quot;system_event\u0026quot;:\n                messages.append({\n                    \u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;,\n                    \u0026quot;content\u0026quot;: f\u0026quot;[System Event] {inp[\u0026#39;event\u0026#39;]}\u0026quot;,\n                })\n    return messages\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e3.2 THINK：LLM 推理\u003c/h3\u003e\n\u003cp\u003eTHINK 阶段是控制循环中最核心的一环——调用 LLM，让它基于当前上下文做出决策。\u003c/p\u003e\n\u003cp\u003e这个阶段要解决三个问题：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e问题一：Context Window 构建\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLLM 的输入不是当前轮次的消息，而是\u003cstrong\u003e从任务开始到现在的完整上下文\u003c/strong\u003e。构建 Context Window 的典型结构：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌─────────────────────────────────────────────┐\n│ System Prompt                               │  固定不变\n│ (角色定义 + 能力边界 + 输出格式要求)           │\n├─────────────────────────────────────────────┤\n│ Tool Definitions                            │  固定不变\n│ (可用工具的 JSON Schema 定义)                │\n├─────────────────────────────────────────────┤\n│ Message History                             │  随轮次增长\n│ (user → assistant → tool → assistant → ...) │\n├─────────────────────────────────────────────┤\n│ Current Turn Input                          │  当前轮次\n│ (本轮 OBSERVE 阶段归一化的输入)              │\n└─────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e问题二：Token 预算控制\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eContext Window 有上限（4K / 8K / 128K / 200K），而每一轮循环都会增加 message history。如果不加控制，几轮之后就会超限。\u003c/p\u003e\n\u003cp\u003e常见的预算控制策略：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e策略\u003c/th\u003e\n\u003cth\u003e实现方式\u003c/th\u003e\n\u003cth\u003e适用场景\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e硬截断\u003c/td\u003e\n\u003ctd\u003e只保留最近 N 条消息\u003c/td\u003e\n\u003ctd\u003e简单场景\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e滑动窗口\u003c/td\u003e\n\u003ctd\u003eSystem Prompt 固定 + 最近 K 轮对话\u003c/td\u003e\n\u003ctd\u003e工具调用场景\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e摘要压缩\u003c/td\u003e\n\u003ctd\u003e将早期对话用 LLM 生成摘要后替换\u003c/td\u003e\n\u003ctd\u003e长对话场景\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e优先级保留\u003c/td\u003e\n\u003ctd\u003e按消息重要性排序，低优先级先丢弃\u003c/td\u003e\n\u003ctd\u003e复杂多步任务\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef _build_context(self, new_messages: list[dict]) -\u0026gt; list[dict]:\n    \u0026quot;\u0026quot;\u0026quot;构建符合 Token 预算的 Context Window\u0026quot;\u0026quot;\u0026quot;\n    self.message_history.extend(new_messages)\n\n    context = [self.system_prompt] + self.tool_definitions\n    remaining_budget = self.max_tokens - self._count_tokens(context)\n\n    # 从最新消息开始向前填充，直到预算耗尽\n    selected = []\n    for msg in reversed(self.message_history):\n        msg_tokens = self._count_tokens([msg])\n        if msg_tokens \u0026gt; remaining_budget:\n            break\n        selected.insert(0, msg)\n        remaining_budget -= msg_tokens\n\n    return context + selected\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e问题三：LLM 输出解析\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLLM 的返回可能是纯文本回答（任务完成），也可能是工具调用请求。需要根据返回类型决定下一步状态转移：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef think(self, context: list[dict]) -\u0026gt; ThinkResult:\n    \u0026quot;\u0026quot;\u0026quot;调用 LLM 进行推理\u0026quot;\u0026quot;\u0026quot;\n    response = self.client.chat.completions.create(\n        model=self.model,\n        messages=context,\n        tools=self.tool_schemas,\n    )\n    choice = response.choices[0]\n\n    if choice.finish_reason == \u0026quot;tool_calls\u0026quot;:\n        return ThinkResult(\n            action=\u0026quot;tool_call\u0026quot;,\n            tool_calls=choice.message.tool_calls,\n            raw_message=choice.message,\n        )\n    else:\n        return ThinkResult(\n            action=\u0026quot;answer\u0026quot;,\n            content=choice.message.content,\n            raw_message=choice.message,\n        )\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e3.3 ACT：执行层\u003c/h3\u003e\n\u003cp\u003eACT 阶段负责\u003cstrong\u003e执行 THINK 阶段决定的动作\u003c/strong\u003e——通常是调用工具（Tool Calling）。\u003c/p\u003e\n\u003cp\u003e执行层的核心挑战不是\u0026quot;调用工具\u0026quot;本身，而是以下几个工程问题：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e同步 vs 异步执行\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e同步执行（Simple）：\n  think → call_tool_1 → wait → call_tool_2 → wait → reflect\n  延迟 = T1 + T2\n\n异步 / 并行执行（Optimized）：\n  think → call_tool_1 ─┬─→ reflect\n        → call_tool_2 ─┘\n  延迟 = max(T1, T2)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e当 LLM 在一次返回中请求多个工具调用（parallel tool calling）时，应该并行执行以降低延迟：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport asyncio\n\nasync def act(self, tool_calls: list[ToolCall]) -\u0026gt; list[dict]:\n    \u0026quot;\u0026quot;\u0026quot;并行执行多个工具调用\u0026quot;\u0026quot;\u0026quot;\n    tasks = [self._execute_tool(tc) for tc in tool_calls]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    tool_results = []\n    for tc, result in zip(tool_calls, results):\n        if isinstance(result, Exception):\n            tool_results.append({\n                \u0026quot;type\u0026quot;: \u0026quot;tool_result\u0026quot;,\n                \u0026quot;call_id\u0026quot;: tc.id,\n                \u0026quot;status\u0026quot;: \u0026quot;error\u0026quot;,\n                \u0026quot;content\u0026quot;: f\u0026quot;Tool \u0026#39;{tc.function.name}\u0026#39; failed: {result}\u0026quot;,\n            })\n        else:\n            tool_results.append({\n                \u0026quot;type\u0026quot;: \u0026quot;tool_result\u0026quot;,\n                \u0026quot;call_id\u0026quot;: tc.id,\n                \u0026quot;status\u0026quot;: \u0026quot;success\u0026quot;,\n                \u0026quot;content\u0026quot;: str(result),\n            })\n    return tool_results\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e执行安全\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e工具执行不是无条件信任的。需要考虑：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e超时控制\u003c/strong\u003e：每个工具调用必须有 timeout，防止阻塞整个循环\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e结果大小限制\u003c/strong\u003e：工具返回值可能非常大（比如查数据库返回 10 万行），需要截断\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e权限校验\u003c/strong\u003e：某些工具（文件写入、网络请求、代码执行）需要额外的权限检查\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e沙箱执行\u003c/strong\u003e：代码执行类工具应该在沙箱中运行\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e3.4 REFLECT：输出质量评估\u003c/h3\u003e\n\u003cp\u003eREFLECT 阶段回答一个关键问题：\u003cstrong\u003e上一步的执行结果是否满意？是继续、重试还是停止？\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e这个阶段有两种实现方式：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e方式一：隐式反思——让 LLM 在下一轮 THINK 中自行判断\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e这是最简单的方式。把工具返回值直接送进下一轮 THINK，让 LLM 自己决定是否需要修正。大多数框架（如 OpenAI Assistants API）默认采用这种方式。\u003c/p\u003e\n\u003cp\u003e优点：实现简单，不增加额外的 LLM 调用。\u003c/p\u003e\n\u003cp\u003e缺点：LLM 可能\u0026quot;自信地\u0026quot;忽略错误，特别是在返回值看起来合理但语义错误的情况下。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e方式二：显式反思——用独立的 LLM 调用进行自我评估\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef reflect(self, action_result: dict, task_goal: str) -\u0026gt; ReflectResult:\n    \u0026quot;\u0026quot;\u0026quot;显式反思：评估执行结果\u0026quot;\u0026quot;\u0026quot;\n    prompt = f\u0026quot;\u0026quot;\u0026quot;评估以下工具执行结果是否达成了任务目标。\n\n任务目标: {task_goal}\n执行结果: {json.dumps(action_result, ensure_ascii=False)}\n\n请回答：\n1. 结果是否正确？(yes/no)\n2. 是否需要进一步行动？(yes/no)\n3. 如果需要，下一步应该做什么？\n\u0026quot;\u0026quot;\u0026quot;\n    response = self.client.chat.completions.create(\n        model=self.model,\n        messages=[{\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: prompt}],\n    )\n    # 解析反思结果...\n    return ReflectResult(\n        is_correct=...,\n        needs_more_action=...,\n        next_step_hint=...,\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eTrade-off 分析：\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e维度\u003c/th\u003e\n\u003cth\u003e隐式反思\u003c/th\u003e\n\u003cth\u003e显式反思\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003eToken 消耗\u003c/td\u003e\n\u003ctd\u003e低\u003c/td\u003e\n\u003ctd\u003e高（额外一次 LLM 调用）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e质量把控\u003c/td\u003e\n\u003ctd\u003e依赖 LLM 自觉\u003c/td\u003e\n\u003ctd\u003e有独立的质量评估\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e延迟\u003c/td\u003e\n\u003ctd\u003e低\u003c/td\u003e\n\u003ctd\u003e增加一轮 LLM 延迟\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e适用场景\u003c/td\u003e\n\u003ctd\u003e简单工具调用\u003c/td\u003e\n\u003ctd\u003e复杂推理链、高准确性要求\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e实际工程中，常用的折中方案是：\u003cstrong\u003e对关键步骤用显式反思，对常规步骤用隐式反思\u003c/strong\u003e。\u003c/p\u003e\n\u003ch3\u003e3.5 终止条件：什么时候停下来？\u003c/h3\u003e\n\u003cp\u003e一个 Agent 如果不知道什么时候停，就是一个烧钱的死循环。终止条件的设计是 Control Loop 中最容易被忽视、但对生产环境最重要的部分。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef should_stop(self, state: LoopState) -\u0026gt; tuple[bool, str]:\n    \u0026quot;\u0026quot;\u0026quot;判断是否应该终止循环\u0026quot;\u0026quot;\u0026quot;\n    # 1. LLM 认为任务完成\n    if state.last_think_result.action == \u0026quot;answer\u0026quot;:\n        return True, \u0026quot;task_completed\u0026quot;\n\n    # 2. 达到最大轮次\n    if state.turn_count \u0026gt;= self.max_turns:\n        return True, \u0026quot;max_turns_exceeded\u0026quot;\n\n    # 3. Token 预算耗尽\n    if state.total_tokens \u0026gt;= self.token_budget:\n        return True, \u0026quot;token_budget_exceeded\u0026quot;\n\n    # 4. 连续错误过多\n    if state.consecutive_errors \u0026gt;= self.max_consecutive_errors:\n        return True, \u0026quot;too_many_errors\u0026quot;\n\n    # 5. 死循环检测（重复输出相同内容）\n    if self._detect_loop(state.recent_outputs):\n        return True, \u0026quot;loop_detected\u0026quot;\n\n    return False, \u0026quot;\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e各终止条件的设计考量：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003emax_turns\u003c/strong\u003e：硬上限，防止失控。一般设 10-30 轮。过小会导致复杂任务被截断，过大会导致 Token 浪费\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003etoken_budget\u003c/strong\u003e：成本控制。根据业务场景设定每次交互的 Token 上限\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003econsecutive_errors\u003c/strong\u003e：容错阈值。工具偶尔失败是正常的，但连续 3 次以上通常意味着系统性问题\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eloop_detected\u003c/strong\u003e：死循环检测。如果 Agent 连续 N 轮输出相同或高度相似的内容，说明它陷入了无效循环\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e4. 两种主流 Loop 模式对比\u003c/h2\u003e\n\u003ch3\u003e4.1 ReAct 模式\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eReAct（Reason + Act）\u003c/strong\u003e 是目前最主流的 Agent Loop 模式，由 Yao et al. 2022 提出。其核心思想是让 LLM 交替进行推理和行动：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌──────────────────────────────────────────────────────┐\n│                   ReAct Loop                         │\n│                                                      │\n│  ┌─────────┐    ┌─────────┐    ┌─────────────────┐  │\n│  │ Thought │ →  │ Action  │ →  │  Observation    │  │\n│  │(LLM推理)│    │(工具调用)│    │(工具返回值)      │  │\n│  └─────────┘    └─────────┘    └────────┬────────┘  │\n│       ▲                                  │          │\n│       └──────────────────────────────────┘          │\n│                  循环直到完成                         │\n└──────────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e一个典型的 ReAct 执行轨迹（Trace）：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eThought: 用户想知道北京今天的天气。我需要调用天气 API。\nAction:  get_weather(city=\u0026quot;北京\u0026quot;)\nObservation: {\u0026quot;temp\u0026quot;: 28, \u0026quot;condition\u0026quot;: \u0026quot;晴\u0026quot;, \u0026quot;humidity\u0026quot;: 45}\n\nThought: 已经获取到天气数据，我可以直接回答用户。\nAnswer:  北京今天晴天，气温 28°C，湿度 45%。\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eReAct 的优势：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e每一步都基于最新的观察结果做决策，\u003cstrong\u003e适应性强\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThought 过程可见，\u003cstrong\u003e可解释性好\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e实现简单，与 Tool Calling API 天然契合\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eReAct 的劣势：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e逐步决策，无法全局优化执行顺序\u003c/li\u003e\n\u003cli\u003e每一步都需要一次 LLM 调用，\u003cstrong\u003e延迟累积\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e对于需要协调多个子任务的复杂场景，容易陷入局部最优\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e4.2 Plan-then-Execute 模式\u003c/h3\u003e\n\u003cp\u003e与 ReAct 的\u0026quot;走一步看一步\u0026quot;不同，Plan-then-Execute 先生成一个\u003cstrong\u003e完整的执行计划\u003c/strong\u003e，然后按计划依次执行：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌──────────────────────────────────────────────────────────┐\n│              Plan-then-Execute Loop                       │\n│                                                          │\n│  ┌──────────────────────────────────────┐                │\n│  │           Planning Phase             │                │\n│  │  Input → LLM → [Step1, Step2, ...]   │                │\n│  └───────────────┬──────────────────────┘                │\n│                  │                                        │\n│                  ▼                                        │\n│  ┌──────────────────────────────────────┐                │\n│  │         Execution Phase              │                │\n│  │  Step1 → Execute → Result1           │                │\n│  │  Step2 → Execute → Result2           │                │\n│  │  ...                                 │                │\n│  └───────────────┬──────────────────────┘                │\n│                  │                                        │\n│                  ▼                                        │\n│  ┌──────────────────────────────────────┐                │\n│  │    Replan (if needed)                │                │\n│  │  检查是否需要调整计划                   │                │\n│  └──────────────────────────────────────┘                │\n└──────────────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e执行轨迹示例：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ePlan:\n  1. 查询北京天气\n  2. 查询上海天气\n  3. 对比两地天气差异\n  4. 生成出行建议\n\nExecute Step 1: get_weather(city=\u0026quot;北京\u0026quot;) → {\u0026quot;temp\u0026quot;: 28, \u0026quot;condition\u0026quot;: \u0026quot;晴\u0026quot;}\nExecute Step 2: get_weather(city=\u0026quot;上海\u0026quot;) → {\u0026quot;temp\u0026quot;: 32, \u0026quot;condition\u0026quot;: \u0026quot;多云\u0026quot;}\nExecute Step 3: (LLM 对比分析)\nExecute Step 4: (LLM 生成建议)\n\nAnswer: ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e4.3 Trade-off 分析\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e                        灵活性\n                          ▲\n                          │\n                 ReAct ●  │\n                          │\n                          │        ● Hybrid\n                          │          (ReAct + Plan)\n                          │\n              Plan-then   │\n              -Execute ●  │\n                          │\n                          └──────────────────→ 效率\n                                          (LLM 调用次数)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e维度\u003c/th\u003e\n\u003cth\u003eReAct\u003c/th\u003e\n\u003cth\u003ePlan-then-Execute\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e灵活性\u003c/td\u003e\n\u003ctd\u003e高。每步实时调整\u003c/td\u003e\n\u003ctd\u003e低。偏离计划时需要 Replan\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLLM 调用次数\u003c/td\u003e\n\u003ctd\u003e多（每步一次推理）\u003c/td\u003e\n\u003ctd\u003e少（规划一次 + 执行时可能不需要 LLM）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e可控性\u003c/td\u003e\n\u003ctd\u003e低。难以预测执行路径\u003c/td\u003e\n\u003ctd\u003e高。计划可审核、可修改\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e适合场景\u003c/td\u003e\n\u003ctd\u003e工具调用为主、步骤不确定\u003c/td\u003e\n\u003ctd\u003e多步骤、有依赖关系、需要全局协调\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e错误恢复\u003c/td\u003e\n\u003ctd\u003e自然。下一步可以直接修正\u003c/td\u003e\n\u003ctd\u003e需要 Replan 机制\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e人类干预\u003c/td\u003e\n\u003ctd\u003e难以在中途插入\u003c/td\u003e\n\u003ctd\u003e容易。可以审核和修改计划\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e实际工程建议：\u003c/strong\u003e 大多数场景从 ReAct 开始。当你发现 Agent 频繁在多步任务中\u0026quot;迷路\u0026quot;或做出低效的工具调用序列时，再考虑引入 Plan-then-Execute 或混合模式。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e5. 状态管理\u003c/h2\u003e\n\u003cp\u003eControl Loop 的状态管理决定了 Agent 的\u003cstrong\u003e持久性\u003c/strong\u003e和\u003cstrong\u003e可恢复性\u003c/strong\u003e。\u003c/p\u003e\n\u003ch3\u003e5.1 Stateless Agent\u003c/h3\u003e\n\u003cp\u003eStateless Agent 不维护执行状态，所有上下文通过 \u003cstrong\u003emessage history\u003c/strong\u003e 传递。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eRequest 1:  [system, user_msg_1]                     → response_1\nRequest 2:  [system, user_msg_1, response_1, user_2] → response_2\nRequest 3:  [system, user_msg_1, response_1, user_2, response_2, user_3] → response_3\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e特点：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e实现最简单，无需持久化\u003c/li\u003e\n\u003cli\u003e每次请求都是自包含的\u003c/li\u003e\n\u003cli\u003emessage history 不断膨胀，最终超过 Context Window\u003c/li\u003e\n\u003cli\u003e不支持暂停/恢复\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这是大多数 \u0026quot;chat completion\u0026quot; 应用的工作方式。适合单轮或短对话场景。\u003c/p\u003e\n\u003ch3\u003e5.2 Stateful Agent\u003c/h3\u003e\n\u003cp\u003eStateful Agent 维护一个独立的 \u003cstrong\u003eexecution state\u003c/strong\u003e，它不仅包含 message history，还包含任务进度、中间结果、工具状态等信息。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e@dataclass\nclass ExecutionState:\n    \u0026quot;\u0026quot;\u0026quot;Agent 执行状态\u0026quot;\u0026quot;\u0026quot;\n    session_id: str\n    status: AgentState\n    turn_count: int\n    message_history: list[dict]\n\n    # 任务状态\n    task_goal: str\n    current_plan: list[str] | None\n    completed_steps: list[str]\n\n    # 资源消耗\n    total_input_tokens: int\n    total_output_tokens: int\n\n    # 错误追踪\n    consecutive_errors: int\n    error_log: list[dict]\n\n    # 时间戳\n    created_at: float\n    updated_at: float\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e5.3 状态持久化方案\u003c/h3\u003e\n\u003cp\u003e当 Agent 需要支持暂停/恢复、跨进程执行、或长时间运行时，执行状态必须持久化。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌─────────────┐     ┌──────────────┐     ┌──────────────┐\n│   In-Memory  │     │    Redis     │     │   Database   │\n│  (dict/obj)  │     │  (KV Store)  │     │ (PostgreSQL) │\n├─────────────┤     ├──────────────┤     ├──────────────┤\n│ 最快         │     │ 快，支持 TTL  │     │ 持久可靠     │\n│ 进程重启丢失  │     │ 跨进程共享    │     │ 支持查询分析  │\n│ 单进程使用    │     │ 重启后可保留  │     │ 适合生产环境  │\n│ 适合开发/测试 │     │ 适合 session  │     │ 适合审计追溯  │\n└─────────────┘     └──────────────┘     └──────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eCheckpoint 与恢复\u003c/strong\u003e 是 Stateful Agent 的核心能力。思路很直接：在每轮循环的关键节点保存一次快照，异常恢复时从最近的快照重新开始。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass CheckpointManager:\n    def save(self, state: ExecutionState) -\u0026gt; str:\n        \u0026quot;\u0026quot;\u0026quot;保存 checkpoint，返回 checkpoint_id\u0026quot;\u0026quot;\u0026quot;\n        snapshot = {\n            \u0026quot;state\u0026quot;: asdict(state),\n            \u0026quot;timestamp\u0026quot;: time.time(),\n        }\n        checkpoint_id = f\u0026quot;{state.session_id}:{state.turn_count}\u0026quot;\n        self.store.set(checkpoint_id, json.dumps(snapshot))\n        return checkpoint_id\n\n    def restore(self, checkpoint_id: str) -\u0026gt; ExecutionState:\n        \u0026quot;\u0026quot;\u0026quot;从 checkpoint 恢复执行状态\u0026quot;\u0026quot;\u0026quot;\n        snapshot = json.loads(self.store.get(checkpoint_id))\n        return ExecutionState(**snapshot[\u0026quot;state\u0026quot;])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e实际系统中，checkpoint 的保存频率需要权衡：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e每轮都保存\u003c/strong\u003e：恢复粒度最细，但写入开销大\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e关键节点保存\u003c/strong\u003e（如每次工具调用前后）：开销适中，覆盖最重要的故障场景\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e定时保存\u003c/strong\u003e：实现简单，但可能丢失最近几轮的状态\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e6. 完整代码实现\u003c/h2\u003e\n\u003cp\u003e下面是一个最小但完整的 Agent Control Loop 实现。不依赖任何框架，仅使用 Python 标准库 + OpenAI SDK。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\u0026quot;\u0026quot;\u0026quot;\nMinimal Agent Control Loop\n不依赖任何框架，纯 Python + OpenAI SDK\n\u0026quot;\u0026quot;\u0026quot;\nimport json\nimport time\nfrom enum import Enum\nfrom dataclasses import dataclass, field\nfrom openai import OpenAI\n\n\nclass State(Enum):\n    OBSERVE = \u0026quot;observe\u0026quot;\n    THINK = \u0026quot;think\u0026quot;\n    ACT = \u0026quot;act\u0026quot;\n    REFLECT = \u0026quot;reflect\u0026quot;\n    DONE = \u0026quot;done\u0026quot;\n    ERROR = \u0026quot;error\u0026quot;\n\n\n@dataclass\nclass LoopContext:\n    messages: list[dict] = field(default_factory=list)\n    turn: int = 0\n    total_tokens: int = 0\n    consecutive_errors: int = 0\n    recent_outputs: list[str] = field(default_factory=list)\n\n\n# ── Tool Registry ────────────────────────────────────\n\nTOOL_FUNCTIONS = {}\n\ndef register_tool(name: str, description: str, parameters: dict):\n    \u0026quot;\u0026quot;\u0026quot;装饰器：注册工具函数及其 schema\u0026quot;\u0026quot;\u0026quot;\n    def decorator(fn):\n        TOOL_FUNCTIONS[name] = {\n            \u0026quot;fn\u0026quot;: fn,\n            \u0026quot;schema\u0026quot;: {\n                \u0026quot;type\u0026quot;: \u0026quot;function\u0026quot;,\n                \u0026quot;function\u0026quot;: {\n                    \u0026quot;name\u0026quot;: name,\n                    \u0026quot;description\u0026quot;: description,\n                    \u0026quot;parameters\u0026quot;: parameters,\n                },\n            },\n        }\n        return fn\n    return decorator\n\n\n@register_tool(\n    name=\u0026quot;get_weather\u0026quot;,\n    description=\u0026quot;获取指定城市的当前天气\u0026quot;,\n    parameters={\n        \u0026quot;type\u0026quot;: \u0026quot;object\u0026quot;,\n        \u0026quot;properties\u0026quot;: {\n            \u0026quot;city\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;城市名称\u0026quot;},\n        },\n        \u0026quot;required\u0026quot;: [\u0026quot;city\u0026quot;],\n    },\n)\ndef get_weather(city: str) -\u0026gt; str:\n    # 示例实现，实际中调用真实 API\n    return json.dumps({\u0026quot;city\u0026quot;: city, \u0026quot;temp\u0026quot;: 28, \u0026quot;condition\u0026quot;: \u0026quot;晴\u0026quot;})\n\n\n# ── Agent Control Loop ───────────────────────────────\n\nclass Agent:\n    def __init__(\n        self,\n        system_prompt: str,\n        model: str = \u0026quot;gpt-4o\u0026quot;,\n        max_turns: int = 15,\n        token_budget: int = 50_000,\n        max_consecutive_errors: int = 3,\n    ):\n        self.client = OpenAI()\n        self.model = model\n        self.system_prompt = system_prompt\n        self.max_turns = max_turns\n        self.token_budget = token_budget\n        self.max_errors = max_consecutive_errors\n        self.tool_schemas = [t[\u0026quot;schema\u0026quot;] for t in TOOL_FUNCTIONS.values()]\n\n    def run(self, user_input: str) -\u0026gt; str:\n        ctx = LoopContext()\n        ctx.messages = [\n            {\u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;, \u0026quot;content\u0026quot;: self.system_prompt},\n            {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: user_input},\n        ]\n        state = State.THINK  # 首轮输入已就绪，直接进入 THINK\n\n        while state not in (State.DONE, State.ERROR):\n            match state:\n                case State.THINK:\n                    state, ctx = self._think(ctx)\n                case State.ACT:\n                    state, ctx = self._act(ctx)\n                case State.REFLECT:\n                    state, ctx = self._reflect(ctx)\n            ctx.turn += 1\n\n        # 提取最终回答\n        for msg in reversed(ctx.messages):\n            if msg[\u0026quot;role\u0026quot;] == \u0026quot;assistant\u0026quot; and msg.get(\u0026quot;content\u0026quot;):\n                return msg[\u0026quot;content\u0026quot;]\n        return \u0026quot;[Agent finished without a final answer]\u0026quot;\n\n    def _think(self, ctx: LoopContext) -\u0026gt; tuple[State, LoopContext]:\n        \u0026quot;\u0026quot;\u0026quot;调用 LLM 推理\u0026quot;\u0026quot;\u0026quot;\n        try:\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=ctx.messages,\n                tools=self.tool_schemas or None,\n            )\n        except Exception as e:\n            ctx.consecutive_errors += 1\n            ctx.messages.append({\n                \u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;,\n                \u0026quot;content\u0026quot;: f\u0026quot;[LLM Error] {e}\u0026quot;,\n            })\n            if ctx.consecutive_errors \u0026gt;= self.max_errors:\n                return State.ERROR, ctx\n            return State.THINK, ctx  # 重试\n\n        # 记录 token 消耗\n        usage = response.usage\n        ctx.total_tokens += (usage.prompt_tokens + usage.completion_tokens)\n        ctx.consecutive_errors = 0\n\n        choice = response.choices[0]\n        assistant_msg = choice.message.model_dump()\n        ctx.messages.append(assistant_msg)\n\n        # 决定下一状态\n        if choice.message.tool_calls:\n            return State.ACT, ctx\n        else:\n            return State.DONE, ctx\n\n    def _act(self, ctx: LoopContext) -\u0026gt; tuple[State, LoopContext]:\n        \u0026quot;\u0026quot;\u0026quot;执行工具调用\u0026quot;\u0026quot;\u0026quot;\n        assistant_msg = ctx.messages[-1]\n        tool_calls = assistant_msg.get(\u0026quot;tool_calls\u0026quot;, [])\n\n        for tc in tool_calls:\n            fn_name = tc[\u0026quot;function\u0026quot;][\u0026quot;name\u0026quot;]\n            fn_args = json.loads(tc[\u0026quot;function\u0026quot;][\u0026quot;arguments\u0026quot;])\n\n            tool_entry = TOOL_FUNCTIONS.get(fn_name)\n            if not tool_entry:\n                result = f\u0026quot;Error: unknown tool \u0026#39;{fn_name}\u0026#39;\u0026quot;\n            else:\n                try:\n                    result = tool_entry[\u0026quot;fn\u0026quot;](**fn_args)\n                except Exception as e:\n                    result = f\u0026quot;Error: tool \u0026#39;{fn_name}\u0026#39; raised {type(e).__name__}: {e}\u0026quot;\n                    ctx.consecutive_errors += 1\n\n            ctx.messages.append({\n                \u0026quot;role\u0026quot;: \u0026quot;tool\u0026quot;,\n                \u0026quot;tool_call_id\u0026quot;: tc[\u0026quot;id\u0026quot;],\n                \u0026quot;content\u0026quot;: str(result),\n            })\n\n        return State.REFLECT, ctx\n\n    def _reflect(self, ctx: LoopContext) -\u0026gt; tuple[State, LoopContext]:\n        \u0026quot;\u0026quot;\u0026quot;反思：检查终止条件\u0026quot;\u0026quot;\u0026quot;\n        # 最大轮次\n        if ctx.turn \u0026gt;= self.max_turns:\n            ctx.messages.append({\n                \u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;,\n                \u0026quot;content\u0026quot;: \u0026quot;[Agent stopped: max turns exceeded]\u0026quot;,\n            })\n            return State.ERROR, ctx\n\n        # Token 预算\n        if ctx.total_tokens \u0026gt;= self.token_budget:\n            ctx.messages.append({\n                \u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;,\n                \u0026quot;content\u0026quot;: \u0026quot;[Agent stopped: token budget exceeded]\u0026quot;,\n            })\n            return State.ERROR, ctx\n\n        # 连续错误\n        if ctx.consecutive_errors \u0026gt;= self.max_errors:\n            return State.ERROR, ctx\n\n        # 死循环检测：最近 3 次输出相同\n        tool_results = [\n            m[\u0026quot;content\u0026quot;] for m in ctx.messages[-6:]\n            if m.get(\u0026quot;role\u0026quot;) == \u0026quot;tool\u0026quot;\n        ]\n        if len(tool_results) \u0026gt;= 3 and len(set(tool_results[-3:])) == 1:\n            ctx.messages.append({\n                \u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;,\n                \u0026quot;content\u0026quot;: \u0026quot;[Agent stopped: loop detected]\u0026quot;,\n            })\n            return State.ERROR, ctx\n\n        # 继续下一轮推理\n        return State.THINK, ctx\n\n\n# ── 使用示例 ─────────────────────────────────────────\n\nif __name__ == \u0026quot;__main__\u0026quot;:\n    agent = Agent(\n        system_prompt=\u0026quot;你是一个天气助手。使用 get_weather 工具回答天气问题。\u0026quot;,\n        max_turns=10,\n    )\n    answer = agent.run(\u0026quot;北京今天天气怎么样？\u0026quot;)\n    print(answer)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这段代码约 130 行，涵盖了 Control Loop 的所有核心要素：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e状态机驱动的循环控制\u003c/li\u003e\n\u003cli\u003e工具注册与动态调用\u003c/li\u003e\n\u003cli\u003eLLM 异常重试\u003c/li\u003e\n\u003cli\u003eToken 消耗追踪\u003c/li\u003e\n\u003cli\u003e多种终止条件（max_turns / token_budget / consecutive_errors / loop_detected）\u003c/li\u003e\n\u003cli\u003e工具执行错误处理\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e它不是生产级代码，但足以说明 Control Loop 的核心机制。在此基础上增加异步执行、状态持久化、日志追踪，就能逐步演进为生产级实现。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e7. 错误处理策略\u003c/h2\u003e\n\u003cp\u003e生产环境中，Agent Control Loop 最常遇到的四类错误：\u003c/p\u003e\n\u003ch3\u003e7.1 Tool 调用失败\u003c/h3\u003e\n\u003cp\u003e工具调用失败是最高频的错误。正确的处理方式不是抛异常终止，而是\u003cstrong\u003e将错误信息作为 Observation 返回给 LLM\u003c/strong\u003e，让它决定如何应对。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 错误的做法：直接终止\ntry:\n    result = call_tool(name, args)\nexcept Exception:\n    raise  # Agent 直接崩溃\n\n# 正确的做法：将错误反馈给 LLM\ntry:\n    result = call_tool(name, args)\nexcept TimeoutError:\n    result = \u0026quot;Tool timed out after 30s. Consider using different parameters.\u0026quot;\nexcept ValueError as e:\n    result = f\u0026quot;Invalid arguments: {e}. Please check parameter types.\u0026quot;\nexcept Exception as e:\n    result = f\u0026quot;Tool failed: {type(e).__name__}: {e}\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLLM 在收到错误信息后，通常能自主修正——换一组参数重试、换一个工具、或者告知用户当前无法完成任务。\u003c/p\u003e\n\u003ch3\u003e7.2 LLM 返回格式异常\u003c/h3\u003e\n\u003cp\u003eLLM 偶尔会返回不符合预期的格式：JSON 不合法、tool_call 参数缺失、content 为空等。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef _parse_tool_call_safe(self, tool_call) -\u0026gt; tuple[str, dict]:\n    \u0026quot;\u0026quot;\u0026quot;安全解析工具调用参数\u0026quot;\u0026quot;\u0026quot;\n    name = tool_call.function.name\n    try:\n        args = json.loads(tool_call.function.arguments)\n    except json.JSONDecodeError:\n        # LLM 返回了非法 JSON，尝试修复或跳过\n        args = {}\n        self.logger.warning(\n            f\u0026quot;Invalid JSON in tool_call arguments: \u0026quot;\n            f\u0026quot;{tool_call.function.arguments}\u0026quot;\n        )\n    return name, args\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e7.3 超时处理\u003c/h3\u003e\n\u003cp\u003e整个 Agent 执行需要有全局超时，防止无限挂起：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport signal\n\nclass TimeoutError(Exception):\n    pass\n\ndef run_with_timeout(fn, timeout_seconds: int, *args, **kwargs):\n    \u0026quot;\u0026quot;\u0026quot;为函数执行添加超时限制\u0026quot;\u0026quot;\u0026quot;\n    def handler(signum, frame):\n        raise TimeoutError(f\u0026quot;Execution timed out after {timeout_seconds}s\u0026quot;)\n\n    old_handler = signal.signal(signal.SIGALRM, handler)\n    signal.alarm(timeout_seconds)\n    try:\n        return fn(*args, **kwargs)\n    finally:\n        signal.alarm(0)\n        signal.signal(signal.SIGALRM, old_handler)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e7.4 死循环检测\u003c/h3\u003e\n\u003cp\u003e当 Agent 陷入死循环时，它会反复执行相同的操作序列。检测策略：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef _detect_loop(self, messages: list[dict], window: int = 6) -\u0026gt; bool:\n    \u0026quot;\u0026quot;\u0026quot;检测 Agent 是否陷入重复循环\u0026quot;\u0026quot;\u0026quot;\n    recent = messages[-window:]\n\n    # 策略 1：完全重复检测\n    contents = [m.get(\u0026quot;content\u0026quot;, \u0026quot;\u0026quot;) for m in recent if m[\u0026quot;role\u0026quot;] == \u0026quot;assistant\u0026quot;]\n    if len(contents) \u0026gt;= 3 and len(set(contents[-3:])) == 1:\n        return True\n\n    # 策略 2：工具调用序列重复检测\n    tool_calls = []\n    for m in recent:\n        if m.get(\u0026quot;tool_calls\u0026quot;):\n            for tc in m[\u0026quot;tool_calls\u0026quot;]:\n                tool_calls.append(f\u0026quot;{tc[\u0026#39;function\u0026#39;][\u0026#39;name\u0026#39;]}:{tc[\u0026#39;function\u0026#39;][\u0026#39;arguments\u0026#39;]}\u0026quot;)\n\n    if len(tool_calls) \u0026gt;= 4:\n        half = len(tool_calls) // 2\n        if tool_calls[:half] == tool_calls[half:2*half]:\n            return True\n\n    return False\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e8. 性能考量\u003c/h2\u003e\n\u003ch3\u003e8.1 Token 消耗与循环次数的关系\u003c/h3\u003e\n\u003cp\u003eAgent Control Loop 的 Token 消耗不是线性增长，而是\u003cstrong\u003e二次增长\u003c/strong\u003e——因为每一轮都要携带之前所有轮次的 message history。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e轮次    新增消息 Token    累计 Context Token    本轮总消耗\n1       T               S + T                S + T\n2       T               S + 2T               S + 2T\n3       T               S + 3T               S + 3T\n...\nN       T               S + NT               S + NT\n\n总消耗 = N*S + T*(1+2+...+N) = N*S + T*N*(N+1)/2\n\n其中 S = System Prompt Token 数，T = 平均每轮消息 Token 数\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这意味着 \u003cstrong\u003e10 轮的 Agent 消耗的 Token 不是 1 轮的 10 倍，而可能是 55 倍\u003c/strong\u003e。这对成本控制至关重要。\u003c/p\u003e\n\u003ch3\u003e8.2 Context Window 膨胀问题\u003c/h3\u003e\n\u003cp\u003e随着轮次增加，Context Window 持续膨胀，导致：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e延迟增加\u003c/strong\u003e：LLM 推理时间与输入 Token 数正相关\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e成本增加\u003c/strong\u003e：按 Token 计费，输入越长越贵\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e质量下降\u003c/strong\u003e：过长的 Context 会导致 LLM \u0026quot;注意力分散\u0026quot;，关键信息被淹没（lost in the middle 问题）\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e8.3 消息压缩/摘要策略\u003c/h3\u003e\n\u003cp\u003e应对 Context Window 膨胀的核心策略：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e策略一：滑动窗口\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e只保留最近 K 轮对话，丢弃更早的历史。简单粗暴但有效。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef _sliding_window(self, messages: list[dict], keep_last: int = 10) -\u0026gt; list[dict]:\n    system_msgs = [m for m in messages if m[\u0026quot;role\u0026quot;] == \u0026quot;system\u0026quot;]\n    non_system = [m for m in messages if m[\u0026quot;role\u0026quot;] != \u0026quot;system\u0026quot;]\n    return system_msgs + non_system[-keep_last:]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e策略二：摘要压缩\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e当 message history 超过阈值时，用 LLM 对早期对话生成摘要，替换原始消息。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef _compress_history(self, messages: list[dict], threshold: int = 20) -\u0026gt; list[dict]:\n    if len(messages) \u0026lt;= threshold:\n        return messages\n\n    # 将早期消息压缩为摘要\n    early = messages[1:-threshold]  # 跳过 system prompt，保留最近的\n    summary_prompt = (\n        \u0026quot;请用 3-5 句话总结以下对话的关键信息和已完成的操作：\\n\u0026quot;\n        + \u0026quot;\\n\u0026quot;.join(m.get(\u0026quot;content\u0026quot;, \u0026quot;\u0026quot;) for m in early if m.get(\u0026quot;content\u0026quot;))\n    )\n\n    summary = self.client.chat.completions.create(\n        model=\u0026quot;gpt-4o-mini\u0026quot;,  # 用小模型做摘要，节省成本\n        messages=[{\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: summary_prompt}],\n    ).choices[0].message.content\n\n    return (\n        [messages[0]]  # system prompt\n        + [{\u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;, \u0026quot;content\u0026quot;: f\u0026quot;[Earlier conversation summary] {summary}\u0026quot;}]\n        + messages[-threshold:]\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e策略三：选择性保留\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e不是所有消息都同等重要。工具的原始返回值（可能非常长）通常可以只保留摘要：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef _trim_tool_results(self, messages: list[dict], max_len: int = 500) -\u0026gt; list[dict]:\n    \u0026quot;\u0026quot;\u0026quot;截断过长的工具返回值\u0026quot;\u0026quot;\u0026quot;\n    trimmed = []\n    for m in messages:\n        if m[\u0026quot;role\u0026quot;] == \u0026quot;tool\u0026quot; and len(m.get(\u0026quot;content\u0026quot;, \u0026quot;\u0026quot;)) \u0026gt; max_len:\n            m = {**m, \u0026quot;content\u0026quot;: m[\u0026quot;content\u0026quot;][:max_len] + \u0026quot;\\n...[truncated]\u0026quot;}\n        trimmed.append(m)\n    return trimmed\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e三种策略的对比：\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e策略\u003c/th\u003e\n\u003cth\u003e信息保留\u003c/th\u003e\n\u003cth\u003e实现成本\u003c/th\u003e\n\u003cth\u003eToken 节省\u003c/th\u003e\n\u003cth\u003e适用场景\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e滑动窗口\u003c/td\u003e\n\u003ctd\u003e低\u003c/td\u003e\n\u003ctd\u003e极低\u003c/td\u003e\n\u003ctd\u003e高\u003c/td\u003e\n\u003ctd\u003e短对话、工具调用为主\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e摘要压缩\u003c/td\u003e\n\u003ctd\u003e中\u003c/td\u003e\n\u003ctd\u003e中（需要额外 LLM 调用）\u003c/td\u003e\n\u003ctd\u003e高\u003c/td\u003e\n\u003ctd\u003e长对话、需要历史上下文\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e选择性保留\u003c/td\u003e\n\u003ctd\u003e高\u003c/td\u003e\n\u003ctd\u003e低\u003c/td\u003e\n\u003ctd\u003e中\u003c/td\u003e\n\u003ctd\u003e工具返回值较大的场景\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e实际工程中，通常\u003cstrong\u003e组合使用\u003c/strong\u003e：先用选择性保留截断大结果，再用滑动窗口控制总长度，在关键节点用摘要压缩保留全局上下文。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e9. 小结与进一步思考\u003c/h2\u003e\n\u003cp\u003e本文从状态机模型出发，完整地拆解了 Agent Control Loop 的核心抽象：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eOBSERVE\u003c/strong\u003e 负责输入归一化——将各种来源的信息统一为 LLM 可理解的 message 格式\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTHINK\u003c/strong\u003e 是核心推理阶段——管理 Context Window、控制 Token 预算、解析 LLM 输出\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eACT\u003c/strong\u003e 是执行层——处理工具调用的同步/异步执行、超时控制、安全隔离\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eREFLECT\u003c/strong\u003e 负责质量评估——决定是继续、重试还是终止\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e终止条件\u003c/strong\u003e是成本和安全的兜底——max_turns、token_budget、error_threshold、loop_detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e我们对比了 ReAct 和 Plan-then-Execute 两种主流模式，分析了 Stateless 与 Stateful 两种状态管理策略，并实现了一个不依赖任何框架的完整 Control Loop。\u003c/p\u003e\n\u003cp\u003e但控制循环只是 Agent 运行时的骨架。它的灵魂在于 \u003cstrong\u003eTool Calling\u003c/strong\u003e——正是工具让 Agent 从\u0026quot;能说会道的语言模型\u0026quot;变成\u0026quot;能做事的智能体\u0026quot;。\u003c/p\u003e\n\u003cp\u003e在下一篇 \u003cstrong\u003e《Tool Calling Deep Dive: 让 LLM 成为可编程接口》\u003c/strong\u003e 中，我们会深入工具调用的设计哲学：JSON Schema 作为契约、Tool Registry 的实现、参数校验、错误传播，以及 Structured Output 为什么优于自由文本。\u003c/p\u003e\n\u003cp\u003e留几个值得进一步思考的问题：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eControl Loop 的嵌套\u003c/strong\u003e：当一个 Agent 的工具是另一个 Agent 时，控制循环如何嵌套？外层循环和内层循环的终止条件如何协调？\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e人机协作中的循环\u003c/strong\u003e：如何在 Control Loop 中优雅地插入人类审批节点？这和 Stateful Agent 的 checkpoint 机制有什么关系？\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e流式输出与控制循环\u003c/strong\u003e：当 Agent 需要边思考边输出（streaming）时，状态机模型还适用吗？需要做哪些调整？\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e多模态输入的归一化\u003c/strong\u003e：当 OBSERVE 阶段接收的不只是文本，还有图片、音频、视频时，输入归一化策略如何演化？\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e系列导航\u003c/strong\u003e：本文是 Agentic 系列的第 04 篇。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e上一篇：\u003ca href=\"/blog/engineering/agentic/03-Agent%20vs%20Workflow%20vs%20Automation\"\u003e03 | Agent vs Workflow vs Automation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下一篇：\u003ca href=\"/blog/engineering/agentic/05-Tool%20Calling%20Deep%20Dive\"\u003e05 | Tool Calling Deep Dive\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e完整目录：\u003ca href=\"/blog/engineering/agentic/01-From%20LLM%20to%20Agent\"\u003e01 | From LLM to Agent\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"article\",null,{\"className\":\"min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"nav\",null,{\"className\":\"flex items-center gap-1 text-sm mb-4\",\"children\":[[\"$\",\"$L13\",null,{\"href\":\"/blog/page/1\",\"className\":\"text-gray-500 hover:text-blue-600 transition-colors\",\"children\":\"博客\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"/\"}],[\"$\",\"$L13\",null,{\"href\":\"/blog/category/engineering/page/1\",\"className\":\"text-gray-500 hover:text-blue-600 transition-colors\",\"children\":\"Engineering\"}],[[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"/\"}],[\"$\",\"$L13\",null,{\"href\":\"/blog/category/engineering/agentic/page/1\",\"className\":\"text-blue-600 hover:text-blue-700 transition-colors\",\"children\":\"Agentic 系统\"}]]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center mb-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"w-4 h-4 mr-2 text-gray-400\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z\"}]}],[\"$\",\"time\",null,{\"dateTime\":\"2025-12-28\",\"children\":\"2025年12月28日\"}]]}]}],[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold text-gray-900 mb-6 text-center\",\"children\":\"Agent Runtime from Scratch: 不依赖框架构建 Agent\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2 mb-6 justify-center\",\"children\":[[\"$\",\"$L13\",\"Agentic\",{\"href\":\"/blog/tag/Agentic/page/1/\",\"className\":\"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors\",\"children\":\"Agentic\"}],[\"$\",\"$L13\",\"AI Engineering\",{\"href\":\"/blog/tag/AI%20Engineering/page/1/\",\"className\":\"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors\",\"children\":\"AI Engineering\"}],[\"$\",\"$L13\",\"Runtime\",{\"href\":\"/blog/tag/Runtime/page/1/\",\"className\":\"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors\",\"children\":\"Runtime\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"max-w-5xl mx-auto\",\"children\":[\"$\",\"$L14\",null,{\"content\":\"$15\"}]}],[\"$\",\"$10\",null,{\"fallback\":[\"$\",\"div\",null,{\"className\":\"mt-12 pt-8 border-t border-gray-200\",\"children\":\"加载导航中...\"}],\"children\":[\"$\",\"$L16\",null,{\"globalNav\":{\"prev\":{\"slug\":\"engineering/middleware/深入理解AQS：Java并发的基石\",\"title\":\"深入理解AQS：Java并发的基石\",\"description\":\"系统性剖析 AbstractQueuedSynchronizer（AQS）的设计思想、核心数据结构、加锁解锁流程，并通过 ReentrantLock 源码深入理解其工作原理，最后梳理 AQS 在 JUC 中的典型应用场景。\",\"pubDate\":\"2025-12-28\",\"tags\":[\"Java\",\"并发编程\",\"AQS\",\"ReentrantLock\",\"JUC\"],\"heroImage\":\"$undefined\",\"content\":\"$17\"},\"next\":{\"slug\":\"engineering/agentic/08-Memory Architecture\",\"title\":\"Memory Architecture: Agent 的状态与记忆体系\",\"description\":\"LLM 是无状态的，但 Agent 必须有状态。本文系统拆解 Agent 记忆的四层架构——Conversation Buffer、Working Memory、Episodic Memory、Semantic Memory，从认知科学类比出发，深入每一层的设计原理、存储方案、读写策略与 Context Window 管理，附完整 Python 实现。\",\"pubDate\":\"2026-01-02\",\"tags\":[\"Agentic\",\"AI Engineering\",\"Memory\"],\"heroImage\":\"$undefined\",\"content\":\"$18\"}},\"tagNav\":{\"Agentic\":{\"prev\":{\"slug\":\"engineering/agentic/06-Prompt Engineering for Agents\",\"title\":\"Prompt Engineering for Agents: 面向 Agent 的提示词工程\",\"description\":\"Agent 的 Prompt 不是聊天提示词，而是系统接口规范。本文系统拆解 Agent Prompt 的分层架构、四种关键设计模式（Router / Planner / Executor / Reflector）、Chain-of-Thought 的 Agent 化应用、Few-shot vs Zero-shot 的场景选择、Prompt 工程化实践（模板化 / 版本控制 / 测试 / 组合），以及 Context Window 管理策略。\",\"pubDate\":\"2025-12-23\",\"tags\":[\"Agentic\",\"AI Engineering\",\"Prompt Engineering\"],\"heroImage\":\"$undefined\",\"content\":\"$19\"},\"next\":\"$5:props:children:props:children:props:children:2:props:children:props:globalNav:next\"},\"AI Engineering\":{\"prev\":\"$5:props:children:props:children:props:children:2:props:children:props:tagNav:Agentic:prev\",\"next\":\"$5:props:children:props:children:props:children:2:props:children:props:globalNav:next\"},\"Runtime\":{\"prev\":{\"slug\":\"engineering/agentic/04-The Agent Control Loop\",\"title\":\"The Agent Control Loop: Agent 运行时的核心抽象\",\"description\":\"Agent 的本质不是一次函数调用，而是一个可中断的控制循环。本文从状态机模型出发，深入剖析 Agent Control Loop 的每个阶段——OBSERVE、THINK、ACT、REFLECT，对比 ReAct 与 Plan-then-Execute 两种主流模式，讨论状态管理、错误处理与性能优化策略，并给出一个不依赖任何框架的完整 Python 实现。\",\"pubDate\":\"2025-12-14\",\"tags\":[\"Agentic\",\"AI Engineering\",\"Runtime\"],\"heroImage\":\"$undefined\",\"content\":\"$1a\"},\"next\":null}}}]}],[\"$\",\"$L1b\",null,{}]]}]}]}]\n"])</script><script>self.__next_f.push([1,"8:null\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n7:null\n"])</script><script>self.__next_f.push([1,"a:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Agent Runtime from Scratch: 不依赖框架构建 Agent - Skyfalling Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"不依赖 LangChain 等框架，从零实现一个功能完整的 Agent Runtime。逐模块构建 LLMClient、ToolRegistry、ToolExecutor、MessageManager 和核心控制循环，包含并行工具调用、Streaming、超时控制、死循环检测等高级特性，附完整可运行代码。\"}],[\"$\",\"meta\",\"2\",{\"property\":\"og:title\",\"content\":\"Agent Runtime from Scratch: 不依赖框架构建 Agent\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:description\",\"content\":\"不依赖 LangChain 等框架，从零实现一个功能完整的 Agent Runtime。逐模块构建 LLMClient、ToolRegistry、ToolExecutor、MessageManager 和核心控制循环，包含并行工具调用、Streaming、超时控制、死循环检测等高级特性，附完整可运行代码。\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"5\",{\"property\":\"article:published_time\",\"content\":\"2025-12-28\"}],[\"$\",\"meta\",\"6\",{\"property\":\"article:author\",\"content\":\"Skyfalling\"}],[\"$\",\"meta\",\"7\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:title\",\"content\":\"Agent Runtime from Scratch: 不依赖框架构建 Agent\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:description\",\"content\":\"不依赖 LangChain 等框架，从零实现一个功能完整的 Agent Runtime。逐模块构建 LLMClient、ToolRegistry、ToolExecutor、MessageManager 和核心控制循环，包含并行工具调用、Streaming、超时控制、死循环检测等高级特性，附完整可运行代码。\"}],[\"$\",\"link\",\"10\",{\"rel\":\"shortcut icon\",\"href\":\"/favicon.png\"}],[\"$\",\"link\",\"11\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"link\",\"12\",{\"rel\":\"icon\",\"href\":\"/favicon.png\"}],[\"$\",\"link\",\"13\",{\"rel\":\"apple-touch-icon\",\"href\":\"/favicon.png\"}]],\"error\":null,\"digest\":\"$undefined\"}\n12:{\"metadata\":\"$a:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>