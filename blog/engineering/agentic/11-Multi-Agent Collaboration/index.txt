1:"$Sreact.fragment"
2:I[10616,["6874","static/chunks/6874-7791217feaf05c17.js","7177","static/chunks/app/layout-142e67ac4336647c.js"],"default"]
3:I[87555,[],""]
4:I[31295,[],""]
6:I[59665,[],"OutletBoundary"]
9:I[74911,[],"AsyncMetadataOutlet"]
b:I[59665,[],"ViewportBoundary"]
d:I[59665,[],"MetadataBoundary"]
f:I[26614,[],""]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/fffdcdb4fb651185.css","style"]
0:{"P":null,"b":"bJbIT2Kmv3sjcEJSOV0Wp","p":"","c":["","blog","engineering","agentic","11-Multi-Agent%20Collaboration",""],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","engineering/agentic/11-Multi-Agent%20Collaboration","c"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/fffdcdb4fb651185.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"zh-CN","children":["$","body",null,{"className":"__className_f367f3","children":["$","div",null,{"className":"min-h-screen flex flex-col","children":[["$","$L2",null,{}],["$","main",null,{"className":"flex-1","children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"bg-[var(--background)]","children":["$","div",null,{"className":"mx-auto max-w-7xl px-6 py-12 lg:px-8","children":["$","p",null,{"className":"text-center text-xs leading-5 text-gray-400","children":["© ",2026," Skyfalling"]}]}]}]]}]}]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","engineering/agentic/11-Multi-Agent%20Collaboration","c"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L5",null,["$","$L6",null,{"children":["$L7","$L8",["$","$L9",null,{"promise":"$@a"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","Na22R0sG0bsmAz7342ZTMv",{"children":[["$","$Lb",null,{"children":"$Lc"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$Ld",null,{"children":"$Le"}]]}],false]],"m":"$undefined","G":["$f","$undefined"],"s":false,"S":true}
10:"$Sreact.suspense"
11:I[74911,[],"AsyncMetadata"]
13:I[6874,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],""]
14:I[32923,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
16:I[40780,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
1a:I[85300,["6874","static/chunks/6874-7791217feaf05c17.js","968","static/chunks/968-d7155a2506e36f1d.js","6909","static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js"],"default"]
e:["$","div",null,{"hidden":true,"children":["$","$10",null,{"fallback":null,"children":["$","$L11",null,{"promise":"$@12"}]}]}]
15:Tf833,<h1>Multi-Agent Collaboration: 多 Agent 协作模式与架构</h1>
<blockquote>
<p>一个人可以走得很快，但一群人才能走得很远。Agent 也是如此。</p>
<p>本文是 Agentic 系列第 11 篇。前 10 篇我们一直在讨论单个 Agent 如何更聪明——更好的记忆、更强的工具、更深的规划。这一篇，我们把视角从&quot;个体智能&quot;拉升到&quot;集体智能&quot;：当一个 Agent 不够用时，多个 Agent 如何协作？</p>
</blockquote>
<hr>
<h2>1. 为什么单 Agent 不够</h2>
<h3>1.1 一个类比：从独立开发者到工程团队</h3>
<p>想象你是一个全栈工程师，独自完成一个项目。前端、后端、数据库、DevOps、测试、文档——全部一个人扛。小项目可以，但当系统规模增长到一定程度，你会发现：</p>
<ul>
<li><strong>注意力是瓶颈</strong>：你不可能同时想着 CSS 布局和数据库索引优化</li>
<li><strong>专业化有上限</strong>：一个人很难同时成为安全专家、性能专家和 UX 专家</li>
<li><strong>效率有天花板</strong>：就算你是 10x 工程师，你的时间也是串行的</li>
<li><strong>单点风险</strong>：你生病了，整个项目就停了</li>
</ul>
<p>这就是人类发明&quot;团队协作&quot;的原因。Agent 面临完全相同的结构性限制。</p>
<h3>1.2 Single-Agent 的四个天花板</h3>
<p><strong>天花板一：Context Window 限制</strong></p>
<p>一个 Agent 的 System Prompt 需要包含：角色定义、工具描述、输出格式约束、领域知识、示例。当你试图让一个 Agent 同时承担搜索、分析、写作、代码生成、数据可视化等多个职能时，光是工具描述就可能占据数万 token。留给实际任务执行的上下文空间被严重压缩。</p>
<pre><code>一个&quot;全能&quot; Agent 的 Context 分配：

┌─────────────────────────────────────────────────┐
│ System Prompt (角色 + 规则)         ~2,000 tokens │
│ Tool Schemas (15 个工具)            ~6,000 tokens │
│ 领域知识 (RAG 检索结果)             ~4,000 tokens │
│ 对话历史                            ~8,000 tokens │
│ 当前任务 + 中间状态                 ~3,000 tokens │
├─────────────────────────────────────────────────┤
│ 剩余可用空间                        ~9,000 tokens │ ← 越来越捉襟见肘
│ (128K 窗口下比例更好，但工具越多问题越突出)         │
└─────────────────────────────────────────────────┘
</code></pre>
<p>更关键的是，研究表明 LLM 在超长上下文中存在&quot;Lost in the Middle&quot;问题——中间位置的信息检索准确率显著下降。塞得越多，每条信息被有效利用的概率越低。</p>
<p><strong>天花板二：专业化限制</strong></p>
<p>一个 System Prompt 很难让 LLM 同时扮演好多个角色。你告诉它&quot;你是一个严谨的数据分析师&quot;，它分析数据时很好；但同一个 prompt 里你又说&quot;你也是一个有创意的文案写手&quot;，这两种人格的行为模式是矛盾的。严谨和创意在同一个 prompt 中互相干扰，最终两个角色都做不好。</p>
<p>这不是 prompt engineering 的技巧问题，而是注意力分配的结构性问题——一个 LLM 调用只有一个 attention 分布，强调了分析的严谨性，就必然削弱了文案的创造性。</p>
<p><strong>天花板三：可靠性限制</strong></p>
<p>单 Agent 是一个 Single Point of Failure。如果它在第 5 步推理出错（比如工具调用参数写错），整个任务链路都会受到污染。虽然我们在第 10 篇讨论了 Reflection 和自我纠错，但自我纠错的前提是&quot;能发现自己错了&quot;——而 LLM 对自身错误的检测能力是有限的。</p>
<p><strong>天花板四：并行度限制</strong></p>
<p>单 Agent 的执行是串行的——一次 LLM 调用，等待结果，再进行下一次。如果一个任务可以分解为三个独立子任务（比如同时搜索三个数据源），单 Agent 只能顺序执行，浪费了大量时间。</p>
<pre><code>Single-Agent 串行执行：

  Task ──→ [Search A] ──→ [Search B] ──→ [Search C] ──→ [Synthesize]
                                                         Total: ~40s

Multi-Agent 并行执行：

           ┌─→ [Search A] ─┐
  Task ──→ ├─→ [Search B] ─┼──→ [Synthesize]
           └─→ [Search C] ─┘
                              Total: ~15s
</code></pre>
<hr>
<h2>2. Multi-Agent 的四种协作模式</h2>
<p>当我们决定使用多个 Agent 时，第一个架构问题是：<strong>它们之间的协作关系是什么？</strong> 不同的关系模式适用于不同的场景，选错模式比用错框架更致命。</p>
<h3>2.1 模式一：Supervisor-Worker（上级分配型）</h3>
<pre><code>                    ┌──────────────────┐
                    │    Supervisor    │
                    │   (任务分解 +    │
                    │    结果合成)     │
                    └──────┬───────────┘
                           │
              ┌────────────┼────────────┐
              │            │            │
              ▼            ▼            ▼
       ┌──────────┐ ┌──────────┐ ┌──────────┐
       │ Worker A │ │ Worker B │ │ Worker C │
       │ (搜索)   │ │ (分析)   │ │ (写作)   │
       └──────────┘ └──────────┘ └──────────┘
              │            │            │
              └────────────┼────────────┘
                           │
                           ▼
                    ┌──────────────────┐
                    │    Supervisor    │
                    │   (收集 + 合成   │
                    │    最终输出)     │
                    └──────────────────┘
</code></pre>
<p><strong>工作流程</strong>：</p>
<ol>
<li>Supervisor Agent 接收用户任务</li>
<li>Supervisor 将任务分解为子任务，分配给不同的 Worker Agent</li>
<li>每个 Worker 独立执行各自的子任务</li>
<li>Supervisor 收集所有 Worker 的结果，合成最终输出</li>
</ol>
<p><strong>核心特征</strong>：</p>
<ul>
<li>有一个明确的中央协调者</li>
<li>Worker 之间不直接通信，只与 Supervisor 交互</li>
<li>Supervisor 负责全局决策，Worker 负责局部执行</li>
</ul>
<p><strong>适用场景</strong>：任务可以明确分解的场景。比如撰写一篇技术调研报告：Search Agent 负责信息搜集，Analyze Agent 负责数据分析，Write Agent 负责报告撰写。Supervisor 负责协调整个流程。</p>
<p><strong>Trade-off</strong>：Supervisor 是单点——如果 Supervisor 对任务的分解不合理，所有 Worker 的努力都会被浪费。此外，Supervisor 本身也是一个 LLM 调用，它对任务的理解能力决定了整个系统的上限。</p>
<h3>2.2 模式二：Peer-to-Peer（平等协商型）</h3>
<pre><code>       ┌──────────┐          ┌──────────┐
       │ Agent A  │◀────────▶│ Agent B  │
       │ (作者)   │          │ (审稿人) │
       └────┬─────┘          └────┬─────┘
            │                     │
            │    ┌──────────┐     │
            └───▶│ Agent C  │◀────┘
                 │ (编辑)   │
                 └──────────┘

       消息流是双向的，没有固定的上下级关系
       每个 Agent 都可以发起对话、提出意见、做出决策
</code></pre>
<p><strong>工作流程</strong>：</p>
<ol>
<li>多个 Agent 地位平等，通过消息传递进行协商</li>
<li>没有中央协调者——Agent 之间直接通信</li>
<li>通过多轮对话达成共识或完成任务</li>
</ol>
<p><strong>核心特征</strong>：</p>
<ul>
<li>去中心化</li>
<li>Agent 之间直接消息传递</li>
<li>适合需要多视角碰撞的任务</li>
</ul>
<p><strong>适用场景</strong>：辩论式分析（多个 Agent 从不同立场论证）、代码审查（Author Agent 写代码，Reviewer Agent 审查，双方来回沟通直到代码质量达标）、多角度决策（乐观分析师 + 悲观分析师 + 风险评估师共同评估一个投资决策）。</p>
<p><strong>Trade-off</strong>：没有中央协调意味着可能出现无限循环（两个 Agent 互相不同意，永远达不成共识）。需要额外的终止机制——最大轮次限制、外部仲裁者、投票制度等。调试也更困难，因为没有一个中心节点可以观察全局状态。</p>
<h3>2.3 模式三：Pipeline（流水线型）</h3>
<pre><code>  Input                                                          Output
    │                                                              ▲
    ▼                                                              │
┌────────┐    ┌────────┐    ┌────────┐    ┌────────┐    ┌────────┐
│ Draft  │───▶│ Review │───▶│  Edit  │───▶│  Fact  │───▶│ Format │
│ Agent  │    │ Agent  │    │ Agent  │    │ Check  │    │ Agent  │
│        │    │        │    │        │    │ Agent  │    │        │
└────────┘    └────────┘    └────────┘    └────────┘    └────────┘

  Stage 1       Stage 2       Stage 3       Stage 4       Stage 5
  生成初稿      审查质量       修改完善      事实核查       格式化输出
</code></pre>
<p><strong>工作流程</strong>：</p>
<ol>
<li>Agent 按顺序串联，形成流水线</li>
<li>上游 Agent 的输出是下游 Agent 的输入</li>
<li>每个 Agent 专注于一个处理阶段</li>
</ol>
<p><strong>核心特征</strong>：</p>
<ul>
<li>类似 Unix 管道：<code>cmd1 | cmd2 | cmd3</code></li>
<li>数据单向流动</li>
<li>每个阶段的 Agent 有明确、单一的职责</li>
</ul>
<p><strong>适用场景</strong>：内容生产流水线（起草 -&gt; 审查 -&gt; 编辑 -&gt; 排版）、数据处理管道（提取 -&gt; 清洗 -&gt; 转换 -&gt; 加载）、多阶段审批（初审 -&gt; 复审 -&gt; 终审）。</p>
<p><strong>Trade-off</strong>：流水线是严格串行的——上游不完成，下游无法开始。如果中间某个 Agent 输出质量差，后续所有阶段都会受影响（错误传播）。但好处是架构简单、易于理解和调试、每个阶段可以独立优化。</p>
<h3>2.4 模式四：Dynamic Routing（动态路由型）</h3>
<pre><code>                    ┌──────────────────┐
                    │   Router Agent   │
                    │ (意图识别 + 路由) │
                    └──────┬───────────┘
                           │
              ┌────────────┼────────────┐
              │            │            │
              ▼            ▼            ▼
       ┌──────────┐ ┌──────────┐ ┌──────────┐
       │ 技术支持  │ │ 售后服务  │ │ 销售咨询  │
       │ Agent    │ │ Agent    │ │ Agent    │
       │          │ │          │ │          │
       │ 处理技术  │ │ 处理退款  │ │ 处理购买  │
       │ 故障排查  │ │ 换货投诉  │ │ 产品推荐  │
       └──────────┘ └──────────┘ └──────────┘

  路由依据：用户输入的意图分类
  每个专家 Agent 有独立的 System Prompt、Tools、知识库
</code></pre>
<p><strong>工作流程</strong>：</p>
<ol>
<li>Router Agent 接收用户输入</li>
<li>根据意图分类，将请求路由到对应的专家 Agent</li>
<li>专家 Agent 处理请求并返回结果</li>
<li>必要时 Router 可以在专家之间进行二次路由</li>
</ol>
<p><strong>核心特征</strong>：</p>
<ul>
<li>一个轻量级的 Router 做决策</li>
<li>多个重量级的专家 Agent 做执行</li>
<li>Router 可以用简单模型（快速、便宜），专家用强大模型（准确、深入）</li>
</ul>
<p><strong>适用场景</strong>：客服系统（技术问题 -&gt; 技术 Agent，退款问题 -&gt; 售后 Agent）、多领域知识问答（医疗问题 -&gt; 医疗 Agent，法律问题 -&gt; 法律 Agent）、代码助手（Python 问题 -&gt; Python 专家，Rust 问题 -&gt; Rust 专家）。</p>
<p><strong>Trade-off</strong>：路由准确率是整个系统的瓶颈——路由错了，后面再专业也没用。模糊意图（&quot;我买的东西有技术问题&quot;——这是技术支持还是售后？）需要特殊处理。一种常见策略是允许 Router 在不确定时同时咨询多个专家，再综合判断。</p>
<h3>2.5 四种模式的对比决策</h3>
<table>
<thead>
<tr>
<th>维度</th>
<th>Supervisor-Worker</th>
<th>Peer-to-Peer</th>
<th>Pipeline</th>
<th>Dynamic Routing</th>
</tr>
</thead>
<tbody><tr>
<td>控制结构</td>
<td>中心化</td>
<td>去中心化</td>
<td>线性</td>
<td>分发型</td>
</tr>
<tr>
<td>通信模式</td>
<td>星形</td>
<td>网状</td>
<td>链式</td>
<td>扇出</td>
</tr>
<tr>
<td>并行度</td>
<td>高（Worker 并行）</td>
<td>中</td>
<td>低（严格串行）</td>
<td>高（请求级并行）</td>
</tr>
<tr>
<td>适用复杂度</td>
<td>高</td>
<td>中</td>
<td>中</td>
<td>低-中</td>
</tr>
<tr>
<td>调试难度</td>
<td>中</td>
<td>高</td>
<td>低</td>
<td>低</td>
</tr>
<tr>
<td>典型场景</td>
<td>报告生成、项目规划</td>
<td>辩论、审查</td>
<td>内容流水线</td>
<td>客服、问答路由</td>
</tr>
</tbody></table>
<p><strong>决策原则</strong>：</p>
<ul>
<li>任务可以并行分解 -&gt; Supervisor-Worker</li>
<li>需要多视角碰撞 -&gt; Peer-to-Peer</li>
<li>处理有明确阶段 -&gt; Pipeline</li>
<li>请求类型多样，专家各有擅长 -&gt; Dynamic Routing</li>
<li>不确定？先从最简单的 Pipeline 开始，逐步演进</li>
</ul>
<hr>
<h2>3. Agent 间通信机制</h2>
<p>多个 Agent 之间需要交换信息，通信机制的选择直接影响系统的可扩展性、耦合度和调试难度。</p>
<h3>3.1 共享内存（Blackboard Pattern）</h3>
<p>所有 Agent 读写同一个共享状态存储。这是最简单直接的通信方式。</p>
<pre><code>       ┌──────────┐   ┌──────────┐   ┌──────────┐
       │ Agent A  │   │ Agent B  │   │ Agent C  │
       └────┬─────┘   └────┬─────┘   └────┬─────┘
            │  read/write   │  read/write   │
            ▼              ▼              ▼
       ┌──────────────────────────────────────────┐
       │           Shared Blackboard              │
       │                                          │
       │  { &quot;search_results&quot;: [...],              │
       │    &quot;analysis&quot;: {...},                    │
       │    &quot;draft&quot;: &quot;...&quot;,                       │
       │    &quot;status&quot;: {&quot;search&quot;: &quot;done&quot;, ...} }   │
       └──────────────────────────────────────────┘
</code></pre>
<pre><code class="language-python">from dataclasses import dataclass, field
from typing import Any
import threading


@dataclass
class Blackboard:
    &quot;&quot;&quot;共享黑板：所有 Agent 的公共状态空间&quot;&quot;&quot;
    _state: dict[str, Any] = field(default_factory=dict)
    _lock: threading.Lock = field(default_factory=threading.Lock)
    _history: list[dict] = field(default_factory=list)

    def read(self, key: str) -&gt; Any:
        with self._lock:
            return self._state.get(key)

    def write(self, key: str, value: Any, author: str = &quot;unknown&quot;):
        with self._lock:
            self._history.append({
                &quot;action&quot;: &quot;write&quot;,
                &quot;key&quot;: key,
                &quot;author&quot;: author,
                &quot;old_value&quot;: self._state.get(key),
                &quot;new_value&quot;: value,
            })
            self._state[key] = value

    def read_all(self) -&gt; dict[str, Any]:
        with self._lock:
            return dict(self._state)
</code></pre>
<p><strong>优点</strong>：实现简单，Agent 之间完全解耦（不需要知道彼此的存在），天然支持任意读写模式。</p>
<p><strong>缺点</strong>：共享状态意味着潜在的竞争条件——两个 Agent 同时写同一个 key 怎么办？需要锁机制或更复杂的冲突解决策略。随着 Agent 数量增加，Blackboard 可能成为瓶颈。</p>
<h3>3.2 消息传递（Message Passing）</h3>
<p>Agent 之间通过显式的消息进行通信。每个 Agent 有自己的收件箱。</p>
<pre><code>       ┌──────────┐         ┌──────────┐
       │ Agent A  │──msg───▶│ Agent B  │
       │          │◀──msg───│          │
       └──────────┘         └──────────┘
            │                     ▲
            │         msg         │
            ▼                     │
       ┌──────────┐              │
       │ Agent C  │──────msg─────┘
       └──────────┘
</code></pre>
<pre><code class="language-python">from dataclasses import dataclass, field
from collections import defaultdict
from queue import Queue


@dataclass
class Message:
    sender: str
    receiver: str
    content: Any
    msg_type: str = &quot;default&quot;  # &quot;task&quot;, &quot;result&quot;, &quot;feedback&quot;, &quot;error&quot;


class MessageBus:
    &quot;&quot;&quot;点对点消息传递&quot;&quot;&quot;

    def __init__(self):
        self._queues: dict[str, Queue] = defaultdict(Queue)

    def send(self, message: Message):
        self._queues[message.receiver].put(message)

    def receive(self, agent_id: str, timeout: float = None) -&gt; Message | None:
        try:
            return self._queues[agent_id].get(timeout=timeout)
        except Exception:
            return None

    def has_messages(self, agent_id: str) -&gt; bool:
        return not self._queues[agent_id].empty()
</code></pre>
<p><strong>优点</strong>：通信关系显式、可追踪、可审计。每条消息都有明确的发送者和接收者。</p>
<p><strong>缺点</strong>：Agent 需要知道其他 Agent 的存在（至少知道 ID），耦合度比 Blackboard 高。如果通信拓扑复杂（多对多），消息管理会变得困难。</p>
<h3>3.3 事件驱动（Event Bus）</h3>
<p>Agent 通过发布/订阅事件进行间接通信。Agent 不需要知道谁会消费它的事件。</p>
<pre><code>       ┌──────────┐   ┌──────────┐   ┌──────────┐
       │ Agent A  │   │ Agent B  │   │ Agent C  │
       │ pub: X   │   │ sub: X   │   │ sub: X,Y │
       └────┬─────┘   └────┬─────┘   └────┬─────┘
            │  publish      │  subscribe   │
            ▼              ▼              ▼
       ┌──────────────────────────────────────────┐
       │              Event Bus                    │
       │                                          │
       │  topic &quot;search_done&quot;  → [Agent B, C]     │
       │  topic &quot;analysis_done&quot; → [Agent C]        │
       │  topic &quot;error&quot;        → [Supervisor]      │
       └──────────────────────────────────────────┘
</code></pre>
<pre><code class="language-python">from collections import defaultdict
from typing import Callable


class EventBus:
    &quot;&quot;&quot;发布/订阅事件总线&quot;&quot;&quot;

    def __init__(self):
        self._subscribers: dict[str, list[Callable]] = defaultdict(list)
        self._event_log: list[dict] = []

    def subscribe(self, topic: str, handler: Callable):
        self._subscribers[topic].append(handler)

    def publish(self, topic: str, data: Any, publisher: str = &quot;unknown&quot;):
        event = {&quot;topic&quot;: topic, &quot;data&quot;: data, &quot;publisher&quot;: publisher}
        self._event_log.append(event)
        for handler in self._subscribers.get(topic, []):
            handler(event)

    def get_event_log(self) -&gt; list[dict]:
        return list(self._event_log)
</code></pre>
<p><strong>优点</strong>：Agent 之间完全解耦——发布者不知道有谁在监听，订阅者不知道事件从哪里来。扩展性好，新增 Agent 只需订阅相关事件。</p>
<p><strong>缺点</strong>：事件流难以追踪——&quot;这个事件是谁发的？谁处理了？处理结果在哪里？&quot;调试时需要完整的事件日志。事件顺序可能不确定，需要额外的排序机制。</p>
<h3>3.4 通信机制对比</h3>
<table>
<thead>
<tr>
<th>维度</th>
<th>Blackboard</th>
<th>Message Passing</th>
<th>Event Bus</th>
</tr>
</thead>
<tbody><tr>
<td>耦合度</td>
<td>低（通过 key 间接通信）</td>
<td>中（需要知道目标 Agent）</td>
<td>低（通过 topic 间接通信）</td>
</tr>
<tr>
<td>实现复杂度</td>
<td>低</td>
<td>中</td>
<td>中</td>
</tr>
<tr>
<td>调试友好度</td>
<td>中（看状态快照）</td>
<td>高（消息链路清晰）</td>
<td>低（事件流分散）</td>
</tr>
<tr>
<td>并发安全</td>
<td>需要锁/MVCC</td>
<td>天然安全（队列隔离）</td>
<td>需要考虑处理顺序</td>
</tr>
<tr>
<td>适用模式</td>
<td>Supervisor-Worker</td>
<td>Peer-to-Peer</td>
<td>Pipeline, 事件驱动架构</td>
</tr>
<tr>
<td>可观测性</td>
<td>状态快照</td>
<td>消息轨迹</td>
<td>事件日志</td>
</tr>
</tbody></table>
<p><strong>实践建议</strong>：大多数 Multi-Agent 系统可以从 Blackboard 开始——它最简单，且对 Supervisor-Worker 模式特别友好。当系统复杂度增长到需要解耦 Agent 间关系时，再考虑 Event Bus。Message Passing 适合 Agent 之间有明确的、频繁的双向交互的场景。</p>
<hr>
<h2>4. 完整实现：Supervisor-Worker 协作框架</h2>
<p>下面用 Python 从零实现一个 Supervisor-Worker 框架。这不依赖任何 Agent 框架，完全基于第一性原理构建。</p>
<h3>4.1 基础抽象</h3>
<pre><code class="language-python">import json
import asyncio
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Any


# ---- LLM 调用抽象（与具体 SDK 解耦）----

async def call_llm(
    messages: list[dict],
    model: str = &quot;gpt-4o&quot;,
    response_format: dict | None = None,
) -&gt; str:
    &quot;&quot;&quot;LLM 调用的统一接口（简化版，生产中替换为真实 SDK 调用）&quot;&quot;&quot;
    import openai
    client = openai.AsyncOpenAI()
    kwargs = {&quot;model&quot;: model, &quot;messages&quot;: messages}
    if response_format:
        kwargs[&quot;response_format&quot;] = response_format
    response = await client.chat.completions.create(**kwargs)
    return response.choices[0].message.content


# ---- 任务与结果的数据结构 ----

@dataclass
class Task:
    &quot;&quot;&quot;一个可执行的子任务&quot;&quot;&quot;
    task_id: str
    description: str
    assigned_to: str = &quot;&quot;          # Worker Agent 名称
    context: dict = field(default_factory=dict)  # 来自上游的上下文
    status: str = &quot;pending&quot;        # pending | running | done | failed
    result: str = &quot;&quot;
    error: str = &quot;&quot;


@dataclass
class TeamResult:
    &quot;&quot;&quot;团队执行的最终结果&quot;&quot;&quot;
    success: bool
    output: str
    tasks: list[Task]
    total_tokens: int = 0
    total_llm_calls: int = 0
</code></pre>
<h3>4.2 Worker Agent</h3>
<p>每个 Worker 是一个专注于特定领域的 Agent，拥有独立的 System Prompt 和能力边界。</p>
<pre><code class="language-python">class WorkerAgent:
    &quot;&quot;&quot;Worker Agent：接收子任务，独立执行，返回结果&quot;&quot;&quot;

    def __init__(self, name: str, system_prompt: str, model: str = &quot;gpt-4o&quot;):
        self.name = name
        self.system_prompt = system_prompt
        self.model = model
        self._call_count = 0

    async def execute(self, task: Task) -&gt; Task:
        &quot;&quot;&quot;执行一个子任务&quot;&quot;&quot;
        task.status = &quot;running&quot;
        task.assigned_to = self.name

        messages = [
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: self.system_prompt},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: self._build_prompt(task)},
        ]

        try:
            result = await call_llm(messages, model=self.model)
            self._call_count += 1
            task.result = result
            task.status = &quot;done&quot;
        except Exception as e:
            task.error = str(e)
            task.status = &quot;failed&quot;

        return task

    def _build_prompt(self, task: Task) -&gt; str:
        prompt = f&quot;## 任务\n{task.description}\n&quot;
        if task.context:
            prompt += f&quot;\n## 上下文信息\n{json.dumps(task.context, ensure_ascii=False, indent=2)}\n&quot;
        prompt += &quot;\n请完成上述任务，直接输出结果。&quot;
        return prompt
</code></pre>
<h3>4.3 Supervisor Agent</h3>
<p>Supervisor 负责三件事：任务分解、任务分配、结果合成。</p>
<pre><code class="language-python">DECOMPOSE_PROMPT = &quot;&quot;&quot;你是一个任务分解专家。给定一个复杂任务，将其分解为可以独立执行的子任务。

可用的 Worker 及其能力：
{workers_description}

请将任务分解为子任务，并指定每个子任务应该分配给哪个 Worker。
输出 JSON 格式：
{{
  &quot;subtasks&quot;: [
    {{
      &quot;task_id&quot;: &quot;task_1&quot;,
      &quot;description&quot;: &quot;具体的子任务描述&quot;,
      &quot;assigned_to&quot;: &quot;worker 名称&quot;,
      &quot;depends_on&quot;: []
    }}
  ]
}}

注意：
- 每个子任务应该足够具体，让 Worker 能独立完成
- depends_on 标明依赖关系（某个子任务需要等另一个完成后才能开始）
- 尽可能让子任务并行执行以提高效率
&quot;&quot;&quot;

SYNTHESIZE_PROMPT = &quot;&quot;&quot;你是一个结果合成专家。多个专业 Agent 已经分别完成了子任务。
请根据它们的结果，合成一个完整、连贯、高质量的最终输出。

原始任务：{original_task}

各子任务的执行结果：
{subtask_results}

请整合以上信息，生成最终的完整输出。确保：
1. 信息完整，没有遗漏
2. 逻辑连贯，前后一致
3. 去除重复内容
4. 保持专业质量
&quot;&quot;&quot;


class SupervisorAgent:
    &quot;&quot;&quot;Supervisor Agent：任务分解、分配、合成&quot;&quot;&quot;

    def __init__(self, model: str = &quot;gpt-4o&quot;):
        self.model = model
        self._call_count = 0

    async def decompose(
        self, task: str, workers: dict[str, WorkerAgent]
    ) -&gt; list[Task]:
        &quot;&quot;&quot;将复杂任务分解为子任务&quot;&quot;&quot;
        workers_desc = &quot;\n&quot;.join(
            f&quot;- {name}: {w.system_prompt[:200]}&quot;
            for name, w in workers.items()
        )

        messages = [
            {
                &quot;role&quot;: &quot;system&quot;,
                &quot;content&quot;: DECOMPOSE_PROMPT.format(
                    workers_description=workers_desc
                ),
            },
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: task},
        ]

        result = await call_llm(
            messages,
            model=self.model,
            response_format={&quot;type&quot;: &quot;json_object&quot;},
        )
        self._call_count += 1

        parsed = json.loads(result)
        tasks = []
        for st in parsed.get(&quot;subtasks&quot;, []):
            tasks.append(Task(
                task_id=st[&quot;task_id&quot;],
                description=st[&quot;description&quot;],
                assigned_to=st.get(&quot;assigned_to&quot;, &quot;&quot;),
            ))
        return tasks

    async def synthesize(
        self, original_task: str, completed_tasks: list[Task]
    ) -&gt; str:
        &quot;&quot;&quot;合成所有 Worker 的结果&quot;&quot;&quot;
        results_text = &quot;\n\n&quot;.join(
            f&quot;### {t.task_id} ({t.assigned_to})\n{t.result}&quot;
            for t in completed_tasks
            if t.status == &quot;done&quot;
        )

        messages = [
            {
                &quot;role&quot;: &quot;system&quot;,
                &quot;content&quot;: SYNTHESIZE_PROMPT.format(
                    original_task=original_task,
                    subtask_results=results_text,
                ),
            },
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请合成最终结果。&quot;},
        ]

        result = await call_llm(messages, model=self.model)
        self._call_count += 1
        return result
</code></pre>
<h3>4.4 AgentTeam：编排层</h3>
<p>AgentTeam 管理多个 Agent 的生命周期、通信和执行流程。</p>
<pre><code class="language-python">class AgentTeam:
    &quot;&quot;&quot;Agent 团队：管理 Supervisor + Workers 的协作&quot;&quot;&quot;

    def __init__(self, supervisor: SupervisorAgent):
        self.supervisor = supervisor
        self.workers: dict[str, WorkerAgent] = {}
        self.blackboard = Blackboard()
        self.execution_log: list[dict] = []

    def add_worker(self, worker: WorkerAgent):
        self.workers[worker.name] = worker

    async def run(self, task: str, max_retries: int = 2) -&gt; TeamResult:
        &quot;&quot;&quot;执行完整的 Multi-Agent 协作流程&quot;&quot;&quot;
        self._log(&quot;team&quot;, f&quot;接收任务: {task[:100]}...&quot;)

        # Phase 1: Supervisor 分解任务
        self._log(&quot;supervisor&quot;, &quot;开始任务分解&quot;)
        subtasks = await self.supervisor.decompose(task, self.workers)
        self._log(&quot;supervisor&quot;, f&quot;分解为 {len(subtasks)} 个子任务&quot;)

        for st in subtasks:
            self._log(&quot;supervisor&quot;, f&quot;  {st.task_id} -&gt; {st.assigned_to}: {st.description[:80]}&quot;)

        # Phase 2: Workers 并行执行（考虑依赖关系）
        completed = await self._execute_tasks(subtasks, max_retries)

        # Phase 3: Supervisor 合成结果
        self._log(&quot;supervisor&quot;, &quot;开始合成结果&quot;)
        final_output = await self.supervisor.synthesize(task, completed)
        self._log(&quot;supervisor&quot;, &quot;合成完成&quot;)

        # 汇总统计
        total_calls = self.supervisor._call_count + sum(
            w._call_count for w in self.workers.values()
        )

        return TeamResult(
            success=all(t.status == &quot;done&quot; for t in completed),
            output=final_output,
            tasks=completed,
            total_llm_calls=total_calls,
        )

    async def _execute_tasks(
        self, tasks: list[Task], max_retries: int
    ) -&gt; list[Task]:
        &quot;&quot;&quot;执行子任务，支持并行和重试&quot;&quot;&quot;
        completed = []
        pending = list(tasks)

        while pending:
            # 找出当前可以执行的任务（依赖已满足）
            ready = []
            still_pending = []
            completed_ids = {t.task_id for t in completed}

            for task in pending:
                deps = task.context.get(&quot;depends_on&quot;, [])
                if all(d in completed_ids for d in deps):
                    ready.append(task)
                else:
                    still_pending.append(task)

            if not ready:
                # 没有可执行的任务但还有待处理的 -&gt; 可能存在循环依赖
                self._log(&quot;team&quot;, &quot;警告: 检测到无法满足的依赖关系&quot;)
                break

            # 并行执行所有就绪的任务
            results = await asyncio.gather(*[
                self._execute_single(task, max_retries)
                for task in ready
            ])

            for task in results:
                completed.append(task)
                # 将结果写入 Blackboard，供后续任务使用
                if task.status == &quot;done&quot;:
                    self.blackboard.write(
                        task.task_id, task.result, author=task.assigned_to
                    )

            pending = still_pending

        return completed

    async def _execute_single(
        self, task: Task, max_retries: int
    ) -&gt; Task:
        &quot;&quot;&quot;执行单个任务，带重试&quot;&quot;&quot;
        worker = self.workers.get(task.assigned_to)
        if not worker:
            task.status = &quot;failed&quot;
            task.error = f&quot;未找到 Worker: {task.assigned_to}&quot;
            return task

        # 将 Blackboard 上的相关信息注入任务上下文
        task.context[&quot;blackboard&quot;] = self.blackboard.read_all()

        for attempt in range(max_retries + 1):
            self._log(worker.name, f&quot;执行 {task.task_id} (尝试 {attempt + 1})&quot;)
            result = await worker.execute(task)

            if result.status == &quot;done&quot;:
                self._log(worker.name, f&quot;{task.task_id} 完成&quot;)
                return result

            self._log(worker.name, f&quot;{task.task_id} 失败: {result.error}&quot;)

            if attempt &lt; max_retries:
                self._log(worker.name, f&quot;准备重试 {task.task_id}&quot;)

        return result

    def _log(self, source: str, message: str):
        entry = {&quot;source&quot;: source, &quot;message&quot;: message}
        self.execution_log.append(entry)
</code></pre>
<h3>4.5 组装示例：技术调研报告</h3>
<pre><code class="language-python">async def main():
    &quot;&quot;&quot;示例：用 Multi-Agent 团队撰写一篇技术调研报告&quot;&quot;&quot;

    # 创建 Supervisor
    supervisor = SupervisorAgent(model=&quot;gpt-4o&quot;)

    # 创建专业化的 Worker Agent
    search_agent = WorkerAgent(
        name=&quot;searcher&quot;,
        system_prompt=(
            &quot;你是一个信息搜索专家。你的任务是根据给定的主题，&quot;
            &quot;整理出全面的信息摘要，包括关键事实、数据、案例。&quot;
            &quot;输出结构化的搜索结果，标注来源和可信度。&quot;
        ),
    )

    analyze_agent = WorkerAgent(
        name=&quot;analyst&quot;,
        system_prompt=(
            &quot;你是一个技术分析专家。你的任务是根据搜索结果和原始数据，&quot;
            &quot;进行深度分析，提炼洞察，识别趋势、风险和机会。&quot;
            &quot;输出包含数据支撑的分析报告。&quot;
        ),
    )

    write_agent = WorkerAgent(
        name=&quot;writer&quot;,
        system_prompt=(
            &quot;你是一个技术写作专家。你的任务是根据分析结果，&quot;
            &quot;撰写结构清晰、逻辑严谨、可读性强的技术报告。&quot;
            &quot;确保使用专业术语，并配有合适的章节结构。&quot;
        ),
    )

    # 组建团队
    team = AgentTeam(supervisor=supervisor)
    team.add_worker(search_agent)
    team.add_worker(analyze_agent)
    team.add_worker(write_agent)

    # 执行任务
    result = await team.run(
        &quot;撰写一篇关于 LLM Agent 在企业客服场景落地的技术调研报告，&quot;
        &quot;包括行业现状、主流技术方案对比、落地挑战和建议。&quot;
    )

    print(f&quot;成功: {result.success}&quot;)
    print(f&quot;LLM 调用次数: {result.total_llm_calls}&quot;)
    print(f&quot;\n最终输出:\n{result.output[:500]}...&quot;)

    # 查看执行日志
    print(&quot;\n执行链路:&quot;)
    for entry in team.execution_log:
        print(f&quot;  [{entry[&#39;source&#39;]}] {entry[&#39;message&#39;]}&quot;)


# asyncio.run(main())
</code></pre>
<p>这段代码展示了核心的协作模式。生产系统中还需要补充：Token 用量追踪、超时控制、Worker 健康检查、结果缓存等。但架构骨架已经清晰——Supervisor 负责全局调度，Worker 负责局部执行，Blackboard 负责状态共享，AgentTeam 负责生命周期管理。</p>
<hr>
<h2>5. 状态管理的复杂性</h2>
<p>Multi-Agent 系统的状态管理比 Single-Agent 复杂一个数量级。核心难题在于：多个 Agent 同时操作状态，如何保证一致性。</p>
<h3>5.1 共享状态 vs 独立状态</h3>
<pre><code>方案 A：共享状态                     方案 B：独立状态
┌─────────────────┐                ┌──────────┐  ┌──────────┐  ┌──────────┐
│  Global State   │                │ State A  │  │ State B  │  │ State C  │
│                 │                │ (Agent A │  │ (Agent B │  │ (Agent C │
│ Agent A ──write │                │  独占)   │  │  独占)   │  │  独占)   │
│ Agent B ──write │                └──────────┘  └──────────┘  └──────────┘
│ Agent C ──write │                      │              │              │
└─────────────────┘                      └──────────────┼──────────────┘
                                                        ▼
                                                  合并/同步层
</code></pre>
<p><strong>共享状态</strong>的优点是 Agent 之间信息同步即时，任何 Agent 都能看到最新全局状态。缺点是需要处理并发冲突。适合 Supervisor-Worker 模式——Supervisor 需要看到所有 Worker 的进度。</p>
<p><strong>独立状态</strong>的优点是无并发问题，每个 Agent 完全自主。缺点是 Agent 之间信息同步有延迟，需要显式的合并机制。适合 Pipeline 模式——每个阶段独立处理，只在交接时传递状态。</p>
<h3>5.2 冲突解决策略</h3>
<p>当两个 Agent 同时修改同一个状态时，需要冲突解决。常见策略：</p>
<pre><code class="language-python">class ConflictResolver:
    &quot;&quot;&quot;状态冲突解决器&quot;&quot;&quot;

    @staticmethod
    def last_writer_wins(old_value, new_value_a, new_value_b, timestamp_a, timestamp_b):
        &quot;&quot;&quot;最后写入者胜出——简单但可能丢失数据&quot;&quot;&quot;
        return new_value_a if timestamp_a &gt; timestamp_b else new_value_b

    @staticmethod
    def merge_append(old_value, new_value_a, new_value_b):
        &quot;&quot;&quot;合并追加——适用于列表类型的状态&quot;&quot;&quot;
        if isinstance(old_value, list):
            merged = list(old_value)
            if isinstance(new_value_a, list):
                merged.extend(new_value_a)
            if isinstance(new_value_b, list):
                merged.extend(new_value_b)
            return merged
        return new_value_b  # fallback

    @staticmethod
    async def llm_resolve(old_value, new_value_a, new_value_b, context: str):
        &quot;&quot;&quot;用 LLM 判断如何合并冲突——最灵活但最贵&quot;&quot;&quot;
        prompt = (
            f&quot;两个 Agent 同时修改了同一个状态。\n&quot;
            f&quot;原始值: {old_value}\n&quot;
            f&quot;Agent A 的修改: {new_value_a}\n&quot;
            f&quot;Agent B 的修改: {new_value_b}\n&quot;
            f&quot;上下文: {context}\n&quot;
            f&quot;请决定最终值应该是什么，并解释原因。&quot;
        )
        return await call_llm([{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}])
</code></pre>
<p>实践中，大多数 Multi-Agent 系统通过架构设计来避免冲突，而不是在运行时解决冲突。最有效的方法是<strong>状态分区</strong>——每个 Agent 只写自己负责的状态区域，避免多 Agent 写同一个 key。这也是 Supervisor-Worker 模式天然的优势：每个 Worker 写自己的结果 key，只有 Supervisor 读所有 key。</p>
<hr>
<h2>6. 错误处理与容错</h2>
<p>Multi-Agent 系统的错误处理比 Single-Agent 更复杂，因为错误的传播路径更多。</p>
<h3>6.1 Worker 失败</h3>
<p>Worker 失败是最常见的情况。处理策略按优先级：</p>
<pre><code>Worker 失败处理决策树：

  Worker 执行失败
       │
       ▼
  ┌─ 是否可重试？ ─── 是 ──→ 重试（最多 N 次）──→ 成功？──→ 继续
  │      │                                          │
  │     否                                         否
  │      │                                          │
  │      ▼                                          ▼
  │  ┌─ 有替代 Worker？ ─── 是 ──→ 分配给替代 Worker
  │  │      │
  │  │     否
  │  │      │
  │  │      ▼
  │  │  ┌─ 该子任务是关键路径？
  │  │  │      │            │
  │  │  │     是           否
  │  │  │      │            │
  │  │  │      ▼            ▼
  │  │  │  整体任务失败   降级处理（跳过该子任务，
  │  │  │                 标记结果为不完整）
</code></pre>
<pre><code class="language-python">class ResilientAgentTeam(AgentTeam):
    &quot;&quot;&quot;增强容错能力的 Agent 团队&quot;&quot;&quot;

    def __init__(self, supervisor: SupervisorAgent):
        super().__init__(supervisor)
        self.fallback_workers: dict[str, list[str]] = {}  # Worker 降级链

    def set_fallback(self, worker_name: str, fallbacks: list[str]):
        &quot;&quot;&quot;设置 Worker 的降级替代链&quot;&quot;&quot;
        self.fallback_workers[worker_name] = fallbacks

    async def _execute_single(self, task: Task, max_retries: int) -&gt; Task:
        &quot;&quot;&quot;增强版：支持 Worker 降级&quot;&quot;&quot;
        # 尝试主 Worker
        result = await super()._execute_single(task, max_retries)
        if result.status == &quot;done&quot;:
            return result

        # 主 Worker 失败，尝试降级 Worker
        fallbacks = self.fallback_workers.get(task.assigned_to, [])
        for fb_name in fallbacks:
            self._log(&quot;team&quot;, f&quot;降级: {task.assigned_to} -&gt; {fb_name}&quot;)
            task.assigned_to = fb_name
            task.status = &quot;pending&quot;
            task.error = &quot;&quot;
            result = await super()._execute_single(task, max_retries=1)
            if result.status == &quot;done&quot;:
                return result

        return result
</code></pre>
<h3>6.2 Supervisor 失败</h3>
<p>Supervisor 失败更严重——它是中央协调者，失败意味着整个任务无法继续。处理策略：</p>
<ul>
<li><strong>外部监控</strong>：在 AgentTeam 之上设置一个非 LLM 的监控层，检测 Supervisor 的健康状态</li>
<li><strong>Supervisor 冗余</strong>：准备一个备用 Supervisor（可以用不同的模型），主 Supervisor 失败时切换</li>
<li><strong>Checkpoint 机制</strong>：Supervisor 在每个决策点保存状态快照，失败后从最近的 Checkpoint 恢复</li>
</ul>
<pre><code class="language-python">async def run_with_checkpoint(self, task: str) -&gt; TeamResult:
    &quot;&quot;&quot;带 Checkpoint 的执行流程&quot;&quot;&quot;
    checkpoint = {&quot;phase&quot;: &quot;init&quot;, &quot;subtasks&quot;: [], &quot;completed&quot;: []}

    try:
        # Phase 1: 分解
        checkpoint[&quot;phase&quot;] = &quot;decompose&quot;
        subtasks = await self.supervisor.decompose(task, self.workers)
        checkpoint[&quot;subtasks&quot;] = subtasks

        # Phase 2: 执行
        checkpoint[&quot;phase&quot;] = &quot;execute&quot;
        completed = await self._execute_tasks(subtasks, max_retries=2)
        checkpoint[&quot;completed&quot;] = completed

        # Phase 3: 合成
        checkpoint[&quot;phase&quot;] = &quot;synthesize&quot;
        output = await self.supervisor.synthesize(task, completed)

        return TeamResult(success=True, output=output, tasks=completed)

    except Exception as e:
        self._log(&quot;team&quot;, f&quot;失败于阶段 {checkpoint[&#39;phase&#39;]}: {e}&quot;)
        # 可以从 checkpoint 恢复，跳过已完成的阶段
        return TeamResult(
            success=False,
            output=f&quot;任务在 {checkpoint[&#39;phase&#39;]} 阶段失败: {e}&quot;,
            tasks=checkpoint.get(&quot;completed&quot;, []),
        )
</code></pre>
<h3>6.3 死锁检测</h3>
<p>在 Peer-to-Peer 模式中，两个 Agent 可能互相等待对方的回复，形成死锁。</p>
<pre><code>死锁场景：

  Agent A: &quot;请 Agent B 先确认方案&quot;
           ↓ 等待 B
  Agent B: &quot;请 Agent A 先提供数据&quot;
           ↓ 等待 A
  → 无限等待
</code></pre>
<p>解决方案：</p>
<pre><code class="language-python">class DeadlockDetector:
    &quot;&quot;&quot;简单的死锁检测器&quot;&quot;&quot;

    def __init__(self, timeout_seconds: float = 60):
        self.timeout = timeout_seconds
        self._waiting: dict[str, str] = {}  # agent_id -&gt; waiting_for_agent_id

    def register_wait(self, agent_id: str, waiting_for: str):
        self._waiting[agent_id] = waiting_for
        # 检测环形等待
        if self._has_cycle(agent_id):
            raise DeadlockError(
                f&quot;检测到死锁: {self._trace_cycle(agent_id)}&quot;
            )

    def _has_cycle(self, start: str) -&gt; bool:
        visited = set()
        current = start
        while current in self._waiting:
            if current in visited:
                return True
            visited.add(current)
            current = self._waiting[current]
        return False

    def _trace_cycle(self, start: str) -&gt; str:
        chain = [start]
        current = self._waiting.get(start, &quot;&quot;)
        while current != start and current:
            chain.append(current)
            current = self._waiting.get(current, &quot;&quot;)
        chain.append(start)
        return &quot; -&gt; &quot;.join(chain)


class DeadlockError(Exception):
    pass
</code></pre>
<hr>
<h2>7. Multi-Agent 的成本问题</h2>
<p>成本是 Multi-Agent 系统必须正视的问题。它不只是&quot;贵一点&quot;的问题——可能是&quot;贵一个数量级&quot;的问题。</p>
<h3>7.1 成本模型</h3>
<pre><code>Single-Agent 执行一个任务的 Token 消耗：

  1 x System Prompt   +  N x (Context + Response)
  ~1,000 tokens          ~3,000 tokens x 5 iterations
                         = ~16,000 tokens


Multi-Agent (Supervisor + 3 Workers) 的 Token 消耗：

  Supervisor 分解:   ~4,000 tokens   (System Prompt + 任务分解)
  Worker A 执行:     ~8,000 tokens   (System Prompt + 执行)
  Worker B 执行:     ~8,000 tokens   (System Prompt + 执行)
  Worker C 执行:     ~8,000 tokens   (System Prompt + 执行)
  Supervisor 合成:   ~6,000 tokens   (收集所有结果 + 合成)
                     ──────────────
  Total:             ~34,000 tokens   ← 约 2x Single-Agent

  如果 Worker 内部也有多轮迭代，消耗会更高。
</code></pre>
<h3>7.2 什么时候 Multi-Agent 的收益大于成本</h3>
<p>不是所有场景都值得用 Multi-Agent。一个简单的决策框架：</p>
<pre><code>                        任务复杂度
                    低 ─────────── 高
                    │               │
  专业化需求  低    │  Single-Agent │  Single-Agent
              │    │  (够用)       │  + Better Prompt
              │    │               │
              高    │  Single-Agent │  Multi-Agent ✓
                    │  + Tools      │  (值得投入)
                    │               │
</code></pre>
<p>Multi-Agent 在以下条件下收益最大：</p>
<ol>
<li><strong>任务天然可并行</strong>：子任务之间独立性高，Multi-Agent 通过并行执行缩短总耗时，即使 token 消耗增加，时间成本下降</li>
<li><strong>专业化收益显著</strong>：专家 Agent 在自己的领域比通用 Agent 的输出质量显著更高，质量提升值得额外成本</li>
<li><strong>Single-Agent 已经到达能力瓶颈</strong>：Context Window 不够、单个 prompt 角色冲突、输出质量不稳定</li>
<li><strong>任务的商业价值足够高</strong>：生成一份价值数万元的分析报告，多花几美元的 API 费用是可以接受的</li>
</ol>
<h3>7.3 成本优化策略</h3>
<pre><code class="language-python">class CostAwareTeam(AgentTeam):
    &quot;&quot;&quot;成本感知的 Agent 团队&quot;&quot;&quot;

    def __init__(self, supervisor, token_budget: int = 100_000):
        super().__init__(supervisor)
        self.token_budget = token_budget
        self.token_used = 0

    def _select_model_for_task(self, task: Task) -&gt; str:
        &quot;&quot;&quot;根据任务复杂度选择模型——不是所有子任务都需要最强模型&quot;&quot;&quot;
        if task.context.get(&quot;complexity&quot;) == &quot;low&quot;:
            return &quot;gpt-4o-mini&quot;     # 简单任务用小模型
        elif task.context.get(&quot;complexity&quot;) == &quot;high&quot;:
            return &quot;gpt-4o&quot;          # 复杂任务用大模型
        else:
            return &quot;gpt-4o-mini&quot;     # 默认用小模型，够用即可

    def _should_continue(self) -&gt; bool:
        &quot;&quot;&quot;预算检查&quot;&quot;&quot;
        if self.token_used &gt;= self.token_budget:
            self._log(&quot;team&quot;, f&quot;Token 预算耗尽 ({self.token_used}/{self.token_budget})&quot;)
            return False
        return True
</code></pre>
<p>关键原则：<strong>Router 和 Supervisor 可以用轻量模型，只有需要深度推理的 Worker 才用重量级模型。</strong> 这类似人类组织中，项目经理不需要是技术最强的人，但专家必须在各自领域足够专业。</p>
<hr>
<h2>8. Multi-Agent 的调试挑战</h2>
<p>Multi-Agent 系统的调试难度是 Single-Agent 的平方级增长——不仅每个 Agent 内部可能出错，Agent 之间的交互也可能出错。</p>
<h3>8.1 执行链路追踪</h3>
<p>每次 Multi-Agent 执行都应该生成一个完整的 Trace，记录每个 Agent 的每次 LLM 调用、输入、输出和耗时。</p>
<pre><code class="language-python">import time
import uuid
from dataclasses import dataclass, field


@dataclass
class Span:
    &quot;&quot;&quot;一个执行跨度（对应一次 Agent 操作）&quot;&quot;&quot;
    span_id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])
    parent_id: str = &quot;&quot;
    agent_name: str = &quot;&quot;
    operation: str = &quot;&quot;          # &quot;decompose&quot;, &quot;execute&quot;, &quot;synthesize&quot;
    input_summary: str = &quot;&quot;
    output_summary: str = &quot;&quot;
    start_time: float = 0.0
    end_time: float = 0.0
    token_count: int = 0
    status: str = &quot;running&quot;      # running | done | failed
    children: list = field(default_factory=list)

    @property
    def duration_ms(self) -&gt; float:
        return (self.end_time - self.start_time) * 1000


class Tracer:
    &quot;&quot;&quot;Multi-Agent 执行链路追踪器&quot;&quot;&quot;

    def __init__(self):
        self.root_span: Span | None = None
        self._span_stack: list[Span] = []

    def start_span(self, agent_name: str, operation: str, input_summary: str = &quot;&quot;) -&gt; Span:
        span = Span(
            agent_name=agent_name,
            operation=operation,
            input_summary=input_summary[:200],
            start_time=time.time(),
        )
        if self._span_stack:
            parent = self._span_stack[-1]
            span.parent_id = parent.span_id
            parent.children.append(span)
        else:
            self.root_span = span

        self._span_stack.append(span)
        return span

    def end_span(self, output_summary: str = &quot;&quot;, status: str = &quot;done&quot;):
        if self._span_stack:
            span = self._span_stack.pop()
            span.end_time = time.time()
            span.output_summary = output_summary[:200]
            span.status = status

    def print_trace(self, span: Span = None, indent: int = 0):
        &quot;&quot;&quot;打印可视化的执行链路&quot;&quot;&quot;
        span = span or self.root_span
        if not span:
            return

        prefix = &quot;  &quot; * indent
        status_icon = &quot;OK&quot; if span.status == &quot;done&quot; else &quot;FAIL&quot;
        print(
            f&quot;{prefix}[{status_icon}] {span.agent_name}.{span.operation} &quot;
            f&quot;({span.duration_ms:.0f}ms)&quot;
        )
        if span.input_summary:
            print(f&quot;{prefix}  IN:  {span.input_summary[:80]}&quot;)
        if span.output_summary:
            print(f&quot;{prefix}  OUT: {span.output_summary[:80]}&quot;)

        for child in span.children:
            self.print_trace(child, indent + 1)
</code></pre>
<p>输出示例：</p>
<pre><code>[OK] supervisor.decompose (2340ms)
  IN:  撰写一篇关于 LLM Agent 在企业客服场景落地的技术调研报告...
  OUT: {&quot;subtasks&quot;: [{&quot;task_id&quot;: &quot;task_1&quot;, ...}, ...]}
  [OK] searcher.execute (5120ms)
    IN:  搜索 LLM Agent 客服场景的行业现状和主流方案...
    OUT: ## 行业现状\n1. 2024 年全球智能客服市场规模...
  [OK] analyst.execute (4800ms)
    IN:  分析搜索结果，提炼关键洞察和趋势...
    OUT: ## 分析结论\n1. 技术成熟度：LLM 客服处于...
  [OK] writer.execute (6200ms)
    IN:  根据分析结果撰写完整的技术调研报告...
    OUT: # LLM Agent 企业客服落地技术调研报告\n\n## 1. 执行摘要...
[OK] supervisor.synthesize (3100ms)
  IN:  请合成最终结果。
  OUT: # LLM Agent 企业客服落地技术调研报告（终稿）...
</code></pre>
<h3>8.2 Bug 复现</h3>
<p>Multi-Agent 场景的 bug 复现特别困难，因为：</p>
<ul>
<li>LLM 输出是非确定性的——相同输入可能产生不同输出</li>
<li>Agent 之间的交互是动态的——执行路径取决于中间结果</li>
<li>并发执行的时序不确定——Worker A 和 B 谁先完成可能影响最终结果</li>
</ul>
<p>应对策略：</p>
<ol>
<li><strong>记录完整的 LLM 输入/输出</strong>：在 Trace 中保存每次 LLM 调用的完整 messages 和 response，不只是摘要</li>
<li><strong>Deterministic Replay</strong>：用固定的 seed 和 temperature=0 复现执行，或者直接 mock LLM 响应</li>
<li><strong>快照式调试</strong>：在每个 Agent 决策点保存完整的 Blackboard 状态快照，出问题时可以回溯到任意时间点</li>
</ol>
<pre><code class="language-python">class ReplayableTeam(AgentTeam):
    &quot;&quot;&quot;可回放的 Agent 团队——记录完整的 LLM 交互供复现&quot;&quot;&quot;

    def __init__(self, supervisor):
        super().__init__(supervisor)
        self._llm_recordings: list[dict] = []

    def record_llm_call(self, agent_name: str, messages: list[dict], response: str):
        self._llm_recordings.append({
            &quot;agent&quot;: agent_name,
            &quot;messages&quot;: messages,
            &quot;response&quot;: response,
            &quot;timestamp&quot;: time.time(),
        })

    def save_recording(self, path: str):
        &quot;&quot;&quot;保存录制数据，用于后续回放和调试&quot;&quot;&quot;
        with open(path, &quot;w&quot;) as f:
            json.dump(self._llm_recordings, f, ensure_ascii=False, indent=2)
</code></pre>
<h3>8.3 可观测性设计</h3>
<p>一个生产级 Multi-Agent 系统至少需要以下可观测性指标：</p>
<table>
<thead>
<tr>
<th>指标类别</th>
<th>具体指标</th>
<th>目的</th>
</tr>
</thead>
<tbody><tr>
<td><strong>延迟</strong></td>
<td>每个 Agent 的执行时间、端到端总时间</td>
<td>定位性能瓶颈</td>
</tr>
<tr>
<td><strong>成本</strong></td>
<td>每个 Agent 的 Token 消耗、总消耗</td>
<td>成本监控和预算控制</td>
</tr>
<tr>
<td><strong>质量</strong></td>
<td>任务成功率、重试次数、降级次数</td>
<td>评估系统可靠性</td>
</tr>
<tr>
<td><strong>链路</strong></td>
<td>完整的 Trace（Agent、操作、输入、输出）</td>
<td>问题排查</td>
</tr>
<tr>
<td><strong>状态</strong></td>
<td>Blackboard 的状态变更历史</td>
<td>数据流追踪</td>
</tr>
<tr>
<td><strong>通信</strong></td>
<td>Agent 间消息数量、消息大小</td>
<td>通信效率分析</td>
</tr>
</tbody></table>
<hr>
<h2>9. 设计 Multi-Agent 系统的决策清单</h2>
<p>在你决定构建 Multi-Agent 系统之前，逐一回答以下问题：</p>
<p><strong>必要性验证</strong>：</p>
<ul>
<li>单个 Agent 真的不够吗？是否尝试过优化 prompt、增加工具、使用更强的模型？</li>
<li>任务是否天然需要多角色/多视角？还是只是因为你觉得&quot;多 Agent 更酷&quot;？</li>
<li>团队的 LLM API 预算能否支撑多 Agent 的额外消耗？</li>
</ul>
<p><strong>架构选择</strong>：</p>
<ul>
<li>任务结构更接近哪种模式？Supervisor-Worker / Peer-to-Peer / Pipeline / Dynamic Routing？</li>
<li>Agent 之间需要什么样的通信？单向传递 / 双向协商 / 广播通知？</li>
<li>状态应该共享还是独立？冲突解决策略是什么？</li>
</ul>
<p><strong>工程保障</strong>：</p>
<ul>
<li>每个 Agent 的失败影响范围是什么？有降级方案吗？</li>
<li>如何追踪一个请求在多个 Agent 之间的完整执行链路？</li>
<li>如何测试多 Agent 协作的正确性——单元测试（单个 Agent）+ 集成测试（Agent 交互）？</li>
</ul>
<hr>
<h2>10. 结语与展望</h2>
<p>本文是 Phase 3（How to Scale Agent Intelligence）的最后一篇。在 Phase 3 的四篇文章中，我们从单个 Agent 的四个维度进行了升级：</p>
<pre><code>Phase 3 知识路线：

  第 08 篇 Memory       → Agent 有了&quot;记忆&quot;
  第 09 篇 RAG          → Agent 有了&quot;外部知识&quot;
  第 10 篇 Planning     → Agent 有了&quot;规划和反思&quot;
  第 11 篇 Multi-Agent  → Agent 有了&quot;团队协作&quot;（本文）
</code></pre>
<p>至此，我们已经拥有构建一个&quot;聪明的&quot; Agent 系统所需的全部核心概念。但&quot;聪明&quot;不等于&quot;可用&quot;。一个在本地跑通 demo 的 Multi-Agent 系统，距离生产环境还有巨大的鸿沟——框架选型、协议标准化、可观测性、安全性、成本控制、评估体系。</p>
<p>这正是 Phase 4（How to Ship Agents to Production）要解决的问题：</p>
<ul>
<li><strong>下一篇（12）</strong>：LangChain vs LangGraph —— 你应该用框架还是自己写？框架的价值边界在哪里？我们会从 Chain 和 Graph 两种抽象出发，讨论框架在什么时候是加速器，什么时候是束缚。</li>
<li><strong>第 13 篇</strong>：MCP and Tool Protocol —— Agent 的工具需要标准化。MCP 协议如何让不同 Agent 共享工具？工具的发现、声明、权限控制。</li>
<li><strong>第 14 篇</strong>：Production-Grade Agent Systems —— 最后一篇，打通最后一公里：评估、安全、成本、灰度、监控。</li>
</ul>
<h3>进一步思考</h3>
<p><strong>关于协作模式的演化</strong>：本文介绍的四种模式是&quot;纯模式&quot;。真实系统中，你很可能需要混合模式——比如 Supervisor-Worker 的 Worker 内部用 Pipeline，或者 Dynamic Routing 的专家 Agent 内部用 Peer-to-Peer 辩论。如何设计这种嵌套的多层协作结构，是一个值得深入探索的方向。</p>
<p><strong>关于 Agent 的涌现行为</strong>：当多个 Agent 协作时，是否会出现超越单个 Agent 能力的&quot;涌现行为&quot;？还是说 Multi-Agent 的上限永远被最强的那个 Agent 决定？这个问题在学术界尚无定论，但从实践角度看，好的协作架构确实能产出超越任何单个 Agent 的结果——正如一个好的工程团队能完成任何个人都无法独自完成的项目。</p>
<p><strong>关于 Human-in-the-Loop</strong>：本文讨论的全是 Agent-to-Agent 的协作。但在生产环境中，最重要的&quot;Agent&quot;可能是人类。如何设计一个 Multi-Agent 系统，让人类能在关键节点介入、审核和纠正？Human-Agent 协作可能比 Agent-Agent 协作更有实用价值，也更有挑战性。</p>
<hr>
<blockquote>
<p><strong>系列导航</strong>：本文是 Agentic 系列的第 11 篇。</p>
<ul>
<li>上一篇：<a href="/blog/engineering/agentic/10-Planning%20and%20Reflection">10 | Planning and Reflection</a></li>
<li>下一篇：<a href="/blog/engineering/agentic/12-LangChain%20vs%20LangGraph">12 | LangChain vs LangGraph</a></li>
<li>完整目录：<a href="/blog/engineering/agentic/01-From%20LLM%20to%20Agent">01 | From LLM to Agent</a></li>
</ul>
</blockquote>
17:T51ba,<blockquote>
<p>微服务架构的核心难题不是技术选型，而是<strong>如何找到正确的服务边界</strong>。拆分得太粗，和单体无异；拆分得太细，分布式的复杂性会吞噬所有收益。领域驱动设计（DDD）提供了一套系统性的方法论，帮助我们从业务本质出发，找到合理的拆分边界。本文将从 DDD 的核心概念出发，结合电商领域的实例，完整展示如何基于 DDD 构建微服务。</p>
</blockquote>
<h2>微服务的本质：不是&quot;小&quot;，而是&quot;界限清晰&quot;</h2>
<p>微服务中的&quot;微&quot;虽然表示服务的规模，但它并不是微服务架构的核心标准。Adrian Cockcroft 对微服务有一个精炼的定义：</p>
<blockquote>
<p>&quot;面向服务的架构由具有<strong>界限上下文</strong>、<strong>松散耦合</strong>的元素组成。&quot;</p>
</blockquote>
<p>一个真正的微服务架构应当具备以下特征：</p>
<table>
<thead>
<tr>
<th>特征</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>业务边界清晰</td>
<td>服务以业务上下文为中心，而非技术抽象</td>
</tr>
<tr>
<td>实现细节隐藏</td>
<td>通过意图接口暴露功能，不泄露内部实现</td>
</tr>
<tr>
<td>数据独立</td>
<td>服务不共享数据库，每个服务拥有自己的数据存储</td>
</tr>
<tr>
<td>故障快速恢复</td>
<td>具备容错和弹性能力</td>
</tr>
<tr>
<td>独立部署</td>
<td>团队可以自主、频繁地发布变更</td>
</tr>
<tr>
<td>自动化文化</td>
<td>自动化测试、持续集成、持续交付</td>
</tr>
</tbody></table>
<p>归纳起来：<strong>松散耦合的面向服务架构，每个服务封装在定义良好的界限上下文中，支持快速、频繁且可靠的交付。</strong></p>
<p>微服务的强大之处在于：<strong>边界内建立高内聚，边界外建立低耦合</strong>——倾向于一起改变的事物应该放在一起。但说起来容易做起来难，业务在不断发展，设想也随之改变。因此，<strong>重构能力</strong>是设计系统时必须考虑的关键问题。</p>
<h2>DDD 核心概念速览</h2>
<p>领域驱动设计（Domain-Driven Design）因 Eric Evans 的同名著作而闻名，它是一组思想、原则和模式，帮助我们基于业务领域的底层模型来设计软件系统。</p>
<h3>基本术语</h3>
<table>
<thead>
<tr>
<th>概念</th>
<th>定义</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td><strong>领域（Domain）</strong></td>
<td>组织所从事的业务范围</td>
<td>零售、电子商务</td>
</tr>
<tr>
<td><strong>子域（Subdomain）</strong></td>
<td>领域下的业务单元，一个领域由多个子域组成</td>
<td>目录、购物车、履约、支付</td>
</tr>
<tr>
<td><strong>统一语言（Ubiquitous Language）</strong></td>
<td>开发人员与领域专家共同使用的、表达业务模型的语言</td>
<td>&quot;商品&quot;、&quot;订单&quot;、&quot;履约&quot;</td>
</tr>
<tr>
<td><strong>界限上下文（Bounded Context）</strong></td>
<td>模型的有效边界，同一术语在不同上下文中含义不同</td>
<td>见下文详述</td>
</tr>
</tbody></table>
<h3>界限上下文：同一个词，不同的含义</h3>
<p>以电商系统中的 <strong>&quot;Item&quot;（商品）</strong> 为例，它在不同的上下文中有着截然不同的含义：</p>
<table>
<thead>
<tr>
<th>上下文</th>
<th>&quot;Item&quot; 的含义</th>
<th>关注的属性</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Catalog（目录）</strong></td>
<td>可出售的产品</td>
<td>名称、描述、价格、图片、分类</td>
</tr>
<tr>
<td><strong>Cart（购物车）</strong></td>
<td>客户添加到购物车的商品选项</td>
<td>SKU、数量、选中状态</td>
</tr>
<tr>
<td><strong>Fulfillment（履约）</strong></td>
<td>将要运送给客户的仓库物料</td>
<td>仓库位置、重量、物流单号</td>
</tr>
</tbody></table>
<p>通过将这些模型分离并隔离在各自的边界内，我们可以自由地表达这些模型而不产生歧义。</p>
<blockquote>
<p><strong>子域 vs 界限上下文</strong>：子域属于<strong>问题空间</strong>（业务如何看待问题），界限上下文属于<strong>解决方案空间</strong>（如何实现问题的解决方案）。理论上一个子域可以有多个界限上下文，但我们努力做到每个子域只有一个。</p>
</blockquote>
<h2>从界限上下文到微服务</h2>
<h3>界限上下文 ≠ 微服务</h3>
<p>每个界限上下文都能直接映射为一个微服务吗？<strong>不一定</strong>。</p>
<p>以&quot;定价&quot;界限上下文为例，它可能包含三个不同的模型：</p>
<table>
<thead>
<tr>
<th>模型（聚合）</th>
<th>职责</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Price（价格）</strong></td>
<td>管理目录商品的价格</td>
</tr>
<tr>
<td><strong>Priced Items（定价项）</strong></td>
<td>计算商品列表的总价</td>
</tr>
<tr>
<td><strong>Discounts（折扣）</strong></td>
<td>管理和应用各类折扣规则</td>
</tr>
</tbody></table>
<p>如果把这三个模型放在一个服务中，随着时间推移，界限可能变得模糊，职责开始重叠，最终退化为&quot;大泥球&quot;。</p>
<h3>聚合（Aggregate）：更精细的拆分单元</h3>
<p>DDD 中的<strong>聚合</strong>是由相关模型组成的自包含单元，是<strong>数据变更的原子边界</strong>。</p>
<blockquote>
<p>聚合是关联对象的集群，被视为数据变更的单元。外部引用仅限于指定聚合的一个成员——<strong>聚合根（Aggregate Root）</strong>。在聚合的边界内需应用一组一致性规则。</p>
</blockquote>
<p>聚合的核心约束：</p>
<ul>
<li><strong>一致性在单个聚合内保证</strong>：跨聚合的一致性只能做到最终一致</li>
<li><strong>只能通过已发布的接口修改聚合</strong>：外部不能绕过聚合根直接操作内部对象</li>
<li><strong>任何违反这些规则的行为都有让应用退化为大泥球的风险</strong></li>
</ul>
<h3>拆分策略：从保守到激进</h3>
<table>
<thead>
<tr>
<th>策略</th>
<th>适用场景</th>
<th>优势</th>
<th>风险</th>
</tr>
</thead>
<tbody><tr>
<td>一个界限上下文 = 一个微服务</td>
<td>领域模糊、业务初期</td>
<td>保守安全，避免过早拆分</td>
<td>服务可能过大</td>
</tr>
<tr>
<td>一个聚合 = 一个微服务</td>
<td>领域清晰、边界确定</td>
<td>粒度精细，独立演进</td>
<td>分布式复杂度高</td>
</tr>
<tr>
<td>一个界限上下文 = 多个微服务</td>
<td>上下文内聚合边界清晰</td>
<td>兼顾灵活与可控</td>
<td>需要精确的聚合划分</td>
</tr>
</tbody></table>
<blockquote>
<p>对于不完全了解的业务领域，建议从<strong>保守策略</strong>开始：将整个界限上下文及其聚合组成单个微服务。确保聚合之间通过接口充分隔离，后续再拆分的成本会低得多。<strong>将两个微服务合并为一个的成本远高于将一个微服务拆分为两个</strong>。</p>
</blockquote>
<h2>上下文映射：精确划分服务边界</h2>
<p>上下文映射（Context Mapping）用于识别和定义各种界限上下文和聚合之间的关系。它帮助我们回答一个关键问题：<strong>这些服务之间应该如何协作？</strong></p>
<h3>一个错误的设计示例</h3>
<p>以电商支付场景为例，假设有三个服务都需要处理支付：</p>
<table>
<thead>
<tr>
<th>服务</th>
<th>支付相关操作</th>
</tr>
</thead>
<tbody><tr>
<td>购物车服务</td>
<td>在线支付授权</td>
</tr>
<tr>
<td>订单服务</td>
<td>订单履约后结算</td>
</tr>
<tr>
<td>联络中心服务</td>
<td>支付重试、变更支付方式</td>
</tr>
</tbody></table>
<p>如果每个服务都内嵌支付聚合并直接对接支付网关，会产生严重问题：</p>
<ul>
<li><strong>一致性不可保证</strong>：支付聚合分散在多个服务中，无法强制执行不变性</li>
<li><strong>并发冲突</strong>：联络中心更改支付方式时，订单服务可能正在用旧方式结算</li>
<li><strong>变更扩散</strong>：支付网关的任何变更都要改动多个服务、多个团队</li>
</ul>
<h3>重新定义服务边界</h3>
<p>通过上下文映射，将支付聚合收拢到一个独立的<strong>支付服务</strong>中：</p>
<table>
<thead>
<tr>
<th>改造项</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>支付服务独立</strong></td>
<td>支付聚合有了专属的界限上下文，不变量在单个服务边界内管理</td>
</tr>
<tr>
<td><strong>反腐层（ACL）</strong></td>
<td>在支付服务和支付网关之间加入适配层，隔离核心领域模型与第三方数据模型</td>
</tr>
<tr>
<td><strong>购物车→支付</strong></td>
<td>同步 API 调用，因为下单时需要即时的支付授权反馈</td>
</tr>
<tr>
<td><strong>订单→支付</strong></td>
<td>异步事件驱动，订单服务发出域事件，支付服务监听并完成结算</td>
</tr>
<tr>
<td><strong>联络中心→支付</strong></td>
<td>异步事件驱动，变更支付方式时发出事件，支付服务撤销旧卡、处理新卡</td>
</tr>
</tbody></table>
<p>核心原则：<strong>微服务架构的成败取决于聚合之间的低耦合以及聚合之内的高内聚。</strong></p>
<h2>事件风暴：协作式的服务边界发现</h2>
<p>事件风暴（Event Storming）是 Alberto Brandolini 提出的一种轻量级的协作建模技术，它是识别聚合和微服务边界的另一种必不可少的工具。</p>
<h3>什么是事件风暴？</h3>
<p>简单来说，事件风暴是团队在一起进行的头脑风暴，目标是识别系统中发生的各种<strong>领域事件</strong>和<strong>业务流程</strong>。</p>
<p>工作方式：</p>
<ol>
<li>所有相关团队在同一个房间（物理或虚拟）</li>
<li>在白板上用不同颜色的便利贴标记事件、命令、聚合和策略</li>
<li>识别重叠概念、模糊的领域语言和冲突的业务流程</li>
<li>对相关模型进行分组，重新定义聚合边界</li>
</ol>
<h3>便利贴颜色约定</h3>
<table>
<thead>
<tr>
<th>颜色</th>
<th>含义</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>橙色</td>
<td><strong>领域事件</strong>（已发生的事实）</td>
<td>&quot;订单已创建&quot;、&quot;支付已完成&quot;</td>
</tr>
<tr>
<td>蓝色</td>
<td><strong>命令</strong>（触发事件的动作）</td>
<td>&quot;创建订单&quot;、&quot;取消订单&quot;</td>
</tr>
<tr>
<td>黄色</td>
<td><strong>聚合</strong>（命令作用的对象）</td>
<td>&quot;订单&quot;、&quot;支付&quot;、&quot;库存&quot;</td>
</tr>
<tr>
<td>紫色</td>
<td><strong>策略/规则</strong>（事件触发的后续逻辑）</td>
<td>&quot;支付完成后发送确认邮件&quot;</td>
</tr>
<tr>
<td>红色</td>
<td><strong>热点/问题</strong>（需要讨论的疑问）</td>
<td>&quot;退款流程和订单取消是否耦合？&quot;</td>
</tr>
</tbody></table>
<h3>事件风暴的产出</h3>
<p>一次成功的事件风暴通常会产出：</p>
<ul>
<li><strong>重新定义的聚合列表</strong>：这些可能成为新的微服务</li>
<li><strong>领域事件清单</strong>：需要在微服务之间流动的事件</li>
<li><strong>命令清单</strong>：外部用户或其他服务直接调用的操作</li>
<li><strong>团队共识</strong>：对领域、统一语言和精确服务边界的共同理解</li>
</ul>
<h2>微服务间的通信：拥抱最终一致性</h2>
<h3>从单体到微服务的一致性挑战</h3>
<p>在单体应用中，多个聚合在同一个进程边界内，可以在一个事务中完成：客户下单 → 扣减库存 → 发送邮件。所有操作要么都成功，要么都失败。</p>
<p>但微服务化后，这些聚合分散到了不同的分布式系统中。根据 <strong>CAP 定理</strong>：</p>
<blockquote>
<p>一个分布式系统只能同时满足三个特性中的两个：<strong>一致性（C）</strong>、<strong>可用性（A）</strong>、<strong>分区容错（P）</strong>。</p>
</blockquote>
<p>在现实系统中，分区容错（P）是不可协商的——网络不可靠、虚拟机可以宕机、区域延迟可能恶化。因此我们只能在<strong>可用性</strong>和<strong>一致性</strong>之间选择。而在现代互联网应用中，牺牲可用性通常也不可接受。</p>
<p><strong>结论：基于最终一致性设计应用程序。</strong></p>
<h3>事件驱动架构</h3>
<p>微服务可以将聚合上发生的重要变更以<strong>领域事件（Domain Event）</strong> 的形式发出，感兴趣的服务监听这些事件并在自己的领域内执行相应操作。</p>
<p>以&quot;订单取消&quot;为例：</p>
<pre><code>订单服务发布事件：OrderCancelled
  → 支付服务监听 → 执行退款
  → 库存服务监听 → 调整商品库存
  → 通知服务监听 → 发送取消确认邮件
</code></pre>
<p>这种方式避免了两种耦合：</p>
<table>
<thead>
<tr>
<th>耦合类型</th>
<th>事件驱动如何避免</th>
</tr>
</thead>
<tbody><tr>
<td><strong>行为耦合</strong></td>
<td>一个领域无需规定其他领域应该做什么</td>
</tr>
<tr>
<td><strong>时间耦合</strong></td>
<td>一个流程的完成不依赖于所有系统同时可用</td>
</tr>
</tbody></table>
<h3>事件驱动的可靠性保障</h3>
<table>
<thead>
<tr>
<th>角色</th>
<th>保障措施</th>
</tr>
</thead>
<tbody><tr>
<td><strong>生产者</strong></td>
<td>确保事件<strong>至少发出一次</strong>（At Least Once），失败时有回退机制重新触发</td>
</tr>
<tr>
<td><strong>消费者</strong></td>
<td>以<strong>幂等方式</strong>消费事件，同一事件重复到达不产生副作用</td>
</tr>
<tr>
<td><strong>事件排序</strong></td>
<td>事件可能乱序到达，消费者用时间戳或版本号保证正确性</td>
</tr>
</tbody></table>
<h3>何时仍需同步调用？</h3>
<p>并非所有场景都适合事件驱动。当需要<strong>即时反馈</strong>时（如购物车→支付授权），仍需同步 API 调用。但要注意：</p>
<ul>
<li>同步调用引入了<strong>行为耦合</strong>和<strong>时间耦合</strong></li>
<li>被调用服务不可用时，调用方也会受影响</li>
</ul>
<p><strong>缓解策略</strong>：同步调用作为主路径，辅以基于事件或批处理的异步重试作为降级方案。在用户体验、系统弹性和运营成本之间做好权衡。</p>
<blockquote>
<p><strong>何时应该合并而非拆分？</strong> 如果发现两个聚合之间需要强 ACID 事务，这是一个强烈的信号——它们可能应该属于同一个聚合。在拆分之前，事件风暴和上下文映射可以帮助我们及早识别这些依赖关系。</p>
</blockquote>
<h2>BFF 模式：解耦前端与领域服务</h2>
<h3>问题：服务为了迎合调用者而变形</h3>
<p>微服务架构中一个常见的反模式是：<strong>域服务为了满足前端的特定数据需求而编排其他服务</strong>。</p>
<p>以&quot;订单详情页&quot;为例，页面需要同时展示订单信息和退款信息。如果让订单服务调用退款服务来组装复合响应：</p>
<ul>
<li>订单服务的自治性降低：退款聚合的变更会影响订单服务</li>
<li>增加故障点：退款服务宕机时订单服务也受影响</li>
<li>变更成本高：前端需求变化时需要两个团队同时改动</li>
</ul>
<h3>解决方案：Backend for Frontends（BFF）</h3>
<p>BFF 是由<strong>消费者团队</strong>（前端团队）创建和维护的后端服务，负责：</p>
<ul>
<li>对多个域服务进行集成和编排</li>
<li>为前端提供定制化的数据契约</li>
<li>根据不同终端（Web/Mobile）优化响应格式和体积</li>
</ul>
<table>
<thead>
<tr>
<th>对比</th>
<th>无 BFF</th>
<th>有 BFF</th>
</tr>
</thead>
<tbody><tr>
<td>数据编排</td>
<td>域服务互相调用，或前端直接调多个服务</td>
<td>BFF 统一编排，域服务保持纯粹</td>
</tr>
<tr>
<td>变更自主性</td>
<td>前端需求变化要改多个域服务</td>
<td>前端团队自主改 BFF</td>
</tr>
<tr>
<td>性能优化</td>
<td>移动端可能获取过多冗余数据</td>
<td>可按终端定制负载大小</td>
</tr>
<tr>
<td>技术选型</td>
<td>受域服务 API 限制</td>
<td>BFF 可采用 GraphQL 等灵活方案</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>尽早构建 BFF 服务</strong>，可以避免两种不良后果：域服务被迫支持跨域编排，或前端不得不直接调用多个后端服务。</p>
</blockquote>
<h2>从单体到微服务：拆分路线图</h2>
<p>将以上所有工具整合，从单体拆分到微服务的推荐路径：</p>
<h3>第一步：战略设计（Strategic Design）</h3>
<ol>
<li><strong>识别子域</strong>：与领域专家一起梳理业务，划分子域</li>
<li><strong>定义界限上下文</strong>：为每个子域确定解决方案的边界</li>
<li><strong>建立统一语言</strong>：在每个上下文内建立一致的业务术语</li>
</ol>
<h3>第二步：战术发现（Tactical Discovery）</h3>
<ol start="4">
<li><strong>事件风暴</strong>：跨团队协作，识别领域事件、命令、聚合和热点问题</li>
<li><strong>上下文映射</strong>：绘制上下文之间的依赖关系和协作模式</li>
<li><strong>识别聚合</strong>：在每个上下文内找到自包含的数据变更单元</li>
</ol>
<h3>第三步：服务划分（Service Decomposition）</h3>
<ol start="7">
<li><strong>确定服务边界</strong>：根据聚合和上下文映射，确定每个微服务的边界</li>
<li><strong>设计通信方式</strong>：区分同步调用和异步事件，优先使用事件驱动</li>
<li><strong>规划 BFF 层</strong>：为不同终端设计专属的后端聚合层</li>
</ol>
<h3>第四步：渐进式拆分（Incremental Migration）</h3>
<ol start="10">
<li><strong>从边缘开始</strong>：先拆分耦合最少、边界最清晰的服务</li>
<li><strong>绞杀者模式</strong>：新功能用微服务实现，老功能逐步迁移</li>
<li><strong>持续验证</strong>：每拆分一个服务，验证边界是否正确，必要时调整</li>
</ol>
<h2>DDD 战略设计与战术设计的关系</h2>
<p>很多团队在实践 DDD 时过度关注<strong>战术设计</strong>（实体、值对象、聚合根、仓储等代码层面的模式），而忽视了<strong>战略设计</strong>（子域、界限上下文、上下文映射）。对于微服务架构而言，战略设计的价值远大于战术设计：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>战略设计</th>
<th>战术设计</th>
</tr>
</thead>
<tbody><tr>
<td>关注点</td>
<td>服务边界、团队协作、系统结构</td>
<td>代码结构、领域模型、设计模式</td>
</tr>
<tr>
<td>影响范围</td>
<td>整个系统架构</td>
<td>单个服务内部</td>
</tr>
<tr>
<td>决策成本</td>
<td>错误的边界划分代价极高</td>
<td>内部重构成本相对可控</td>
</tr>
<tr>
<td>适用阶段</td>
<td>架构设计初期</td>
<td>服务实现阶段</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>先做对战略设计（找到正确的边界），再做好战术设计（在边界内写好代码）。</strong> 边界划错了，代码写得再漂亮也是徒劳。</p>
</blockquote>
<h2>总结</h2>
<p>基于 DDD 构建微服务的核心认知：</p>
<ol>
<li><strong>微服务的本质是界限清晰</strong>，不是规模小。边界内高内聚，边界外低耦合</li>
<li><strong>界限上下文是服务拆分的起点</strong>，但不是终点——聚合才是更精细的拆分单元</li>
<li><strong>上下文映射揭示服务间的真实依赖</strong>，帮助我们避免聚合被错误地分散到多个服务中</li>
<li><strong>事件风暴是最有效的协作式建模工具</strong>，它能让团队在分解前就达成共识</li>
<li><strong>拥抱最终一致性</strong>，优先使用事件驱动架构，减少服务间的行为耦合和时间耦合</li>
<li><strong>BFF 模式解耦前端与域服务</strong>，让域服务专注于核心业务逻辑</li>
<li><strong>先保守后激进</strong>：不确定时将整个上下文作为一个服务，确保聚合间接口隔离，后续再拆分</li>
<li><strong>合并的成本远高于拆分</strong>：将两个数据库合并为一个，远比将一个数据库拆为两个要困难</li>
</ol>
<blockquote>
<p>DDD 不是银弹，它是一种思考方式。它引导我们从业务本质出发，用结构化的方法找到正确的服务边界。在微服务架构中，<strong>找到正确的边界比选择正确的技术栈重要十倍</strong>。</p>
</blockquote>
18:Td1b4,<h1>LangChain vs LangGraph: 框架的价值与边界</h1>
<blockquote>
<p>框架是加速器，不是必需品。它替你做了决策——有些决策是好的，有些会在深夜的生产事故中反噬你。</p>
<p>本文是 Agentic 系列第 12 篇。前面 11 篇我们从零构建了 Agent 的每一个组件——控制循环、工具调用、记忆、规划、多 Agent 协作。现在是时候回过头来，以工程师的视角冷静审视：框架提供了什么，隐藏了什么，限制了什么。</p>
</blockquote>
<hr>
<h2>1. 开篇：你真的需要框架吗？</h2>
<p>这个问题的答案不是&quot;需要&quot;或&quot;不需要&quot;，而是&quot;取决于&quot;。</p>
<p>如果你已经读完本系列前 7 篇文章（从控制循环到自研 Runtime），你已经具备了从零构建一个 Agent 系统的能力。你知道 Tool Calling 的 JSON Schema 契约，知道控制循环的 Observe-Think-Plan-Act-Reflect-Update 六阶段，知道 Memory 的短期/长期分层，知道 Planner 的 ReAct 与分层规划。</p>
<p>这时候你面临一个决策：</p>
<pre><code>选择 A：自己实现所有组件，完全掌控
选择 B：使用框架，快速启动，接受其抽象和约束
选择 C：理解框架的实现，选择性地借鉴或使用其部分模块
</code></pre>
<p>大多数成熟的工程团队最终会走向选择 C。但要做到选择 C，你必须先深入理解框架到底在做什么。这就是本文的目的。</p>
<hr>
<h2>2. 为什么需要框架</h2>
<p>框架存在是有道理的。在深入批判之前，先公正地承认它们解决了哪些真实的工程问题。</p>
<h3>2.1 减少重复代码</h3>
<p>每一个 Agent 系统都需要处理以下样板代码：</p>
<ul>
<li><strong>工具注册与调度</strong>：维护一个 <code>tool_name → callable</code> 的映射表，处理参数校验和错误捕获</li>
<li><strong>消息格式管理</strong>：构造和维护 <code>messages</code> 列表，处理不同角色（system/user/assistant/tool）的消息格式</li>
<li><strong>LLM 调用封装</strong>：处理 API 差异（OpenAI、Anthropic、本地模型的接口都不同）、流式输出、重试、降级</li>
<li><strong>状态序列化</strong>：将 Agent 的运行状态持久化到数据库或文件系统</li>
</ul>
<p>这些代码在每个项目中高度相似，但又充满细节（比如 OpenAI 的 <code>tool_calls</code> 和 Anthropic 的 <code>tool_use</code> 格式差异）。框架把这些细节屏蔽了。</p>
<h3>2.2 社区生态</h3>
<p>成熟框架最大的资产不是代码，而是生态：</p>
<ul>
<li><strong>预置 Tool 集成</strong>：搜索引擎（Tavily、SerpAPI）、数据库（SQL、MongoDB）、文件系统、浏览器等，开箱即用</li>
<li><strong>预置 Retriever</strong>：支持各种向量数据库（Pinecone、Weaviate、Chroma、FAISS）的统一接口</li>
<li><strong>文档与教程</strong>：从入门到进阶的学习路径</li>
<li><strong>社区问答</strong>：遇到问题时有人讨论、有 issue 可以搜索</li>
</ul>
<h3>2.3 最佳实践封装</h3>
<p>框架将社区沉淀的设计模式编码为默认行为：</p>
<ul>
<li>ReAct 模式的标准实现</li>
<li>Retrieval-Augmented Generation 的标准 pipeline</li>
<li>对话记忆的滑动窗口管理</li>
<li>工具调用的错误处理和重试</li>
</ul>
<p>对于刚接触 Agent 开发的团队，这些封装可以避免很多常见的设计错误。</p>
<h3>2.4 快速原型验证</h3>
<p>当你需要在两天内验证一个想法是否可行时，框架的价值最大化。10 行代码就能跑通一个带工具调用的 Agent 原型，比从零实现快一个数量级。</p>
<pre><code class="language-python"># 10 行代码验证一个想法——这是框架的甜蜜点
from langchain_openai import ChatOpenAI
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.tools.tavily_search import TavilySearchResults

llm = ChatOpenAI(model=&quot;gpt-4o&quot;)
tools = [TavilySearchResults(max_results=3)]
prompt = ChatPromptTemplate.from_messages([
    (&quot;system&quot;, &quot;You are a helpful research assistant.&quot;),
    (&quot;human&quot;, &quot;{input}&quot;),
    (&quot;placeholder&quot;, &quot;{agent_scratchpad}&quot;),
])
agent = create_tool_calling_agent(llm, tools, prompt)
executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
result = executor.invoke({&quot;input&quot;: &quot;2025 年 AI Agent 领域有哪些重要进展？&quot;})
</code></pre>
<p>这段代码在 5 分钟内就能跑通。但如果你打算把它部署到生产环境——请继续往下读。</p>
<hr>
<h2>3. LangChain 深入分析</h2>
<p>LangChain 是 AI Agent 领域生态最大的框架，也是争议最多的框架。我们不吹不黑，从架构和工程两个维度来分析。</p>
<h3>3.1 核心抽象</h3>
<p>LangChain 的设计围绕四个核心抽象：</p>
<table>
<thead>
<tr>
<th>抽象</th>
<th>本质</th>
<th>职责</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Chain</strong></td>
<td>链式调用</td>
<td>将多个步骤串联为顺序执行的管道</td>
</tr>
<tr>
<td><strong>Agent</strong></td>
<td>工具选择 + 循环</td>
<td>LLM 自主决定调用哪个工具，循环直到完成</td>
</tr>
<tr>
<td><strong>Memory</strong></td>
<td>对话状态管理</td>
<td>维护对话历史，支持滑动窗口、摘要等策略</td>
</tr>
<tr>
<td><strong>Retriever</strong></td>
<td>知识检索</td>
<td>从向量数据库或其他数据源检索相关文档</td>
</tr>
</tbody></table>
<p>这四个抽象之间的关系可以用下图表示：</p>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                    LangChain Architecture                │
├─────────────────────────────────────────────────────────┤
│                                                         │
│   ┌──────────┐    ┌──────────┐    ┌──────────────────┐  │
│   │  Chain   │    │  Agent   │    │  AgentExecutor   │  │
│   │          │    │          │    │  (Control Loop)  │  │
│   │ step1 →  │    │ LLM +   │    │                  │  │
│   │ step2 →  │    │ Tools +  │    │  while not done: │  │
│   │ step3    │    │ Prompt   │    │    plan()        │  │
│   └────┬─────┘    └────┬─────┘    │    execute()     │  │
│        │               │          │    observe()     │  │
│        │               └──────────┤                  │  │
│        │                          └────────┬─────────┘  │
│        │                                   │            │
│   ┌────▼───────────────────────────────────▼─────────┐  │
│   │              LLM Abstraction Layer               │  │
│   │  ChatOpenAI │ ChatAnthropic │ ChatOllama │ ...   │  │
│   └────────────────────┬─────────────────────────────┘  │
│                        │                                │
│   ┌────────────────────▼─────────────────────────────┐  │
│   │                  Memory                          │  │
│   │  ConversationBufferMemory │ ConversationSummary  │  │
│   │  VectorStoreMemory │ EntityMemory │ ...          │  │
│   └──────────────────────────────────────────────────┘  │
│                                                         │
│   ┌──────────────────────────────────────────────────┐  │
│   │                  Retriever                       │  │
│   │  VectorStoreRetriever │ BM25 │ MultiQuery │ ... │  │
│   └──────────────────────────────────────────────────┘  │
│                                                         │
│   ┌──────────────────────────────────────────────────┐  │
│   │                  Tools                           │  │
│   │  Search │ Calculator │ SQL │ FileSystem │ ...    │  │
│   └──────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────┘
</code></pre>
<h3>3.2 代码示例：用 LangChain 实现工具调用 Agent</h3>
<p>下面用 LangChain 实现一个能查天气和创建日程的 Agent，同时标注每一层抽象的存在：</p>
<pre><code class="language-python">from langchain_openai import ChatOpenAI
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool

# --- 第 1 层抽象：@tool 装饰器 ---
# LangChain 用装饰器将普通函数包装为 Tool 对象
# 自动从类型注解和 docstring 生成 JSON Schema
@tool
def get_weather(city: str, date: str) -&gt; str:
    &quot;&quot;&quot;获取指定城市在指定日期的天气预报。

    Args:
        city: 城市名称，例如 &quot;北京&quot;
        date: 日期，格式 YYYY-MM-DD
    &quot;&quot;&quot;
    # 实际调用天气 API
    return f&#39;{{&quot;city&quot;: &quot;{city}&quot;, &quot;date&quot;: &quot;{date}&quot;, &quot;temp&quot;: &quot;31°C&quot;, &quot;condition&quot;: &quot;多云转雷阵雨&quot;}}&#39;

@tool
def create_reminder(title: str, time: str, note: str) -&gt; str:
    &quot;&quot;&quot;创建一个日程提醒。

    Args:
        title: 提醒标题
        time: 提醒时间，ISO 8601 格式
        note: 提醒备注内容
    &quot;&quot;&quot;
    return f&#39;{{&quot;status&quot;: &quot;created&quot;, &quot;title&quot;: &quot;{title}&quot;, &quot;time&quot;: &quot;{time}&quot;}}&#39;

# --- 第 2 层抽象：LLM 封装 ---
# ChatOpenAI 封装了 OpenAI API 的调用细节
llm = ChatOpenAI(model=&quot;gpt-4o&quot;, temperature=0)

# --- 第 3 层抽象：Prompt Template ---
# ChatPromptTemplate 管理消息的组装逻辑
prompt = ChatPromptTemplate.from_messages([
    (&quot;system&quot;, &quot;你是一个智能助手，可以查询天气和管理日程。今天是 2025-09-01。&quot;),
    (&quot;human&quot;, &quot;{input}&quot;),
    MessagesPlaceholder(variable_name=&quot;agent_scratchpad&quot;),  # Agent 的工作记忆
])

# --- 第 4 层抽象：Agent 构造 ---
# create_tool_calling_agent 将 LLM + Tools + Prompt 组合为一个 Agent
tools = [get_weather, create_reminder]
agent = create_tool_calling_agent(llm, tools, prompt)

# --- 第 5 层抽象：AgentExecutor ---
# AgentExecutor 提供控制循环：调用 Agent → 执行工具 → 反馈结果 → 循环
executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,       # 输出每一步的推理过程
    max_iterations=10,  # 最大循环次数
    handle_parsing_errors=True,  # 自动处理 LLM 输出格式错误
)

# --- 运行 ---
result = executor.invoke({&quot;input&quot;: &quot;帮我查看明天北京的天气，然后创建一个提醒&quot;})
print(result[&quot;output&quot;])
</code></pre>
<p>数一数：从你的业务逻辑（两个工具函数）到最终执行，经过了 <strong>5 层抽象</strong>。每一层都在&quot;帮你做决策&quot;——消息格式、工具注册方式、控制循环策略、错误处理逻辑、输出解析方式。</p>
<h3>3.3 优点</h3>
<p><strong>1. 生态最大、集成最多</strong></p>
<p>截至 2025 年，LangChain 拥有 AI Agent 框架领域最庞大的集成生态：</p>
<ul>
<li>70+ LLM 提供商（OpenAI、Anthropic、Google、Mistral、本地模型等）</li>
<li>50+ 向量数据库</li>
<li>100+ 预置工具</li>
<li>30+ Document Loader（PDF、HTML、CSV、Notion、Confluence 等）</li>
</ul>
<p><strong>2. 社区活跃</strong></p>
<p>GitHub 上最活跃的 AI 项目之一。遇到问题时，StackOverflow 和 GitHub Issues 中大概率能找到讨论。</p>
<p><strong>3. 上手快</strong></p>
<p>对于 PoC（Proof of Concept）和原型验证，LangChain 能让你在几小时内从零到一跑通一个完整的 Agent。</p>
<p><strong>4. 抽象统一</strong></p>
<p>不同 LLM 提供商的 API 差异被封装在统一接口下。切换 OpenAI → Anthropic 只需要换一行代码（理论上如此，实际上有细微差异）。</p>
<h3>3.4 问题</h3>
<p>以下不是主观吐槽，而是在生产环境中反复遇到的工程问题。</p>
<p><strong>问题 1：过度抽象——简单的事情被包了太多层</strong></p>
<p>考虑一个最基本的需求：调用 LLM 并获取结构化输出。</p>
<pre><code class="language-python"># 不用框架：3 行代码，直白清晰
import openai
response = openai.chat.completions.create(
    model=&quot;gpt-4o&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;分析这段文本的情感&quot;}],
    response_format={&quot;type&quot;: &quot;json_object&quot;},
)
result = json.loads(response.choices[0].message.content)

# 用 LangChain：需要理解 ChatOpenAI、BaseOutputParser、RunnableSequence、
# StrOutputParser vs JsonOutputParser、LCEL 管道语法...
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template(&quot;分析这段文本的情感: {text}&quot;)
llm = ChatOpenAI(model=&quot;gpt-4o&quot;)
parser = JsonOutputParser()
chain = prompt | llm | parser  # LCEL 管道语法
result = chain.invoke({&quot;text&quot;: &quot;这个产品太棒了&quot;})
</code></pre>
<p>LangChain 版本代码量更多不是问题——问题在于它引入了多个你需要理解的新概念（<code>ChatPromptTemplate</code>、<code>JsonOutputParser</code>、LCEL 管道操作符 <code>|</code>），而这些概念只是在封装原本就很简单的操作。</p>
<p><strong>问题 2：调试困难——错误信息穿过多层封装后难以定位</strong></p>
<p>当 LangChain 链条中的某一环出错时，错误堆栈可能长达 20-30 层，涉及 <code>RunnableSequence</code>、<code>RunnableParallel</code>、<code>RunnableLambda</code> 等内部抽象。你需要在这些框架内部类之间导航，才能找到真正的错误源。</p>
<pre><code># 真实场景中的错误堆栈（简化版）
Traceback:
  langchain_core/runnables/base.py      RunnableSequence.invoke()
  langchain_core/runnables/base.py      RunnableSequence._invoke()
  langchain_core/runnables/base.py      Runnable.invoke()
  langchain_core/runnables/base.py      RunnableLambda.invoke()
  langchain/agents/output_parsers.py    ToolsAgentOutputParser.parse()
  ...
  # 15 层之后...
  你的代码.py                            你的函数()   ← 真正的问题在这里
</code></pre>
<p>在生产环境的 3 AM 报警中，这种调试体验是痛苦的。</p>
<p><strong>问题 3：版本混乱——API 变动频繁</strong></p>
<p>LangChain 在快速迭代中经历了多次重大 API 变更：</p>
<ul>
<li><code>langchain</code> → <code>langchain-core</code> + <code>langchain-community</code> 的包拆分</li>
<li><code>LLMChain</code> → LCEL（LangChain Expression Language）的范式转换</li>
<li><code>initialize_agent</code> → <code>create_tool_calling_agent</code> 的 Agent 创建方式变更</li>
<li>Memory 接口的多次重构</li>
</ul>
<p>6 个月前写的代码，今天大概率跑不通。网上的教程和 StackOverflow 答案大量过时。对于需要长期维护的生产系统，这是一个严重的风险。</p>
<p><strong>问题 4：&quot;Chain&quot; 思维的局限——线性链无法表达复杂的分支和循环</strong></p>
<p>LangChain 的核心抽象是 &quot;Chain&quot;——链式调用。这个模型对于线性流水线（A → B → C）非常优雅，但现实中的 Agent 逻辑往往是非线性的：</p>
<pre><code>线性 Chain 能表达的：

    A ──→ B ──→ C ──→ D
    (检索)  (摘要)  (格式化) (输出)


现实中 Agent 需要的：

    A ──→ B ──→ C ──→ D
    │     │     ▲     │
    │     ├─→ E ─┘     │     ← 条件分支
    │     │             │
    │     └─→ F ──→ G ──┘     ← 并行执行
    │           │
    └───────────┘              ← 循环重试
</code></pre>
<p>LangChain 的 LCEL 可以通过 <code>RunnableBranch</code> 和 <code>RunnableParallel</code> 实现一些分支和并行，但语法变得复杂且不直观。这正是 LangGraph 诞生的原因。</p>
<hr>
<h2>4. LangGraph 深入分析</h2>
<p>LangGraph 是 LangChain 团队推出的下一代框架，核心思想是用<strong>有向图（Directed Graph）</strong> 替代<strong>链（Chain）</strong> 作为基础抽象。这不是一个小改动——它从根本上改变了 Agent 逻辑的表达方式。</p>
<h3>4.1 核心抽象</h3>
<p>LangGraph 的设计围绕四个概念：</p>
<table>
<thead>
<tr>
<th>抽象</th>
<th>本质</th>
<th>对应的计算模型</th>
</tr>
</thead>
<tbody><tr>
<td><strong>State</strong></td>
<td>共享状态对象</td>
<td>状态机的 State</td>
</tr>
<tr>
<td><strong>Node</strong></td>
<td>一个函数</td>
<td>状态机的 State Handler</td>
</tr>
<tr>
<td><strong>Edge</strong></td>
<td>节点间的连接</td>
<td>状态机的 Transition</td>
</tr>
<tr>
<td><strong>Graph</strong></td>
<td>节点和边的组合</td>
<td>有限状态机（FSM）</td>
</tr>
</tbody></table>
<p>核心思想：<strong>Agent 的执行流程就是一个状态机。</strong> 每个节点是一个处理函数，每条边是一个转移条件，整个图定义了 Agent 的所有可能执行路径。</p>
<pre><code>┌─────────────────────────────────────────────────────────────┐
│                   LangGraph State Machine                    │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   ┌─────────────────────────────────────────────────────┐   │
│   │                  Shared State                       │   │
│   │  {messages: [...], tool_results: {...}, plan: [...]} │   │
│   └────────────────────────┬────────────────────────────┘   │
│                            │                                │
│               ┌────────────▼────────────┐                   │
│               │       START             │                   │
│               └────────────┬────────────┘                   │
│                            │                                │
│               ┌────────────▼────────────┐                   │
│               │      agent_node         │                   │
│               │   (LLM Reasoning)       │                   │
│               └────────────┬────────────┘                   │
│                            │                                │
│               ┌────────────▼────────────┐                   │
│              ╱    should_continue?       ╲                   │
│             ╱  (Conditional Edge)         ╲                  │
│            ╱                               ╲                 │
│      tool_calls?                      no tool_calls?        │
│           │                                │                │
│  ┌────────▼─────────┐          ┌──────────▼──────────┐     │
│  │    tool_node      │          │       END            │     │
│  │  (Execute Tools)  │          │   (Return Result)    │     │
│  └────────┬──────────┘          └─────────────────────┘     │
│           │                                                 │
│           └──────────────────┐                              │
│                              │ (feed tool results back)     │
│               ┌──────────────▼──────────┐                   │
│               │      agent_node         │ ← 回到推理节点    │
│               └─────────────────────────┘                   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<p>这个图可以清晰地表达：</p>
<ul>
<li><strong>循环</strong>：<code>agent_node → tool_node → agent_node</code>（工具调用循环）</li>
<li><strong>分支</strong>：<code>should_continue?</code> 条件路由</li>
<li><strong>终止</strong>：到达 <code>END</code> 节点时退出</li>
</ul>
<h3>4.2 代码示例：用 LangGraph 实现同一个 Agent</h3>
<p>用 LangGraph 实现与上文 LangChain 相同的天气查询 + 日程创建 Agent：</p>
<pre><code class="language-python">from typing import Annotated, TypedDict
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode


# ============================================================
# Step 1: 定义共享状态（State）
# ============================================================
# 这是 LangGraph 与 LangChain 的核心差异：
# 显式定义 Agent 的完整状态结构
class AgentState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]  # 消息列表，自动追加


# ============================================================
# Step 2: 定义工具（和 LangChain 相同）
# ============================================================
@tool
def get_weather(city: str, date: str) -&gt; str:
    &quot;&quot;&quot;获取指定城市在指定日期的天气预报。&quot;&quot;&quot;
    return f&#39;{{&quot;city&quot;: &quot;{city}&quot;, &quot;date&quot;: &quot;{date}&quot;, &quot;temp&quot;: &quot;31°C&quot;, &quot;condition&quot;: &quot;多云转雷阵雨&quot;}}&#39;

@tool
def create_reminder(title: str, time: str, note: str) -&gt; str:
    &quot;&quot;&quot;创建一个日程提醒。&quot;&quot;&quot;
    return f&#39;{{&quot;status&quot;: &quot;created&quot;, &quot;title&quot;: &quot;{title}&quot;, &quot;time&quot;: &quot;{time}&quot;}}&#39;

tools = [get_weather, create_reminder]


# ============================================================
# Step 3: 定义节点（Node）
# ============================================================
llm = ChatOpenAI(model=&quot;gpt-4o&quot;, temperature=0).bind_tools(tools)

def agent_node(state: AgentState) -&gt; dict:
    &quot;&quot;&quot;推理节点：LLM 根据当前状态决定下一步&quot;&quot;&quot;
    system_message = {
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: &quot;你是一个智能助手，可以查询天气和管理日程。今天是 2025-09-01。&quot;
    }
    messages = [system_message] + state[&quot;messages&quot;]
    response = llm.invoke(messages)
    return {&quot;messages&quot;: [response]}

# ToolNode 是 LangGraph 的内置节点，自动执行工具调用
tool_node = ToolNode(tools)


# ============================================================
# Step 4: 定义边（Edge）—— 条件路由
# ============================================================
def should_continue(state: AgentState) -&gt; str:
    &quot;&quot;&quot;条件路由：检查最后一条消息是否包含工具调用&quot;&quot;&quot;
    last_message = state[&quot;messages&quot;][-1]
    if hasattr(last_message, &quot;tool_calls&quot;) and last_message.tool_calls:
        return &quot;tools&quot;     # 有工具调用 → 去 tool_node
    return &quot;end&quot;           # 无工具调用 → 任务完成


# ============================================================
# Step 5: 构建图（Graph）
# ============================================================
graph_builder = StateGraph(AgentState)

# 添加节点
graph_builder.add_node(&quot;agent&quot;, agent_node)
graph_builder.add_node(&quot;tools&quot;, tool_node)

# 添加边
graph_builder.add_edge(START, &quot;agent&quot;)                        # 入口 → 推理
graph_builder.add_conditional_edges(&quot;agent&quot;, should_continue, {
    &quot;tools&quot;: &quot;tools&quot;,                                         # 推理 → 工具执行
    &quot;end&quot;: END,                                               # 推理 → 结束
})
graph_builder.add_edge(&quot;tools&quot;, &quot;agent&quot;)                      # 工具执行 → 回到推理

# 编译图
graph = graph_builder.compile()


# ============================================================
# Step 6: 运行
# ============================================================
result = graph.invoke({
    &quot;messages&quot;: [HumanMessage(content=&quot;帮我查看明天北京的天气，然后创建一个提醒&quot;)]
})

# 输出最终结果
for message in result[&quot;messages&quot;]:
    print(f&quot;[{message.type}] {message.content}&quot;)
</code></pre>
<p>对比 LangChain 版本，LangGraph 的关键差异：</p>
<ol>
<li><strong>显式状态定义</strong>：<code>AgentState</code> 明确声明了 Agent 运行时的完整状态</li>
<li><strong>显式控制流</strong>：<code>add_edge</code> 和 <code>add_conditional_edges</code> 让执行路径一目了然</li>
<li><strong>图可视化</strong>：编译后的 <code>graph</code> 可以直接渲染为流程图，便于理解和调试</li>
<li><strong>没有隐藏的循环</strong>：循环通过 <code>tools → agent</code> 的边显式定义，而不是藏在 <code>AgentExecutor</code> 内部</li>
</ol>
<h3>4.3 优点</h3>
<p><strong>1. 状态机模型比 Chain 更强大</strong></p>
<p>Chain 只能表达线性流水线。Graph 可以表达任意拓扑——分支、循环、并行、条件汇聚。这与现实中 Agent 的执行逻辑天然匹配。</p>
<p><strong>2. 确定性的控制流 + 非确定性的 LLM 决策</strong></p>
<p>这是 LangGraph 最精妙的设计哲学：</p>
<pre><code>确定性（代码定义）：            非确定性（LLM 决定）：
├── 有哪些节点                 ├── 每个节点内部的推理
├── 节点间如何连接              ├── 工具选择和参数
├── 条件路由的判断逻辑          ├── 是否继续循环
└── 状态的数据结构              └── 最终输出内容
</code></pre>
<p>图的拓扑结构是确定性的（你在编译时就知道所有可能的执行路径），但每一步走哪条路径是 LLM 在运行时决定的。这实现了<strong>可预测的系统行为</strong>与<strong>灵活的智能决策</strong>之间的平衡。</p>
<p><strong>3. Checkpoint 支持——暂停、恢复、Time-Travel</strong></p>
<p>LangGraph 内置了状态检查点机制。这意味着：</p>
<pre><code class="language-python">from langgraph.checkpoint.memory import MemorySaver

# 带 checkpoint 的图
checkpointer = MemorySaver()
graph = graph_builder.compile(checkpointer=checkpointer)

# 运行时传入 thread_id
config = {&quot;configurable&quot;: {&quot;thread_id&quot;: &quot;user-123&quot;}}
result = graph.invoke({&quot;messages&quot;: [HumanMessage(content=&quot;查天气&quot;)]}, config)

# 可以暂停、恢复、回放
# - 暂停：interrupt_before=[&quot;tool_node&quot;] 在工具执行前暂停，等待人类审批
# - 恢复：再次 invoke 同一个 thread_id，从上次中断点继续
# - Time-travel：回滚到任意 checkpoint，重新执行
</code></pre>
<p>这在 Human-in-the-Loop（人机协作）场景中极其有价值——Agent 可以在执行敏感操作前暂停，等待人类确认。</p>
<p><strong>4. 可以表达复杂的多 Agent 架构</strong></p>
<p>上一篇我们讨论的 Supervisor/Worker 模式、并行 Agent 协作，在 LangGraph 中可以自然地表达为图结构：</p>
<pre><code>                 ┌──────────────┐
                 │  Supervisor  │
                 └──────┬───────┘
                        │
              ┌─────────┼─────────┐
              ▼         ▼         ▼
        ┌──────────┐ ┌──────┐ ┌──────────┐
        │ Researcher│ │Coder │ │ Reviewer │
        └──────────┘ └──────┘ └──────────┘
              │         │         │
              └─────────┼─────────┘
                        ▼
                 ┌──────────────┐
                 │  Supervisor  │ ← 回到 Supervisor 决定是否继续
                 └──────────────┘
</code></pre>
<h3>4.4 问题</h3>
<p><strong>问题 1：学习曲线较陡</strong></p>
<p>LangGraph 要求你理解状态机、有向图、条件路由等概念。对于习惯了&quot;调用一个函数就能跑&quot;的开发者来说，需要一段适应期。</p>
<p>特别是 <code>Annotated[list[BaseMessage], add_messages]</code> 这样的状态定义语法（使用 <code>Annotated</code> 类型指定 reducer 函数），对 Python 类型系统不熟悉的开发者可能感到困惑。</p>
<p><strong>问题 2：状态定义需要提前规划</strong></p>
<p>在 LangChain 中，你可以随意传递数据，框架会帮你管理。在 LangGraph 中，所有状态必须在 <code>AgentState</code> 中预先定义。这意味着你需要在写代码之前就想清楚 Agent 需要哪些状态。</p>
<pre><code class="language-python"># 如果开发到一半发现需要新的状态字段，
# 你需要修改 State 定义，并确保所有节点兼容
class AgentState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    plan: list[str]                    # 后来加的
    current_step: int                  # 后来加的
    tool_results: dict[str, str]       # 后来加的
    retry_count: int                   # 后来加的
    # ... 状态会越来越复杂
</code></pre>
<p>对于探索性的开发来说，这种&quot;先定义后使用&quot;的约束会拖慢迭代速度。</p>
<p><strong>问题 3：小任务过度工程化</strong></p>
<p>如果你的 Agent 逻辑就是&quot;调用 LLM → 可能调用工具 → 返回结果&quot;这个简单循环，用 LangGraph 定义 State、Node、Edge、Conditional Edge 就像是用大炮打蚊子。</p>
<pre><code class="language-python"># 一个简单的 ReAct Agent，用 LangGraph 需要 40+ 行图定义代码
# 用原生 Python 只需要一个 while 循环：
while True:
    response = llm.chat(messages, tools=tools)
    if not response.tool_calls:
        return response.content
    for tc in response.tool_calls:
        result = execute_tool(tc)
        messages.append(tool_message(tc.id, result))
</code></pre>
<p>当你的 Agent 逻辑不涉及复杂的分支和并行时，LangGraph 的开销不值得。</p>
<hr>
<h2>5. 其他框架概览</h2>
<p>除了 LangChain 和 LangGraph，AI Agent 领域还有多个值得关注的框架。以下不深入展开，重点给出定位和适用场景。</p>
<h3>5.1 框架定位速览</h3>
<table>
<thead>
<tr>
<th>框架</th>
<th>开发者</th>
<th>核心抽象</th>
<th>定位</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>LangChain</strong></td>
<td>LangChain Inc.</td>
<td>Chain（链式调用）</td>
<td>通用 AI 应用框架</td>
<td>原型验证、RAG、简单 Agent</td>
</tr>
<tr>
<td><strong>LangGraph</strong></td>
<td>LangChain Inc.</td>
<td>Graph（状态机）</td>
<td>复杂 Agent 编排</td>
<td>多步推理、Human-in-the-Loop、多 Agent</td>
</tr>
<tr>
<td><strong>CrewAI</strong></td>
<td>CrewAI Inc.</td>
<td>Crew + Agent + Task</td>
<td>多 Agent 协作</td>
<td>角色扮演式多 Agent 工作流</td>
</tr>
<tr>
<td><strong>AutoGen</strong></td>
<td>Microsoft</td>
<td>Agent + Conversation</td>
<td>多 Agent 对话</td>
<td>研究型多 Agent 系统、代码生成</td>
</tr>
<tr>
<td><strong>Semantic Kernel</strong></td>
<td>Microsoft</td>
<td>Kernel + Plugin + Planner</td>
<td>企业级 AI 编排</td>
<td>企业应用集成、.NET 生态</td>
</tr>
<tr>
<td><strong>Haystack</strong></td>
<td>deepset</td>
<td>Pipeline + Component</td>
<td>RAG 专用</td>
<td>文档检索、知识问答</td>
</tr>
<tr>
<td><strong>DSPy</strong></td>
<td>Stanford NLP</td>
<td>Module + Signature + Optimizer</td>
<td>Prompt 优化</td>
<td>需要自动调优 Prompt 的系统</td>
</tr>
</tbody></table>
<h3>5.2 简要点评</h3>
<p><strong>CrewAI</strong> 的核心思路是&quot;角色扮演&quot;——你定义多个 Agent，每个 Agent 有一个角色（Researcher、Writer、Reviewer），然后把一个任务分配给这个&quot;团队&quot;。这个抽象直观好懂，但在复杂场景中角色定义和任务分配的灵活性不足。</p>
<pre><code class="language-python"># CrewAI 的核心抽象：角色 + 任务 + 团队
from crewai import Agent, Task, Crew

researcher = Agent(role=&quot;Researcher&quot;, goal=&quot;查找相关信息&quot;, ...)
writer = Agent(role=&quot;Writer&quot;, goal=&quot;撰写报告&quot;, ...)
task1 = Task(description=&quot;研究 AI Agent 的最新进展&quot;, agent=researcher)
task2 = Task(description=&quot;基于研究结果撰写报告&quot;, agent=writer)
crew = Crew(agents=[researcher, writer], tasks=[task1, task2])
result = crew.kickoff()
</code></pre>
<p><strong>AutoGen</strong>（Microsoft）强调多 Agent 之间的对话作为协作机制。Agent 之间通过消息传递交互，可以构建复杂的对话流程。适合研究和实验性项目，生产部署的工程支持较弱。</p>
<p><strong>Semantic Kernel</strong>（Microsoft）面向企业用户，强调与现有企业系统的集成。如果你的技术栈是 .NET/C#，或者需要与 Microsoft 365/Azure 深度集成，Semantic Kernel 是更自然的选择。</p>
<p><strong>Haystack</strong>（deepset）不试图做通用 Agent 框架，而是专注于 RAG pipeline。如果你的核心需求是文档检索和知识问答（而不是 Agent 的自主决策和工具调用），Haystack 的 Pipeline 抽象比 LangChain 更干净。</p>
<p><strong>DSPy</strong>（Stanford NLP）走了一条完全不同的路——它不是一个 Agent 运行时框架，而是一个 Prompt 优化框架。核心思想是把 Prompt 当作可学习的参数，通过编译和优化自动找到最佳 Prompt。适合对 Prompt 质量有极高要求的场景。</p>
<h3>5.3 框架选型决策树</h3>
<pre><code>你的核心需求是什么？
│
├─── 快速原型 / PoC
│    └─→ LangChain（生态最大，上手最快）
│
├─── 复杂 Agent 逻辑（分支/循环/并行）
│    └─→ LangGraph（状态机模型天然适合）
│
├─── 多 Agent 协作
│    ├─── 角色扮演式 → CrewAI
│    ├─── 对话式协作 → AutoGen
│    └─── 图编排式   → LangGraph
│
├─── RAG / 知识问答
│    ├─── 需要灵活性  → LangChain + Retriever
│    └─── 需要干净抽象 → Haystack
│
├─── 企业级集成（.NET / Azure）
│    └─→ Semantic Kernel
│
├─── Prompt 自动优化
│    └─→ DSPy
│
└─── 生产系统（需要精细控制）
     └─→ 自研，或只使用框架的底层模块
</code></pre>
<hr>
<h2>6. 框架 vs 自研的决策矩阵</h2>
<p>这是本文最重要的一节。不存在&quot;框架一定好&quot;或&quot;自研一定好&quot;的结论——关键是根据你的具体场景做出理性决策。</p>
<h3>6.1 决策矩阵</h3>
<table>
<thead>
<tr>
<th>考量因素</th>
<th>倾向选框架</th>
<th>倾向选自研</th>
</tr>
</thead>
<tbody><tr>
<td><strong>项目阶段</strong></td>
<td>原型验证、MVP</td>
<td>生产系统、需要长期维护</td>
</tr>
<tr>
<td><strong>团队规模</strong></td>
<td>1-3 人小团队</td>
<td>5+ 人专职 AI 团队</td>
</tr>
<tr>
<td><strong>定制化程度</strong></td>
<td>标准 ReAct/RAG 模式</td>
<td>有独特的控制流或状态管理需求</td>
</tr>
<tr>
<td><strong>调试要求</strong></td>
<td>能接受黑盒</td>
<td>需要完全可观测、可追踪</td>
</tr>
<tr>
<td><strong>性能要求</strong></td>
<td>对 latency 不敏感</td>
<td>需要极致优化每一毫秒</td>
</tr>
<tr>
<td><strong>依赖容忍度</strong></td>
<td>能接受第三方依赖的版本变化</td>
<td>需要完全掌控依赖</td>
</tr>
<tr>
<td><strong>上线时间</strong></td>
<td>2 周内上线</td>
<td>3 个月以上的工程周期</td>
</tr>
<tr>
<td><strong>团队 AI 经验</strong></td>
<td>初次接触 Agent 开发</td>
<td>对 Agent 架构有深入理解</td>
</tr>
</tbody></table>
<h3>6.2 常见场景分析</h3>
<p><strong>场景 1：初创团队做 AI 产品的 MVP</strong></p>
<p>推荐：LangChain（快速原型）→ 验证产品方向 → 决定是否重写</p>
<p>理由：此时最大的风险不是技术债，而是方向错误。花 3 个月自研一个完美的 Agent Runtime，结果发现用户不需要 Agent——这才是最大的浪费。用框架在 2 周内验证想法，确认方向后再决定技术路线。</p>
<p><strong>场景 2：大厂 AI 平台团队</strong></p>
<p>推荐：自研核心 Runtime + 选择性使用框架的底层模块</p>
<p>理由：大厂有足够的工程资源，且对可靠性、可观测性、安全性的要求远超框架的默认支持。自研 Runtime 可以完全掌控控制循环、状态管理、错误处理、日志追踪。但可以借鉴框架的设计模式，或使用框架的工具集成层（比如 LangChain 的 Tool/Retriever 集成）。</p>
<p><strong>场景 3：企业内部的 AI 助手</strong></p>
<p>推荐：LangGraph（如果逻辑复杂）或 LangChain（如果逻辑简单）</p>
<p>理由：企业内部项目通常有明确的需求边界和合理的 SLA 要求，框架能满足大部分需求。LangGraph 的 Human-in-the-Loop 支持对企业审批流程特别有用。</p>
<p><strong>场景 4：研究实验</strong></p>
<p>推荐：AutoGen 或自研轻量框架</p>
<p>理由：研究需要最大的灵活性来尝试新想法。框架的抽象可能限制实验空间。但如果实验涉及多 Agent 交互，AutoGen 的对话式抽象可以减少样板代码。</p>
<h3>6.3 一个务实的折中方案</h3>
<p>在实践中，最常见的成熟方案是<strong>分层使用框架</strong>：</p>
<pre><code>┌─────────────────────────────────────────────────┐
│              你的应用层代码                        │
│         (业务逻辑、API 接口、用户交互)              │
├─────────────────────────────────────────────────┤
│              自研 Agent Runtime                    │
│    (控制循环、状态管理、错误处理、可观测性)          │
├───────────────┬─────────────────────────────────┤
│  自研工具调度   │   框架的集成模块（可选使用）       │
│  自研消息管理   │   LangChain Tool/Retriever       │
│  自研状态存储   │   LangChain Document Loader      │
│               │   LangChain Embedding 接口        │
├───────────────┴─────────────────────────────────┤
│              LLM Provider SDK                     │
│         (openai, anthropic, etc.)                 │
└─────────────────────────────────────────────────┘
</code></pre>
<p>核心思路：</p>
<ul>
<li><strong>控制循环自研</strong>：这是 Agent 最核心的逻辑，也是最需要定制的部分。用 40-60 行 Python 就能实现一个健壮的控制循环（回顾第 07 篇）</li>
<li><strong>LLM 调用用原生 SDK</strong>：OpenAI SDK 和 Anthropic SDK 本身就很好用，不需要再包一层</li>
<li><strong>工具集成可以借用框架</strong>：LangChain 的 Tool 生态确实强大。你可以只 <code>pip install langchain-community</code> 来使用其预置工具，而不用采纳整个框架</li>
<li><strong>状态管理自研</strong>：根据你的持久化需求（Redis、PostgreSQL、内存）定制</li>
</ul>
<p>这个方案的好处是：你在最关键的层面保留了完全掌控力，同时在最不需要掌控的层面（第三方服务的集成）借助了框架的生态。</p>
<hr>
<h2>7. 框架的正确使用姿势</h2>
<p>无论你最终选择什么方案，以下原则都适用。</p>
<h3>7.1 理解原理再用框架</h3>
<p>这正是本系列前 7 篇文章的价值。当你理解了控制循环的六个阶段、Tool Calling 的 JSON Schema 契约、Memory 的分层架构之后，框架在你眼中就不再是黑盒——它只是这些原理的一种实现。</p>
<pre><code>不理解原理时使用框架：
    框架 = 黑魔法（出错时手足无措）

理解原理后使用框架：
    框架 = 已知原理的一种实现（出错时知道去哪里找原因）
</code></pre>
<p>具体来说：</p>
<ul>
<li>当 LangChain 的 <code>AgentExecutor</code> 出错时，你知道它内部在跑一个控制循环，可以猜测问题出在哪个阶段</li>
<li>当 LangGraph 的状态转移出现异常时，你知道这本质上是一个状态机的转移条件判断错误</li>
<li>当框架的 Memory 管理不符合你的需求时，你知道自己需要什么样的记忆架构，可以替换或扩展</li>
</ul>
<h3>7.2 不要被框架限制思维</h3>
<p>框架提供了一组默认的设计模式。这些模式覆盖了 80% 的常见场景，但你的场景可能落在剩下的 20%。</p>
<p><strong>反模式</strong>：为了适配框架的抽象而扭曲自己的业务逻辑。</p>
<pre><code class="language-python"># 反模式：业务逻辑需要 Agent 在两个工具的结果之间做比较，
# 但框架不直接支持，于是你&quot;发明&quot;了一个假工具来绕过限制

@tool
def compare_results(result_a: str, result_b: str) -&gt; str:
    &quot;&quot;&quot;比较两个结果（实际上这应该是 Agent 内部的推理逻辑，不是工具）&quot;&quot;&quot;
    # 这不应该是一个 Tool —— 这是把框架的抽象当成了唯一的解法
    return llm.invoke(f&quot;比较: {result_a} vs {result_b}&quot;)
</code></pre>
<p><strong>正确做法</strong>：框架不支持的逻辑，用原生代码实现，然后插入到框架的流程中（或者干脆不用框架处理这部分）。</p>
<h3>7.3 框架代码是最好的学习材料</h3>
<p>即使你决定自研，框架的源码仍然是宝贵的学习资源。以下是几个值得阅读的代码文件：</p>
<ul>
<li><strong>LangGraph 的 <code>StateGraph</code></strong>：理解如何用 Python 实现一个状态机运行时</li>
<li><strong>LangChain 的 <code>ToolNode</code></strong>：理解如何将 LLM 的 tool_call 输出映射为实际的函数调用</li>
<li><strong>LangChain 的 <code>ChatOpenAI</code></strong>：理解如何封装 LLM Provider 的 API 差异</li>
<li><strong>LangGraph 的 <code>MemorySaver</code></strong>：理解 checkpoint 和状态持久化的实现</li>
</ul>
<p>阅读源码时，关注的不是具体的 API，而是<strong>设计决策</strong>：为什么这样抽象？这个 trade-off 是什么？有没有更好的方案？</p>
<h3>7.4 随时准备好替换或去掉框架</h3>
<p>一个健康的架构应该允许你在不重写业务逻辑的情况下替换底层框架。实现方式：</p>
<pre><code class="language-python"># 定义你自己的接口（不依赖任何框架）
from abc import ABC, abstractmethod

class BaseLLM(ABC):
    @abstractmethod
    def chat(self, messages: list[dict], tools: list[dict] | None = None) -&gt; dict:
        ...

class BaseToolExecutor(ABC):
    @abstractmethod
    def execute(self, tool_name: str, args: dict) -&gt; str:
        ...

class BaseMemory(ABC):
    @abstractmethod
    def get_messages(self, limit: int = 20) -&gt; list[dict]:
        ...
    @abstractmethod
    def add_message(self, message: dict) -&gt; None:
        ...


# 框架实现（可替换）
class LangChainLLM(BaseLLM):
    def __init__(self):
        from langchain_openai import ChatOpenAI
        self._llm = ChatOpenAI(model=&quot;gpt-4o&quot;)

    def chat(self, messages, tools=None):
        # 将你的接口适配为 LangChain 接口
        ...

# 原生实现（可替换）
class NativeLLM(BaseLLM):
    def __init__(self):
        import openai
        self._client = openai.OpenAI()

    def chat(self, messages, tools=None):
        response = self._client.chat.completions.create(
            model=&quot;gpt-4o&quot;, messages=messages, tools=tools
        )
        ...


# 你的 Agent 代码只依赖自己的接口
class MyAgent:
    def __init__(self, llm: BaseLLM, tools: BaseToolExecutor, memory: BaseMemory):
        self.llm = llm
        self.tools = tools
        self.memory = memory

    def run(self, user_input: str) -&gt; str:
        # 业务逻辑不依赖任何框架
        ...
</code></pre>
<p>这不是过度设计——这是<strong>依赖倒置原则</strong>在 Agent 架构中的直接应用。当框架发生 breaking change（LangChain 几乎每季度都有）时，你只需要修改适配层，而不是重写整个系统。</p>
<hr>
<h2>8. LangChain vs LangGraph：直接对比</h2>
<p>最后，用一张表格直接对比 LangChain 和 LangGraph 在各维度的差异：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>LangChain</th>
<th>LangGraph</th>
</tr>
</thead>
<tbody><tr>
<td><strong>核心抽象</strong></td>
<td>Chain（线性管道）</td>
<td>Graph（有向状态机）</td>
</tr>
<tr>
<td><strong>控制流表达</strong></td>
<td>线性为主，分支/循环需要 hack</td>
<td>天然支持分支、循环、并行</td>
</tr>
<tr>
<td><strong>状态管理</strong></td>
<td>隐式（框架内部管理）</td>
<td>显式（开发者定义 State 类型）</td>
</tr>
<tr>
<td><strong>学习曲线</strong></td>
<td>低（上手快）</td>
<td>中等（需要理解状态机概念）</td>
</tr>
<tr>
<td><strong>调试体验</strong></td>
<td>差（多层抽象遮蔽错误源）</td>
<td>中等（图结构可视化，但状态流转需追踪）</td>
</tr>
<tr>
<td><strong>适合场景</strong></td>
<td>简单 Agent、RAG、原型验证</td>
<td>复杂 Agent、多 Agent、Human-in-the-Loop</td>
</tr>
<tr>
<td><strong>生态集成</strong></td>
<td>最丰富</td>
<td>继承 LangChain 生态</td>
</tr>
<tr>
<td><strong>Human-in-the-Loop</strong></td>
<td>不原生支持</td>
<td>原生 Checkpoint + Interrupt 支持</td>
</tr>
<tr>
<td><strong>多 Agent</strong></td>
<td>需要自行编排</td>
<td>原生支持子图嵌套</td>
</tr>
<tr>
<td><strong>生产就绪度</strong></td>
<td>中等（需要大量自定义）</td>
<td>较高（状态持久化、检查点内置）</td>
</tr>
<tr>
<td><strong>灵活性</strong></td>
<td>框架约束多，突破框架难</td>
<td>图定义灵活，但需要提前规划</td>
</tr>
<tr>
<td><strong>版本稳定性</strong></td>
<td>差（API 频繁变更）</td>
<td>较好（API 相对稳定）</td>
</tr>
</tbody></table>
<p><strong>总结</strong>：如果 LangChain 是一条<strong>传送带</strong>（把东西从 A 运到 B），那么 LangGraph 就是一张<strong>铁路网</strong>（可以在任意站点之间调度列车）。传送带简单高效，铁路网灵活强大——选哪个取决于你要运的东西有多复杂。</p>
<hr>
<h2>9. 结语与进一步思考</h2>
<h3>核心立场回顾</h3>
<p>本文的核心立场可以用三句话概括：</p>
<ol>
<li><p><strong>框架是加速器，不是必需品。</strong> 它加速了开发，但也隐藏了复杂性。当隐藏的复杂性成为你的瓶颈时，框架就从加速器变成了减速器。</p>
</li>
<li><p><strong>理解原理比掌握框架更重要。</strong> 框架会变（LangChain 已经经历了多次 API 大改），但控制循环、状态管理、工具调用的基本原理不会变。前 7 篇文章构建的知识，是你评估和使用任何框架的基础。</p>
</li>
<li><p><strong>最好的架构是&quot;框架可替换&quot;的架构。</strong> 把框架当作可插拔的实现层，而不是系统的骨架。你的业务逻辑应该依赖自己定义的接口，而不是某个框架的 API。</p>
</li>
</ol>
<h3>框架解决了&quot;怎么写&quot;，协议解决&quot;怎么连接&quot;</h3>
<p>框架帮你解决了一个 Agent 内部的组件编排问题：如何组织 LLM 调用、工具执行、状态管理。但当你有多个 Agent、多个工具提供者、多个模型时，一个更根本的问题浮现出来：</p>
<blockquote>
<p>这些组件之间用什么协议通信？工具如何被发现和注册？能力如何被声明和协商？</p>
</blockquote>
<p>这不是框架能解决的问题——这需要<strong>协议（Protocol）</strong>。下一篇我们将讨论 MCP（Model Context Protocol），看看 Agent 工具生态的协议化未来。</p>
<h3>留给读者的思考</h3>
<p><strong>关于框架的未来</strong>：LLM 本身的能力在快速增强。当模型原生支持复杂的多步推理（如 o1/o3 的 chain-of-thought）、原生支持长对话记忆（如 Gemini 的长上下文窗口）、原生支持工具调用时，框架的价值会被压缩还是放大？换句话说——当 LLM 足够强时，我们还需要框架在中间做多少事？</p>
<p><strong>关于抽象的代价</strong>：每一层抽象都在隐藏复杂性。隐藏复杂性是好事（让你专注于业务逻辑），但也是坏事（让你在出问题时无法理解系统行为）。在 Agent 这样本身就充满不确定性的系统中，你能接受多少&quot;隐藏的复杂性&quot;？</p>
<p><strong>关于生态锁定</strong>：选择一个框架意味着接受它的抽象、它的生态、它的更新节奏、它的设计理念。当框架的方向与你的需求分叉时，迁移的成本有多高？这个成本是否在你的决策时被低估了？</p>
<p>这些问题没有标准答案。但作为 AI 工程师，能够清晰地提出这些问题，本身就是一种重要的能力。</p>
<hr>
<blockquote>
<p><strong>系列导航</strong>：本文是 Agentic 系列的第 12 篇。</p>
<ul>
<li>上一篇：<a href="/blog/engineering/agentic/11-Multi-Agent%20Collaboration">11 | Multi-Agent Collaboration</a></li>
<li>下一篇：<a href="/blog/engineering/agentic/13-MCP%20and%20Tool%20Protocol">13 | MCP and Tool Protocol</a></li>
<li>完整目录：<a href="/blog/engineering/agentic/01-From%20LLM%20to%20Agent">01 | From LLM to Agent</a></li>
</ul>
</blockquote>
19:T9ed3,<h1>Planning and Reflection: 从 ReAct 到分层规划与自我纠错</h1>
<blockquote>
<p>LLM 的 next-token prediction 天生是&quot;短视&quot;的——它只看到当前 token 的概率分布，不会思考十步之后的结局。规划（Planning）让 Agent 具备&quot;远视&quot;能力，反思（Reflection）让 Agent 具备&quot;纠错&quot;能力。二者结合，是 Agent 从&quot;工具调用器&quot;进化为&quot;问题解决者&quot;的关键。</p>
<p>本文是 Agentic 系列的第 10 篇。我们将从规划范式的演进出发，深入分析 ReAct、Plan-and-Execute、Tree-of-Thought、Hierarchical Planning 四种规划模式，再系统探讨 Reflection 机制的设计与陷阱。</p>
</blockquote>
<hr>
<h2>1. 为什么 Agent 需要规划和反思</h2>
<p>LLM 的核心训练目标是 next-token prediction：给定前文，预测最可能的下一个 token。这种机制天然缺乏两种能力：</p>
<ul>
<li><strong>前瞻（Lookahead）</strong>：生成第一步时不会考虑&quot;这个决定在第五步会导致什么后果&quot;——每一步都选局部最优，但局部最优的叠加不等于全局最优。</li>
<li><strong>回溯（Backtrack）</strong>：一旦生成了一段文本就不会主动回头修正，即使中间步骤出了错，后续 token 也会基于错误的前提继续生成。</li>
</ul>
<p><strong>规划（Planning）</strong> 弥补前瞻缺陷——在执行前把大目标拆成子目标，考虑步骤间的依赖和顺序。<strong>反思（Reflection）</strong> 弥补回溯缺陷——在执行后检查结果、分析错误、决定重试或调整。</p>
<pre><code>没有规划的 Agent：走一步看一步（Greedy, Reactive）
有规划的 Agent：先想好路线再出发（Deliberate, Proactive）
有反思的 Agent：走错了能发现、能纠正（Self-correcting）
</code></pre>
<p>二者结合，Agent 才能从&quot;工具调用器&quot;进化为&quot;问题解决者&quot;。</p>
<hr>
<h2>2. 规划范式的演进</h2>
<pre><code>   2022              2023 early         2023 mid            2023+ now
    │                    │                  │                    │
    ▼                    ▼                  ▼                    ▼
┌────────┐      ┌──────────────┐    ┌──────────────┐   ┌────────────────┐
│No Plan │─────▶│    ReAct     │───▶│Plan-and-Exec │──▶│ Hierarchical   │
│直接回答 │      │Thought-Act-  │    │先规划再执行   │   │  Planning      │
└────────┘      │Observation   │    └──────────────┘   │ 多层级分解     │
                └──────┬───────┘                       └────────────────┘
                       │           ┌──────────────┐           ▲
                       └──────────▶│Tree-of-Thought│──────────┘
                                   │多路径搜索     │
                                   └──────────────┘

能力维度：单步回答 ──▶ 逐步推理 ──▶ 全局规划 ──▶ 多路径探索 ──▶ 递归分解
</code></pre>
<table>
<thead>
<tr>
<th>范式</th>
<th>核心思想</th>
<th>解决了什么</th>
<th>新的问题</th>
</tr>
</thead>
<tbody><tr>
<td>No Planning</td>
<td>LLM 直接回答</td>
<td>—</td>
<td>无法处理多步任务</td>
</tr>
<tr>
<td>ReAct</td>
<td>交替 Thought-Action-Observation</td>
<td>多步推理+行动</td>
<td>Greedy，缺乏全局视野</td>
</tr>
<tr>
<td>Plan-and-Execute</td>
<td>先规划再逐步执行</td>
<td>全局视野，可追踪</td>
<td>计划可能过时，修正成本高</td>
</tr>
<tr>
<td>Tree-of-Thought</td>
<td>多条路径搜索选优</td>
<td>探索多种可能性</td>
<td>成本倍增</td>
</tr>
<tr>
<td>Hierarchical</td>
<td>多层级递归分解</td>
<td>处理真正复杂的任务</td>
<td>架构复杂，调试困难</td>
</tr>
</tbody></table>
<hr>
<h2>3. ReAct 深入分析</h2>
<h3>3.1 原理：Reason + Act 交替进行</h3>
<p>ReAct（Yao et al., 2022）让 LLM 在推理（Thought）和行动（Action）之间交替，每次行动后观察结果（Observation），再基于观察继续推理。</p>
<pre><code>User Question
     │
     ▼
┌──────────┐     ┌──────────┐     ┌──────────────┐
│ Thought  │────▶│  Action  │────▶│ Observation  │
│ (推理)   │     │ (行动)   │     │ (观察结果)    │
└──────────┘     └──────────┘     └──────┬───────┘
     ▲                                    │
     └────────────────────────────────────┘
</code></pre>
<h3>3.2 ReAct Prompt 模板</h3>
<pre><code class="language-python">REACT_SYSTEM_PROMPT = &quot;&quot;&quot;You operate in a loop of Thought, Action, Observation.

- Thought: Analyze the situation and decide the next step.
- Action: Call a tool. Format: Action: tool_name({&quot;param&quot;: &quot;value&quot;})
- Observation: Review the tool&#39;s result.

When ready, respond: Final Answer: &lt;your answer&gt;

Available tools:
{tool_descriptions}

Rules:
1. Always think before acting.
2. If a tool fails, analyze why and try differently.
3. Do not fabricate information — use only tool results.
&quot;&quot;&quot;
</code></pre>
<h3>3.3 优点与缺点</h3>
<p><strong>优点</strong>：灵活自适应（每步可根据 Observation 调整）、实现简单（while 循环 + prompt）、可解释性强（Thought 暴露推理过程）、容错好（失败后下一步可换策略）。</p>
<p><strong>缺点</strong>：Greedy / 短视（不考虑长期后果）、效率低（每步完整 LLM 调用）、上下文膨胀（步骤越多 token 越多）、容易循环（重复同一失败策略）。</p>
<h3>3.4 Python 实现</h3>
<pre><code class="language-python">import json
from dataclasses import dataclass
from typing import Callable
import openai

@dataclass
class Tool:
    name: str
    description: str
    parameters: dict
    function: Callable

class ReActAgent:
    def __init__(self, model: str = &quot;gpt-4o&quot;, tools: list[Tool] | None = None,
                 max_iterations: int = 10):
        self.model = model
        self.tools = {t.name: t for t in (tools or [])}
        self.max_iterations = max_iterations
        self.client = openai.OpenAI()

    def _build_system_prompt(self) -&gt; str:
        tool_desc = &quot;\n&quot;.join(
            f&quot;- {t.name}: {t.description}&quot; for t in self.tools.values()
        )
        return REACT_SYSTEM_PROMPT.format(tool_descriptions=tool_desc)

    def _parse_action(self, text: str) -&gt; tuple[str, dict] | None:
        for line in text.split(&quot;\n&quot;):
            if line.strip().startswith(&quot;Action:&quot;):
                action_str = line.strip()[len(&quot;Action:&quot;):].strip()
                paren = action_str.find(&quot;(&quot;)
                if paren == -1:
                    return None
                name = action_str[:paren].strip()
                params_str = action_str[paren + 1:].rstrip(&quot;)&quot;)
                params = json.loads(params_str) if params_str else {}
                return name, params
        return None

    def _execute_tool(self, name: str, params: dict) -&gt; str:
        if name not in self.tools:
            return f&quot;Error: Unknown tool &#39;{name}&#39;&quot;
        try:
            return str(self.tools[name].function(**params))
        except Exception as e:
            return f&quot;Error: {e}&quot;

    def run(self, query: str) -&gt; str:
        messages = [
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: self._build_system_prompt()},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: query},
        ]
        for _ in range(self.max_iterations):
            resp = self.client.chat.completions.create(
                model=self.model, messages=messages, temperature=0.0,
            )
            text = resp.choices[0].message.content
            messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: text})

            if &quot;Final Answer:&quot; in text:
                return text.split(&quot;Final Answer:&quot;)[-1].strip()

            action = self._parse_action(text)
            if action is None:
                messages.append({&quot;role&quot;: &quot;user&quot;,
                                 &quot;content&quot;: &quot;Provide a valid Action or Final Answer.&quot;})
                continue

            observation = self._execute_tool(*action)
            messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;Observation: {observation}&quot;})

        return &quot;Reached max iterations without final answer.&quot;
</code></pre>
<p>注意：随着迭代增加 <code>messages</code> 不断膨胀，token 消耗呈线性增长。超过 5-6 步的任务需要考虑上下文压缩（如摘要历史步骤）。</p>
<hr>
<h2>4. Plan-and-Execute 模式</h2>
<h3>4.1 原理：先规划再执行</h3>
<p>Plan-and-Execute 将规划与执行分离：先用一次 LLM 调用生成完整计划，再逐个执行子任务，必要时触发 Replanning。</p>
<pre><code>┌────────────┐       Plan: [S1, S2, S3]      ┌────────────┐
│  Planner   │──────────────────────────────▶│  Executor  │
│ (全局规划)  │                                │ (逐步执行)  │
└────────────┘                                └─────┬──────┘
      ▲                                             │ 执行失败
      │            ┌─────────────┐                  │
      └────────────│  Replanner  │◀─────────────────┘
                   │ (动态修正)   │
                   └─────────────┘
</code></pre>
<h3>4.2 Planner / Executor 分离的优势</h3>
<ol>
<li><strong>关注点分离</strong>：Planner 负责&quot;做什么&quot;，Executor 负责&quot;怎么做&quot;，可以分别用不同模型优化</li>
<li><strong>可并行</strong>：无依赖的步骤可以并行执行</li>
<li><strong>可追踪</strong>：计划本身是结构化数据，便于监控和审计</li>
<li><strong>可中断恢复</strong>：执行到一半中断后可从某一步重启</li>
</ol>
<h3>4.3 计划的动态修正</h3>
<p>三种 Replan 策略：<strong>完全重新规划</strong>（全局优化但可能丢弃已有成果）、<strong>局部修正</strong>（成本低但可能保留错误前提）、<strong>条件触发</strong>（仅在步骤失败或偏差超阈值时 Replan）。生产中通常用条件触发 + 局部修正的组合。</p>
<h3>4.4 Python 实现</h3>
<pre><code class="language-python">from dataclasses import dataclass, field

@dataclass
class PlanStep:
    id: int
    description: str
    tool: str | None = None
    depends_on: list[int] = field(default_factory=list)
    status: str = &quot;pending&quot;   # pending / completed / failed
    result: str | None = None

PLANNER_PROMPT = &quot;&quot;&quot;Decompose the goal into concrete steps (max 7).
Available tools: {tool_names}
Output JSON: {{&quot;goal&quot;: &quot;...&quot;, &quot;steps&quot;: [{{&quot;id&quot;: 1, &quot;description&quot;: &quot;...&quot;,
&quot;tool&quot;: &quot;tool_name or null&quot;, &quot;depends_on&quot;: []}}]}}&quot;&quot;&quot;

class PlanAndExecuteAgent:
    def __init__(self, tools: dict[str, Tool],
                 planner_model: str = &quot;gpt-4o&quot;,
                 executor_model: str = &quot;gpt-4o-mini&quot;,
                 max_replans: int = 3):
        self.tools = tools
        self.planner_model = planner_model
        self.executor_model = executor_model
        self.max_replans = max_replans
        self.client = openai.OpenAI()

    def _create_plan(self, goal: str) -&gt; list[PlanStep]:
        resp = self.client.chat.completions.create(
            model=self.planner_model,
            messages=[
                {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: PLANNER_PROMPT.format(
                    tool_names=&quot;, &quot;.join(self.tools.keys()))},
                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: goal},
            ],
            response_format={&quot;type&quot;: &quot;json_object&quot;},
        )
        data = json.loads(resp.choices[0].message.content)
        return [PlanStep(**s) for s in data[&quot;steps&quot;]]

    def _execute_step(self, step: PlanStep, context: dict) -&gt; str:
        if step.tool and step.tool in self.tools:
            param_resp = self.client.chat.completions.create(
                model=self.executor_model,
                messages=[{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;:
                    f&quot;Call tool &#39;{step.tool}&#39; for: {step.description}\n&quot;
                    f&quot;Context: {json.dumps(context)}\nReturn JSON params only.&quot;}],
                response_format={&quot;type&quot;: &quot;json_object&quot;},
            )
            params = json.loads(param_resp.choices[0].message.content)
            return str(self.tools[step.tool].function(**params))
        resp = self.client.chat.completions.create(
            model=self.executor_model,
            messages=[{&quot;role&quot;: &quot;user&quot;,
                       &quot;content&quot;: f&quot;Task: {step.description}\nContext: {json.dumps(context)}&quot;}],
        )
        return resp.choices[0].message.content

    def run(self, goal: str) -&gt; str:
        steps = self._create_plan(goal)
        context = {}
        for replan in range(self.max_replans + 1):
            for step in steps:
                if step.status == &quot;completed&quot;:
                    continue
                deps_met = all(
                    any(s.id == d and s.status == &quot;completed&quot; for s in steps)
                    for d in step.depends_on
                )
                if not deps_met:
                    continue
                try:
                    step.result = self._execute_step(step, context)
                    step.status = &quot;completed&quot;
                    context[f&quot;step_{step.id}&quot;] = step.result
                except Exception as e:
                    step.status = &quot;failed&quot;
                    step.result = str(e)
                    steps = self._replan(goal, steps, step)
                    break
            if all(s.status == &quot;completed&quot; for s in steps):
                return self._synthesize(goal, context)
        return &quot;Exceeded max replans.&quot;

    def _replan(self, goal, steps, failed) -&gt; list[PlanStep]:
        # 将已完成步骤 + 失败信息交给 Planner 重新规划
        completed = [{&quot;id&quot;: s.id, &quot;result&quot;: s.result}
                     for s in steps if s.status == &quot;completed&quot;]
        resp = self.client.chat.completions.create(
            model=self.planner_model,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;:
                f&quot;Replan. Goal: {goal}\nCompleted: {json.dumps(completed)}\n&quot;
                f&quot;Failed step: {failed.description} -&gt; {failed.result}&quot;}],
            response_format={&quot;type&quot;: &quot;json_object&quot;},
        )
        data = json.loads(resp.choices[0].message.content)
        return [PlanStep(**s) for s in data[&quot;steps&quot;]]

    def _synthesize(self, goal, context):
        resp = self.client.chat.completions.create(
            model=self.planner_model,
            messages=[{&quot;role&quot;: &quot;user&quot;,
                       &quot;content&quot;: f&quot;Goal: {goal}\nResults: {json.dumps(context)}\n&quot;
                       &quot;Synthesize a final answer.&quot;}],
        )
        return resp.choices[0].message.content
</code></pre>
<p>Planner 用 <code>gpt-4o</code>（强规划），Executor 用 <code>gpt-4o-mini</code>（快执行）——这是生产中常见的成本优化手段。</p>
<hr>
<h2>5. Tree-of-Thought</h2>
<h3>5.1 原理</h3>
<p>Tree-of-Thought（ToT，Yao et al. 2023）模拟人类&quot;深思熟虑&quot;：同时考虑多条推理路径，评估每条的前景，选择最优的继续深入。</p>
<pre><code>                       Root (问题)
                      /     |     \
                   Th1     Th2    Th3      ← 生成多个候选 Thought
                  /   \     |    /   \
               T1a   T1b  T2a  T3a  T3b   ← 继续展开
                ✗      ✓    ✗    ✓    ✗    ← 评估函数打分，剪枝
</code></pre>
<p>三个核心组件：<strong>Thought Generator</strong>（每步生成 k 个候选）、<strong>State Evaluator</strong>（对候选打分）、<strong>Search Algorithm</strong>（BFS 或 DFS）。</p>
<h3>5.2 BFS vs DFS</h3>
<ul>
<li><strong>BFS</strong>：每层展开 k 个，评估后保留 top-k 进入下一层。适合步骤少、每步选择多的问题。总调用 ≈ k x depth x 2（生成+评估）。</li>
<li><strong>DFS</strong>：选当前最优一路深入，死胡同时回溯。适合步骤多、每步选择少的问题。最好 O(depth)，最坏 O(k^depth)。</li>
</ul>
<h3>5.3 评估函数设计</h3>
<ol>
<li><strong>LLM 自评</strong>：让 LLM 对每个 Thought 打分。简单但可能有系统性偏见。</li>
<li><strong>投票法</strong>：多次评估取多数。更稳健但成本更高。</li>
<li><strong>外部验证</strong>：可验证的问题（数学/代码）用外部工具检查。最可靠但适用范围有限。</li>
</ol>
<h3>5.4 Trade-off：质量 vs 成本</h3>
<pre><code>方法           LLM 调用次数      质量    适用场景
─────────────  ──────────────   ─────   ──────────
ReAct(单路径)   O(steps)         基准    大多数任务
ToT-BFS        O(k * d * 2)     高      创意/数学/方案选型
ToT-DFS        O(k^d) 最坏      中-高   深度推理
</code></pre>
<p>k=3, d=3 时 ToT 可能需要 40+ 次 LLM 调用，ReAct 只需 5-6 次——<strong>8-10 倍成本差距</strong>。只有当正确性要求高且存在多条有意义的推理路径时，ToT 的投入才有回报。</p>
<h3>5.5 Python 实现</h3>
<pre><code class="language-python">import json
from dataclasses import dataclass, field
import openai

@dataclass
class ThoughtNode:
    &quot;&quot;&quot;搜索树中的节点，每个节点代表一条推理路径的当前状态&quot;&quot;&quot;
    state: str                           # 当前推理状态（累积的 thought 文本）
    score: float = 0.0                   # 评估函数打分
    depth: int = 0
    children: list[&quot;ThoughtNode&quot;] = field(default_factory=list)

class TreeOfThought:
    def __init__(self, model: str = &quot;gpt-4o&quot;, k: int = 3, max_depth: int = 3):
        &quot;&quot;&quot;
        k: 每层生成的候选 thought 数量（BFS 宽度）
        max_depth: 搜索树最大深度
        &quot;&quot;&quot;
        self.model = model
        self.k = k
        self.max_depth = max_depth
        self.client = openai.OpenAI()

    def generate_thoughts(self, problem: str, current_state: str) -&gt; list[str]:
        &quot;&quot;&quot;生成 k 个候选 thought&quot;&quot;&quot;
        resp = self.client.chat.completions.create(
            model=self.model,
            messages=[{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;:
                f&quot;Given the problem and current reasoning state, &quot;
                f&quot;generate exactly {self.k} distinct next-step thoughts.\n&quot;
                f&#39;Return JSON: {{&quot;thoughts&quot;: [&quot;thought1&quot;, &quot;thought2&quot;, ...]}}&#39;},
                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;:
                f&quot;Problem: {problem}\nCurrent state: {current_state or &#39;(start)&#39;}&quot;}],
            response_format={&quot;type&quot;: &quot;json_object&quot;},
        )
        data = json.loads(resp.choices[0].message.content)
        return data[&quot;thoughts&quot;][:self.k]

    def evaluate_thought(self, problem: str, state: str) -&gt; float:
        &quot;&quot;&quot;评估当前推理状态的前景，返回 0-1 分数&quot;&quot;&quot;
        resp = self.client.chat.completions.create(
            model=self.model,
            messages=[{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;:
                &quot;Evaluate how promising this reasoning state is for solving the problem.\n&quot;
                &#39;Return JSON: {&quot;score&quot;: 0.0-1.0, &quot;reason&quot;: &quot;...&quot;}&#39;},
                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;:
                f&quot;Problem: {problem}\nReasoning so far: {state}&quot;}],
            response_format={&quot;type&quot;: &quot;json_object&quot;},
        )
        data = json.loads(resp.choices[0].message.content)
        return float(data[&quot;score&quot;])

    def solve(self, problem: str) -&gt; str:
        &quot;&quot;&quot;BFS 搜索：每层生成 k 个候选，评估后保留 top-k 进入下一层&quot;&quot;&quot;
        # 初始化：根节点
        current_level = [ThoughtNode(state=&quot;&quot;, depth=0)]

        for depth in range(self.max_depth):
            candidates: list[ThoughtNode] = []

            for node in current_level:
                # 为每个节点生成 k 个候选 thought
                thoughts = self.generate_thoughts(problem, node.state)
                for thought in thoughts:
                    new_state = f&quot;{node.state}\nStep {depth+1}: {thought}&quot;.strip()
                    score = self.evaluate_thought(problem, new_state)
                    child = ThoughtNode(state=new_state, score=score, depth=depth+1)
                    node.children.append(child)
                    candidates.append(child)

            # 保留 top-k 进入下一层（BFS 剪枝）
            candidates.sort(key=lambda n: n.score, reverse=True)
            current_level = candidates[:self.k]

        # 返回最终得分最高的推理路径
        best = max(current_level, key=lambda n: n.score)
        return best.state
</code></pre>
<p>核心观察：BFS 宽度 <code>k</code> 和搜索深度 <code>max_depth</code> 共同控制质量-成本的 trade-off。<code>k</code> 越大，每层探索的候选越多，找到好路径的概率越高，但 LLM 调用次数以 O(k² × d) 增长（每层 k 个节点各生成 k 个候选 + k 次评估）。实践中 k=2<del>3、depth=2</del>3 是较好的起点，可根据任务复杂度动态调整。</p>
<hr>
<h2>6. 分层规划（Hierarchical Planning）</h2>
<p>当任务复杂到&quot;设计并实现用户权限系统&quot;这种级别时，一层计划无法覆盖从架构到实现的所有粒度。分层规划通过<strong>递归分解</strong>解决：高层拆子目标，低层拆具体动作。</p>
<pre><code>高层规划器 (Strategic)
├─ 子目标1: 设计数据模型
│   └─ 低层规划器 (Tactical)
│       ├─ Action: 分析需求
│       ├─ Action: 设计 ER 图
│       └─ Action: 定义 API Schema
├─ 子目标2: 实现认证模块
│   └─ 低层规划器
│       ├─ Action: 实现 JWT 签发
│       └─ Action: 编写测试
└─ 子目标3: 实现授权模块
    └─ 低层规划器
        ├─ Action: 实现 RBAC
        └─ Action: 集成测试
</code></pre>
<h3>6.1 递归分解的终止条件</h3>
<ol>
<li><strong>原子性</strong>：任务可用单次工具调用完成 → 停止分解</li>
<li><strong>深度限制</strong>：最大 2-3 层，防止过度分解</li>
<li><strong>预算约束</strong>：剩余 token 预算不足以继续分解 → 当前粒度直接执行</li>
</ol>
<pre><code class="language-python">class HierarchicalPlanner:
    def __init__(self, client: openai.OpenAI, model=&quot;gpt-4o&quot;, max_depth=3):
        self.client, self.model, self.max_depth = client, model, max_depth

    def decompose(self, goal: str, depth: int = 0) -&gt; dict:
        if depth &gt;= self.max_depth:
            return {&quot;type&quot;: &quot;action&quot;, &quot;description&quot;: goal}

        resp = self.client.chat.completions.create(
            model=self.model,
            messages=[{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;:
                &quot;Decide if this goal is atomic or compound.\n&quot;
                &#39;Atomic: {&quot;type&quot;:&quot;action&quot;,&quot;description&quot;:&quot;...&quot;}\n&#39;
                &#39;Compound: {&quot;type&quot;:&quot;goal&quot;,&quot;description&quot;:&quot;...&quot;,&quot;subgoals&quot;:[&quot;...&quot;,]}&#39;},
                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: goal}],
            response_format={&quot;type&quot;: &quot;json_object&quot;},
        )
        node = json.loads(resp.choices[0].message.content)
        if node[&quot;type&quot;] == &quot;action&quot;:
            return node
        node[&quot;children&quot;] = [self.decompose(sg, depth+1) for sg in node.get(&quot;subgoals&quot;,[])]
        return node
</code></pre>
<p>实践中 2 层（Strategic + Tactical）通常够用。3 层以上的调试成本会快速失控。</p>
<h3>6.2 执行层：递归执行分解后的计划</h3>
<p><code>HierarchicalPlanner</code> 只负责分解，执行需要单独的 Executor。核心逻辑：叶节点（type=&quot;action&quot;）直接调用 LLM 或工具执行，分支节点（type=&quot;goal&quot;）递归执行所有子节点并聚合结果。</p>
<pre><code class="language-python">@dataclass
class ExecutionResult:
    description: str
    output: str
    success: bool
    children: list[&quot;ExecutionResult&quot;] = field(default_factory=list)

class HierarchicalExecutor:
    def __init__(self, client: openai.OpenAI, model: str = &quot;gpt-4o-mini&quot;,
                 tools: dict[str, Callable] | None = None):
        self.client = client
        self.model = model
        self.tools = tools or {}

    def execute(self, node: dict) -&gt; ExecutionResult:
        &quot;&quot;&quot;递归执行分解后的计划树&quot;&quot;&quot;
        desc = node.get(&quot;description&quot;, &quot;&quot;)

        # 叶节点：直接执行
        if node[&quot;type&quot;] == &quot;action&quot;:
            output = self._execute_action(desc)
            return ExecutionResult(description=desc, output=output, success=True)

        # 分支节点：递归执行所有子节点
        child_results = [self.execute(child) for child in node.get(&quot;children&quot;, [])]
        all_success = all(r.success for r in child_results)

        # 聚合子节点结果
        summary = self._aggregate(desc, child_results)
        return ExecutionResult(
            description=desc, output=summary,
            success=all_success, children=child_results,
        )

    def _execute_action(self, action: str) -&gt; str:
        &quot;&quot;&quot;执行单个原子动作——优先使用工具，否则 fallback 到 LLM&quot;&quot;&quot;
        for tool_name, tool_fn in self.tools.items():
            if tool_name.lower() in action.lower():
                return str(tool_fn(action))
        resp = self.client.chat.completions.create(
            model=self.model,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;Execute this task: {action}&quot;}],
        )
        return resp.choices[0].message.content

    def _aggregate(self, goal: str, results: list[ExecutionResult]) -&gt; str:
        &quot;&quot;&quot;将子节点执行结果聚合为父目标的总结&quot;&quot;&quot;
        parts = &quot;\n&quot;.join(f&quot;- {r.description}: {r.output[:200]}&quot; for r in results)
        resp = self.client.chat.completions.create(
            model=self.model,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;:
                f&quot;Goal: {goal}\nSub-results:\n{parts}\nSummarize the overall outcome.&quot;}],
        )
        return resp.choices[0].message.content
</code></pre>
<p>分解与执行分离的好处：<code>HierarchicalPlanner</code> 可以用强模型（gpt-4o）做规划，<code>HierarchicalExecutor</code> 用快模型（gpt-4o-mini）做执行，兼顾规划质量和执行成本。同时，执行层可以独立替换——例如将 <code>_execute_action</code> 改为调用真实 API 或 Code Interpreter，而不影响规划逻辑。</p>
<hr>
<h2>7. Reflection（反思）机制</h2>
<h3>7.1 为什么需要反思</h3>
<p>Agent 有三类常见失败：LLM 输出错误（幻觉/逻辑错误）、工具执行失败（超时/参数错误）、计划不可行（前提假设不成立）。没有反思，错误会<strong>无意识地传播</strong>——第 2 步的错成为第 3 步的输入，错误不断累积。</p>
<h3>7.2 Self-Critique</h3>
<p>用同一个 LLM 评估自己的输出。理论支持：LLM 在<strong>验证</strong>上通常比<strong>生成</strong>更强（就像检查别人的代码比自己写更容易）。但盲区在于：LLM 的系统性偏见在生成和评估中是一致的。</p>
<h3>7.3 结构化反思</h3>
<pre><code class="language-python">@dataclass
class ReflectionResult:
    what_went_well: list[str]
    what_went_wrong: list[str]
    root_cause: str
    what_to_do_next: str
    should_retry: bool
    confidence: float  # 0-1

REFLECTION_PROMPT = &quot;&quot;&quot;Analyze this execution result.
Goal: {goal} | Steps: {steps} | Result: {result}
Return JSON: {{&quot;what_went_well&quot;:[], &quot;what_went_wrong&quot;:[], &quot;root_cause&quot;:&quot;&quot;,
&quot;what_to_do_next&quot;:&quot;&quot;, &quot;should_retry&quot;: bool, &quot;confidence&quot;: 0.0-1.0}}&quot;&quot;&quot;
</code></pre>
<h3>7.4 Retry Budget 与 Stop Condition</h3>
<p>反思不能无限循环。必须有 Stop Condition：</p>
<pre><code>                  反思完成
                     │
          ┌──────────▼──────────┐   是
          │ 质量 &gt;= 阈值？       │─────▶ 返回结果
          └──────────┬──────────┘
                     │ 否
          ┌──────────▼──────────┐   是
          │ 达到最大重试？       │─────▶ 返回最好的结果
          └──────────┬──────────┘
                     │ 否
          ┌──────────▼──────────┐   是
          │ 改进幅度 &lt; 阈值？    │─────▶ 停止（再试也没用）
          └──────────┬──────────┘
                     │ 否
          ┌──────────▼──────────┐   是
          │ 成本超出预算？       │─────▶ 返回当前结果
          └──────────┬──────────┘
                     │ 否
                  继续重试
</code></pre>
<p>四个条件形成<strong>多层安全网</strong>：质量达标是正常退出，最大重试和成本预算是硬性保底，改进幅度检测是&quot;聪明的&quot;提前退出。</p>
<h3>7.5 代码实现</h3>
<pre><code class="language-python">@dataclass
class ReflectionPolicy:
    max_retries: int = 3
    quality_threshold: float = 0.7
    improvement_threshold: float = 0.1
    cost_limit_tokens: int = 10000

class ReflectiveAgent:
    def __init__(self, base_agent: ReActAgent, policy: ReflectionPolicy,
                 model: str = &quot;gpt-4o-mini&quot;):
        self.base_agent = base_agent
        self.policy = policy
        self.model = model
        self.client = openai.OpenAI()

    def _reflect(self, goal, steps, result) -&gt; ReflectionResult:
        resp = self.client.chat.completions.create(
            model=self.model,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: REFLECTION_PROMPT.format(
                goal=goal, steps=json.dumps(steps), result=result)}],
            response_format={&quot;type&quot;: &quot;json_object&quot;},
        )
        return ReflectionResult(**json.loads(resp.choices[0].message.content))

    def run(self, goal: str) -&gt; str:
        best_result, best_score = None, 0.0
        history = []

        for attempt in range(self.policy.max_retries + 1):
            # 执行（重试时注入反思结论）
            if attempt == 0:
                result = self.base_agent.run(goal)
            else:
                enhanced = (f&quot;{goal}\n\nPrevious issues: {reflection.what_went_wrong}&quot;
                           f&quot;\nRoot cause: {reflection.root_cause}&quot;
                           f&quot;\nSuggestion: {reflection.what_to_do_next}&quot;)
                result = self.base_agent.run(enhanced)

            reflection = self._reflect(goal, history, result)

            # Stop conditions
            if reflection.confidence &gt;= self.policy.quality_threshold:
                best_result, best_score = result, reflection.confidence
                break
            if not reflection.should_retry:
                break
            if attempt &gt; 0 and (reflection.confidence - best_score) &lt; self.policy.improvement_threshold:
                break  # 改进幅度不足，再试也没用

            # 更新最优结果（放在 stop condition 之后，避免 improvement 检查失效）
            if reflection.confidence &gt; best_score:
                best_result, best_score = result, reflection.confidence

            history.append({&quot;attempt&quot;: attempt, &quot;issues&quot;: reflection.what_went_wrong})

        return best_result or result
</code></pre>
<hr>
<h2>8. Reflection 的陷阱</h2>
<h3>8.1 无限循环</h3>
<p>Agent 不断反思但不改进——反思发现了问题却没有提供有效的改进方向。解法：<code>improvement_threshold</code> 检测，连续两轮质量差距 &lt; 0.1 直接停止。</p>
<h3>8.2 过度反思</h3>
<p>简单任务（&quot;今天天气怎么样&quot;）也要三轮反思，浪费 3-4 倍 token。解法：引入复杂度判断，简单任务跳过反思。</p>
<pre><code class="language-python">def needs_reflection(task: str, result: str) -&gt; bool:
    &quot;&quot;&quot;简单任务不值得反思&quot;&quot;&quot;
    if len(result) &lt; 100:  # 结果很短 → 可能是简单查询
        return False
    simple_patterns = [&quot;什么是&quot;, &quot;查一下&quot;, &quot;告诉我&quot;]
    return not any(p in task for p in simple_patterns)
</code></pre>
<h3>8.3 成本爆炸</h3>
<p>每次反思是完整 LLM 调用，包含完整上下文。对策：(1) 反思用小模型（GPT-4o-mini）；(2) 压缩上下文传摘要版本；(3) 采样反思（30% 的执行触发反思而非 100%）。</p>
<h3>8.4 合理的 Reflection 策略</h3>
<pre><code>Q1: 任务的错误成本高吗？
  高 → 启用反思    低 → 跳过

Q2: 错误可自动检测吗？
  是（代码可测试） → 外部验证（更可靠更便宜）
  否（文案质量）   → LLM Self-Critique

Q3: 预算够吗？
  够   → 结构化反思 + 多轮重试
  不够 → 单轮 Self-Critique

Q4: 延迟敏感吗？
  是 → 最多一轮，超时直接返回
  否 → 多轮直到质量达标
</code></pre>
<hr>
<h2>9. 规划模式选型指南</h2>
<table>
<thead>
<tr>
<th>场景</th>
<th>推荐模式</th>
<th>原因</th>
</tr>
</thead>
<tbody><tr>
<td>简单工具调用（查天气、算术）</td>
<td><strong>ReAct</strong></td>
<td>1-2 步完成，规划是过度设计</td>
</tr>
<tr>
<td>多步研究（竞品分析、技术调研）</td>
<td><strong>Plan-and-Execute</strong></td>
<td>需要全局视野和步骤追踪</td>
</tr>
<tr>
<td>创意/数学/代码</td>
<td><strong>Tree-of-Thought</strong></td>
<td>需探索多条路径并选最优</td>
</tr>
<tr>
<td>复杂项目（系统设计）</td>
<td><strong>Hierarchical</strong></td>
<td>粒度跨度大，需递归分解</td>
</tr>
<tr>
<td>高可靠（金融/法律）</td>
<td><strong>Plan-and-Execute + Reflection</strong></td>
<td>全局规划 + 结果验证</td>
</tr>
<tr>
<td>实时交互（客服/对话）</td>
<td><strong>ReAct</strong></td>
<td>延迟敏感，逐步响应</td>
</tr>
<tr>
<td>长时任务（数据管道）</td>
<td><strong>Hierarchical + Plan-Exec</strong></td>
<td>可中断、可恢复、可并行</td>
</tr>
</tbody></table>
<p><strong>二维决策矩阵：</strong></p>
<pre><code>                  任务步骤少            任务步骤多
             ┌──────────────────┬──────────────────┐
 确定性高    │  ReAct            │  Plan-and-Exec   │
 (路径清晰)  │ (甚至不需要Agent)  │                  │
             ├──────────────────┼──────────────────┤
 确定性低    │  Tree-of-Thought  │  Hierarchical    │
 (需要探索)  │                   │  + Reflection    │
             └──────────────────┴──────────────────┘
</code></pre>
<p><strong>模式组合</strong>在生产中很常见：Hierarchical + Plan-and-Execute（高层分解子目标，内部用 Plan-Exec 执行）；ReAct + Reflection（逐步执行，每 N 步检查方向）。关键原则：<strong>从 ReAct 开始，只有当它的局限性确实成为瓶颈时再升级。</strong></p>
<hr>
<h2>10. 结语：规划的边界与 Multi-Agent 的必要性</h2>
<p>规划和反思让单个 Agent 从&quot;走一步看一步&quot;进化到&quot;先想后做再检查&quot;。但单 Agent 的规划能力终有上限：</p>
<ul>
<li><strong>上下文窗口限制</strong>：任务涉及的知识和状态超出 context window 时，单 Agent 力不从心</li>
<li><strong>专业性限制</strong>：一个 Agent 很难同时擅长编码、写作和数据分析——就像一个人很难同时是程序员、设计师和产品经理</li>
<li><strong>执行效率限制</strong>：单 Agent 串行执行，即使计划中的步骤可以并行</li>
</ul>
<p>当这些限制成为瓶颈，你需要的不是更好的规划算法，而是<strong>多个 Agent 的协作</strong>——每个 Agent 专注于擅长领域，由 Orchestrator 协调。这正是下一篇的主题：<strong>Multi-Agent Collaboration: 多 Agent 协作模式与架构。</strong></p>
<hr>
<blockquote>
<p><strong>进一步思考：</strong></p>
<ol>
<li>规划质量高度依赖 LLM 对任务域的理解。如果 LLM 从未见过某类任务，能否通过 few-shot examples 注入领域知识来提升规划质量？</li>
<li>&quot;LLM 评估 LLM&quot; 的反思机制在多大程度上可靠？是否能引入外部验证信号（代码测试、人类反馈）来补强？</li>
<li>Tree-of-Thought 的搜索空间是指数级的。能否借鉴 AlphaGo 的 MCTS 来更高效搜索？Reasoning model（如 o1、o3）是否已在内部做了类似的事情？</li>
<li>规划和反思的 token 成本显著。能否缓存和复用已有的计划，为相似任务跳过规划阶段？</li>
</ol>
</blockquote>
<hr>
<blockquote>
<p><strong>系列导航</strong>：本文是 Agentic 系列的第 10 篇。</p>
<ul>
<li>上一篇：<a href="/blog/engineering/agentic/09-RAG%20as%20Cognitive%20Memory">09 | RAG as Cognitive Memory</a></li>
<li>下一篇：<a href="/blog/engineering/agentic/11-Multi-Agent%20Collaboration">11 | Multi-Agent Collaboration</a></li>
<li>完整目录：<a href="/blog/engineering/agentic/01-From%20LLM%20to%20Agent">01 | From LLM to Agent</a></li>
</ul>
</blockquote>
5:["$","article",null,{"className":"min-h-screen","children":["$","div",null,{"className":"mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12","children":[["$","header",null,{"className":"mb-8","children":[["$","nav",null,{"className":"flex items-center gap-1 text-sm mb-4","children":[["$","$L13",null,{"href":"/blog/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"博客"}],["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/page/1","className":"text-gray-500 hover:text-blue-600 transition-colors","children":"Engineering"}],[["$","span",null,{"className":"text-gray-300","children":"/"}],["$","$L13",null,{"href":"/blog/category/engineering/agentic/page/1","className":"text-blue-600 hover:text-blue-700 transition-colors","children":"Agentic 系统"}]]]}],["$","div",null,{"className":"flex items-center mb-6","children":["$","div",null,{"className":"inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal","children":[["$","svg",null,{"className":"w-4 h-4 mr-2 text-gray-400","fill":"none","stroke":"currentColor","viewBox":"0 0 24 24","children":["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","strokeWidth":2,"d":"M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z"}]}],["$","time",null,{"dateTime":"2026-01-17","children":"2026年01月17日"}]]}]}],["$","h1",null,{"className":"text-4xl font-bold text-gray-900 mb-6 text-center","children":"Multi-Agent Collaboration: 多 Agent 协作模式与架构"}],["$","div",null,{"className":"flex flex-wrap gap-2 mb-6 justify-center","children":[["$","$L13","Agentic",{"href":"/blog/tag/Agentic/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"Agentic"}],["$","$L13","AI Engineering",{"href":"/blog/tag/AI%20Engineering/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"AI Engineering"}],["$","$L13","Multi-Agent",{"href":"/blog/tag/Multi-Agent/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"Multi-Agent"}]]}]]}],["$","div",null,{"className":"max-w-5xl mx-auto","children":["$","$L14",null,{"content":"$15"}]}],["$","$10",null,{"fallback":["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":"加载导航中..."}],"children":["$","$L16",null,{"globalNav":{"prev":{"slug":"engineering/domain/基于DDD构建微服务：从战略设计到落地实践","title":"基于DDD构建微服务：从战略设计到落地实践","description":"深入探讨领域驱动设计（DDD）如何指导微服务的拆分与设计。从界限上下文、聚合、上下文映射到事件风暴，系统性地阐述 DDD 的战略设计工具如何帮助我们找到正确的服务边界，并通过事件驱动架构和 BFF 模式解决微服务间的通信与协作问题。","pubDate":"2026-01-15","tags":["DDD","微服务","领域驱动设计","架构设计","事件驱动"],"heroImage":"$undefined","content":"$17"},"next":{"slug":"engineering/agentic/12-LangChain vs LangGraph","title":"LangChain vs LangGraph: 框架的价值与边界","description":"Agentic 系列第 12 篇。客观审视 AI Agent 框架的价值与局限。深入分析 LangChain 的抽象模型与陷阱、LangGraph 的状态机优势与学习曲线，横向对比 CrewAI、AutoGen、Semantic Kernel 等框架，最终给出框架 vs 自研的决策矩阵。核心立场：理解原理再用框架，框架是加速器而非必需品。","pubDate":"2026-01-22","tags":["Agentic","AI Engineering","Framework"],"heroImage":"$undefined","content":"$18"}},"tagNav":{"Agentic":{"prev":{"slug":"engineering/agentic/10-Planning and Reflection","title":"Planning and Reflection: 从 ReAct 到分层规划与自我纠错","description":"Agentic 系列第 10 篇。深入剖析 Agent 规划（Planning）与反思（Reflection）的核心机制——从 ReAct 的交替推理、Plan-and-Execute 的全局视野、Tree-of-Thought 的多路径搜索，到分层规划的递归分解，再到结构化反思与自我纠错。包含完整 Python 实现、决策分析与 trade-off 讨论。","pubDate":"2026-01-12","tags":["Agentic","AI Engineering","Planning"],"heroImage":"$undefined","content":"$19"},"next":"$5:props:children:props:children:props:children:2:props:children:props:globalNav:next"},"AI Engineering":{"prev":"$5:props:children:props:children:props:children:2:props:children:props:tagNav:Agentic:prev","next":"$5:props:children:props:children:props:children:2:props:children:props:globalNav:next"},"Multi-Agent":{"prev":null,"next":null}}}]}],["$","$L1a",null,{}]]}]}]}]
8:null
c:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
7:null
a:{"metadata":[["$","title","0",{"children":"Multi-Agent Collaboration: 多 Agent 协作模式与架构 - Skyfalling Blog"}],["$","meta","1",{"name":"description","content":"单个 Agent 的能力有天花板——Context Window 有限、专业化受限、单点故障、串行瓶颈。本文系统拆解多 Agent 协作的四种核心模式（Supervisor-Worker、Peer-to-Peer、Pipeline、Dynamic Routing），深入 Agent 间通信机制、状态管理、错误处理与成本控制，并用 Python 从零实现一个 Supervisor-Worker 协作框架。"}],["$","meta","2",{"property":"og:title","content":"Multi-Agent Collaboration: 多 Agent 协作模式与架构"}],["$","meta","3",{"property":"og:description","content":"单个 Agent 的能力有天花板——Context Window 有限、专业化受限、单点故障、串行瓶颈。本文系统拆解多 Agent 协作的四种核心模式（Supervisor-Worker、Peer-to-Peer、Pipeline、Dynamic Routing），深入 Agent 间通信机制、状态管理、错误处理与成本控制，并用 Python 从零实现一个 Supervisor-Worker 协作框架。"}],["$","meta","4",{"property":"og:type","content":"article"}],["$","meta","5",{"property":"article:published_time","content":"2026-01-17"}],["$","meta","6",{"property":"article:author","content":"Skyfalling"}],["$","meta","7",{"name":"twitter:card","content":"summary"}],["$","meta","8",{"name":"twitter:title","content":"Multi-Agent Collaboration: 多 Agent 协作模式与架构"}],["$","meta","9",{"name":"twitter:description","content":"单个 Agent 的能力有天花板——Context Window 有限、专业化受限、单点故障、串行瓶颈。本文系统拆解多 Agent 协作的四种核心模式（Supervisor-Worker、Peer-to-Peer、Pipeline、Dynamic Routing），深入 Agent 间通信机制、状态管理、错误处理与成本控制，并用 Python 从零实现一个 Supervisor-Worker 协作框架。"}],["$","link","10",{"rel":"shortcut icon","href":"/favicon.png"}],["$","link","11",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","link","12",{"rel":"icon","href":"/favicon.png"}],["$","link","13",{"rel":"apple-touch-icon","href":"/favicon.png"}]],"error":null,"digest":"$undefined"}
12:{"metadata":"$a:metadata","error":null,"digest":"$undefined"}
