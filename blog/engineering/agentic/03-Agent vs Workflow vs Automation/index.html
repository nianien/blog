<!DOCTYPE html><html lang="zh-CN"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/232416e7c3a1ca7e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-42d55485b4428e47.js"/><script src="/_next/static/chunks/4bd1b696-8ec333fca6b38e39.js" async=""></script><script src="/_next/static/chunks/1684-a2aac8a674e5d38c.js" async=""></script><script src="/_next/static/chunks/main-app-2791dc86ed05573e.js" async=""></script><script src="/_next/static/chunks/6874-7791217feaf05c17.js" async=""></script><script src="/_next/static/chunks/app/layout-142e67ac4336647c.js" async=""></script><script src="/_next/static/chunks/968-d7155a2506e36f1d.js" async=""></script><script src="/_next/static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js" async=""></script><meta name="next-size-adjust" content=""/><title>Agent vs Workflow vs Automation: 选对抽象才是关键 - Skyfalling Blog</title><meta name="description" content="不是所有问题都需要 Agent。本文系统比较 Rule-based Automation、Workflow/DAG、Agent 三种执行范式，从确定性、成本、可观测性等维度给出选型框架，帮助工程师在真实场景中选对抽象层次。"/><meta property="og:title" content="Agent vs Workflow vs Automation: 选对抽象才是关键"/><meta property="og:description" content="不是所有问题都需要 Agent。本文系统比较 Rule-based Automation、Workflow/DAG、Agent 三种执行范式，从确定性、成本、可观测性等维度给出选型框架，帮助工程师在真实场景中选对抽象层次。"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-12-09"/><meta property="article:author" content="Skyfalling"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Agent vs Workflow vs Automation: 选对抽象才是关键"/><meta name="twitter:description" content="不是所有问题都需要 Agent。本文系统比较 Rule-based Automation、Workflow/DAG、Agent 三种执行范式，从确定性、成本、可观测性等维度给出选型框架，帮助工程师在真实场景中选对抽象层次。"/><link rel="shortcut icon" href="/favicon.png"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><link rel="icon" href="/favicon.png"/><link rel="apple-touch-icon" href="/favicon.png"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_f367f3"><div hidden=""><!--$--><!--/$--></div><div class="min-h-screen flex flex-col"><header class="bg-[var(--background)]"><nav class="mx-auto flex max-w-7xl items-center justify-between p-6 lg:px-8" aria-label="Global"><div class="flex lg:flex-1"><a class="-m-1.5 p-1.5" href="/"><span class="sr-only">Skyfalling Blog</span><span class="text-2xl font-bold text-gray-900">Skyfalling</span></a></div><div class="flex lg:hidden"><button type="button" class="-m-2.5 inline-flex items-center justify-center rounded-md p-2.5 text-gray-700"><span class="sr-only">打开主菜单</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></button></div><div class="hidden lg:flex lg:gap-x-12"><a class="text-base font-semibold leading-6 transition-colors text-gray-900 hover:text-blue-600" href="/">首页</a><a class="text-base font-semibold leading-6 transition-colors text-blue-600 border-b-2 border-blue-600 pb-1" href="/blog/">博客</a><a class="text-base font-semibold leading-6 transition-colors text-gray-900 hover:text-blue-600" href="/about/">关于</a></div><div class="hidden lg:flex lg:flex-1 lg:justify-end"></div></nav></header><main class="flex-1"><article class="min-h-screen"><div class="mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8"><div class="rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12"><header class="mb-8"><nav class="flex items-center gap-1 text-sm mb-4"><a class="text-gray-500 hover:text-blue-600 transition-colors" href="/blog/page/1/">博客</a><span class="text-gray-300">/</span><a class="text-gray-500 hover:text-blue-600 transition-colors" href="/blog/category/engineering/page/1/">Engineering</a><span class="text-gray-300">/</span><a class="text-blue-600 hover:text-blue-700 transition-colors" href="/blog/category/engineering/agentic/page/1/">Agentic 系统</a></nav><div class="flex items-center mb-6"><div class="inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal"><svg class="w-4 h-4 mr-2 text-gray-400" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z"></path></svg><time dateTime="2025-12-09">2025年12月09日</time></div></div><h1 class="text-4xl font-bold text-gray-900 mb-6 text-center">Agent vs Workflow vs Automation: 选对抽象才是关键</h1><div class="flex flex-wrap gap-2 mb-6 justify-center"><a class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors" href="/blog/tag/Agentic/page/1/">Agentic</a><a class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors" href="/blog/tag/AI%20Engineering/page/1/">AI Engineering</a><a class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors" href="/blog/tag/Architecture/page/1/">Architecture</a></div></header><div class="max-w-5xl mx-auto"><div class="prose prose-lg prose-gray mx-auto max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:text-blue-700 prose-strong:text-gray-900 prose-strong:font-semibold prose-li:text-gray-700 prose-hr:border-gray-300"><h1>Agent vs Workflow vs Automation: 选对抽象才是关键</h1>
<blockquote>
<p>系列第 03 篇。上一篇我们讲了&quot;LLM 本身不是 Agent&quot;，这一篇要回答一个更实际的问题：<strong>你的问题，真的需要 Agent 吗？</strong></p>
</blockquote>
<hr>
<h2>1. 开篇：Agent 万能论的陷阱</h2>
<p>2024 年以来，&quot;Agent&quot; 这个词已经被严重滥用。打开任何一篇技术文章，似乎所有系统都应该被重写为 Agent——客服要 Agent、ETL 要 Agent、运维要 Agent、审批要 Agent。</p>
<p>但现实是：<strong>大部分生产系统中，80% 的任务用 if/else 和 DAG 就能解决，且解决得更好。</strong></p>
<p>Agent 不是银弹。它是一种特定的执行范式，适用于特定的问题空间。盲目使用 Agent 的代价是：更高的 Token 成本、更长的延迟、更难的调试、更差的可预测性。</p>
<p>这篇文章的目标很简单：帮你建立一个清晰的选型框架。面对一个具体问题，你应该能在 30 秒内判断——<strong>用 Automation、用 Workflow、还是用 Agent。</strong></p>
<hr>
<h2>2. 三种执行范式</h2>
<h3>2.1 Rule-based Automation</h3>
<p><strong>定义</strong>：用预定义规则驱动的全自动执行。输入确定，规则确定，输出确定。</p>
<p>典型实现：if/else 逻辑、Rule Engine（Drools、Rete）、Cron Job、Event Trigger。</p>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                 Rule-based Automation                    │
│                                                         │
│   Input ──→ [Rule Match] ──→ Action A                   │
│                  │                                      │
│                  ├──→ Action B                           │
│                  │                                      │
│                  └──→ Action C                           │
│                                                         │
│   特征：路径在编写时完全确定，运行时无决策               │
│   类比：铁轨上的火车，轨道已铺好                         │
└─────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>核心特征</strong>：</p>
<ul>
<li>零运行时决策——所有分支在代码 / 规则编写时就已确定</li>
<li>确定性：相同输入永远产生相同输出</li>
<li>延迟极低（微秒到毫秒级）</li>
<li>可解释性最强——每一步都可以追溯到具体规则</li>
</ul>
<pre><code class="language-python"># 典型的 Rule-based Automation
class AlertRule:
    def __init__(self, metric: str, threshold: float, action: str):
        self.metric = metric
        self.threshold = threshold
        self.action = action

class RuleEngine:
    def __init__(self):
        self.rules: list[AlertRule] = []

    def add_rule(self, rule: AlertRule):
        self.rules.append(rule)

    def evaluate(self, metrics: dict[str, float]) -&gt; list[str]:
        &quot;&quot;&quot;对每条指标做规则匹配，返回触发的动作列表&quot;&quot;&quot;
        actions = []
        for rule in self.rules:
            value = metrics.get(rule.metric)
            if value is not None and value &gt; rule.threshold:
                actions.append(rule.action)
        return actions

# 使用
engine = RuleEngine()
engine.add_rule(AlertRule(&quot;cpu_usage&quot;, 90.0, &quot;scale_up&quot;))
engine.add_rule(AlertRule(&quot;error_rate&quot;, 5.0, &quot;page_oncall&quot;))
engine.add_rule(AlertRule(&quot;disk_usage&quot;, 85.0, &quot;cleanup_logs&quot;))

triggered = engine.evaluate({&quot;cpu_usage&quot;: 95.0, &quot;error_rate&quot;: 2.0})
# → [&quot;scale_up&quot;]   — 完全确定，完全可预测
</code></pre>
<h3>2.2 Workflow / DAG</h3>
<p><strong>定义</strong>：预定义步骤的有序编排。步骤之间有依赖关系，可以有条件分支，但所有可能的路径在设计时已知。</p>
<p>典型实现：Airflow、Temporal、Prefect、Step Functions、BPMN Engine。</p>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                    Workflow / DAG                        │
│                                                         │
│   Start ──→ [Step A] ──→ [Step B] ──┬──→ [Step C]      │
│                                     │                   │
│                                     └──→ [Step D]      │
│                          │                    │         │
│                          └────────┬───────────┘         │
│                                   ▼                     │
│                              [Step E] ──→ End           │
│                                                         │
│   特征：路径在设计时确定，运行时按条件选择分支           │
│   类比：地铁线路图，站点和换乘规则预先设定               │
└─────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>核心特征</strong>：</p>
<ul>
<li>步骤预定义，依赖关系显式声明</li>
<li>有条件分支，但分支的数量和逻辑在设计时确定</li>
<li>支持重试、超时、补偿（Compensation）</li>
<li>可视化程度高——DAG 本身就是文档</li>
</ul>
<pre><code class="language-python"># 典型的 Workflow / DAG 定义（伪代码，框架无关）
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Callable

class StepStatus(Enum):
    PENDING = &quot;pending&quot;
    RUNNING = &quot;running&quot;
    SUCCESS = &quot;success&quot;
    FAILED = &quot;failed&quot;
    SKIPPED = &quot;skipped&quot;

@dataclass
class Step:
    name: str
    fn: Callable
    depends_on: list[str] = field(default_factory=list)
    condition: Callable | None = None  # 条件分支
    retry_count: int = 3
    timeout_seconds: int = 300

class DAGExecutor:
    def __init__(self):
        self.steps: dict[str, Step] = {}
        self.results: dict[str, Any] = {}
        self.status: dict[str, StepStatus] = {}

    def add_step(self, step: Step):
        self.steps[step.name] = step
        self.status[step.name] = StepStatus.PENDING

    def _can_run(self, step: Step) -&gt; bool:
        &quot;&quot;&quot;检查依赖是否全部完成&quot;&quot;&quot;
        for dep in step.depends_on:
            if self.status.get(dep) != StepStatus.SUCCESS:
                return False
        return True

    def _should_run(self, step: Step) -&gt; bool:
        &quot;&quot;&quot;检查条件分支&quot;&quot;&quot;
        if step.condition is None:
            return True
        return step.condition(self.results)

    def run(self, initial_context: dict):
        self.results.update(initial_context)
        # 简化的拓扑排序执行（生产实现应支持并行）
        remaining = set(self.steps.keys())
        while remaining:
            runnable = [
                name for name in remaining
                if self._can_run(self.steps[name])
            ]
            if not runnable:
                raise RuntimeError(&quot;DAG has unresolvable dependencies&quot;)
            for name in runnable:
                step = self.steps[name]
                remaining.remove(name)
                if not self._should_run(step):
                    self.status[name] = StepStatus.SKIPPED
                    continue
                self.status[name] = StepStatus.RUNNING
                try:
                    self.results[name] = step.fn(self.results)
                    self.status[name] = StepStatus.SUCCESS
                except Exception:
                    self.status[name] = StepStatus.FAILED
                    raise
</code></pre>
<h3>2.3 Agent</h3>
<p><strong>定义</strong>：LLM 驱动的动态决策执行。每一步做什么，由 LLM 在运行时根据当前状态决定。路径不确定，在执行前无法预知。</p>
<p>典型实现：ReAct Loop、LangGraph Agent、AutoGPT、自研 Agent Runtime。</p>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                       Agent                             │
│                                                         │
│   Input ──→ [LLM: 观察+思考] ──→ [Tool A] ──┐          │
│                     ▲                        │          │
│                     │                        ▼          │
│                     │            [LLM: 观察+思考]       │
│                     │                  │     │          │
│                     │         ┌────────┘     │          │
│                     │         ▼              ▼          │
│                     ├──── [Tool C]      [Tool B]        │
│                     │         │              │          │
│                     │         ▼              ▼          │
│                     └── [LLM: 够了吗？] ──→ Output      │
│                                                         │
│   特征：路径在运行时动态生成，每一步由 LLM 决定         │
│   类比：出租车司机，根据实时路况随时调整路线             │
└─────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>核心特征</strong>：</p>
<ul>
<li>运行时决策——下一步做什么由 LLM 在当前上下文中推理得出</li>
<li>非确定性：相同输入可能走不同路径（temperature &gt; 0 时尤为明显）</li>
<li>能处理模糊、开放、未预见的输入</li>
<li>每一步决策都需要 LLM 推理，延迟和成本显著高于前两者</li>
</ul>
<pre><code class="language-python"># 典型的 Agent Loop（极简实现）
from typing import Any

class Tool:
    def __init__(self, name: str, description: str, fn: callable):
        self.name = name
        self.description = description
        self.fn = fn

class Agent:
    def __init__(self, llm_client, tools: list[Tool], max_steps: int = 10):
        self.llm = llm_client
        self.tools = {t.name: t for t in tools}
        self.max_steps = max_steps

    def run(self, user_input: str) -&gt; str:
        messages = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input}]
        tool_descriptions = [
            {&quot;name&quot;: t.name, &quot;description&quot;: t.description}
            for t in self.tools.values()
        ]

        for step in range(self.max_steps):
            # LLM 决定下一步：调用工具，还是直接回答
            response = self.llm.chat(
                messages=messages,
                tools=tool_descriptions,
            )

            if response.is_final_answer:
                return response.content

            # LLM 选择了一个工具
            tool_name = response.tool_call.name
            tool_args = response.tool_call.arguments
            tool_result = self.tools[tool_name].fn(**tool_args)

            # 将工具结果加入上下文，进入下一轮循环
            messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: response.raw})
            messages.append({&quot;role&quot;: &quot;tool&quot;, &quot;content&quot;: str(tool_result)})

        return &quot;达到最大步数限制，未能完成任务。&quot;
</code></pre>
<p>注意上面代码的关键区别：<strong>Automation 和 Workflow 的控制流是代码写死的，Agent 的控制流是 LLM 在运行时生成的。</strong> 这是三者的本质差异。</p>
<hr>
<h2>3. 决策维度分析</h2>
<h3>3.1 对比总览</h3>
<table>
<thead>
<tr>
<th>维度</th>
<th>Rule-based Automation</th>
<th>Workflow / DAG</th>
<th>Agent</th>
</tr>
</thead>
<tbody><tr>
<td><strong>确定性</strong></td>
<td>完全确定</td>
<td>路径确定，结果依赖外部</td>
<td>不确定</td>
</tr>
<tr>
<td><strong>可解释性</strong></td>
<td>极强（规则可追溯）</td>
<td>强（DAG 可视化）</td>
<td>弱（LLM 是黑盒）</td>
</tr>
<tr>
<td><strong>延迟</strong></td>
<td>μs - ms</td>
<td>ms - min（取决于步骤）</td>
<td>s - min（LLM 推理）</td>
</tr>
<tr>
<td><strong>单次成本</strong></td>
<td>几乎为零</td>
<td>低（计算资源）</td>
<td>高（Token 费用）</td>
</tr>
<tr>
<td><strong>可靠性</strong></td>
<td>极高</td>
<td>高（有重试/补偿）</td>
<td>中等（LLM 可能幻觉）</td>
</tr>
<tr>
<td><strong>可观测性</strong></td>
<td>高（日志即文档）</td>
<td>高（DAG 天然可视化）</td>
<td>低（需要额外 Trace）</td>
</tr>
<tr>
<td><strong>灵活性</strong></td>
<td>低（新规则需改代码）</td>
<td>中（新步骤需改 DAG）</td>
<td>高（Prompt 即可调整）</td>
</tr>
<tr>
<td><strong>处理模糊输入</strong></td>
<td>不支持</td>
<td>有限支持</td>
<td>原生支持</td>
</tr>
<tr>
<td><strong>开发复杂度</strong></td>
<td>低</td>
<td>中</td>
<td>高</td>
</tr>
</tbody></table>
<h3>3.2 逐维度展开</h3>
<p><strong>确定性 vs 不确定性</strong></p>
<p>这是最重要的选型维度。问自己一个问题：<strong>给定相同的输入，系统是否必须产生相同的输出？</strong></p>
<ul>
<li>如果答案是&quot;必须&quot;——不要用 Agent。Rule-based Automation 或 Workflow 是正确选择。</li>
<li>如果答案是&quot;不一定，但结果需要在合理范围内&quot;——Agent 可以考虑，但要加 Guardrail。</li>
<li>如果答案是&quot;每次可以不同，只要合理就行&quot;——Agent 是自然选择。</li>
</ul>
<p>金融交易、订单状态流转、计费逻辑——这些场景如果引入 Agent 的非确定性，后果不堪设想。</p>
<p><strong>可解释性</strong></p>
<p>生产系统出了问题，你需要回答&quot;为什么系统做了这个决策&quot;。</p>
<ul>
<li>Rule Engine：直接查看匹配了哪条规则，一目了然。</li>
<li>Workflow：查看 DAG 执行日志，哪个步骤走了哪个分支，完全透明。</li>
<li>Agent：LLM 的推理过程是一段自然语言（Chain of Thought），但它可能是事后合理化，并不一定反映真实的&quot;推理过程&quot;。</li>
</ul>
<p>在合规要求高的领域（金融、医疗、法律），可解释性不是 nice-to-have，而是硬性要求。</p>
<p><strong>成本</strong></p>
<p>这一点经常被低估。以一个中等复杂度的任务为例：</p>
<pre><code>Rule Engine:  ~0 成本（CPU 时间可忽略）
Workflow:     ~$0.001（计算资源 + 存储）
Agent:        ~$0.01 - $0.50（取决于步骤数和模型选择）
              3 步 Agent × GPT-4 级别 ≈ 每次 $0.03-0.10
              如果日调用量 100K，月成本 = $3,000 - $10,000
</code></pre>
<p>当你把 Agent 用在本该用 Rule Engine 解决的问题上，你是在用 100 倍的成本获得更差的可靠性。</p>
<p><strong>可靠性</strong></p>
<ul>
<li>Rule Engine：只要规则正确，就永远正确。故障模式是规则覆盖不全。</li>
<li>Workflow：支持重试、幂等、补偿事务。成熟的 Workflow Engine 可以做到 99.99% 可靠。</li>
<li>Agent：LLM 可能幻觉、可能选错工具、可能陷入循环。即使加了 Guardrail，端到端成功率通常在 85%-95%（复杂任务更低）。</li>
</ul>
<p><strong>可观测性</strong></p>
<ul>
<li>Rule Engine：每次执行记录匹配规则和动作，日志本身就是完整的审计轨迹。</li>
<li>Workflow：DAG 执行引擎天然提供步骤级别的状态、耗时、输入输出。Airflow 的 UI 就是最好的例子。</li>
<li>Agent：你需要自己构建 Trace 系统——记录每一轮 LLM 的输入、输出、选择的工具、工具的返回值、Token 消耗。没有这些，Agent 在生产环境中就是一个黑盒。</li>
</ul>
<hr>
<h2>4. 场景分析</h2>
<p>抽象的对比不如具体场景有说服力。下面逐个分析。</p>
<h3>4.1 数据 ETL Pipeline → Workflow</h3>
<p><strong>场景</strong>：每天从 3 个数据源抽取数据，清洗、转换、加载到数据仓库。</p>
<p><strong>选 Workflow 的理由</strong>：</p>
<ul>
<li>步骤完全确定：Extract → Transform → Load，不需要运行时决策</li>
<li>步骤间有明确的依赖关系：Transform 必须在 Extract 之后</li>
<li>需要精确的重试和失败补偿：某个数据源失败了，只重跑那个分支</li>
<li>需要调度：每天凌晨 3 点执行</li>
<li>需要回填（Backfill）：补跑历史数据</li>
</ul>
<p><strong>为什么不用 Agent</strong>：</p>
<p>ETL 不需要&quot;思考下一步做什么&quot;——步骤是固定的。用 Agent 意味着每次运行都要花 Token 让 LLM &quot;重新发现&quot;这些固定步骤，纯属浪费。更危险的是，LLM 可能在某次运行中&quot;创造性地&quot;跳过某个步骤或改变转换逻辑。</p>
<h3>4.2 客服问答 → Agent</h3>
<p><strong>场景</strong>：用户通过聊天窗口提问，系统需要理解意图、查询知识库、可能需要查订单、可能需要转人工。</p>
<p><strong>选 Agent 的理由</strong>：</p>
<ul>
<li>输入是自然语言，意图不确定，无法枚举所有可能</li>
<li>处理路径取决于用户说了什么——可能一步就能回答，也可能需要查 3 个系统</li>
<li>需要上下文理解和多轮对话能力</li>
<li>&quot;足够好&quot;的回答即可，不需要 100% 确定性</li>
</ul>
<p><strong>为什么不用 Workflow</strong>：</p>
<p>你无法预定义所有可能的对话路径。用户可能问&quot;我的订单到哪了&quot;，也可能问&quot;你们支持退款吗&quot;，也可能在同一轮对话中先问订单再问退款政策。Workflow 的路径是编译期确定的，处理不了这种运行时的动态性。</p>
<h3>4.3 定时报表生成 → Automation</h3>
<p><strong>场景</strong>：每周一早上 9 点，从数据库查询上周的销售数据，生成 Excel 报表，发送到指定邮箱。</p>
<p><strong>选 Automation 的理由</strong>：</p>
<ul>
<li>触发条件确定：Cron 定时</li>
<li>逻辑确定：SQL 查询 → 格式化 → 发送</li>
<li>不需要编排复杂依赖</li>
<li>不需要任何&quot;智能&quot;——SQL 和模板都是写死的</li>
</ul>
<p><strong>为什么不用 Workflow 或 Agent</strong>：</p>
<p>Workflow 是大炮打蚊子——这里没有复杂的步骤依赖和分支。Agent 更是离谱——你不需要 LLM 来执行 <code>SELECT SUM(amount) FROM orders WHERE date &gt;= &#39;2025-07-28&#39;</code>。</p>
<h3>4.4 代码审查助手 → Agent</h3>
<p><strong>场景</strong>：PR 提交后，自动分析代码变更，给出审查意见：安全隐患、性能问题、风格建议。</p>
<p><strong>选 Agent 的理由</strong>：</p>
<ul>
<li>代码变更是非结构化的，无法穷举所有模式</li>
<li>需要&quot;理解&quot;代码语义，而非简单的模式匹配（静态分析工具已经覆盖了模式匹配的部分）</li>
<li>审查意见需要结合上下文（这个函数在项目中是怎么用的？改动会影响什么？）</li>
<li>Agent 可以调用多种工具：读取文件、运行测试、查看 Git 历史</li>
</ul>
<p><strong>为什么不用 Rule Engine</strong>：</p>
<p>Rule Engine 只能匹配预定义模式（如&quot;函数超过 100 行&quot;），无法理解语义层面的问题（如&quot;这个 API 调用没有处理超时&quot;）。实际上，最好的方案是 <strong>Rule Engine + Agent</strong>——先用 Linter/SAST 做确定性检查，再用 Agent 做语义级审查。</p>
<h3>4.5 订单状态流转 → Workflow</h3>
<p><strong>场景</strong>：电商订单从创建到完成的状态机——待支付 → 已支付 → 拣货中 → 已发货 → 已签收 → 已完成。</p>
<p><strong>选 Workflow 的理由</strong>：</p>
<ul>
<li>状态和转换规则完全确定：已支付才能拣货，已发货才能签收</li>
<li>每个状态转换都有明确的触发条件（支付回调、物流推送）</li>
<li>需要事务保证：状态转换必须原子性，不能出现&quot;钱扣了但订单还是待支付&quot;</li>
<li>需要补偿机制：支付超时需要自动取消</li>
<li>0 容忍非确定性——用户的钱不能有任何模糊</li>
</ul>
<p><strong>为什么不用 Agent</strong>：</p>
<p>这个问题需要反复强调：<strong>涉及金钱和状态一致性的流程，绝对不能用 Agent。</strong> LLM 的幻觉在这里不是&quot;回答不太准确&quot;，而是&quot;用户的钱没了但订单没更新&quot;。</p>
<h3>4.6 智能运维（AIOps）→ 混合架构</h3>
<p><strong>场景</strong>：监控告警触发后，自动诊断根因并执行修复。</p>
<p><strong>为什么需要混合</strong>：</p>
<p>这个场景天然分为确定性部分和不确定性部分——</p>
<ul>
<li>确定性部分（Automation）：告警规则匹配、阈值判断、常见故障的自动修复（CPU 高 → 扩容，磁盘满 → 清理日志）</li>
<li>不确定性部分（Agent）：复杂故障的根因分析——Agent 可以查看日志、查询指标、检查最近的部署变更，综合判断根因</li>
<li>编排部分（Workflow）：整个处理流程的骨架——告警接收 → 去重 → 分级 → 自动修复 / 智能诊断 → 通知</li>
</ul>
<pre><code>告警触发
   │
   ▼
[Automation: 告警去重 + 分级]
   │
   ├──→ P4/P3 已知模式 ──→ [Automation: 自动修复]
   │                              │
   │                              ▼
   │                         [通知 Oncall]
   │
   └──→ P2/P1 或未知模式 ──→ [Agent: 根因分析]
                                   │
                                   ├──→ 找到根因 ──→ [Automation: 执行修复]
                                   │
                                   └──→ 无法确定 ──→ [升级到人工]
</code></pre>
<p>这才是 Agent 的正确用法——<strong>只在真正需要&quot;智能&quot;的环节使用 Agent，其余部分用更可靠、更便宜的范式处理。</strong></p>
<hr>
<h2>5. 混合架构：三者如何共存</h2>
<p>真实的生产系统很少只用一种范式。更常见的模式是：</p>
<pre><code>┌──────────────────────────────────────────────────────────────┐
│                    混合架构全景                                │
│                                                              │
│  ┌──────────────────────────────────────────────────┐        │
│  │              Workflow / DAG（骨架层）              │        │
│  │                                                  │        │
│  │  Step 1          Step 2          Step 3          │        │
│  │  ┌──────────┐   ┌──────────┐   ┌──────────┐     │        │
│  │  │Automation│──→│  Agent   │──→│Automation│     │        │
│  │  │数据预处理│   │语义分析  │   │结果写入  │     │        │
│  │  └──────────┘   └──────────┘   └──────────┘     │        │
│  │       │              │              │            │        │
│  │       ▼              ▼              ▼            │        │
│  │  确定性操作     LLM 推理       确定性操作        │        │
│  │  延迟: 10ms    延迟: 2-5s     延迟: 50ms        │        │
│  │  成本: ~0      成本: $0.02    成本: ~0           │        │
│  └──────────────────────────────────────────────────┘        │
│                                                              │
│  设计原则：                                                  │
│  1. Workflow 负责编排和容错（重试、超时、补偿）              │
│  2. Automation 处理所有确定性步骤                            │
│  3. Agent 只出现在需要&quot;理解&quot;和&quot;推理&quot;的节点                  │
│  4. Agent 的输出经过验证后才进入下一步                       │
└──────────────────────────────────────────────────────────────┘
</code></pre>
<h3>5.1 设计原则</h3>
<p><strong>原则一：Agent 是 Workflow 的节点，不是整个 Workflow</strong></p>
<p>一个常见的错误是让 Agent 控制整个流程——从数据获取到处理到存储全部由 Agent 决定。正确的做法是：Workflow 定义骨架（步骤顺序、依赖关系、容错策略），Agent 只负责其中需要推理的那一步。</p>
<pre><code class="language-python"># 错误做法：让 Agent 控制整个流程
agent.run(&quot;从数据库读取用户评论，分析情感，把结果写回数据库&quot;)
# Agent 可能：用错 SQL、忘记写回、写入格式错误...

# 正确做法：Workflow 控制流程，Agent 只做推理
def step_1_extract(ctx):
    &quot;&quot;&quot;确定性步骤：用固定 SQL 读取数据&quot;&quot;&quot;
    return db.query(&quot;SELECT id, comment FROM reviews WHERE date = %s&quot;, ctx[&quot;date&quot;])

def step_2_analyze(ctx):
    &quot;&quot;&quot;Agent 步骤：对每条评论做情感分析&quot;&quot;&quot;
    results = []
    for review in ctx[&quot;step_1_extract&quot;]:
        sentiment = agent.run(
            f&quot;分析以下评论的情感倾向(positive/negative/neutral):\n{review[&#39;comment&#39;]}&quot;
        )
        results.append({&quot;id&quot;: review[&quot;id&quot;], &quot;sentiment&quot;: sentiment})
    return results

def step_3_load(ctx):
    &quot;&quot;&quot;确定性步骤：用固定逻辑写回数据库&quot;&quot;&quot;
    for item in ctx[&quot;step_2_analyze&quot;]:
        db.execute(
            &quot;UPDATE reviews SET sentiment = %s WHERE id = %s&quot;,
            (item[&quot;sentiment&quot;], item[&quot;id&quot;])
        )

# Workflow 定义
workflow.add_step(Step(&quot;extract&quot;, step_1_extract))
workflow.add_step(Step(&quot;analyze&quot;, step_2_analyze, depends_on=[&quot;extract&quot;]))
workflow.add_step(Step(&quot;load&quot;, step_3_load, depends_on=[&quot;analyze&quot;]))
</code></pre>
<p><strong>原则二：Agent 的输出必须经过验证</strong></p>
<p>Agent 的输出是非确定性的。在混合架构中，Agent 节点和下游确定性节点之间，必须有一个验证层。</p>
<pre><code class="language-python">def step_2_analyze_with_validation(ctx):
    &quot;&quot;&quot;Agent 步骤 + 输出验证&quot;&quot;&quot;
    VALID_SENTIMENTS = {&quot;positive&quot;, &quot;negative&quot;, &quot;neutral&quot;}
    results = []
    for review in ctx[&quot;step_1_extract&quot;]:
        sentiment = agent.run(f&quot;分析情感倾向: {review[&#39;comment&#39;]}&quot;)
        # 验证 Agent 输出
        sentiment = sentiment.strip().lower()
        if sentiment not in VALID_SENTIMENTS:
            sentiment = &quot;neutral&quot;  # fallback
            log.warning(f&quot;Agent 返回了无效的情感值，已 fallback: review_id={review[&#39;id&#39;]}&quot;)
        results.append({&quot;id&quot;: review[&quot;id&quot;], &quot;sentiment&quot;: sentiment})
    return results
</code></pre>
<p><strong>原则三：确定性部分永远优先用 Automation</strong></p>
<p>如果一个步骤的输入输出可以完全预定义，就不要用 Agent。这不是技术保守，而是工程理性——用最简单的工具解决问题，把复杂性预算留给真正需要的地方。</p>
<hr>
<h2>6. Agent 的隐性成本</h2>
<p>这一节讲的是大部分&quot;Agent 教程&quot;不会告诉你的东西。</p>
<h3>6.1 Token 成本</h3>
<p>Agent 的每一步决策都需要调用 LLM。一个 5 步 Agent 执行一次任务的 Token 消耗：</p>
<pre><code>第 1 步: System Prompt (500) + User Input (200) + Response (300)    = 1,000 tokens
第 2 步: 上一轮上下文 (1,000) + Tool Result (500) + Response (400)  = 1,900 tokens
第 3 步: 上一轮上下文 (1,900) + Tool Result (300) + Response (350)  = 2,550 tokens
第 4 步: 上一轮上下文 (2,550) + Tool Result (800) + Response (400)  = 3,750 tokens
第 5 步: 上一轮上下文 (3,750) + Tool Result (200) + Response (500)  = 4,450 tokens
                                                          ─────────────────────
                                                          总计: ~13,650 tokens
</code></pre>
<p>注意上下文是累积的——每一步都要重新发送之前所有的对话历史。这意味着 <strong>Token 消耗是超线性增长的</strong>。步骤越多，后期每一步的成本越高。</p>
<p>以 GPT-4o 为例（$2.5/1M input, $10/1M output），上面这个 5 步 Agent 单次执行成本约 $0.03-0.05。看似不多，但如果日调用量 10 万次，月成本就是 $90,000-$150,000。</p>
<p><strong>优化策略</strong>：</p>
<ul>
<li>上下文压缩：每 N 步对历史做一次摘要</li>
<li>选择合适的模型：简单决策用小模型，关键决策用大模型</li>
<li>缓存：对相同输入的 Agent 结果做缓存（注意非确定性问题）</li>
<li>减少 Agent 步骤：通过更好的 Prompt 和工具设计，减少所需的推理轮次</li>
</ul>
<h3>6.2 延迟</h3>
<p>LLM 的推理延迟通常在 500ms-5s 之间（取决于模型和输出长度）。一个 5 步 Agent 的端到端延迟：</p>
<pre><code>5 步 × 平均 1.5s/步 = 7.5s

加上工具调用时间（网络请求、数据库查询等），实际延迟可能在 10-15s。
</code></pre>
<p>对比：</p>
<ul>
<li>Rule Engine 处理同样的逻辑：&lt; 10ms</li>
<li>Workflow 执行 5 个确定性步骤：&lt; 500ms</li>
</ul>
<p><strong>在延迟敏感的场景（如支付、交易、实时推荐），Agent 的延迟是不可接受的。</strong></p>
<h3>6.3 不可预测性</h3>
<p>这是 Agent 最被低估的问题。</p>
<pre><code class="language-python"># 同样的输入，Agent 可能走出完全不同的路径

# 第一次运行
# Step 1: 调用 search_database → 找到 3 条记录
# Step 2: 调用 analyze_data → 生成分析
# Step 3: 返回结果
# 总计: 3 步, 耗时 4s, 成本 $0.02

# 第二次运行（完全相同的输入）
# Step 1: 调用 search_database → 找到 3 条记录
# Step 2: 调用 search_web → 想找更多信息（为什么？LLM 这次觉得不够）
# Step 3: 调用 search_database → 用新的关键词再查一次
# Step 4: 调用 analyze_data → 生成分析
# Step 5: 觉得分析不够好，调用 analyze_data → 重新生成
# Step 6: 返回结果
# 总计: 6 步, 耗时 10s, 成本 $0.06
</code></pre>
<p>这意味着你<strong>无法预测 Agent 的执行时间和成本</strong>。在需要做容量规划和 SLA 承诺的生产系统中，这是一个严重的问题。</p>
<h3>6.4 调试困难</h3>
<p>确定性系统的 Bug 可以精确复现：相同的输入 + 相同的代码 = 相同的 Bug。</p>
<p>Agent 不行。因为：</p>
<ol>
<li>LLM 的输出本身带有随机性（即使 temperature=0，不同批次推理也可能有微小差异）</li>
<li>工具调用的结果可能随时间变化（数据库内容变了、API 返回变了）</li>
<li>上下文窗口中的信息累积，前几步的微小差异会被放大</li>
</ol>
<p><strong>调试 Agent 的正确做法</strong>：</p>
<ul>
<li>完整记录每一步的输入（包括完整的 messages 列表）和输出</li>
<li>记录每次工具调用的参数和返回值</li>
<li>记录 Token 使用量和延迟</li>
<li>支持&quot;回放&quot;——用记录的数据重新走一遍流程（但要注意，即使相同输入，LLM 也可能给出不同输出）</li>
</ul>
<hr>
<h2>7. 选型决策树</h2>
<p>面对一个具体需求，按以下流程判断：</p>
<pre><code>                    你的任务需要&quot;理解&quot;自然语言
                    或处理模糊/开放式输入吗？
                           │
                    ┌──────┴──────┐
                    │             │
                   Yes           No
                    │             │
                    ▼             ▼
             结果需要 100%     任务步骤之间有
             确定性吗？        复杂依赖关系吗？
                │                    │
          ┌─────┴─────┐        ┌─────┴─────┐
          │           │        │           │
         Yes         No       Yes         No
          │           │        │           │
          ▼           ▼        ▼           ▼
      先用规则     ┌──────┐  Workflow    Automation
      处理能处     │Agent │  / DAG      (Rule/Cron)
      理的部分     └──┬───┘
      用 Agent        │
      处理剩余        ▼
      (混合架构)   可以接受 $0.01-0.10/次
                   的成本和 2-10s 的延迟吗？
                          │
                    ┌─────┴─────┐
                    │           │
                   Yes         No
                    │           │
                    ▼           ▼
                  Agent     重新审视需求：
                            能否拆分为
                            确定性 + 模糊性部分？
                            → 混合架构
</code></pre>
<p><strong>速查表</strong>：</p>
<table>
<thead>
<tr>
<th>如果你的任务是...</th>
<th>推荐范式</th>
<th>理由</th>
</tr>
</thead>
<tbody><tr>
<td>固定逻辑 + 定时触发</td>
<td>Automation</td>
<td>无需编排，无需推理</td>
</tr>
<tr>
<td>多步骤 + 有依赖 + 确定性</td>
<td>Workflow</td>
<td>需要编排，不需要推理</td>
</tr>
<tr>
<td>理解自然语言 + 动态决策</td>
<td>Agent</td>
<td>需要推理</td>
</tr>
<tr>
<td>大部分确定 + 少量模糊</td>
<td>Workflow + Agent 节点</td>
<td>编排确定部分，推理模糊部分</td>
</tr>
<tr>
<td>简单触发 + 复杂诊断</td>
<td>Automation + Agent</td>
<td>触发用规则，诊断用推理</td>
</tr>
</tbody></table>
<hr>
<h2>8. 常见误区</h2>
<p>在结束之前，总结几个我在实际项目中反复见到的选型错误。</p>
<p><strong>误区一：因为&quot;想用 AI&quot;而选 Agent</strong></p>
<p>技术选型应该从问题出发，不是从解决方案出发。&quot;我们想用 AI&quot; 不是选 Agent 的理由，&quot;用户输入是自然语言且意图不可穷举&quot; 才是。</p>
<p><strong>误区二：用 Agent 替代状态机</strong></p>
<p>订单流转、审批流程、工单生命周期——这些有限状态机（FSM）问题有成熟的解决方案。把它们交给 Agent 不会让系统更智能，只会让它更不可靠。</p>
<p><strong>误区三：Agent 做完所有事</strong></p>
<p>让 Agent 既负责决策又负责执行。正确做法是：Agent 只负责&quot;决定做什么&quot;（What），具体的执行（How）交给确定性系统。例如 Agent 决定&quot;需要给用户退款&quot;，但实际调用退款 API 的逻辑是固定的代码，不是 Agent 自己拼 HTTP 请求。</p>
<p><strong>误区四：忽视 Agent 的失败模式</strong></p>
<p>Agent 会失败。它会幻觉、会陷入循环、会选错工具、会超时。你的系统设计必须考虑：Agent 失败了怎么办？有没有 Fallback？有没有人工兜底？最大重试次数是多少？</p>
<hr>
<h2>9. 总结</h2>
<p>回到开篇的问题：你的问题，真的需要 Agent 吗？</p>
<p>三条准则：</p>
<ol>
<li><strong>能用规则解决的，不要用 Workflow；能用 Workflow 解决的，不要用 Agent。</strong> 选择复杂度最低的范式，降低的是长期维护成本。</li>
<li><strong>Agent 的正确位置是&quot;最后一英里的模糊性&quot;。</strong> 在混合架构中，让确定性系统处理 80% 的工作，Agent 只处理那 20% 需要&quot;理解&quot;和&quot;推理&quot;的部分。</li>
<li><strong>Agent 是有代价的，而且代价比你想象的高。</strong> Token 成本、延迟、不可预测性、调试难度——这些隐性成本在规模化后会成为真实的痛点。</li>
</ol>
<p>选对抽象，才是真正的技术判断力。</p>
<hr>
<blockquote>
<p><strong>系列导航</strong>：本文是 Agentic 系列的第 03 篇。</p>
<ul>
<li>上一篇：<a href="/blog/engineering/agentic/02-From%20Prompt%20to%20Agent">02 | From Prompt to Agent</a></li>
<li>下一篇：<a href="/blog/engineering/agentic/04-The%20Agent%20Control%20Loop">04 | The Agent Control Loop</a></li>
<li>完整目录：<a href="/blog/engineering/agentic/01-From%20LLM%20to%20Agent">01 | From LLM to Agent</a></li>
</ul>
</blockquote>
</div></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><div class="mt-12 pt-8 border-t border-gray-200">加载导航中...</div><!--/$--><div class="mt-16 border-t border-gray-200 pt-8"><div class="mx-auto max-w-3xl"><h3 class="text-2xl font-bold text-gray-900 mb-8">评论</h3></div></div></div></div></article><!--$--><!--/$--></main><footer class="bg-[var(--background)]"><div class="mx-auto max-w-7xl px-6 py-12 lg:px-8"><p class="text-center text-xs leading-5 text-gray-400">© <!-- -->2026<!-- --> Skyfalling</p></div></footer></div><script src="/_next/static/chunks/webpack-42d55485b4428e47.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[10616,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"7177\",\"static/chunks/app/layout-142e67ac4336647c.js\"],\"default\"]\n3:I[87555,[],\"\"]\n4:I[31295,[],\"\"]\n6:I[59665,[],\"OutletBoundary\"]\n9:I[74911,[],\"AsyncMetadataOutlet\"]\nb:I[59665,[],\"ViewportBoundary\"]\nd:I[59665,[],\"MetadataBoundary\"]\nf:I[26614,[],\"\"]\n:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/232416e7c3a1ca7e.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"Zxm2PPmP1ogY3qYZcx7aw\",\"p\":\"\",\"c\":[\"\",\"blog\",\"engineering\",\"agentic\",\"03-Agent%20vs%20Workflow%20vs%20Automation\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"engineering/agentic/03-Agent%20vs%20Workflow%20vs%20Automation\",\"c\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/232416e7c3a1ca7e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"zh-CN\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_f367f3\",\"children\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex flex-col\",\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"main\",null,{\"className\":\"flex-1\",\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"footer\",null,{\"className\":\"bg-[var(--background)]\",\"children\":[\"$\",\"div\",null,{\"className\":\"mx-auto max-w-7xl px-6 py-12 lg:px-8\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-center text-xs leading-5 text-gray-400\",\"children\":[\"© \",2026,\" Skyfalling\"]}]}]}]]}]}]}]]}],{\"children\":[\"blog\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"engineering/agentic/03-Agent%20vs%20Workflow%20vs%20Automation\",\"c\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",null,[\"$\",\"$L6\",null,{\"children\":[\"$L7\",\"$L8\",[\"$\",\"$L9\",null,{\"promise\":\"$@a\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"MggnObXDnj0SFcSkZFRMcv\",{\"children\":[[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],[\"$\",\"$Ld\",null,{\"children\":\"$Le\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$f\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"10:\"$Sreact.suspense\"\n11:I[74911,[],\"AsyncMetadata\"]\n13:I[6874,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"\"]\n14:I[32923,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"default\"]\n16:I[40780,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"default\"]\n19:I[85300,[\"6874\",\"static/chunks/6874-7791217feaf05c17.js\",\"968\",\"static/chunks/968-d7155a2506e36f1d.js\",\"6909\",\"static/chunks/app/blog/%5B...slug%5D/page-3137b04d8f9ed325.js\"],\"default\"]\ne:[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$10\",null,{\"fallback\":null,\"children\":[\"$\",\"$L11\",null,{\"promise\":\"$@12\"}]}]}]\n15:T9547,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eAgent vs Workflow vs Automation: 选对抽象才是关键\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e系列第 03 篇。上一篇我们讲了\u0026quot;LLM 本身不是 Agent\u0026quot;，这一篇要回答一个更实际的问题：\u003cstrong\u003e你的问题，真的需要 Agent 吗？\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch2\u003e1. 开篇：Agent 万能论的陷阱\u003c/h2\u003e\n\u003cp\u003e2024 年以来，\u0026quot;Agent\u0026quot; 这个词已经被严重滥用。打开任何一篇技术文章，似乎所有系统都应该被重写为 Agent——客服要 Agent、ETL 要 Agent、运维要 Agent、审批要 Agent。\u003c/p\u003e\n\u003cp\u003e但现实是：\u003cstrong\u003e大部分生产系统中，80% 的任务用 if/else 和 DAG 就能解决，且解决得更好。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAgent 不是银弹。它是一种特定的执行范式，适用于特定的问题空间。盲目使用 Agent 的代价是：更高的 Token 成本、更长的延迟、更难的调试、更差的可预测性。\u003c/p\u003e\n\u003cp\u003e这篇文章的目标很简单：帮你建立一个清晰的选型框架。面对一个具体问题，你应该能在 30 秒内判断——\u003cstrong\u003e用 Automation、用 Workflow、还是用 Agent。\u003c/strong\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e2. 三种执行范式\u003c/h2\u003e\n\u003ch3\u003e2.1 Rule-based Automation\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e定义\u003c/strong\u003e：用预定义规则驱动的全自动执行。输入确定，规则确定，输出确定。\u003c/p\u003e\n\u003cp\u003e典型实现：if/else 逻辑、Rule Engine（Drools、Rete）、Cron Job、Event Trigger。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌─────────────────────────────────────────────────────────┐\n│                 Rule-based Automation                    │\n│                                                         │\n│   Input ──→ [Rule Match] ──→ Action A                   │\n│                  │                                      │\n│                  ├──→ Action B                           │\n│                  │                                      │\n│                  └──→ Action C                           │\n│                                                         │\n│   特征：路径在编写时完全确定，运行时无决策               │\n│   类比：铁轨上的火车，轨道已铺好                         │\n└─────────────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e核心特征\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e零运行时决策——所有分支在代码 / 规则编写时就已确定\u003c/li\u003e\n\u003cli\u003e确定性：相同输入永远产生相同输出\u003c/li\u003e\n\u003cli\u003e延迟极低（微秒到毫秒级）\u003c/li\u003e\n\u003cli\u003e可解释性最强——每一步都可以追溯到具体规则\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 典型的 Rule-based Automation\nclass AlertRule:\n    def __init__(self, metric: str, threshold: float, action: str):\n        self.metric = metric\n        self.threshold = threshold\n        self.action = action\n\nclass RuleEngine:\n    def __init__(self):\n        self.rules: list[AlertRule] = []\n\n    def add_rule(self, rule: AlertRule):\n        self.rules.append(rule)\n\n    def evaluate(self, metrics: dict[str, float]) -\u0026gt; list[str]:\n        \u0026quot;\u0026quot;\u0026quot;对每条指标做规则匹配，返回触发的动作列表\u0026quot;\u0026quot;\u0026quot;\n        actions = []\n        for rule in self.rules:\n            value = metrics.get(rule.metric)\n            if value is not None and value \u0026gt; rule.threshold:\n                actions.append(rule.action)\n        return actions\n\n# 使用\nengine = RuleEngine()\nengine.add_rule(AlertRule(\u0026quot;cpu_usage\u0026quot;, 90.0, \u0026quot;scale_up\u0026quot;))\nengine.add_rule(AlertRule(\u0026quot;error_rate\u0026quot;, 5.0, \u0026quot;page_oncall\u0026quot;))\nengine.add_rule(AlertRule(\u0026quot;disk_usage\u0026quot;, 85.0, \u0026quot;cleanup_logs\u0026quot;))\n\ntriggered = engine.evaluate({\u0026quot;cpu_usage\u0026quot;: 95.0, \u0026quot;error_rate\u0026quot;: 2.0})\n# → [\u0026quot;scale_up\u0026quot;]   — 完全确定，完全可预测\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2.2 Workflow / DAG\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e定义\u003c/strong\u003e：预定义步骤的有序编排。步骤之间有依赖关系，可以有条件分支，但所有可能的路径在设计时已知。\u003c/p\u003e\n\u003cp\u003e典型实现：Airflow、Temporal、Prefect、Step Functions、BPMN Engine。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌─────────────────────────────────────────────────────────┐\n│                    Workflow / DAG                        │\n│                                                         │\n│   Start ──→ [Step A] ──→ [Step B] ──┬──→ [Step C]      │\n│                                     │                   │\n│                                     └──→ [Step D]      │\n│                          │                    │         │\n│                          └────────┬───────────┘         │\n│                                   ▼                     │\n│                              [Step E] ──→ End           │\n│                                                         │\n│   特征：路径在设计时确定，运行时按条件选择分支           │\n│   类比：地铁线路图，站点和换乘规则预先设定               │\n└─────────────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e核心特征\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e步骤预定义，依赖关系显式声明\u003c/li\u003e\n\u003cli\u003e有条件分支，但分支的数量和逻辑在设计时确定\u003c/li\u003e\n\u003cli\u003e支持重试、超时、补偿（Compensation）\u003c/li\u003e\n\u003cli\u003e可视化程度高——DAG 本身就是文档\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 典型的 Workflow / DAG 定义（伪代码，框架无关）\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Callable\n\nclass StepStatus(Enum):\n    PENDING = \u0026quot;pending\u0026quot;\n    RUNNING = \u0026quot;running\u0026quot;\n    SUCCESS = \u0026quot;success\u0026quot;\n    FAILED = \u0026quot;failed\u0026quot;\n    SKIPPED = \u0026quot;skipped\u0026quot;\n\n@dataclass\nclass Step:\n    name: str\n    fn: Callable\n    depends_on: list[str] = field(default_factory=list)\n    condition: Callable | None = None  # 条件分支\n    retry_count: int = 3\n    timeout_seconds: int = 300\n\nclass DAGExecutor:\n    def __init__(self):\n        self.steps: dict[str, Step] = {}\n        self.results: dict[str, Any] = {}\n        self.status: dict[str, StepStatus] = {}\n\n    def add_step(self, step: Step):\n        self.steps[step.name] = step\n        self.status[step.name] = StepStatus.PENDING\n\n    def _can_run(self, step: Step) -\u0026gt; bool:\n        \u0026quot;\u0026quot;\u0026quot;检查依赖是否全部完成\u0026quot;\u0026quot;\u0026quot;\n        for dep in step.depends_on:\n            if self.status.get(dep) != StepStatus.SUCCESS:\n                return False\n        return True\n\n    def _should_run(self, step: Step) -\u0026gt; bool:\n        \u0026quot;\u0026quot;\u0026quot;检查条件分支\u0026quot;\u0026quot;\u0026quot;\n        if step.condition is None:\n            return True\n        return step.condition(self.results)\n\n    def run(self, initial_context: dict):\n        self.results.update(initial_context)\n        # 简化的拓扑排序执行（生产实现应支持并行）\n        remaining = set(self.steps.keys())\n        while remaining:\n            runnable = [\n                name for name in remaining\n                if self._can_run(self.steps[name])\n            ]\n            if not runnable:\n                raise RuntimeError(\u0026quot;DAG has unresolvable dependencies\u0026quot;)\n            for name in runnable:\n                step = self.steps[name]\n                remaining.remove(name)\n                if not self._should_run(step):\n                    self.status[name] = StepStatus.SKIPPED\n                    continue\n                self.status[name] = StepStatus.RUNNING\n                try:\n                    self.results[name] = step.fn(self.results)\n                    self.status[name] = StepStatus.SUCCESS\n                except Exception:\n                    self.status[name] = StepStatus.FAILED\n                    raise\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2.3 Agent\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e定义\u003c/strong\u003e：LLM 驱动的动态决策执行。每一步做什么，由 LLM 在运行时根据当前状态决定。路径不确定，在执行前无法预知。\u003c/p\u003e\n\u003cp\u003e典型实现：ReAct Loop、LangGraph Agent、AutoGPT、自研 Agent Runtime。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌─────────────────────────────────────────────────────────┐\n│                       Agent                             │\n│                                                         │\n│   Input ──→ [LLM: 观察+思考] ──→ [Tool A] ──┐          │\n│                     ▲                        │          │\n│                     │                        ▼          │\n│                     │            [LLM: 观察+思考]       │\n│                     │                  │     │          │\n│                     │         ┌────────┘     │          │\n│                     │         ▼              ▼          │\n│                     ├──── [Tool C]      [Tool B]        │\n│                     │         │              │          │\n│                     │         ▼              ▼          │\n│                     └── [LLM: 够了吗？] ──→ Output      │\n│                                                         │\n│   特征：路径在运行时动态生成，每一步由 LLM 决定         │\n│   类比：出租车司机，根据实时路况随时调整路线             │\n└─────────────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e核心特征\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e运行时决策——下一步做什么由 LLM 在当前上下文中推理得出\u003c/li\u003e\n\u003cli\u003e非确定性：相同输入可能走不同路径（temperature \u0026gt; 0 时尤为明显）\u003c/li\u003e\n\u003cli\u003e能处理模糊、开放、未预见的输入\u003c/li\u003e\n\u003cli\u003e每一步决策都需要 LLM 推理，延迟和成本显著高于前两者\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 典型的 Agent Loop（极简实现）\nfrom typing import Any\n\nclass Tool:\n    def __init__(self, name: str, description: str, fn: callable):\n        self.name = name\n        self.description = description\n        self.fn = fn\n\nclass Agent:\n    def __init__(self, llm_client, tools: list[Tool], max_steps: int = 10):\n        self.llm = llm_client\n        self.tools = {t.name: t for t in tools}\n        self.max_steps = max_steps\n\n    def run(self, user_input: str) -\u0026gt; str:\n        messages = [{\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: user_input}]\n        tool_descriptions = [\n            {\u0026quot;name\u0026quot;: t.name, \u0026quot;description\u0026quot;: t.description}\n            for t in self.tools.values()\n        ]\n\n        for step in range(self.max_steps):\n            # LLM 决定下一步：调用工具，还是直接回答\n            response = self.llm.chat(\n                messages=messages,\n                tools=tool_descriptions,\n            )\n\n            if response.is_final_answer:\n                return response.content\n\n            # LLM 选择了一个工具\n            tool_name = response.tool_call.name\n            tool_args = response.tool_call.arguments\n            tool_result = self.tools[tool_name].fn(**tool_args)\n\n            # 将工具结果加入上下文，进入下一轮循环\n            messages.append({\u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;, \u0026quot;content\u0026quot;: response.raw})\n            messages.append({\u0026quot;role\u0026quot;: \u0026quot;tool\u0026quot;, \u0026quot;content\u0026quot;: str(tool_result)})\n\n        return \u0026quot;达到最大步数限制，未能完成任务。\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e注意上面代码的关键区别：\u003cstrong\u003eAutomation 和 Workflow 的控制流是代码写死的，Agent 的控制流是 LLM 在运行时生成的。\u003c/strong\u003e 这是三者的本质差异。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e3. 决策维度分析\u003c/h2\u003e\n\u003ch3\u003e3.1 对比总览\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e维度\u003c/th\u003e\n\u003cth\u003eRule-based Automation\u003c/th\u003e\n\u003cth\u003eWorkflow / DAG\u003c/th\u003e\n\u003cth\u003eAgent\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e确定性\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e完全确定\u003c/td\u003e\n\u003ctd\u003e路径确定，结果依赖外部\u003c/td\u003e\n\u003ctd\u003e不确定\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e可解释性\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e极强（规则可追溯）\u003c/td\u003e\n\u003ctd\u003e强（DAG 可视化）\u003c/td\u003e\n\u003ctd\u003e弱（LLM 是黑盒）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e延迟\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eμs - ms\u003c/td\u003e\n\u003ctd\u003ems - min（取决于步骤）\u003c/td\u003e\n\u003ctd\u003es - min（LLM 推理）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e单次成本\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e几乎为零\u003c/td\u003e\n\u003ctd\u003e低（计算资源）\u003c/td\u003e\n\u003ctd\u003e高（Token 费用）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e可靠性\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e极高\u003c/td\u003e\n\u003ctd\u003e高（有重试/补偿）\u003c/td\u003e\n\u003ctd\u003e中等（LLM 可能幻觉）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e可观测性\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e高（日志即文档）\u003c/td\u003e\n\u003ctd\u003e高（DAG 天然可视化）\u003c/td\u003e\n\u003ctd\u003e低（需要额外 Trace）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e灵活性\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e低（新规则需改代码）\u003c/td\u003e\n\u003ctd\u003e中（新步骤需改 DAG）\u003c/td\u003e\n\u003ctd\u003e高（Prompt 即可调整）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e处理模糊输入\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e不支持\u003c/td\u003e\n\u003ctd\u003e有限支持\u003c/td\u003e\n\u003ctd\u003e原生支持\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e开发复杂度\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e低\u003c/td\u003e\n\u003ctd\u003e中\u003c/td\u003e\n\u003ctd\u003e高\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch3\u003e3.2 逐维度展开\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e确定性 vs 不确定性\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e这是最重要的选型维度。问自己一个问题：\u003cstrong\u003e给定相同的输入，系统是否必须产生相同的输出？\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e如果答案是\u0026quot;必须\u0026quot;——不要用 Agent。Rule-based Automation 或 Workflow 是正确选择。\u003c/li\u003e\n\u003cli\u003e如果答案是\u0026quot;不一定，但结果需要在合理范围内\u0026quot;——Agent 可以考虑，但要加 Guardrail。\u003c/li\u003e\n\u003cli\u003e如果答案是\u0026quot;每次可以不同，只要合理就行\u0026quot;——Agent 是自然选择。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e金融交易、订单状态流转、计费逻辑——这些场景如果引入 Agent 的非确定性，后果不堪设想。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e可解释性\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e生产系统出了问题，你需要回答\u0026quot;为什么系统做了这个决策\u0026quot;。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRule Engine：直接查看匹配了哪条规则，一目了然。\u003c/li\u003e\n\u003cli\u003eWorkflow：查看 DAG 执行日志，哪个步骤走了哪个分支，完全透明。\u003c/li\u003e\n\u003cli\u003eAgent：LLM 的推理过程是一段自然语言（Chain of Thought），但它可能是事后合理化，并不一定反映真实的\u0026quot;推理过程\u0026quot;。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在合规要求高的领域（金融、医疗、法律），可解释性不是 nice-to-have，而是硬性要求。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e成本\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e这一点经常被低估。以一个中等复杂度的任务为例：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eRule Engine:  ~0 成本（CPU 时间可忽略）\nWorkflow:     ~$0.001（计算资源 + 存储）\nAgent:        ~$0.01 - $0.50（取决于步骤数和模型选择）\n              3 步 Agent × GPT-4 级别 ≈ 每次 $0.03-0.10\n              如果日调用量 100K，月成本 = $3,000 - $10,000\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e当你把 Agent 用在本该用 Rule Engine 解决的问题上，你是在用 100 倍的成本获得更差的可靠性。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e可靠性\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRule Engine：只要规则正确，就永远正确。故障模式是规则覆盖不全。\u003c/li\u003e\n\u003cli\u003eWorkflow：支持重试、幂等、补偿事务。成熟的 Workflow Engine 可以做到 99.99% 可靠。\u003c/li\u003e\n\u003cli\u003eAgent：LLM 可能幻觉、可能选错工具、可能陷入循环。即使加了 Guardrail，端到端成功率通常在 85%-95%（复杂任务更低）。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e可观测性\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRule Engine：每次执行记录匹配规则和动作，日志本身就是完整的审计轨迹。\u003c/li\u003e\n\u003cli\u003eWorkflow：DAG 执行引擎天然提供步骤级别的状态、耗时、输入输出。Airflow 的 UI 就是最好的例子。\u003c/li\u003e\n\u003cli\u003eAgent：你需要自己构建 Trace 系统——记录每一轮 LLM 的输入、输出、选择的工具、工具的返回值、Token 消耗。没有这些，Agent 在生产环境中就是一个黑盒。\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e4. 场景分析\u003c/h2\u003e\n\u003cp\u003e抽象的对比不如具体场景有说服力。下面逐个分析。\u003c/p\u003e\n\u003ch3\u003e4.1 数据 ETL Pipeline → Workflow\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e场景\u003c/strong\u003e：每天从 3 个数据源抽取数据，清洗、转换、加载到数据仓库。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e选 Workflow 的理由\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e步骤完全确定：Extract → Transform → Load，不需要运行时决策\u003c/li\u003e\n\u003cli\u003e步骤间有明确的依赖关系：Transform 必须在 Extract 之后\u003c/li\u003e\n\u003cli\u003e需要精确的重试和失败补偿：某个数据源失败了，只重跑那个分支\u003c/li\u003e\n\u003cli\u003e需要调度：每天凌晨 3 点执行\u003c/li\u003e\n\u003cli\u003e需要回填（Backfill）：补跑历史数据\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e为什么不用 Agent\u003c/strong\u003e：\u003c/p\u003e\n\u003cp\u003eETL 不需要\u0026quot;思考下一步做什么\u0026quot;——步骤是固定的。用 Agent 意味着每次运行都要花 Token 让 LLM \u0026quot;重新发现\u0026quot;这些固定步骤，纯属浪费。更危险的是，LLM 可能在某次运行中\u0026quot;创造性地\u0026quot;跳过某个步骤或改变转换逻辑。\u003c/p\u003e\n\u003ch3\u003e4.2 客服问答 → Agent\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e场景\u003c/strong\u003e：用户通过聊天窗口提问，系统需要理解意图、查询知识库、可能需要查订单、可能需要转人工。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e选 Agent 的理由\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e输入是自然语言，意图不确定，无法枚举所有可能\u003c/li\u003e\n\u003cli\u003e处理路径取决于用户说了什么——可能一步就能回答，也可能需要查 3 个系统\u003c/li\u003e\n\u003cli\u003e需要上下文理解和多轮对话能力\u003c/li\u003e\n\u003cli\u003e\u0026quot;足够好\u0026quot;的回答即可，不需要 100% 确定性\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e为什么不用 Workflow\u003c/strong\u003e：\u003c/p\u003e\n\u003cp\u003e你无法预定义所有可能的对话路径。用户可能问\u0026quot;我的订单到哪了\u0026quot;，也可能问\u0026quot;你们支持退款吗\u0026quot;，也可能在同一轮对话中先问订单再问退款政策。Workflow 的路径是编译期确定的，处理不了这种运行时的动态性。\u003c/p\u003e\n\u003ch3\u003e4.3 定时报表生成 → Automation\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e场景\u003c/strong\u003e：每周一早上 9 点，从数据库查询上周的销售数据，生成 Excel 报表，发送到指定邮箱。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e选 Automation 的理由\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e触发条件确定：Cron 定时\u003c/li\u003e\n\u003cli\u003e逻辑确定：SQL 查询 → 格式化 → 发送\u003c/li\u003e\n\u003cli\u003e不需要编排复杂依赖\u003c/li\u003e\n\u003cli\u003e不需要任何\u0026quot;智能\u0026quot;——SQL 和模板都是写死的\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e为什么不用 Workflow 或 Agent\u003c/strong\u003e：\u003c/p\u003e\n\u003cp\u003eWorkflow 是大炮打蚊子——这里没有复杂的步骤依赖和分支。Agent 更是离谱——你不需要 LLM 来执行 \u003ccode\u003eSELECT SUM(amount) FROM orders WHERE date \u0026gt;= \u0026#39;2025-07-28\u0026#39;\u003c/code\u003e。\u003c/p\u003e\n\u003ch3\u003e4.4 代码审查助手 → Agent\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e场景\u003c/strong\u003e：PR 提交后，自动分析代码变更，给出审查意见：安全隐患、性能问题、风格建议。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e选 Agent 的理由\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e代码变更是非结构化的，无法穷举所有模式\u003c/li\u003e\n\u003cli\u003e需要\u0026quot;理解\u0026quot;代码语义，而非简单的模式匹配（静态分析工具已经覆盖了模式匹配的部分）\u003c/li\u003e\n\u003cli\u003e审查意见需要结合上下文（这个函数在项目中是怎么用的？改动会影响什么？）\u003c/li\u003e\n\u003cli\u003eAgent 可以调用多种工具：读取文件、运行测试、查看 Git 历史\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e为什么不用 Rule Engine\u003c/strong\u003e：\u003c/p\u003e\n\u003cp\u003eRule Engine 只能匹配预定义模式（如\u0026quot;函数超过 100 行\u0026quot;），无法理解语义层面的问题（如\u0026quot;这个 API 调用没有处理超时\u0026quot;）。实际上，最好的方案是 \u003cstrong\u003eRule Engine + Agent\u003c/strong\u003e——先用 Linter/SAST 做确定性检查，再用 Agent 做语义级审查。\u003c/p\u003e\n\u003ch3\u003e4.5 订单状态流转 → Workflow\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e场景\u003c/strong\u003e：电商订单从创建到完成的状态机——待支付 → 已支付 → 拣货中 → 已发货 → 已签收 → 已完成。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e选 Workflow 的理由\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e状态和转换规则完全确定：已支付才能拣货，已发货才能签收\u003c/li\u003e\n\u003cli\u003e每个状态转换都有明确的触发条件（支付回调、物流推送）\u003c/li\u003e\n\u003cli\u003e需要事务保证：状态转换必须原子性，不能出现\u0026quot;钱扣了但订单还是待支付\u0026quot;\u003c/li\u003e\n\u003cli\u003e需要补偿机制：支付超时需要自动取消\u003c/li\u003e\n\u003cli\u003e0 容忍非确定性——用户的钱不能有任何模糊\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e为什么不用 Agent\u003c/strong\u003e：\u003c/p\u003e\n\u003cp\u003e这个问题需要反复强调：\u003cstrong\u003e涉及金钱和状态一致性的流程，绝对不能用 Agent。\u003c/strong\u003e LLM 的幻觉在这里不是\u0026quot;回答不太准确\u0026quot;，而是\u0026quot;用户的钱没了但订单没更新\u0026quot;。\u003c/p\u003e\n\u003ch3\u003e4.6 智能运维（AIOps）→ 混合架构\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e场景\u003c/strong\u003e：监控告警触发后，自动诊断根因并执行修复。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e为什么需要混合\u003c/strong\u003e：\u003c/p\u003e\n\u003cp\u003e这个场景天然分为确定性部分和不确定性部分——\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e确定性部分（Automation）：告警规则匹配、阈值判断、常见故障的自动修复（CPU 高 → 扩容，磁盘满 → 清理日志）\u003c/li\u003e\n\u003cli\u003e不确定性部分（Agent）：复杂故障的根因分析——Agent 可以查看日志、查询指标、检查最近的部署变更，综合判断根因\u003c/li\u003e\n\u003cli\u003e编排部分（Workflow）：整个处理流程的骨架——告警接收 → 去重 → 分级 → 自动修复 / 智能诊断 → 通知\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e告警触发\n   │\n   ▼\n[Automation: 告警去重 + 分级]\n   │\n   ├──→ P4/P3 已知模式 ──→ [Automation: 自动修复]\n   │                              │\n   │                              ▼\n   │                         [通知 Oncall]\n   │\n   └──→ P2/P1 或未知模式 ──→ [Agent: 根因分析]\n                                   │\n                                   ├──→ 找到根因 ──→ [Automation: 执行修复]\n                                   │\n                                   └──→ 无法确定 ──→ [升级到人工]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这才是 Agent 的正确用法——\u003cstrong\u003e只在真正需要\u0026quot;智能\u0026quot;的环节使用 Agent，其余部分用更可靠、更便宜的范式处理。\u003c/strong\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e5. 混合架构：三者如何共存\u003c/h2\u003e\n\u003cp\u003e真实的生产系统很少只用一种范式。更常见的模式是：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌──────────────────────────────────────────────────────────────┐\n│                    混合架构全景                                │\n│                                                              │\n│  ┌──────────────────────────────────────────────────┐        │\n│  │              Workflow / DAG（骨架层）              │        │\n│  │                                                  │        │\n│  │  Step 1          Step 2          Step 3          │        │\n│  │  ┌──────────┐   ┌──────────┐   ┌──────────┐     │        │\n│  │  │Automation│──→│  Agent   │──→│Automation│     │        │\n│  │  │数据预处理│   │语义分析  │   │结果写入  │     │        │\n│  │  └──────────┘   └──────────┘   └──────────┘     │        │\n│  │       │              │              │            │        │\n│  │       ▼              ▼              ▼            │        │\n│  │  确定性操作     LLM 推理       确定性操作        │        │\n│  │  延迟: 10ms    延迟: 2-5s     延迟: 50ms        │        │\n│  │  成本: ~0      成本: $0.02    成本: ~0           │        │\n│  └──────────────────────────────────────────────────┘        │\n│                                                              │\n│  设计原则：                                                  │\n│  1. Workflow 负责编排和容错（重试、超时、补偿）              │\n│  2. Automation 处理所有确定性步骤                            │\n│  3. Agent 只出现在需要\u0026quot;理解\u0026quot;和\u0026quot;推理\u0026quot;的节点                  │\n│  4. Agent 的输出经过验证后才进入下一步                       │\n└──────────────────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e5.1 设计原则\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e原则一：Agent 是 Workflow 的节点，不是整个 Workflow\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e一个常见的错误是让 Agent 控制整个流程——从数据获取到处理到存储全部由 Agent 决定。正确的做法是：Workflow 定义骨架（步骤顺序、依赖关系、容错策略），Agent 只负责其中需要推理的那一步。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 错误做法：让 Agent 控制整个流程\nagent.run(\u0026quot;从数据库读取用户评论，分析情感，把结果写回数据库\u0026quot;)\n# Agent 可能：用错 SQL、忘记写回、写入格式错误...\n\n# 正确做法：Workflow 控制流程，Agent 只做推理\ndef step_1_extract(ctx):\n    \u0026quot;\u0026quot;\u0026quot;确定性步骤：用固定 SQL 读取数据\u0026quot;\u0026quot;\u0026quot;\n    return db.query(\u0026quot;SELECT id, comment FROM reviews WHERE date = %s\u0026quot;, ctx[\u0026quot;date\u0026quot;])\n\ndef step_2_analyze(ctx):\n    \u0026quot;\u0026quot;\u0026quot;Agent 步骤：对每条评论做情感分析\u0026quot;\u0026quot;\u0026quot;\n    results = []\n    for review in ctx[\u0026quot;step_1_extract\u0026quot;]:\n        sentiment = agent.run(\n            f\u0026quot;分析以下评论的情感倾向(positive/negative/neutral):\\n{review[\u0026#39;comment\u0026#39;]}\u0026quot;\n        )\n        results.append({\u0026quot;id\u0026quot;: review[\u0026quot;id\u0026quot;], \u0026quot;sentiment\u0026quot;: sentiment})\n    return results\n\ndef step_3_load(ctx):\n    \u0026quot;\u0026quot;\u0026quot;确定性步骤：用固定逻辑写回数据库\u0026quot;\u0026quot;\u0026quot;\n    for item in ctx[\u0026quot;step_2_analyze\u0026quot;]:\n        db.execute(\n            \u0026quot;UPDATE reviews SET sentiment = %s WHERE id = %s\u0026quot;,\n            (item[\u0026quot;sentiment\u0026quot;], item[\u0026quot;id\u0026quot;])\n        )\n\n# Workflow 定义\nworkflow.add_step(Step(\u0026quot;extract\u0026quot;, step_1_extract))\nworkflow.add_step(Step(\u0026quot;analyze\u0026quot;, step_2_analyze, depends_on=[\u0026quot;extract\u0026quot;]))\nworkflow.add_step(Step(\u0026quot;load\u0026quot;, step_3_load, depends_on=[\u0026quot;analyze\u0026quot;]))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e原则二：Agent 的输出必须经过验证\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAgent 的输出是非确定性的。在混合架构中，Agent 节点和下游确定性节点之间，必须有一个验证层。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef step_2_analyze_with_validation(ctx):\n    \u0026quot;\u0026quot;\u0026quot;Agent 步骤 + 输出验证\u0026quot;\u0026quot;\u0026quot;\n    VALID_SENTIMENTS = {\u0026quot;positive\u0026quot;, \u0026quot;negative\u0026quot;, \u0026quot;neutral\u0026quot;}\n    results = []\n    for review in ctx[\u0026quot;step_1_extract\u0026quot;]:\n        sentiment = agent.run(f\u0026quot;分析情感倾向: {review[\u0026#39;comment\u0026#39;]}\u0026quot;)\n        # 验证 Agent 输出\n        sentiment = sentiment.strip().lower()\n        if sentiment not in VALID_SENTIMENTS:\n            sentiment = \u0026quot;neutral\u0026quot;  # fallback\n            log.warning(f\u0026quot;Agent 返回了无效的情感值，已 fallback: review_id={review[\u0026#39;id\u0026#39;]}\u0026quot;)\n        results.append({\u0026quot;id\u0026quot;: review[\u0026quot;id\u0026quot;], \u0026quot;sentiment\u0026quot;: sentiment})\n    return results\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e原则三：确定性部分永远优先用 Automation\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e如果一个步骤的输入输出可以完全预定义，就不要用 Agent。这不是技术保守，而是工程理性——用最简单的工具解决问题，把复杂性预算留给真正需要的地方。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e6. Agent 的隐性成本\u003c/h2\u003e\n\u003cp\u003e这一节讲的是大部分\u0026quot;Agent 教程\u0026quot;不会告诉你的东西。\u003c/p\u003e\n\u003ch3\u003e6.1 Token 成本\u003c/h3\u003e\n\u003cp\u003eAgent 的每一步决策都需要调用 LLM。一个 5 步 Agent 执行一次任务的 Token 消耗：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e第 1 步: System Prompt (500) + User Input (200) + Response (300)    = 1,000 tokens\n第 2 步: 上一轮上下文 (1,000) + Tool Result (500) + Response (400)  = 1,900 tokens\n第 3 步: 上一轮上下文 (1,900) + Tool Result (300) + Response (350)  = 2,550 tokens\n第 4 步: 上一轮上下文 (2,550) + Tool Result (800) + Response (400)  = 3,750 tokens\n第 5 步: 上一轮上下文 (3,750) + Tool Result (200) + Response (500)  = 4,450 tokens\n                                                          ─────────────────────\n                                                          总计: ~13,650 tokens\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e注意上下文是累积的——每一步都要重新发送之前所有的对话历史。这意味着 \u003cstrong\u003eToken 消耗是超线性增长的\u003c/strong\u003e。步骤越多，后期每一步的成本越高。\u003c/p\u003e\n\u003cp\u003e以 GPT-4o 为例（$2.5/1M input, $10/1M output），上面这个 5 步 Agent 单次执行成本约 $0.03-0.05。看似不多，但如果日调用量 10 万次，月成本就是 $90,000-$150,000。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e优化策略\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e上下文压缩：每 N 步对历史做一次摘要\u003c/li\u003e\n\u003cli\u003e选择合适的模型：简单决策用小模型，关键决策用大模型\u003c/li\u003e\n\u003cli\u003e缓存：对相同输入的 Agent 结果做缓存（注意非确定性问题）\u003c/li\u003e\n\u003cli\u003e减少 Agent 步骤：通过更好的 Prompt 和工具设计，减少所需的推理轮次\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e6.2 延迟\u003c/h3\u003e\n\u003cp\u003eLLM 的推理延迟通常在 500ms-5s 之间（取决于模型和输出长度）。一个 5 步 Agent 的端到端延迟：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e5 步 × 平均 1.5s/步 = 7.5s\n\n加上工具调用时间（网络请求、数据库查询等），实际延迟可能在 10-15s。\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e对比：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRule Engine 处理同样的逻辑：\u0026lt; 10ms\u003c/li\u003e\n\u003cli\u003eWorkflow 执行 5 个确定性步骤：\u0026lt; 500ms\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e在延迟敏感的场景（如支付、交易、实时推荐），Agent 的延迟是不可接受的。\u003c/strong\u003e\u003c/p\u003e\n\u003ch3\u003e6.3 不可预测性\u003c/h3\u003e\n\u003cp\u003e这是 Agent 最被低估的问题。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 同样的输入，Agent 可能走出完全不同的路径\n\n# 第一次运行\n# Step 1: 调用 search_database → 找到 3 条记录\n# Step 2: 调用 analyze_data → 生成分析\n# Step 3: 返回结果\n# 总计: 3 步, 耗时 4s, 成本 $0.02\n\n# 第二次运行（完全相同的输入）\n# Step 1: 调用 search_database → 找到 3 条记录\n# Step 2: 调用 search_web → 想找更多信息（为什么？LLM 这次觉得不够）\n# Step 3: 调用 search_database → 用新的关键词再查一次\n# Step 4: 调用 analyze_data → 生成分析\n# Step 5: 觉得分析不够好，调用 analyze_data → 重新生成\n# Step 6: 返回结果\n# 总计: 6 步, 耗时 10s, 成本 $0.06\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这意味着你\u003cstrong\u003e无法预测 Agent 的执行时间和成本\u003c/strong\u003e。在需要做容量规划和 SLA 承诺的生产系统中，这是一个严重的问题。\u003c/p\u003e\n\u003ch3\u003e6.4 调试困难\u003c/h3\u003e\n\u003cp\u003e确定性系统的 Bug 可以精确复现：相同的输入 + 相同的代码 = 相同的 Bug。\u003c/p\u003e\n\u003cp\u003eAgent 不行。因为：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLLM 的输出本身带有随机性（即使 temperature=0，不同批次推理也可能有微小差异）\u003c/li\u003e\n\u003cli\u003e工具调用的结果可能随时间变化（数据库内容变了、API 返回变了）\u003c/li\u003e\n\u003cli\u003e上下文窗口中的信息累积，前几步的微小差异会被放大\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003e调试 Agent 的正确做法\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e完整记录每一步的输入（包括完整的 messages 列表）和输出\u003c/li\u003e\n\u003cli\u003e记录每次工具调用的参数和返回值\u003c/li\u003e\n\u003cli\u003e记录 Token 使用量和延迟\u003c/li\u003e\n\u003cli\u003e支持\u0026quot;回放\u0026quot;——用记录的数据重新走一遍流程（但要注意，即使相同输入，LLM 也可能给出不同输出）\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e7. 选型决策树\u003c/h2\u003e\n\u003cp\u003e面对一个具体需求，按以下流程判断：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e                    你的任务需要\u0026quot;理解\u0026quot;自然语言\n                    或处理模糊/开放式输入吗？\n                           │\n                    ┌──────┴──────┐\n                    │             │\n                   Yes           No\n                    │             │\n                    ▼             ▼\n             结果需要 100%     任务步骤之间有\n             确定性吗？        复杂依赖关系吗？\n                │                    │\n          ┌─────┴─────┐        ┌─────┴─────┐\n          │           │        │           │\n         Yes         No       Yes         No\n          │           │        │           │\n          ▼           ▼        ▼           ▼\n      先用规则     ┌──────┐  Workflow    Automation\n      处理能处     │Agent │  / DAG      (Rule/Cron)\n      理的部分     └──┬───┘\n      用 Agent        │\n      处理剩余        ▼\n      (混合架构)   可以接受 $0.01-0.10/次\n                   的成本和 2-10s 的延迟吗？\n                          │\n                    ┌─────┴─────┐\n                    │           │\n                   Yes         No\n                    │           │\n                    ▼           ▼\n                  Agent     重新审视需求：\n                            能否拆分为\n                            确定性 + 模糊性部分？\n                            → 混合架构\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e速查表\u003c/strong\u003e：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e如果你的任务是...\u003c/th\u003e\n\u003cth\u003e推荐范式\u003c/th\u003e\n\u003cth\u003e理由\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e固定逻辑 + 定时触发\u003c/td\u003e\n\u003ctd\u003eAutomation\u003c/td\u003e\n\u003ctd\u003e无需编排，无需推理\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e多步骤 + 有依赖 + 确定性\u003c/td\u003e\n\u003ctd\u003eWorkflow\u003c/td\u003e\n\u003ctd\u003e需要编排，不需要推理\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e理解自然语言 + 动态决策\u003c/td\u003e\n\u003ctd\u003eAgent\u003c/td\u003e\n\u003ctd\u003e需要推理\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e大部分确定 + 少量模糊\u003c/td\u003e\n\u003ctd\u003eWorkflow + Agent 节点\u003c/td\u003e\n\u003ctd\u003e编排确定部分，推理模糊部分\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e简单触发 + 复杂诊断\u003c/td\u003e\n\u003ctd\u003eAutomation + Agent\u003c/td\u003e\n\u003ctd\u003e触发用规则，诊断用推理\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003chr\u003e\n\u003ch2\u003e8. 常见误区\u003c/h2\u003e\n\u003cp\u003e在结束之前，总结几个我在实际项目中反复见到的选型错误。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e误区一：因为\u0026quot;想用 AI\u0026quot;而选 Agent\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e技术选型应该从问题出发，不是从解决方案出发。\u0026quot;我们想用 AI\u0026quot; 不是选 Agent 的理由，\u0026quot;用户输入是自然语言且意图不可穷举\u0026quot; 才是。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e误区二：用 Agent 替代状态机\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e订单流转、审批流程、工单生命周期——这些有限状态机（FSM）问题有成熟的解决方案。把它们交给 Agent 不会让系统更智能，只会让它更不可靠。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e误区三：Agent 做完所有事\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e让 Agent 既负责决策又负责执行。正确做法是：Agent 只负责\u0026quot;决定做什么\u0026quot;（What），具体的执行（How）交给确定性系统。例如 Agent 决定\u0026quot;需要给用户退款\u0026quot;，但实际调用退款 API 的逻辑是固定的代码，不是 Agent 自己拼 HTTP 请求。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e误区四：忽视 Agent 的失败模式\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAgent 会失败。它会幻觉、会陷入循环、会选错工具、会超时。你的系统设计必须考虑：Agent 失败了怎么办？有没有 Fallback？有没有人工兜底？最大重试次数是多少？\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e9. 总结\u003c/h2\u003e\n\u003cp\u003e回到开篇的问题：你的问题，真的需要 Agent 吗？\u003c/p\u003e\n\u003cp\u003e三条准则：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e能用规则解决的，不要用 Workflow；能用 Workflow 解决的，不要用 Agent。\u003c/strong\u003e 选择复杂度最低的范式，降低的是长期维护成本。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAgent 的正确位置是\u0026quot;最后一英里的模糊性\u0026quot;。\u003c/strong\u003e 在混合架构中，让确定性系统处理 80% 的工作，Agent 只处理那 20% 需要\u0026quot;理解\u0026quot;和\u0026quot;推理\u0026quot;的部分。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAgent 是有代价的，而且代价比你想象的高。\u003c/strong\u003e Token 成本、延迟、不可预测性、调试难度——这些隐性成本在规模化后会成为真实的痛点。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e选对抽象，才是真正的技术判断力。\u003c/p\u003e\n\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e系列导航\u003c/strong\u003e：本文是 Agentic 系列的第 03 篇。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e上一篇：\u003ca href=\"/blog/engineering/agentic/02-From%20Prompt%20to%20Agent\"\u003e02 | From Prompt to Agent\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下一篇：\u003ca href=\"/blog/engineering/agentic/04-The%20Agent%20Control%20Loop\"\u003e04 | The Agent Control Loop\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e完整目录：\u003ca href=\"/blog/engineering/agentic/01-From%20LLM%20to%20Agent\"\u003e01 | From LLM to Agent\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"17:T8130,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eFrom Prompt to Agent: 为什么 LLM 本身不是 Agent\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e当我们说\u0026quot;让 AI 帮我完成这件事\u0026quot;时，我们期望的不是一次文本生成，而是一次\u003cstrong\u003e有目标、有规划、有执行、有反馈\u003c/strong\u003e的任务完成过程。这正是 LLM 和 Agent 的根本区别。\u003c/p\u003e\n\u003cp\u003e本文是 Agentic 系列第 02 篇。上一篇我们绘制了全景地图，这一篇我们回到原点：为什么一个再强大的 LLM，本身也不是 Agent？从\u0026quot;不是什么\u0026quot;出发，才能精确定义\u0026quot;是什么\u0026quot;。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch2\u003e1. LLM 的本质：一个文本到文本的函数\u003c/h2\u003e\n\u003cp\u003e把所有复杂性剥离，LLM 的数学本质极其简洁：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ef(prompt) → response\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e给定一段输入文本（prompt），经过前向推理，输出一段文本（response）。就这样。\u003c/p\u003e\n\u003cp\u003e更严格地说，LLM 做的是\u003cstrong\u003e条件概率采样\u003c/strong\u003e：给定已有 token 序列 \u003ccode\u003e[t₁, t₂, ..., tₙ]\u003c/code\u003e，逐个预测下一个 token 的概率分布 \u003ccode\u003eP(tₙ₊₁ | t₁, ..., tₙ)\u003c/code\u003e，然后按某种策略（greedy、top-k、top-p）从分布中采样。\u003c/p\u003e\n\u003cp\u003e这意味着三个关键性质：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e无状态（Stateless）\u003c/strong\u003e：模型权重在推理时不变，两次相同输入产生的概率分布相同（忽略采样随机性）。模型本身不存储任何关于\u0026quot;之前发生了什么\u0026quot;的信息。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e无副作用（Side-effect Free）\u003c/strong\u003e：模型不会改变外部世界的任何状态——不会写文件、不会调 API、不会修改数据库。它只输出文本。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e无记忆（Memoryless）\u003c/strong\u003e：每次调用都是独立的函数调用。上一次对话的内容，除非你手动拼接进 prompt，否则模型完全不知道。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e用一个 Python 类比，LLM 就是一个纯函数：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef llm(prompt: str) -\u0026gt; str:\n    \u0026quot;\u0026quot;\u0026quot;\n    纯函数：相同输入 → 相同输出分布\n    无副作用：不修改任何外部状态\n    无记忆：不保留任何调用历史\n    \u0026quot;\u0026quot;\u0026quot;\n    tokens = tokenize(prompt)\n    output_tokens = []\n    for _ in range(max_tokens):\n        next_token_probs = model.forward(tokens + output_tokens)\n        next_token = sample(next_token_probs, temperature=0.7)\n        output_tokens.append(next_token)\n        if next_token == EOS:\n            break\n    return detokenize(output_tokens)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这是一个非常优雅的抽象。但正是这个抽象的简洁性，决定了它的局限性。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e2. LLM 的五大局限\u003c/h2\u003e\n\u003ch3\u003e2.1 无记忆：每次对话都是独立宇宙\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e场景\u003c/strong\u003e：你让 LLM 帮你写一个项目方案。第一轮你说了需求，第二轮你补充了约束，第三轮你修改了目标。LLM 怎么\u0026quot;记住\u0026quot;前两轮？\u003c/p\u003e\n\u003cp\u003e答案是：它不记。所谓的\u0026quot;多轮对话\u0026quot;，本质上是\u003cstrong\u003e客户端把历史消息全部拼接进 prompt\u003c/strong\u003e 重新发送。每一轮调用，LLM 都在从零开始阅读整个对话历史。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 所谓\u0026quot;多轮对话\u0026quot;的真相\nmessages = [\n    {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;帮我写个项目方案\u0026quot;},          # 第一轮\n    {\u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;好的，请问项目目标是什么？\u0026quot;},\n    {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;做一个推荐系统\u0026quot;},            # 第二轮\n    {\u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;了解，技术栈偏好？\u0026quot;},\n    {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;用 Python，预算 50 万\u0026quot;},     # 第三轮\n]\n# 每次都是把 *全部* messages 发给 LLM，它并不\u0026quot;记得\u0026quot;前两轮\nresponse = llm.chat(messages)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这带来两个工程问题：一是 \u003cstrong\u003econtext window 有限\u003c/strong\u003e，对话太长会被截断，早期关键信息丢失；二是 \u003cstrong\u003etoken 成本线性增长\u003c/strong\u003e，每一轮都在为重复传输历史对话付费。\u003c/p\u003e\n\u003ch3\u003e2.2 无工具：只能生成文本，不能执行操作\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e场景\u003c/strong\u003e：你问 LLM\u0026quot;现在北京气温多少度？\u0026quot;它会给你一个看起来很自信的答案——但这个答案是从训练数据中\u0026quot;编\u0026quot;出来的，不是实时查询的结果。\u003c/p\u003e\n\u003cp\u003eLLM 不能发 HTTP 请求，不能查数据库，不能读文件系统，不能调用任何外部服务。它唯一的\u0026quot;输出通道\u0026quot;就是文本。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e用户：帮我创建一个 GitHub 仓库叫 my-project\nLLM：好的，已经为您创建了 GitHub 仓库 my-project！  ← 这是幻觉，什么都没发生\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLLM 的\u0026quot;执行\u0026quot;是一种语言层面的模拟——它可以生成看起来像执行结果的文本，但实际上没有任何副作用发生。这是 hallucination 问题在工具层面的体现。\u003c/p\u003e\n\u003ch3\u003e2.3 无规划：只有 next-token prediction，没有 multi-step reasoning\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e场景\u003c/strong\u003e：你让 LLM\u0026quot;规划一次三天的日本旅行\u0026quot;。它会一口气输出一个看起来完整的方案。但这不是\u0026quot;规划\u0026quot;——这是\u0026quot;自回归生成\u0026quot;。它不会先列出约束（预算、时间、兴趣），再枚举可能的方案，再比较 trade-off，再做决策。它只是在逐 token 地预测\u0026quot;下一个最可能的词\u0026quot;。\u003c/p\u003e\n\u003cp\u003e真正的规划需要：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e目标分解\u003c/strong\u003e：把大目标拆成子目标\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e约束满足\u003c/strong\u003e：在多个维度上满足约束条件\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e方案评估\u003c/strong\u003e：对多个候选方案进行比较\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e回溯修正\u003c/strong\u003e：发现某条路不通时能回退\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eLLM 的自回归生成是单向的、线性的，没有回溯机制。它无法在生成第 50 个 token 时\u0026quot;回头修改\u0026quot;第 10 个 token。所有看起来像\u0026quot;规划\u0026quot;的输出，都是语言模式匹配的结果，不是搜索与优化的结果。\u003c/p\u003e\n\u003ch3\u003e2.4 无状态：不知道自己之前做了什么\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e场景\u003c/strong\u003e：你让 LLM 执行一个多步骤任务——先查数据，再分析，再写报告。即使它能生成每一步的文本描述，它也不知道\u0026quot;第一步的结果是什么\u0026quot;，因为它没有一个持久化的状态空间来记录执行进度。\u003c/p\u003e\n\u003cp\u003e无状态和无记忆不同：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e无记忆\u003c/strong\u003e强调的是跨调用的信息丢失\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e无状态\u003c/strong\u003e强调的是在一次任务中，没有结构化的执行状态追踪\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e一个 Agent 需要知道：\u0026quot;我已经完成了步骤 1 和 2，步骤 3 失败了，我需要重试步骤 3\u0026quot;。LLM 没有这个能力。\u003c/p\u003e\n\u003ch3\u003e2.5 无反思：无法评估自己的输出质量\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e场景\u003c/strong\u003e：你让 LLM 写一段代码。它写完了。这段代码是否正确？LLM 不知道。它不会自动运行代码验证，不会检查边界条件，不会评估时间复杂度是否满足要求。\u003c/p\u003e\n\u003cp\u003e更深层的问题是：LLM 无法区分\u0026quot;我确信这是对的\u0026quot;和\u0026quot;我在瞎猜\u0026quot;。它的 confidence 不等于 correctness。一个 softmax 输出 0.95 的概率，并不意味着答案有 95% 的概率是正确的。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e                    LLM 的五大局限\n\n    +----------+----------+----------+----------+----------+\n    |          |          |          |          |          |\n    | 无记忆    | 无工具    | 无规划    | 无状态    | 无反思    |\n    | Memoryless| Toolless | Planless | Stateless| Reflectless|\n    |          |          |          |          |          |\n    | 跨调用    | 只输出    | 单向生成  | 无执行    | 无法自我  |\n    | 信息丢失  | 文本     | 无回溯    | 进度追踪  | 评估质量  |\n    |          |          |          |          |          |\n    +----------+----------+----------+----------+----------+\n                          |\n                          v\n              LLM 需要一个\u0026quot;外壳\u0026quot;来弥补这些局限\n              这个外壳，就是 Agent Runtime\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e3. Agent 的精确定义\u003c/h2\u003e\n\u003ch3\u003e3.1 定义\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eAgent = LLM + Memory + Tools + Planner + Runtime\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e这不是一个松散的隐喻，而是一个精确的组件模型。每个组件有明确的职责边界：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e组件\u003c/th\u003e\n\u003cth\u003e职责\u003c/th\u003e\n\u003cth\u003e类比\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eLLM\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e语义理解、推理、生成\u003c/td\u003e\n\u003ctd\u003e大脑的语言区\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eMemory\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e存储对话历史、任务状态、长期知识\u003c/td\u003e\n\u003ctd\u003e海马体 + 笔记本\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eTools\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e与外部世界交互的能力集合\u003c/td\u003e\n\u003ctd\u003e双手 + 工具箱\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003ePlanner\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e目标分解、行动排序、策略选择\u003c/td\u003e\n\u003ctd\u003e前额叶皮层\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eRuntime\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e控制循环、状态管理、错误处理、生命周期\u003c/td\u003e\n\u003ctd\u003e自主神经系统\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch3\u003e3.2 组件交互模型\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e    +---------------------------------------------------------+\n    |                     Agent Runtime                        |\n    |                                                         |\n    |   +----------+     +----------+     +----------+        |\n    |   |          |     |          |     |          |        |\n    |   |  Memory  |\u0026lt;---\u0026gt;|   LLM    |\u0026lt;---\u0026gt;| Planner  |        |\n    |   |          |     |          |     |          |        |\n    |   +----+-----+     +----+-----+     +----+-----+        |\n    |        |                |                 |              |\n    |        |           +----v-----+           |              |\n    |        +----------\u0026gt;|          |\u0026lt;----------+              |\n    |                    |  Tools   |                          |\n    |                    |          |                          |\n    |                    +----+-----+                          |\n    |                         |                                |\n    +---------------------------------------------------------+\n                              |\n                              v\n                     External World\n                  (APIs, DBs, Files, Users)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e交互流程\u003c/strong\u003e：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eRuntime\u003c/strong\u003e 接收外部输入（用户消息、系统事件）\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRuntime\u003c/strong\u003e 从 \u003cstrong\u003eMemory\u003c/strong\u003e 加载相关上下文\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLLM\u003c/strong\u003e 基于输入 + 上下文进行推理\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePlanner\u003c/strong\u003e（通常由 LLM 驱动）决定下一步行动\u003c/li\u003e\n\u003cli\u003e如果需要执行操作，\u003cstrong\u003eRuntime\u003c/strong\u003e 调度 \u003cstrong\u003eTools\u003c/strong\u003e 执行\u003c/li\u003e\n\u003cli\u003e工具执行结果写回 \u003cstrong\u003eMemory\u003c/strong\u003e，进入下一轮循环\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e关键设计决策：\u003cstrong\u003ePlanner 是一个独立组件，还是 LLM 的一部分？\u003c/strong\u003e 这取决于你对确定性的需求。如果 Planner 由 LLM 驱动（如 ReAct 模式），灵活但不可控；如果 Planner 是硬编码的状态机，可控但不灵活。这个 trade-off 贯穿整个 Agent 架构设计，我们会在第 03 篇深入讨论。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e4. Agent 的核心循环详解\u003c/h2\u003e\n\u003cp\u003eAgent 的运行可以抽象为六个阶段的循环：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e    +-------+     +-------+     +------+\n    |       |     |       |     |      |\n    |Observe+----\u0026gt;| Think +----\u0026gt;| Plan |\n    |       |     |       |     |      |\n    +---^---+     +-------+     +--+---+\n        |                          |\n        |                          v\n    +---+----+                 +---+---+\n    |        |                 |       |\n    | Update |\u0026lt;----+ Reflect  \u0026lt;+ Act   |\n    |        |     |          ||       |\n    +--------+     +----------++-------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e4.1 各阶段详解\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eObserve（感知）\u003c/strong\u003e：收集当前环境信息。这包括用户的最新输入、上一步工具的返回结果、系统级事件（如超时、异常）、从 Memory 中检索的相关上下文。感知阶段的核心问题是\u003cstrong\u003e信息筛选\u003c/strong\u003e——不是所有信息都应该进入 LLM 的上下文，context window 是稀缺资源。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThink（推理）\u003c/strong\u003e：基于感知到的信息，理解当前处境。这是 LLM 最擅长的部分——语义理解、意图识别、情境分析。Think 阶段的输出是对当前状态的结构化理解，而不是最终答案。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePlan（规划）\u003c/strong\u003e：基于对当前状态的理解，决定下一步做什么。Plan 可以是单步的（\u0026quot;调用天气 API\u0026quot;），也可以是多步的（\u0026quot;先查天气，再根据天气决定穿什么，再创建提醒\u0026quot;）。规划的粒度直接影响系统的可控性和灵活性。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAct（执行）\u003c/strong\u003e：执行规划中的动作。可能是调用工具（Tool Calling）、生成文本回复、更新内部状态，或者向用户提问以获取更多信息。执行是唯一产生副作用的阶段。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eReflect（反思）\u003c/strong\u003e：评估执行结果。工具调用成功了吗？返回的数据符合预期吗？是否需要重试或换一个方案？反思是 Agent 与简单 Chain 的关键区别——它引入了\u003cstrong\u003e自我纠错\u003c/strong\u003e的能力。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUpdate（更新）\u003c/strong\u003e：将本轮循环中产生的信息写入 Memory。包括更新对话历史、记录执行结果、修改任务状态。Update 确保下一轮循环有最新的上下文可用。\u003c/p\u003e\n\u003ch3\u003e4.2 最简实现\u003c/h3\u003e\n\u003cp\u003e下面是这个控制循环的 Python 伪代码实现。注意，这不是生产代码，而是用于精确表达架构意图的最简抽象：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom dataclasses import dataclass, field\nfrom typing import Any\n\n@dataclass\nclass AgentState:\n    \u0026quot;\u0026quot;\u0026quot;Agent 的可序列化状态\u0026quot;\u0026quot;\u0026quot;\n    messages: list[dict] = field(default_factory=list)       # 对话历史\n    task_status: str = \u0026quot;pending\u0026quot;                              # 任务状态\n    plan: list[str] = field(default_factory=list)             # 当前计划\n    step_index: int = 0                                       # 执行进度\n    observations: list[Any] = field(default_factory=list)     # 感知缓冲\n\nclass Agent:\n    def __init__(self, llm, tools: dict, max_iterations: int = 10):\n        self.llm = llm\n        self.tools = tools           # {\u0026quot;tool_name\u0026quot;: callable}\n        self.max_iterations = max_iterations\n        self.state = AgentState()\n\n    def run(self, user_input: str) -\u0026gt; str:\n        self.state.messages.append({\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: user_input})\n\n        for i in range(self.max_iterations):\n            # --- Observe ---\n            context = self._observe()\n\n            # --- Think ---\n            thought = self.llm.generate(\n                system_prompt=THINK_PROMPT,\n                messages=context,\n            )\n\n            # --- Plan ---\n            plan = self.llm.generate(\n                system_prompt=PLAN_PROMPT,\n                messages=context + [{\u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;, \u0026quot;content\u0026quot;: thought}],\n                response_format={\u0026quot;type\u0026quot;: \u0026quot;json\u0026quot;, \u0026quot;schema\u0026quot;: PlanSchema},\n            )\n\n            if plan.action == \u0026quot;finish\u0026quot;:\n                return plan.final_answer\n\n            # --- Act ---\n            tool_name = plan.tool_name\n            tool_args = plan.tool_args\n            try:\n                result = self.tools[tool_name](**tool_args)\n            except Exception as e:\n                result = f\u0026quot;Error: {e}\u0026quot;\n\n            # --- Reflect ---\n            reflection = self.llm.generate(\n                system_prompt=REFLECT_PROMPT,\n                messages=context + [\n                    {\u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;, \u0026quot;content\u0026quot;: f\u0026quot;Action: {tool_name}({tool_args})\u0026quot;},\n                    {\u0026quot;role\u0026quot;: \u0026quot;tool\u0026quot;, \u0026quot;content\u0026quot;: str(result)},\n                ],\n            )\n\n            # --- Update ---\n            self.state.messages.append({\u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;, \u0026quot;content\u0026quot;: thought})\n            self.state.messages.append({\u0026quot;role\u0026quot;: \u0026quot;tool\u0026quot;, \u0026quot;content\u0026quot;: str(result)})\n            self.state.observations.append({\n                \u0026quot;step\u0026quot;: i,\n                \u0026quot;action\u0026quot;: tool_name,\n                \u0026quot;result\u0026quot;: result,\n                \u0026quot;reflection\u0026quot;: reflection,\n            })\n\n            if reflection.should_retry:\n                continue  # 重试当前步骤\n            self.state.step_index += 1\n\n        return \u0026quot;达到最大迭代次数，任务未完成。\u0026quot;\n\n    def _observe(self) -\u0026gt; list[dict]:\n        \u0026quot;\u0026quot;\u0026quot;从 Memory 中组装当前上下文\u0026quot;\u0026quot;\u0026quot;\n        # 实际系统中这里会有复杂的上下文压缩、检索等逻辑\n        return self.state.messages[-20:]  # 简化：取最近 20 条消息\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这段代码中有几个值得关注的设计决策：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eThink 和 Plan 分两次 LLM 调用\u003c/strong\u003e：可以使用不同的 system prompt 引导不同的思维模式，也便于独立观测和调试。代价是额外的 latency 和 token 成本。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePlan 使用 Structured Output\u003c/strong\u003e：规划结果以 JSON Schema 约束，确保输出可解析、可校验。这是将 LLM 的非确定性输出转化为确定性执行的关键桥梁。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReflect 独立成阶段\u003c/strong\u003e：而不是合并到下一轮的 Think 中。这使得反思的 prompt 可以专注于\u0026quot;评估\u0026quot;而不是\u0026quot;理解+评估\u0026quot;，通常能得到更准确的自我评价。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003emax_iterations 作为安全阀\u003c/strong\u003e：防止 Agent 陷入无限循环。这是生产系统中必须有的机制，没有它，一个错误的 Reflect 判断就可能导致无限重试。\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch2\u003e5. 从 Chatbot 到 Agent 的光谱\u003c/h2\u003e\n\u003cp\u003eAgent 不是一个二元概念——\u0026quot;是 Agent\u0026quot;或\u0026quot;不是 Agent\u0026quot;。从最简单的 LLM 调用到完整的 Agent 系统，中间存在一个连续的光谱，每向右移动一步，都在引入新的复杂性来换取新的能力。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e确定性 ←─────────────────────────────────────────────────→ 自主性\n\nPure LLM    System     RAG       Tool       ReAct       Full\n            Prompt               Calling    Agent       Agent\n\n  f(x)→y   定制化     知识增强   函数调用   推理+执行    完整系统\n            对话                            循环\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e下表从六个维度对比这个光谱的各个阶段：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e阶段\u003c/th\u003e\n\u003cth\u003e记忆\u003c/th\u003e\n\u003cth\u003e工具\u003c/th\u003e\n\u003cth\u003e规划\u003c/th\u003e\n\u003cth\u003e状态\u003c/th\u003e\n\u003cth\u003e反思\u003c/th\u003e\n\u003cth\u003e典型产品/模式\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003ePure LLM\u003c/td\u003e\n\u003ctd\u003e无\u003c/td\u003e\n\u003ctd\u003e无\u003c/td\u003e\n\u003ctd\u003e无\u003c/td\u003e\n\u003ctd\u003e无\u003c/td\u003e\n\u003ctd\u003e无\u003c/td\u003e\n\u003ctd\u003e单次 API 调用\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e+ System Prompt\u003c/td\u003e\n\u003ctd\u003e无\u003c/td\u003e\n\u003ctd\u003e无\u003c/td\u003e\n\u003ctd\u003e无\u003c/td\u003e\n\u003ctd\u003e无\u003c/td\u003e\n\u003ctd\u003e无\u003c/td\u003e\n\u003ctd\u003e定制化 Chatbot\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e+ RAG\u003c/td\u003e\n\u003ctd\u003e外部知识\u003c/td\u003e\n\u003ctd\u003e检索\u003c/td\u003e\n\u003ctd\u003e无\u003c/td\u003e\n\u003ctd\u003e无\u003c/td\u003e\n\u003ctd\u003e无\u003c/td\u003e\n\u003ctd\u003e知识问答系统\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e+ Tool Calling\u003c/td\u003e\n\u003ctd\u003e会话级\u003c/td\u003e\n\u003ctd\u003e有\u003c/td\u003e\n\u003ctd\u003e单步\u003c/td\u003e\n\u003ctd\u003e无\u003c/td\u003e\n\u003ctd\u003e无\u003c/td\u003e\n\u003ctd\u003eFunction Calling\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e+ Loop（ReAct）\u003c/td\u003e\n\u003ctd\u003e会话级\u003c/td\u003e\n\u003ctd\u003e有\u003c/td\u003e\n\u003ctd\u003e多步\u003c/td\u003e\n\u003ctd\u003e运行时\u003c/td\u003e\n\u003ctd\u003e隐式\u003c/td\u003e\n\u003ctd\u003eReAct Agent\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eFull Agent\u003c/td\u003e\n\u003ctd\u003e长期\u003c/td\u003e\n\u003ctd\u003e有\u003c/td\u003e\n\u003ctd\u003e多步\u003c/td\u003e\n\u003ctd\u003e持久化\u003c/td\u003e\n\u003ctd\u003e显式\u003c/td\u003e\n\u003ctd\u003e自主 Agent 系统\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e每个阶段的跃迁都有明确的 trade-off：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePure LLM → + System Prompt\u003c/strong\u003e：几乎零成本，但能显著改变模型的行为风格和专业度。Trade-off：prompt 越长，留给用户输入的 context window 越少。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e+ System Prompt → + RAG\u003c/strong\u003e：引入外部知识源，解决知识时效性和专业性问题。Trade-off：检索质量直接决定回答质量（garbage in, garbage out），且增加了 latency 和基础设施成本。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e+ RAG → + Tool Calling\u003c/strong\u003e：从\u0026quot;只读\u0026quot;变成\u0026quot;可写\u0026quot;，LLM 可以触发外部操作。Trade-off：引入了安全风险（LLM 可能调用不该调用的工具）和确定性问题（工具调用可能失败）。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e+ Tool Calling → + Loop\u003c/strong\u003e：从单次推理变成多步推理-执行循环。这是质变。Trade-off：循环次数不可预测，token 成本不可预测，调试复杂度指数级上升。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e+ Loop → Full Agent\u003c/strong\u003e：引入持久化记忆和显式反思。Trade-off：系统复杂度大幅提升，需要处理记忆一致性、状态持久化、长时间运行等问题。\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e6. 一个完整的例子\u003c/h2\u003e\n\u003cp\u003e用同一个任务——\u0026quot;帮我查看明天北京的天气并创建日程提醒\u0026quot;——展示不同阶段的实现差异。\u003c/p\u003e\n\u003ch3\u003e6.1 Pure LLM\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eresponse = llm.generate(\u0026quot;帮我查看明天北京的天气并创建日程提醒\u0026quot;)\n# 输出：好的，明天北京的天气大约是 25°C，晴转多云...（纯幻觉，没有真实数据）\n# 日程提醒也不会真的被创建\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e问题：没有真实数据，没有真实执行，一切都是生成的\u0026quot;假\u0026quot;内容。\u003c/p\u003e\n\u003ch3\u003e6.2 LLM + RAG\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 预先检索天气相关知识\nweather_docs = retriever.search(\u0026quot;北京天气预报\u0026quot;)\ncontext = format_docs(weather_docs)\n\nresponse = llm.generate(\n    f\u0026quot;根据以下信息回答用户问题：\\n{context}\\n\\n用户：帮我查看明天北京的天气并创建日程提醒\u0026quot;\n)\n# 输出基于检索到的文档，但如果文档不包含明天的天气（高概率），仍然无法回答\n# 日程提醒依然无法创建\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e问题：RAG 提供了知识，但无法获取实时数据，更无法执行\u0026quot;创建日程\u0026quot;这个写操作。\u003c/p\u003e\n\u003ch3\u003e6.3 LLM + Tool Calling（单步）\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003etools = [\n    {\n        \u0026quot;name\u0026quot;: \u0026quot;get_weather\u0026quot;,\n        \u0026quot;description\u0026quot;: \u0026quot;获取指定城市的天气预报\u0026quot;,\n        \u0026quot;parameters\u0026quot;: {\u0026quot;city\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;date\u0026quot;: \u0026quot;string\u0026quot;}\n    },\n    {\n        \u0026quot;name\u0026quot;: \u0026quot;create_reminder\u0026quot;,\n        \u0026quot;description\u0026quot;: \u0026quot;创建日程提醒\u0026quot;,\n        \u0026quot;parameters\u0026quot;: {\u0026quot;title\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;time\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;note\u0026quot;: \u0026quot;string\u0026quot;}\n    }\n]\n\nresponse = llm.generate(\n    \u0026quot;帮我查看明天北京的天气并创建日程提醒\u0026quot;,\n    tools=tools,\n)\n# LLM 返回一个 tool_call：get_weather(city=\u0026quot;北京\u0026quot;, date=\u0026quot;2025-08-04\u0026quot;)\n# 但只能调用一个工具——它选了查天气，日程提醒怎么办？\n# 需要第二轮调用，但谁来发起？没有循环机制。\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e问题：单步 Tool Calling 只能执行一个动作。多步任务需要外部编排。\u003c/p\u003e\n\u003ch3\u003e6.4 LLM + Tools + Loop（ReAct Agent）\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eagent = Agent(llm=llm, tools={\u0026quot;get_weather\u0026quot;: get_weather, \u0026quot;create_reminder\u0026quot;: create_reminder})\nresult = agent.run(\u0026quot;帮我查看明天北京的天气并创建日程提醒\u0026quot;)\n\n# Agent 内部执行过程：\n#\n# [Iteration 1]\n# Think:  用户想查天气并创建提醒，我需要先查天气，再用天气信息创建提醒。\n# Plan:   调用 get_weather(city=\u0026quot;北京\u0026quot;, date=\u0026quot;2025-08-04\u0026quot;)\n# Act:    → {\u0026quot;temp\u0026quot;: 31, \u0026quot;condition\u0026quot;: \u0026quot;多云转雷阵雨\u0026quot;, \u0026quot;humidity\u0026quot;: 78}\n# Reflect: 成功获取天气数据，接下来需要创建日程提醒。\n# Update:  记录天气数据到 state。\n#\n# [Iteration 2]\n# Think:  已获取天气信息（31°C，多云转雷阵雨），需要创建提醒。\n# Plan:   调用 create_reminder(\n#             title=\u0026quot;明天北京天气提醒\u0026quot;,\n#             time=\u0026quot;2025-08-04T07:00:00\u0026quot;,\n#             note=\u0026quot;31°C，多云转雷阵雨，湿度 78%，建议带伞\u0026quot;\n#         )\n# Act:    → {\u0026quot;status\u0026quot;: \u0026quot;created\u0026quot;, \u0026quot;id\u0026quot;: \u0026quot;rem_abc123\u0026quot;}\n# Reflect: 提醒创建成功。两个子任务都已完成，可以返回最终结果。\n# Plan:   finish\n#\n# 最终输出：\n# \u0026quot;明天北京天气：31°C，多云转雷阵雨，湿度 78%。\n#  已为您创建早上 7:00 的天气提醒，建议带伞。\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这才是我们期望的行为：\u003cstrong\u003e理解意图 → 分解任务 → 逐步执行 → 组合结果\u003c/strong\u003e。注意 Agent 做了几件 Pure LLM 做不到的事：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e任务分解\u003c/strong\u003e：识别出\u0026quot;查天气\u0026quot;和\u0026quot;创建提醒\u0026quot;是两个子任务，且有依赖关系\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e信息传递\u003c/strong\u003e：把第一步的天气数据作为第二步的输入（note 字段）\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e智能补全\u003c/strong\u003e：用户没说提醒时间，Agent 推断了一个合理的时间（早上 7 点）\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e结果整合\u003c/strong\u003e：把多步执行的结果组合成一个连贯的自然语言回复\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e6.5 Full Agent（增加长期记忆与反思）\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Full Agent 在 ReAct 基础上增加：\n\n# 1. 长期记忆：记住用户偏好\nuser_profile = memory.recall(user_id=\u0026quot;u_001\u0026quot;)\n# → {\u0026quot;preferred_reminder_time\u0026quot;: \u0026quot;06:30\u0026quot;, \u0026quot;weather_sensitivity\u0026quot;: \u0026quot;rain\u0026quot;}\n\n# 2. 个性化决策：基于用户历史偏好\n# Agent 不再推断 7:00，而是使用用户偏好的 6:30\n# Agent 知道用户对雨天敏感，会强调带伞建议\n\n# 3. 显式反思：执行后回顾\nreflection = agent.reflect(\n    task=\u0026quot;查天气并创建提醒\u0026quot;,\n    result=result,\n    criteria=[\u0026quot;信息完整性\u0026quot;, \u0026quot;时间合理性\u0026quot;, \u0026quot;个性化程度\u0026quot;]\n)\n# → \u0026quot;时间使用了用户偏好，天气包含了降雨提醒。但缺少穿衣建议，下次可以补充。\u0026quot;\n\n# 4. 记忆更新：学习本次交互\nmemory.store(\n    user_id=\u0026quot;u_001\u0026quot;,\n    fact=\u0026quot;用户关注北京天气，可能是北京居民或近期有出行计划\u0026quot;,\n    source=\u0026quot;interaction_20250803\u0026quot;\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFull Agent 的核心区别在于：\u003cstrong\u003e它在跨会话的时间尺度上持续学习和个性化\u003c/strong\u003e。这需要一个完整的 Memory 架构来支撑——短期会话记忆、长期用户画像、事实知识库——我们将在第 08 篇详细展开。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e7. Agent 的设计哲学\u003c/h2\u003e\n\u003ch3\u003e7.1 LLM as the Reasoning Engine, Not the Entire System\u003c/h3\u003e\n\u003cp\u003e这是 Agent 架构最核心的设计原则。LLM 是推理引擎，不是整个系统。就像汽车的发动机不是汽车本身——你还需要变速箱（Planner）、方向盘（Tools）、仪表盘（Memory）和底盘（Runtime）。\u003c/p\u003e\n\u003cp\u003e这个原则的工程含义是：\u003cstrong\u003e不要让 LLM 做所有事情。\u003c/strong\u003e 让它做它擅长的——语义理解、推理、决策——然后用确定性代码处理其余部分。\u003c/p\u003e\n\u003ch3\u003e7.2 确定性 vs 非确定性的边界\u003c/h3\u003e\n\u003cp\u003eAgent 系统的核心设计问题之一是：\u003cstrong\u003e哪些部分让 LLM 做（非确定性），哪些部分用代码做（确定性）？\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e    确定性 (代码)                        非确定性 (LLM)\n    +-----------------------+          +-----------------------+\n    | 输入校验              |          | 意图理解              |\n    | 工具调度              |          | 工具选择              |\n    | 参数类型检查          |          | 参数填充              |\n    | 权限控制              |          | 上下文摘要            |\n    | 错误重试逻辑          |          | 结果解释              |\n    | 速率限制              |          | 对话策略              |\n    | 日志记录              |          | 异常情况判断          |\n    | 状态持久化            |          | 任务分解              |\n    +-----------------------+          +-----------------------+\n            |                                    |\n            v                                    v\n    可预测、可审计、可测试            灵活、自适应、但不可控\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e决策原则：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e如果逻辑可以穷举，用代码\u003c/strong\u003e。比如\u0026quot;用户必须先登录才能创建日程\u0026quot;——这是业务规则，不需要 LLM 判断。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e如果需要理解自然语言语义，用 LLM\u003c/strong\u003e。比如\u0026quot;用户说\u0026#39;帮我约个会\u0026#39;是什么意思\u0026quot;——这需要语义理解。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e如果错误的代价很高，用代码兜底\u003c/strong\u003e。比如转账操作的金额校验，无论 LLM 怎么说，都必须用代码做最终确认。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e如果需要处理开放域输入，用 LLM\u003c/strong\u003e。比如用户可能用任何方式描述他们的需求，只有 LLM 能处理这种多样性。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e7.3 何时不需要 Agent\u003c/h3\u003e\n\u003cp\u003e并非所有问题都需要 Agent。以下场景用更简单的方案更好：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e固定流程的自动化\u003c/strong\u003e：发票处理、数据同步——用 Workflow（DAG）更可靠\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e单轮问答\u003c/strong\u003e：FAQ、知识检索——LLM + RAG 就够了\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e确定性决策\u003c/strong\u003e：基于规则的审批——规则引擎更合适\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e高吞吐低延迟\u003c/strong\u003e：实时推荐——Agent 的多轮调用延迟太高\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAgent 的最佳应用场景是：\u003cstrong\u003e任务需要多步推理、工具组合使用、且执行路径在运行时才能确定\u003c/strong\u003e。如果执行路径在编译时就能确定，你需要的是 Workflow，不是 Agent。这正是我们下一篇要深入讨论的主题。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e8. 总结与思考\u003c/h2\u003e\n\u003cp\u003e本文从 LLM 的本质出发，论证了为什么 \u003ccode\u003ef(prompt) → response\u003c/code\u003e 不等于 Agent。核心论点可以压缩为一句话：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eLLM 是推理能力的来源，Agent 是将推理能力转化为行动能力的系统。\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e我们建立了三个关键的心智模型：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e组件模型\u003c/strong\u003e：Agent = LLM + Memory + Tools + Planner + Runtime，五个组件各有职责，协作运行。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e循环模型\u003c/strong\u003e：Observe → Think → Plan → Act → Reflect → Update，Agent 通过控制循环将单次推理扩展为多步执行。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e光谱模型\u003c/strong\u003e：从 Pure LLM 到 Full Agent 是一个连续光谱，每一步都有明确的能力增益和复杂性代价。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e进一步思考\u003c/h3\u003e\n\u003cp\u003e在进入下一篇之前，留几个值得深入思考的问题：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e关于 Agent 的边界\u003c/strong\u003e：如果 Planner 是硬编码的（比如一个固定的 DAG），这还算 Agent 吗？如果所有工具都是预定义的、参数是模板化的，LLM 只负责填参数，这算 Agent 还是 Workflow？这个边界在哪里，决定了你在工程实践中应该选择什么样的架构。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e关于 LLM 的演进\u003c/strong\u003e：随着模型能力的增强（更长的 context window、更强的 reasoning、内置的 tool use），LLM 和 Agent 之间的边界是否会逐渐模糊？OpenAI 的 o1/o3 系列通过 chain-of-thought 在模型内部实现了某种程度的\u0026quot;规划\u0026quot;，这是否意味着 Agent Runtime 的部分功能会被吸收进模型本身？\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e关于成本和延迟\u003c/strong\u003e：Agent 的每一轮循环都包含至少一次 LLM 调用。如果一个任务需要 5 轮循环，每轮 3 次 LLM 调用（Think + Plan + Reflect），就是 15 次调用。这个成本和延迟在生产环境中是否可接受？如何在 Agent 的灵活性和系统的性能之间找到平衡点？\u003c/p\u003e\n\u003cp\u003e这些问题没有标准答案，但它们定义了 Agentic 系统设计的核心张力。\u003c/p\u003e\n\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e系列导航\u003c/strong\u003e：本文是 Agentic 系列的第 02 篇。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e上一篇：\u003ca href=\"/blog/engineering/agentic/01-From%20LLM%20to%20Agent\"\u003e01 | From LLM to Agent\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下一篇：\u003ca href=\"/blog/engineering/agentic/03-Agent%20vs%20Workflow%20vs%20Automation\"\u003e03 | Agent vs Workflow vs Automation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e完整目录：\u003ca href=\"/blog/engineering/agentic/01-From%20LLM%20to%20Agent\"\u003e01 | From LLM to Agent\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"18:Tc7d3,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eThe Agent Control Loop: Agent 运行时的核心抽象\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e如果说 LLM 是 Agent 的大脑，那么 Control Loop 就是 Agent 的心跳。\u003c/p\u003e\n\u003cp\u003e大多数教程在讲 Agent 时，上来就接框架、调 API、跑 demo。但如果你不理解 Agent 运行时的核心抽象——控制循环——你永远只是在用别人的黑盒。\u003c/p\u003e\n\u003cp\u003e本文是 Agentic 系列第 04 篇，整个系列的技术基石。我们会从状态机模型出发，逐层拆解 Agent Control Loop 的每一个阶段，给出完整的 Python 实现，并深入分析实际工程中的 trade-off。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch2\u003e1. Agent 的本质：可中断的控制循环\u003c/h2\u003e\n\u003cp\u003e一个常见的误解是把 Agent 等同于\u0026quot;一次 LLM 调用\u0026quot;。实际上，Agent 和 LLM 的关系，类似于操作系统和 CPU 的关系——LLM 是执行推理的计算单元，而 Agent 是管理整个执行生命周期的运行时系统。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLLM 是一个函数：\u003c/strong\u003e \u003ccode\u003ef(prompt) -\u0026gt; completion\u003c/code\u003e，输入文本，输出文本，调用一次就结束。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAgent 是一个循环：\u003c/strong\u003e 它持续运行，在每一轮中观察环境、调用 LLM 进行推理、执行动作、评估结果，然后决定是否继续。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eLLM:    Input ──→ Output            (一次调用)\n\nAgent:  Input ──→ [Observe → Think → Act → Reflect] ──→ ... ──→ Output\n                  └──────── 循环 N 次 ────────────┘     (多轮控制)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这个循环有几个关键特性：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e可中断\u003c/strong\u003e：循环可以在任何阶段暂停，等待外部输入（用户确认、异步工具返回）后恢复\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e有状态\u003c/strong\u003e：循环维护上下文信息，每一轮的输出影响下一轮的输入\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e有终止条件\u003c/strong\u003e：循环不会无限运行，它在满足特定条件时停止\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e可观测\u003c/strong\u003e：循环的每一步都应该是可追踪、可回溯的\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e理解了这一点，Agent 编程的核心问题就变成了：\u003cstrong\u003e如何设计和实现这个控制循环？\u003c/strong\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e2. 状态机模型：形式化定义\u003c/h2\u003e\n\u003cp\u003e要严谨地描述 Control Loop，最自然的方式是用\u003cstrong\u003e有限状态机（FSM）\u003c/strong\u003e。\u003c/p\u003e\n\u003ch3\u003e2.1 状态定义\u003c/h3\u003e\n\u003cp\u003e一个 Agent Control Loop 可以用以下状态集合描述：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom enum import Enum\n\nclass AgentState(Enum):\n    OBSERVE  = \u0026quot;observe\u0026quot;   # 接收并归一化输入\n    THINK    = \u0026quot;think\u0026quot;     # LLM 推理，决定下一步行动\n    ACT      = \u0026quot;act\u0026quot;       # 执行工具调用或产出结果\n    REFLECT  = \u0026quot;reflect\u0026quot;   # 评估执行结果，决定是否继续\n    DONE     = \u0026quot;done\u0026quot;      # 终止：任务完成\n    ERROR    = \u0026quot;error\u0026quot;     # 终止：不可恢复错误\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2.2 状态转移图\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e                    ┌─────────────────────────────────────────┐\n                    │                                         │\n                    ▼                                         │\n              ┌──────────┐                                    │\n   Input ───→│ OBSERVE  │                                    │\n              └────┬─────┘                                    │\n                   │                                         │\n                   ▼                                         │\n              ┌──────────┐    need_action    ┌──────────┐    │\n              │  THINK   │ ───────────────→ │   ACT    │    │\n              └────┬─────┘                   └────┬─────┘    │\n                   │                              │          │\n                   │ has_answer                   │          │\n                   │                              ▼          │\n                   │                        ┌──────────┐     │\n                   │                        │ REFLECT  │ ────┘\n                   │                        └────┬─────┘  continue\n                   │                             │\n                   ▼                             ▼\n              ┌──────────┐                  ┌──────────┐\n              │   DONE   │                  │  ERROR   │\n              └──────────┘                  └──────────┘\n                                       (max_retries exceeded\n                                        / unrecoverable)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e状态转移规则：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e当前状态\u003c/th\u003e\n\u003cth\u003e条件\u003c/th\u003e\n\u003cth\u003e下一状态\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003eOBSERVE\u003c/td\u003e\n\u003ctd\u003e输入就绪\u003c/td\u003e\n\u003ctd\u003eTHINK\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTHINK\u003c/td\u003e\n\u003ctd\u003eLLM 返回 tool_call\u003c/td\u003e\n\u003ctd\u003eACT\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTHINK\u003c/td\u003e\n\u003ctd\u003eLLM 返回最终回答\u003c/td\u003e\n\u003ctd\u003eDONE\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTHINK\u003c/td\u003e\n\u003ctd\u003eLLM 调用异常\u003c/td\u003e\n\u003ctd\u003eERROR\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eACT\u003c/td\u003e\n\u003ctd\u003e工具执行完成\u003c/td\u003e\n\u003ctd\u003eREFLECT\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eACT\u003c/td\u003e\n\u003ctd\u003e工具执行失败\u003c/td\u003e\n\u003ctd\u003eREFLECT (带错误信息)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eREFLECT\u003c/td\u003e\n\u003ctd\u003e需要继续\u003c/td\u003e\n\u003ctd\u003eOBSERVE (将结果作为新输入)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eREFLECT\u003c/td\u003e\n\u003ctd\u003e任务完成\u003c/td\u003e\n\u003ctd\u003eDONE\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eREFLECT\u003c/td\u003e\n\u003ctd\u003e超过重试上限\u003c/td\u003e\n\u003ctd\u003eERROR\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch3\u003e2.3 与 OODA Loop 的对比\u003c/h3\u003e\n\u003cp\u003eAgent Control Loop 并不是凭空发明的，它和军事决策理论中的 \u003cstrong\u003eOODA Loop（Observe-Orient-Decide-Act）\u003c/strong\u003e 有深层的结构对应：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eOODA Loop:          Agent Control Loop:\n┌─────────┐         ┌─────────┐\n│ Observe │ ──────→ │ OBSERVE │  感知环境\n├─────────┤         ├─────────┤\n│ Orient  │ ──────→ │ THINK   │  理解上下文，形成判断\n├─────────┤         │         │\n│ Decide  │ ──────→ │         │  (LLM 在 THINK 中同时完成 Orient+Decide)\n├─────────┤         ├─────────┤\n│  Act    │ ──────→ │  ACT    │  执行行动\n└─────────┘         ├─────────┤\n                    │ REFLECT │  OODA 中没有显式的反思阶段\n                    └─────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e关键区别在于 \u003cstrong\u003eREFLECT 阶段\u003c/strong\u003e。传统 OODA Loop 假设决策者能实时感知行动效果并自然融入下一轮 Observe。但 LLM Agent 不具备这种连续感知能力——它需要一个显式的反思步骤来评估工具返回值、判断是否需要修正。这是 Agent Control Loop 相对于经典决策循环的重要改进。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e3. 循环中每个阶段的深入分析\u003c/h2\u003e\n\u003ch3\u003e3.1 OBSERVE：输入归一化\u003c/h3\u003e\n\u003cp\u003eOBSERVE 阶段的职责是\u003cstrong\u003e收集并归一化各种来源的输入\u003c/strong\u003e，将它们统一为 LLM 可理解的格式。\u003c/p\u003e\n\u003cp\u003e输入来源远不止\u0026quot;用户消息\u0026quot;一种：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e输入来源                    归一化后\n┌─────────────────┐       ┌──────────────────────┐\n│ 用户消息         │ ────→ │ {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;,     │\n│ 工具返回值       │ ────→ │  \u0026quot;content\u0026quot;: \u0026quot;...\u0026quot;}   │\n│ 系统事件         │ ────→ │                      │\n│ 定时触发         │ ────→ │ {\u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;,   │\n│ 外部 Webhook    │ ────→ │  \u0026quot;content\u0026quot;: \u0026quot;...\u0026quot;}   │\n│ 上一轮反思结果   │ ────→ │                      │\n└─────────────────┘       └──────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e输入归一化的核心原则：\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e所有输入都必须序列化为 message 格式\u003c/strong\u003e。不管来源是什么，最终都要变成 \u003ccode\u003e{\u0026quot;role\u0026quot;: ..., \u0026quot;content\u0026quot;: ...}\u003c/code\u003e 的形式，因为 LLM 只理解 message 序列。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e工具返回值需要结构化包装\u003c/strong\u003e。不要直接把原始 JSON 甩给 LLM，要附上工具名称、执行状态和必要的摘要信息。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e输入需要截断和优先级排序\u003c/strong\u003e。当多个输入同时到达时，需要决定哪些放进当前轮次的 Context Window，哪些缓存到下一轮。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef observe(self, raw_inputs: list[dict]) -\u0026gt; list[dict]:\n    \u0026quot;\u0026quot;\u0026quot;将原始输入归一化为 LLM message 格式\u0026quot;\u0026quot;\u0026quot;\n    messages = []\n    for inp in raw_inputs:\n        match inp[\u0026quot;type\u0026quot;]:\n            case \u0026quot;user_message\u0026quot;:\n                messages.append({\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: inp[\u0026quot;text\u0026quot;]})\n            case \u0026quot;tool_result\u0026quot;:\n                messages.append({\n                    \u0026quot;role\u0026quot;: \u0026quot;tool\u0026quot;,\n                    \u0026quot;tool_call_id\u0026quot;: inp[\u0026quot;call_id\u0026quot;],\n                    \u0026quot;content\u0026quot;: self._format_tool_result(inp),\n                })\n            case \u0026quot;system_event\u0026quot;:\n                messages.append({\n                    \u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;,\n                    \u0026quot;content\u0026quot;: f\u0026quot;[System Event] {inp[\u0026#39;event\u0026#39;]}\u0026quot;,\n                })\n    return messages\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e3.2 THINK：LLM 推理\u003c/h3\u003e\n\u003cp\u003eTHINK 阶段是控制循环中最核心的一环——调用 LLM，让它基于当前上下文做出决策。\u003c/p\u003e\n\u003cp\u003e这个阶段要解决三个问题：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e问题一：Context Window 构建\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLLM 的输入不是当前轮次的消息，而是\u003cstrong\u003e从任务开始到现在的完整上下文\u003c/strong\u003e。构建 Context Window 的典型结构：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌─────────────────────────────────────────────┐\n│ System Prompt                               │  固定不变\n│ (角色定义 + 能力边界 + 输出格式要求)           │\n├─────────────────────────────────────────────┤\n│ Tool Definitions                            │  固定不变\n│ (可用工具的 JSON Schema 定义)                │\n├─────────────────────────────────────────────┤\n│ Message History                             │  随轮次增长\n│ (user → assistant → tool → assistant → ...) │\n├─────────────────────────────────────────────┤\n│ Current Turn Input                          │  当前轮次\n│ (本轮 OBSERVE 阶段归一化的输入)              │\n└─────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e问题二：Token 预算控制\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eContext Window 有上限（4K / 8K / 128K / 200K），而每一轮循环都会增加 message history。如果不加控制，几轮之后就会超限。\u003c/p\u003e\n\u003cp\u003e常见的预算控制策略：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e策略\u003c/th\u003e\n\u003cth\u003e实现方式\u003c/th\u003e\n\u003cth\u003e适用场景\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e硬截断\u003c/td\u003e\n\u003ctd\u003e只保留最近 N 条消息\u003c/td\u003e\n\u003ctd\u003e简单场景\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e滑动窗口\u003c/td\u003e\n\u003ctd\u003eSystem Prompt 固定 + 最近 K 轮对话\u003c/td\u003e\n\u003ctd\u003e工具调用场景\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e摘要压缩\u003c/td\u003e\n\u003ctd\u003e将早期对话用 LLM 生成摘要后替换\u003c/td\u003e\n\u003ctd\u003e长对话场景\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e优先级保留\u003c/td\u003e\n\u003ctd\u003e按消息重要性排序，低优先级先丢弃\u003c/td\u003e\n\u003ctd\u003e复杂多步任务\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef _build_context(self, new_messages: list[dict]) -\u0026gt; list[dict]:\n    \u0026quot;\u0026quot;\u0026quot;构建符合 Token 预算的 Context Window\u0026quot;\u0026quot;\u0026quot;\n    self.message_history.extend(new_messages)\n\n    context = [self.system_prompt] + self.tool_definitions\n    remaining_budget = self.max_tokens - self._count_tokens(context)\n\n    # 从最新消息开始向前填充，直到预算耗尽\n    selected = []\n    for msg in reversed(self.message_history):\n        msg_tokens = self._count_tokens([msg])\n        if msg_tokens \u0026gt; remaining_budget:\n            break\n        selected.insert(0, msg)\n        remaining_budget -= msg_tokens\n\n    return context + selected\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e问题三：LLM 输出解析\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLLM 的返回可能是纯文本回答（任务完成），也可能是工具调用请求。需要根据返回类型决定下一步状态转移：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef think(self, context: list[dict]) -\u0026gt; ThinkResult:\n    \u0026quot;\u0026quot;\u0026quot;调用 LLM 进行推理\u0026quot;\u0026quot;\u0026quot;\n    response = self.client.chat.completions.create(\n        model=self.model,\n        messages=context,\n        tools=self.tool_schemas,\n    )\n    choice = response.choices[0]\n\n    if choice.finish_reason == \u0026quot;tool_calls\u0026quot;:\n        return ThinkResult(\n            action=\u0026quot;tool_call\u0026quot;,\n            tool_calls=choice.message.tool_calls,\n            raw_message=choice.message,\n        )\n    else:\n        return ThinkResult(\n            action=\u0026quot;answer\u0026quot;,\n            content=choice.message.content,\n            raw_message=choice.message,\n        )\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e3.3 ACT：执行层\u003c/h3\u003e\n\u003cp\u003eACT 阶段负责\u003cstrong\u003e执行 THINK 阶段决定的动作\u003c/strong\u003e——通常是调用工具（Tool Calling）。\u003c/p\u003e\n\u003cp\u003e执行层的核心挑战不是\u0026quot;调用工具\u0026quot;本身，而是以下几个工程问题：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e同步 vs 异步执行\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e同步执行（Simple）：\n  think → call_tool_1 → wait → call_tool_2 → wait → reflect\n  延迟 = T1 + T2\n\n异步 / 并行执行（Optimized）：\n  think → call_tool_1 ─┬─→ reflect\n        → call_tool_2 ─┘\n  延迟 = max(T1, T2)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e当 LLM 在一次返回中请求多个工具调用（parallel tool calling）时，应该并行执行以降低延迟：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport asyncio\n\nasync def act(self, tool_calls: list[ToolCall]) -\u0026gt; list[dict]:\n    \u0026quot;\u0026quot;\u0026quot;并行执行多个工具调用\u0026quot;\u0026quot;\u0026quot;\n    tasks = [self._execute_tool(tc) for tc in tool_calls]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    tool_results = []\n    for tc, result in zip(tool_calls, results):\n        if isinstance(result, Exception):\n            tool_results.append({\n                \u0026quot;type\u0026quot;: \u0026quot;tool_result\u0026quot;,\n                \u0026quot;call_id\u0026quot;: tc.id,\n                \u0026quot;status\u0026quot;: \u0026quot;error\u0026quot;,\n                \u0026quot;content\u0026quot;: f\u0026quot;Tool \u0026#39;{tc.function.name}\u0026#39; failed: {result}\u0026quot;,\n            })\n        else:\n            tool_results.append({\n                \u0026quot;type\u0026quot;: \u0026quot;tool_result\u0026quot;,\n                \u0026quot;call_id\u0026quot;: tc.id,\n                \u0026quot;status\u0026quot;: \u0026quot;success\u0026quot;,\n                \u0026quot;content\u0026quot;: str(result),\n            })\n    return tool_results\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e执行安全\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e工具执行不是无条件信任的。需要考虑：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e超时控制\u003c/strong\u003e：每个工具调用必须有 timeout，防止阻塞整个循环\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e结果大小限制\u003c/strong\u003e：工具返回值可能非常大（比如查数据库返回 10 万行），需要截断\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e权限校验\u003c/strong\u003e：某些工具（文件写入、网络请求、代码执行）需要额外的权限检查\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e沙箱执行\u003c/strong\u003e：代码执行类工具应该在沙箱中运行\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e3.4 REFLECT：输出质量评估\u003c/h3\u003e\n\u003cp\u003eREFLECT 阶段回答一个关键问题：\u003cstrong\u003e上一步的执行结果是否满意？是继续、重试还是停止？\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e这个阶段有两种实现方式：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e方式一：隐式反思——让 LLM 在下一轮 THINK 中自行判断\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e这是最简单的方式。把工具返回值直接送进下一轮 THINK，让 LLM 自己决定是否需要修正。大多数框架（如 OpenAI Assistants API）默认采用这种方式。\u003c/p\u003e\n\u003cp\u003e优点：实现简单，不增加额外的 LLM 调用。\u003c/p\u003e\n\u003cp\u003e缺点：LLM 可能\u0026quot;自信地\u0026quot;忽略错误，特别是在返回值看起来合理但语义错误的情况下。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e方式二：显式反思——用独立的 LLM 调用进行自我评估\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef reflect(self, action_result: dict, task_goal: str) -\u0026gt; ReflectResult:\n    \u0026quot;\u0026quot;\u0026quot;显式反思：评估执行结果\u0026quot;\u0026quot;\u0026quot;\n    prompt = f\u0026quot;\u0026quot;\u0026quot;评估以下工具执行结果是否达成了任务目标。\n\n任务目标: {task_goal}\n执行结果: {json.dumps(action_result, ensure_ascii=False)}\n\n请回答：\n1. 结果是否正确？(yes/no)\n2. 是否需要进一步行动？(yes/no)\n3. 如果需要，下一步应该做什么？\n\u0026quot;\u0026quot;\u0026quot;\n    response = self.client.chat.completions.create(\n        model=self.model,\n        messages=[{\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: prompt}],\n    )\n    # 解析反思结果...\n    return ReflectResult(\n        is_correct=...,\n        needs_more_action=...,\n        next_step_hint=...,\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eTrade-off 分析：\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e维度\u003c/th\u003e\n\u003cth\u003e隐式反思\u003c/th\u003e\n\u003cth\u003e显式反思\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003eToken 消耗\u003c/td\u003e\n\u003ctd\u003e低\u003c/td\u003e\n\u003ctd\u003e高（额外一次 LLM 调用）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e质量把控\u003c/td\u003e\n\u003ctd\u003e依赖 LLM 自觉\u003c/td\u003e\n\u003ctd\u003e有独立的质量评估\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e延迟\u003c/td\u003e\n\u003ctd\u003e低\u003c/td\u003e\n\u003ctd\u003e增加一轮 LLM 延迟\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e适用场景\u003c/td\u003e\n\u003ctd\u003e简单工具调用\u003c/td\u003e\n\u003ctd\u003e复杂推理链、高准确性要求\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e实际工程中，常用的折中方案是：\u003cstrong\u003e对关键步骤用显式反思，对常规步骤用隐式反思\u003c/strong\u003e。\u003c/p\u003e\n\u003ch3\u003e3.5 终止条件：什么时候停下来？\u003c/h3\u003e\n\u003cp\u003e一个 Agent 如果不知道什么时候停，就是一个烧钱的死循环。终止条件的设计是 Control Loop 中最容易被忽视、但对生产环境最重要的部分。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef should_stop(self, state: LoopState) -\u0026gt; tuple[bool, str]:\n    \u0026quot;\u0026quot;\u0026quot;判断是否应该终止循环\u0026quot;\u0026quot;\u0026quot;\n    # 1. LLM 认为任务完成\n    if state.last_think_result.action == \u0026quot;answer\u0026quot;:\n        return True, \u0026quot;task_completed\u0026quot;\n\n    # 2. 达到最大轮次\n    if state.turn_count \u0026gt;= self.max_turns:\n        return True, \u0026quot;max_turns_exceeded\u0026quot;\n\n    # 3. Token 预算耗尽\n    if state.total_tokens \u0026gt;= self.token_budget:\n        return True, \u0026quot;token_budget_exceeded\u0026quot;\n\n    # 4. 连续错误过多\n    if state.consecutive_errors \u0026gt;= self.max_consecutive_errors:\n        return True, \u0026quot;too_many_errors\u0026quot;\n\n    # 5. 死循环检测（重复输出相同内容）\n    if self._detect_loop(state.recent_outputs):\n        return True, \u0026quot;loop_detected\u0026quot;\n\n    return False, \u0026quot;\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e各终止条件的设计考量：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003emax_turns\u003c/strong\u003e：硬上限，防止失控。一般设 10-30 轮。过小会导致复杂任务被截断，过大会导致 Token 浪费\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003etoken_budget\u003c/strong\u003e：成本控制。根据业务场景设定每次交互的 Token 上限\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003econsecutive_errors\u003c/strong\u003e：容错阈值。工具偶尔失败是正常的，但连续 3 次以上通常意味着系统性问题\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eloop_detected\u003c/strong\u003e：死循环检测。如果 Agent 连续 N 轮输出相同或高度相似的内容，说明它陷入了无效循环\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e4. 两种主流 Loop 模式对比\u003c/h2\u003e\n\u003ch3\u003e4.1 ReAct 模式\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eReAct（Reason + Act）\u003c/strong\u003e 是目前最主流的 Agent Loop 模式，由 Yao et al. 2022 提出。其核心思想是让 LLM 交替进行推理和行动：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌──────────────────────────────────────────────────────┐\n│                   ReAct Loop                         │\n│                                                      │\n│  ┌─────────┐    ┌─────────┐    ┌─────────────────┐  │\n│  │ Thought │ →  │ Action  │ →  │  Observation    │  │\n│  │(LLM推理)│    │(工具调用)│    │(工具返回值)      │  │\n│  └─────────┘    └─────────┘    └────────┬────────┘  │\n│       ▲                                  │          │\n│       └──────────────────────────────────┘          │\n│                  循环直到完成                         │\n└──────────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e一个典型的 ReAct 执行轨迹（Trace）：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eThought: 用户想知道北京今天的天气。我需要调用天气 API。\nAction:  get_weather(city=\u0026quot;北京\u0026quot;)\nObservation: {\u0026quot;temp\u0026quot;: 28, \u0026quot;condition\u0026quot;: \u0026quot;晴\u0026quot;, \u0026quot;humidity\u0026quot;: 45}\n\nThought: 已经获取到天气数据，我可以直接回答用户。\nAnswer:  北京今天晴天，气温 28°C，湿度 45%。\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eReAct 的优势：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e每一步都基于最新的观察结果做决策，\u003cstrong\u003e适应性强\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThought 过程可见，\u003cstrong\u003e可解释性好\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e实现简单，与 Tool Calling API 天然契合\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eReAct 的劣势：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e逐步决策，无法全局优化执行顺序\u003c/li\u003e\n\u003cli\u003e每一步都需要一次 LLM 调用，\u003cstrong\u003e延迟累积\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e对于需要协调多个子任务的复杂场景，容易陷入局部最优\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e4.2 Plan-then-Execute 模式\u003c/h3\u003e\n\u003cp\u003e与 ReAct 的\u0026quot;走一步看一步\u0026quot;不同，Plan-then-Execute 先生成一个\u003cstrong\u003e完整的执行计划\u003c/strong\u003e，然后按计划依次执行：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌──────────────────────────────────────────────────────────┐\n│              Plan-then-Execute Loop                       │\n│                                                          │\n│  ┌──────────────────────────────────────┐                │\n│  │           Planning Phase             │                │\n│  │  Input → LLM → [Step1, Step2, ...]   │                │\n│  └───────────────┬──────────────────────┘                │\n│                  │                                        │\n│                  ▼                                        │\n│  ┌──────────────────────────────────────┐                │\n│  │         Execution Phase              │                │\n│  │  Step1 → Execute → Result1           │                │\n│  │  Step2 → Execute → Result2           │                │\n│  │  ...                                 │                │\n│  └───────────────┬──────────────────────┘                │\n│                  │                                        │\n│                  ▼                                        │\n│  ┌──────────────────────────────────────┐                │\n│  │    Replan (if needed)                │                │\n│  │  检查是否需要调整计划                   │                │\n│  └──────────────────────────────────────┘                │\n└──────────────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e执行轨迹示例：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ePlan:\n  1. 查询北京天气\n  2. 查询上海天气\n  3. 对比两地天气差异\n  4. 生成出行建议\n\nExecute Step 1: get_weather(city=\u0026quot;北京\u0026quot;) → {\u0026quot;temp\u0026quot;: 28, \u0026quot;condition\u0026quot;: \u0026quot;晴\u0026quot;}\nExecute Step 2: get_weather(city=\u0026quot;上海\u0026quot;) → {\u0026quot;temp\u0026quot;: 32, \u0026quot;condition\u0026quot;: \u0026quot;多云\u0026quot;}\nExecute Step 3: (LLM 对比分析)\nExecute Step 4: (LLM 生成建议)\n\nAnswer: ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e4.3 Trade-off 分析\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e                        灵活性\n                          ▲\n                          │\n                 ReAct ●  │\n                          │\n                          │        ● Hybrid\n                          │          (ReAct + Plan)\n                          │\n              Plan-then   │\n              -Execute ●  │\n                          │\n                          └──────────────────→ 效率\n                                          (LLM 调用次数)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e维度\u003c/th\u003e\n\u003cth\u003eReAct\u003c/th\u003e\n\u003cth\u003ePlan-then-Execute\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e灵活性\u003c/td\u003e\n\u003ctd\u003e高。每步实时调整\u003c/td\u003e\n\u003ctd\u003e低。偏离计划时需要 Replan\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLLM 调用次数\u003c/td\u003e\n\u003ctd\u003e多（每步一次推理）\u003c/td\u003e\n\u003ctd\u003e少（规划一次 + 执行时可能不需要 LLM）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e可控性\u003c/td\u003e\n\u003ctd\u003e低。难以预测执行路径\u003c/td\u003e\n\u003ctd\u003e高。计划可审核、可修改\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e适合场景\u003c/td\u003e\n\u003ctd\u003e工具调用为主、步骤不确定\u003c/td\u003e\n\u003ctd\u003e多步骤、有依赖关系、需要全局协调\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e错误恢复\u003c/td\u003e\n\u003ctd\u003e自然。下一步可以直接修正\u003c/td\u003e\n\u003ctd\u003e需要 Replan 机制\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e人类干预\u003c/td\u003e\n\u003ctd\u003e难以在中途插入\u003c/td\u003e\n\u003ctd\u003e容易。可以审核和修改计划\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e实际工程建议：\u003c/strong\u003e 大多数场景从 ReAct 开始。当你发现 Agent 频繁在多步任务中\u0026quot;迷路\u0026quot;或做出低效的工具调用序列时，再考虑引入 Plan-then-Execute 或混合模式。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e5. 状态管理\u003c/h2\u003e\n\u003cp\u003eControl Loop 的状态管理决定了 Agent 的\u003cstrong\u003e持久性\u003c/strong\u003e和\u003cstrong\u003e可恢复性\u003c/strong\u003e。\u003c/p\u003e\n\u003ch3\u003e5.1 Stateless Agent\u003c/h3\u003e\n\u003cp\u003eStateless Agent 不维护执行状态，所有上下文通过 \u003cstrong\u003emessage history\u003c/strong\u003e 传递。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eRequest 1:  [system, user_msg_1]                     → response_1\nRequest 2:  [system, user_msg_1, response_1, user_2] → response_2\nRequest 3:  [system, user_msg_1, response_1, user_2, response_2, user_3] → response_3\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e特点：\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e实现最简单，无需持久化\u003c/li\u003e\n\u003cli\u003e每次请求都是自包含的\u003c/li\u003e\n\u003cli\u003emessage history 不断膨胀，最终超过 Context Window\u003c/li\u003e\n\u003cli\u003e不支持暂停/恢复\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这是大多数 \u0026quot;chat completion\u0026quot; 应用的工作方式。适合单轮或短对话场景。\u003c/p\u003e\n\u003ch3\u003e5.2 Stateful Agent\u003c/h3\u003e\n\u003cp\u003eStateful Agent 维护一个独立的 \u003cstrong\u003eexecution state\u003c/strong\u003e，它不仅包含 message history，还包含任务进度、中间结果、工具状态等信息。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e@dataclass\nclass ExecutionState:\n    \u0026quot;\u0026quot;\u0026quot;Agent 执行状态\u0026quot;\u0026quot;\u0026quot;\n    session_id: str\n    status: AgentState\n    turn_count: int\n    message_history: list[dict]\n\n    # 任务状态\n    task_goal: str\n    current_plan: list[str] | None\n    completed_steps: list[str]\n\n    # 资源消耗\n    total_input_tokens: int\n    total_output_tokens: int\n\n    # 错误追踪\n    consecutive_errors: int\n    error_log: list[dict]\n\n    # 时间戳\n    created_at: float\n    updated_at: float\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e5.3 状态持久化方案\u003c/h3\u003e\n\u003cp\u003e当 Agent 需要支持暂停/恢复、跨进程执行、或长时间运行时，执行状态必须持久化。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e┌─────────────┐     ┌──────────────┐     ┌──────────────┐\n│   In-Memory  │     │    Redis     │     │   Database   │\n│  (dict/obj)  │     │  (KV Store)  │     │ (PostgreSQL) │\n├─────────────┤     ├──────────────┤     ├──────────────┤\n│ 最快         │     │ 快，支持 TTL  │     │ 持久可靠     │\n│ 进程重启丢失  │     │ 跨进程共享    │     │ 支持查询分析  │\n│ 单进程使用    │     │ 重启后可保留  │     │ 适合生产环境  │\n│ 适合开发/测试 │     │ 适合 session  │     │ 适合审计追溯  │\n└─────────────┘     └──────────────┘     └──────────────┘\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eCheckpoint 与恢复\u003c/strong\u003e 是 Stateful Agent 的核心能力。思路很直接：在每轮循环的关键节点保存一次快照，异常恢复时从最近的快照重新开始。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass CheckpointManager:\n    def save(self, state: ExecutionState) -\u0026gt; str:\n        \u0026quot;\u0026quot;\u0026quot;保存 checkpoint，返回 checkpoint_id\u0026quot;\u0026quot;\u0026quot;\n        snapshot = {\n            \u0026quot;state\u0026quot;: asdict(state),\n            \u0026quot;timestamp\u0026quot;: time.time(),\n        }\n        checkpoint_id = f\u0026quot;{state.session_id}:{state.turn_count}\u0026quot;\n        self.store.set(checkpoint_id, json.dumps(snapshot))\n        return checkpoint_id\n\n    def restore(self, checkpoint_id: str) -\u0026gt; ExecutionState:\n        \u0026quot;\u0026quot;\u0026quot;从 checkpoint 恢复执行状态\u0026quot;\u0026quot;\u0026quot;\n        snapshot = json.loads(self.store.get(checkpoint_id))\n        return ExecutionState(**snapshot[\u0026quot;state\u0026quot;])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e实际系统中，checkpoint 的保存频率需要权衡：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e每轮都保存\u003c/strong\u003e：恢复粒度最细，但写入开销大\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e关键节点保存\u003c/strong\u003e（如每次工具调用前后）：开销适中，覆盖最重要的故障场景\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e定时保存\u003c/strong\u003e：实现简单，但可能丢失最近几轮的状态\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e6. 完整代码实现\u003c/h2\u003e\n\u003cp\u003e下面是一个最小但完整的 Agent Control Loop 实现。不依赖任何框架，仅使用 Python 标准库 + OpenAI SDK。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\u0026quot;\u0026quot;\u0026quot;\nMinimal Agent Control Loop\n不依赖任何框架，纯 Python + OpenAI SDK\n\u0026quot;\u0026quot;\u0026quot;\nimport json\nimport time\nfrom enum import Enum\nfrom dataclasses import dataclass, field\nfrom openai import OpenAI\n\n\nclass State(Enum):\n    OBSERVE = \u0026quot;observe\u0026quot;\n    THINK = \u0026quot;think\u0026quot;\n    ACT = \u0026quot;act\u0026quot;\n    REFLECT = \u0026quot;reflect\u0026quot;\n    DONE = \u0026quot;done\u0026quot;\n    ERROR = \u0026quot;error\u0026quot;\n\n\n@dataclass\nclass LoopContext:\n    messages: list[dict] = field(default_factory=list)\n    turn: int = 0\n    total_tokens: int = 0\n    consecutive_errors: int = 0\n    recent_outputs: list[str] = field(default_factory=list)\n\n\n# ── Tool Registry ────────────────────────────────────\n\nTOOL_FUNCTIONS = {}\n\ndef register_tool(name: str, description: str, parameters: dict):\n    \u0026quot;\u0026quot;\u0026quot;装饰器：注册工具函数及其 schema\u0026quot;\u0026quot;\u0026quot;\n    def decorator(fn):\n        TOOL_FUNCTIONS[name] = {\n            \u0026quot;fn\u0026quot;: fn,\n            \u0026quot;schema\u0026quot;: {\n                \u0026quot;type\u0026quot;: \u0026quot;function\u0026quot;,\n                \u0026quot;function\u0026quot;: {\n                    \u0026quot;name\u0026quot;: name,\n                    \u0026quot;description\u0026quot;: description,\n                    \u0026quot;parameters\u0026quot;: parameters,\n                },\n            },\n        }\n        return fn\n    return decorator\n\n\n@register_tool(\n    name=\u0026quot;get_weather\u0026quot;,\n    description=\u0026quot;获取指定城市的当前天气\u0026quot;,\n    parameters={\n        \u0026quot;type\u0026quot;: \u0026quot;object\u0026quot;,\n        \u0026quot;properties\u0026quot;: {\n            \u0026quot;city\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;城市名称\u0026quot;},\n        },\n        \u0026quot;required\u0026quot;: [\u0026quot;city\u0026quot;],\n    },\n)\ndef get_weather(city: str) -\u0026gt; str:\n    # 示例实现，实际中调用真实 API\n    return json.dumps({\u0026quot;city\u0026quot;: city, \u0026quot;temp\u0026quot;: 28, \u0026quot;condition\u0026quot;: \u0026quot;晴\u0026quot;})\n\n\n# ── Agent Control Loop ───────────────────────────────\n\nclass Agent:\n    def __init__(\n        self,\n        system_prompt: str,\n        model: str = \u0026quot;gpt-4o\u0026quot;,\n        max_turns: int = 15,\n        token_budget: int = 50_000,\n        max_consecutive_errors: int = 3,\n    ):\n        self.client = OpenAI()\n        self.model = model\n        self.system_prompt = system_prompt\n        self.max_turns = max_turns\n        self.token_budget = token_budget\n        self.max_errors = max_consecutive_errors\n        self.tool_schemas = [t[\u0026quot;schema\u0026quot;] for t in TOOL_FUNCTIONS.values()]\n\n    def run(self, user_input: str) -\u0026gt; str:\n        ctx = LoopContext()\n        ctx.messages = [\n            {\u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;, \u0026quot;content\u0026quot;: self.system_prompt},\n            {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: user_input},\n        ]\n        state = State.THINK  # 首轮输入已就绪，直接进入 THINK\n\n        while state not in (State.DONE, State.ERROR):\n            match state:\n                case State.THINK:\n                    state, ctx = self._think(ctx)\n                case State.ACT:\n                    state, ctx = self._act(ctx)\n                case State.REFLECT:\n                    state, ctx = self._reflect(ctx)\n            ctx.turn += 1\n\n        # 提取最终回答\n        for msg in reversed(ctx.messages):\n            if msg[\u0026quot;role\u0026quot;] == \u0026quot;assistant\u0026quot; and msg.get(\u0026quot;content\u0026quot;):\n                return msg[\u0026quot;content\u0026quot;]\n        return \u0026quot;[Agent finished without a final answer]\u0026quot;\n\n    def _think(self, ctx: LoopContext) -\u0026gt; tuple[State, LoopContext]:\n        \u0026quot;\u0026quot;\u0026quot;调用 LLM 推理\u0026quot;\u0026quot;\u0026quot;\n        try:\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=ctx.messages,\n                tools=self.tool_schemas or None,\n            )\n        except Exception as e:\n            ctx.consecutive_errors += 1\n            ctx.messages.append({\n                \u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;,\n                \u0026quot;content\u0026quot;: f\u0026quot;[LLM Error] {e}\u0026quot;,\n            })\n            if ctx.consecutive_errors \u0026gt;= self.max_errors:\n                return State.ERROR, ctx\n            return State.THINK, ctx  # 重试\n\n        # 记录 token 消耗\n        usage = response.usage\n        ctx.total_tokens += (usage.prompt_tokens + usage.completion_tokens)\n        ctx.consecutive_errors = 0\n\n        choice = response.choices[0]\n        assistant_msg = choice.message.model_dump()\n        ctx.messages.append(assistant_msg)\n\n        # 决定下一状态\n        if choice.message.tool_calls:\n            return State.ACT, ctx\n        else:\n            return State.DONE, ctx\n\n    def _act(self, ctx: LoopContext) -\u0026gt; tuple[State, LoopContext]:\n        \u0026quot;\u0026quot;\u0026quot;执行工具调用\u0026quot;\u0026quot;\u0026quot;\n        assistant_msg = ctx.messages[-1]\n        tool_calls = assistant_msg.get(\u0026quot;tool_calls\u0026quot;, [])\n\n        for tc in tool_calls:\n            fn_name = tc[\u0026quot;function\u0026quot;][\u0026quot;name\u0026quot;]\n            fn_args = json.loads(tc[\u0026quot;function\u0026quot;][\u0026quot;arguments\u0026quot;])\n\n            tool_entry = TOOL_FUNCTIONS.get(fn_name)\n            if not tool_entry:\n                result = f\u0026quot;Error: unknown tool \u0026#39;{fn_name}\u0026#39;\u0026quot;\n            else:\n                try:\n                    result = tool_entry[\u0026quot;fn\u0026quot;](**fn_args)\n                except Exception as e:\n                    result = f\u0026quot;Error: tool \u0026#39;{fn_name}\u0026#39; raised {type(e).__name__}: {e}\u0026quot;\n                    ctx.consecutive_errors += 1\n\n            ctx.messages.append({\n                \u0026quot;role\u0026quot;: \u0026quot;tool\u0026quot;,\n                \u0026quot;tool_call_id\u0026quot;: tc[\u0026quot;id\u0026quot;],\n                \u0026quot;content\u0026quot;: str(result),\n            })\n\n        return State.REFLECT, ctx\n\n    def _reflect(self, ctx: LoopContext) -\u0026gt; tuple[State, LoopContext]:\n        \u0026quot;\u0026quot;\u0026quot;反思：检查终止条件\u0026quot;\u0026quot;\u0026quot;\n        # 最大轮次\n        if ctx.turn \u0026gt;= self.max_turns:\n            ctx.messages.append({\n                \u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;,\n                \u0026quot;content\u0026quot;: \u0026quot;[Agent stopped: max turns exceeded]\u0026quot;,\n            })\n            return State.ERROR, ctx\n\n        # Token 预算\n        if ctx.total_tokens \u0026gt;= self.token_budget:\n            ctx.messages.append({\n                \u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;,\n                \u0026quot;content\u0026quot;: \u0026quot;[Agent stopped: token budget exceeded]\u0026quot;,\n            })\n            return State.ERROR, ctx\n\n        # 连续错误\n        if ctx.consecutive_errors \u0026gt;= self.max_errors:\n            return State.ERROR, ctx\n\n        # 死循环检测：最近 3 次输出相同\n        tool_results = [\n            m[\u0026quot;content\u0026quot;] for m in ctx.messages[-6:]\n            if m.get(\u0026quot;role\u0026quot;) == \u0026quot;tool\u0026quot;\n        ]\n        if len(tool_results) \u0026gt;= 3 and len(set(tool_results[-3:])) == 1:\n            ctx.messages.append({\n                \u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;,\n                \u0026quot;content\u0026quot;: \u0026quot;[Agent stopped: loop detected]\u0026quot;,\n            })\n            return State.ERROR, ctx\n\n        # 继续下一轮推理\n        return State.THINK, ctx\n\n\n# ── 使用示例 ─────────────────────────────────────────\n\nif __name__ == \u0026quot;__main__\u0026quot;:\n    agent = Agent(\n        system_prompt=\u0026quot;你是一个天气助手。使用 get_weather 工具回答天气问题。\u0026quot;,\n        max_turns=10,\n    )\n    answer = agent.run(\u0026quot;北京今天天气怎么样？\u0026quot;)\n    print(answer)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这段代码约 130 行，涵盖了 Control Loop 的所有核心要素：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e状态机驱动的循环控制\u003c/li\u003e\n\u003cli\u003e工具注册与动态调用\u003c/li\u003e\n\u003cli\u003eLLM 异常重试\u003c/li\u003e\n\u003cli\u003eToken 消耗追踪\u003c/li\u003e\n\u003cli\u003e多种终止条件（max_turns / token_budget / consecutive_errors / loop_detected）\u003c/li\u003e\n\u003cli\u003e工具执行错误处理\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e它不是生产级代码，但足以说明 Control Loop 的核心机制。在此基础上增加异步执行、状态持久化、日志追踪，就能逐步演进为生产级实现。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e7. 错误处理策略\u003c/h2\u003e\n\u003cp\u003e生产环境中，Agent Control Loop 最常遇到的四类错误：\u003c/p\u003e\n\u003ch3\u003e7.1 Tool 调用失败\u003c/h3\u003e\n\u003cp\u003e工具调用失败是最高频的错误。正确的处理方式不是抛异常终止，而是\u003cstrong\u003e将错误信息作为 Observation 返回给 LLM\u003c/strong\u003e，让它决定如何应对。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# 错误的做法：直接终止\ntry:\n    result = call_tool(name, args)\nexcept Exception:\n    raise  # Agent 直接崩溃\n\n# 正确的做法：将错误反馈给 LLM\ntry:\n    result = call_tool(name, args)\nexcept TimeoutError:\n    result = \u0026quot;Tool timed out after 30s. Consider using different parameters.\u0026quot;\nexcept ValueError as e:\n    result = f\u0026quot;Invalid arguments: {e}. Please check parameter types.\u0026quot;\nexcept Exception as e:\n    result = f\u0026quot;Tool failed: {type(e).__name__}: {e}\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLLM 在收到错误信息后，通常能自主修正——换一组参数重试、换一个工具、或者告知用户当前无法完成任务。\u003c/p\u003e\n\u003ch3\u003e7.2 LLM 返回格式异常\u003c/h3\u003e\n\u003cp\u003eLLM 偶尔会返回不符合预期的格式：JSON 不合法、tool_call 参数缺失、content 为空等。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef _parse_tool_call_safe(self, tool_call) -\u0026gt; tuple[str, dict]:\n    \u0026quot;\u0026quot;\u0026quot;安全解析工具调用参数\u0026quot;\u0026quot;\u0026quot;\n    name = tool_call.function.name\n    try:\n        args = json.loads(tool_call.function.arguments)\n    except json.JSONDecodeError:\n        # LLM 返回了非法 JSON，尝试修复或跳过\n        args = {}\n        self.logger.warning(\n            f\u0026quot;Invalid JSON in tool_call arguments: \u0026quot;\n            f\u0026quot;{tool_call.function.arguments}\u0026quot;\n        )\n    return name, args\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e7.3 超时处理\u003c/h3\u003e\n\u003cp\u003e整个 Agent 执行需要有全局超时，防止无限挂起：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport signal\n\nclass TimeoutError(Exception):\n    pass\n\ndef run_with_timeout(fn, timeout_seconds: int, *args, **kwargs):\n    \u0026quot;\u0026quot;\u0026quot;为函数执行添加超时限制\u0026quot;\u0026quot;\u0026quot;\n    def handler(signum, frame):\n        raise TimeoutError(f\u0026quot;Execution timed out after {timeout_seconds}s\u0026quot;)\n\n    old_handler = signal.signal(signal.SIGALRM, handler)\n    signal.alarm(timeout_seconds)\n    try:\n        return fn(*args, **kwargs)\n    finally:\n        signal.alarm(0)\n        signal.signal(signal.SIGALRM, old_handler)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e7.4 死循环检测\u003c/h3\u003e\n\u003cp\u003e当 Agent 陷入死循环时，它会反复执行相同的操作序列。检测策略：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef _detect_loop(self, messages: list[dict], window: int = 6) -\u0026gt; bool:\n    \u0026quot;\u0026quot;\u0026quot;检测 Agent 是否陷入重复循环\u0026quot;\u0026quot;\u0026quot;\n    recent = messages[-window:]\n\n    # 策略 1：完全重复检测\n    contents = [m.get(\u0026quot;content\u0026quot;, \u0026quot;\u0026quot;) for m in recent if m[\u0026quot;role\u0026quot;] == \u0026quot;assistant\u0026quot;]\n    if len(contents) \u0026gt;= 3 and len(set(contents[-3:])) == 1:\n        return True\n\n    # 策略 2：工具调用序列重复检测\n    tool_calls = []\n    for m in recent:\n        if m.get(\u0026quot;tool_calls\u0026quot;):\n            for tc in m[\u0026quot;tool_calls\u0026quot;]:\n                tool_calls.append(f\u0026quot;{tc[\u0026#39;function\u0026#39;][\u0026#39;name\u0026#39;]}:{tc[\u0026#39;function\u0026#39;][\u0026#39;arguments\u0026#39;]}\u0026quot;)\n\n    if len(tool_calls) \u0026gt;= 4:\n        half = len(tool_calls) // 2\n        if tool_calls[:half] == tool_calls[half:2*half]:\n            return True\n\n    return False\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e8. 性能考量\u003c/h2\u003e\n\u003ch3\u003e8.1 Token 消耗与循环次数的关系\u003c/h3\u003e\n\u003cp\u003eAgent Control Loop 的 Token 消耗不是线性增长，而是\u003cstrong\u003e二次增长\u003c/strong\u003e——因为每一轮都要携带之前所有轮次的 message history。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e轮次    新增消息 Token    累计 Context Token    本轮总消耗\n1       T               S + T                S + T\n2       T               S + 2T               S + 2T\n3       T               S + 3T               S + 3T\n...\nN       T               S + NT               S + NT\n\n总消耗 = N*S + T*(1+2+...+N) = N*S + T*N*(N+1)/2\n\n其中 S = System Prompt Token 数，T = 平均每轮消息 Token 数\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这意味着 \u003cstrong\u003e10 轮的 Agent 消耗的 Token 不是 1 轮的 10 倍，而可能是 55 倍\u003c/strong\u003e。这对成本控制至关重要。\u003c/p\u003e\n\u003ch3\u003e8.2 Context Window 膨胀问题\u003c/h3\u003e\n\u003cp\u003e随着轮次增加，Context Window 持续膨胀，导致：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e延迟增加\u003c/strong\u003e：LLM 推理时间与输入 Token 数正相关\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e成本增加\u003c/strong\u003e：按 Token 计费，输入越长越贵\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e质量下降\u003c/strong\u003e：过长的 Context 会导致 LLM \u0026quot;注意力分散\u0026quot;，关键信息被淹没（lost in the middle 问题）\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e8.3 消息压缩/摘要策略\u003c/h3\u003e\n\u003cp\u003e应对 Context Window 膨胀的核心策略：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e策略一：滑动窗口\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e只保留最近 K 轮对话，丢弃更早的历史。简单粗暴但有效。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef _sliding_window(self, messages: list[dict], keep_last: int = 10) -\u0026gt; list[dict]:\n    system_msgs = [m for m in messages if m[\u0026quot;role\u0026quot;] == \u0026quot;system\u0026quot;]\n    non_system = [m for m in messages if m[\u0026quot;role\u0026quot;] != \u0026quot;system\u0026quot;]\n    return system_msgs + non_system[-keep_last:]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e策略二：摘要压缩\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e当 message history 超过阈值时，用 LLM 对早期对话生成摘要，替换原始消息。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef _compress_history(self, messages: list[dict], threshold: int = 20) -\u0026gt; list[dict]:\n    if len(messages) \u0026lt;= threshold:\n        return messages\n\n    # 将早期消息压缩为摘要\n    early = messages[1:-threshold]  # 跳过 system prompt，保留最近的\n    summary_prompt = (\n        \u0026quot;请用 3-5 句话总结以下对话的关键信息和已完成的操作：\\n\u0026quot;\n        + \u0026quot;\\n\u0026quot;.join(m.get(\u0026quot;content\u0026quot;, \u0026quot;\u0026quot;) for m in early if m.get(\u0026quot;content\u0026quot;))\n    )\n\n    summary = self.client.chat.completions.create(\n        model=\u0026quot;gpt-4o-mini\u0026quot;,  # 用小模型做摘要，节省成本\n        messages=[{\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: summary_prompt}],\n    ).choices[0].message.content\n\n    return (\n        [messages[0]]  # system prompt\n        + [{\u0026quot;role\u0026quot;: \u0026quot;system\u0026quot;, \u0026quot;content\u0026quot;: f\u0026quot;[Earlier conversation summary] {summary}\u0026quot;}]\n        + messages[-threshold:]\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e策略三：选择性保留\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e不是所有消息都同等重要。工具的原始返回值（可能非常长）通常可以只保留摘要：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef _trim_tool_results(self, messages: list[dict], max_len: int = 500) -\u0026gt; list[dict]:\n    \u0026quot;\u0026quot;\u0026quot;截断过长的工具返回值\u0026quot;\u0026quot;\u0026quot;\n    trimmed = []\n    for m in messages:\n        if m[\u0026quot;role\u0026quot;] == \u0026quot;tool\u0026quot; and len(m.get(\u0026quot;content\u0026quot;, \u0026quot;\u0026quot;)) \u0026gt; max_len:\n            m = {**m, \u0026quot;content\u0026quot;: m[\u0026quot;content\u0026quot;][:max_len] + \u0026quot;\\n...[truncated]\u0026quot;}\n        trimmed.append(m)\n    return trimmed\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e三种策略的对比：\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e策略\u003c/th\u003e\n\u003cth\u003e信息保留\u003c/th\u003e\n\u003cth\u003e实现成本\u003c/th\u003e\n\u003cth\u003eToken 节省\u003c/th\u003e\n\u003cth\u003e适用场景\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e滑动窗口\u003c/td\u003e\n\u003ctd\u003e低\u003c/td\u003e\n\u003ctd\u003e极低\u003c/td\u003e\n\u003ctd\u003e高\u003c/td\u003e\n\u003ctd\u003e短对话、工具调用为主\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e摘要压缩\u003c/td\u003e\n\u003ctd\u003e中\u003c/td\u003e\n\u003ctd\u003e中（需要额外 LLM 调用）\u003c/td\u003e\n\u003ctd\u003e高\u003c/td\u003e\n\u003ctd\u003e长对话、需要历史上下文\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e选择性保留\u003c/td\u003e\n\u003ctd\u003e高\u003c/td\u003e\n\u003ctd\u003e低\u003c/td\u003e\n\u003ctd\u003e中\u003c/td\u003e\n\u003ctd\u003e工具返回值较大的场景\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e实际工程中，通常\u003cstrong\u003e组合使用\u003c/strong\u003e：先用选择性保留截断大结果，再用滑动窗口控制总长度，在关键节点用摘要压缩保留全局上下文。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e9. 小结与进一步思考\u003c/h2\u003e\n\u003cp\u003e本文从状态机模型出发，完整地拆解了 Agent Control Loop 的核心抽象：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eOBSERVE\u003c/strong\u003e 负责输入归一化——将各种来源的信息统一为 LLM 可理解的 message 格式\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTHINK\u003c/strong\u003e 是核心推理阶段——管理 Context Window、控制 Token 预算、解析 LLM 输出\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eACT\u003c/strong\u003e 是执行层——处理工具调用的同步/异步执行、超时控制、安全隔离\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eREFLECT\u003c/strong\u003e 负责质量评估——决定是继续、重试还是终止\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e终止条件\u003c/strong\u003e是成本和安全的兜底——max_turns、token_budget、error_threshold、loop_detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e我们对比了 ReAct 和 Plan-then-Execute 两种主流模式，分析了 Stateless 与 Stateful 两种状态管理策略，并实现了一个不依赖任何框架的完整 Control Loop。\u003c/p\u003e\n\u003cp\u003e但控制循环只是 Agent 运行时的骨架。它的灵魂在于 \u003cstrong\u003eTool Calling\u003c/strong\u003e——正是工具让 Agent 从\u0026quot;能说会道的语言模型\u0026quot;变成\u0026quot;能做事的智能体\u0026quot;。\u003c/p\u003e\n\u003cp\u003e在下一篇 \u003cstrong\u003e《Tool Calling Deep Dive: 让 LLM 成为可编程接口》\u003c/strong\u003e 中，我们会深入工具调用的设计哲学：JSON Schema 作为契约、Tool Registry 的实现、参数校验、错误传播，以及 Structured Output 为什么优于自由文本。\u003c/p\u003e\n\u003cp\u003e留几个值得进一步思考的问题：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eControl Loop 的嵌套\u003c/strong\u003e：当一个 Agent 的工具是另一个 Agent 时，控制循环如何嵌套？外层循环和内层循环的终止条件如何协调？\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e人机协作中的循环\u003c/strong\u003e：如何在 Control Loop 中优雅地插入人类审批节点？这和 Stateful Agent 的 checkpoint 机制有什么关系？\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e流式输出与控制循环\u003c/strong\u003e：当 Agent 需要边思考边输出（streaming）时，状态机模型还适用吗？需要做哪些调整？\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e多模态输入的归一化\u003c/strong\u003e：当 OBSERVE 阶段接收的不只是文本，还有图片、音频、视频时，输入归一化策略如何演化？\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e系列导航\u003c/strong\u003e：本文是 Agentic 系列的第 04 篇。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e上一篇：\u003ca href=\"/blog/engineering/agentic/03-Agent%20vs%20Workflow%20vs%20Automation\"\u003e03 | Agent vs Workflow vs Automation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下一篇：\u003ca href=\"/blog/engineering/agentic/05-Tool%20Calling%20Deep%20Dive\"\u003e05 | Tool Calling Deep Dive\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e完整目录：\u003ca href=\"/blog/engineering/agentic/01-From%20LLM%20to%20Agent\"\u003e01 | From LLM to Agent\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"article\",null,{\"className\":\"min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"nav\",null,{\"className\":\"flex items-center gap-1 text-sm mb-4\",\"children\":[[\"$\",\"$L13\",null,{\"href\":\"/blog/page/1\",\"className\":\"text-gray-500 hover:text-blue-600 transition-colors\",\"children\":\"博客\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"/\"}],[\"$\",\"$L13\",null,{\"href\":\"/blog/category/engineering/page/1\",\"className\":\"text-gray-500 hover:text-blue-600 transition-colors\",\"children\":\"Engineering\"}],[[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"/\"}],[\"$\",\"$L13\",null,{\"href\":\"/blog/category/engineering/agentic/page/1\",\"className\":\"text-blue-600 hover:text-blue-700 transition-colors\",\"children\":\"Agentic 系统\"}]]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center mb-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"w-4 h-4 mr-2 text-gray-400\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z\"}]}],[\"$\",\"time\",null,{\"dateTime\":\"2025-12-09\",\"children\":\"2025年12月09日\"}]]}]}],[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold text-gray-900 mb-6 text-center\",\"children\":\"Agent vs Workflow vs Automation: 选对抽象才是关键\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2 mb-6 justify-center\",\"children\":[[\"$\",\"$L13\",\"Agentic\",{\"href\":\"/blog/tag/Agentic/page/1/\",\"className\":\"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors\",\"children\":\"Agentic\"}],[\"$\",\"$L13\",\"AI Engineering\",{\"href\":\"/blog/tag/AI%20Engineering/page/1/\",\"className\":\"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors\",\"children\":\"AI Engineering\"}],[\"$\",\"$L13\",\"Architecture\",{\"href\":\"/blog/tag/Architecture/page/1/\",\"className\":\"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors\",\"children\":\"Architecture\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"max-w-5xl mx-auto\",\"children\":[\"$\",\"$L14\",null,{\"content\":\"$15\"}]}],[\"$\",\"$10\",null,{\"fallback\":[\"$\",\"div\",null,{\"className\":\"mt-12 pt-8 border-t border-gray-200\",\"children\":\"加载导航中...\"}],\"children\":[\"$\",\"$L16\",null,{\"globalNav\":{\"prev\":{\"slug\":\"engineering/agentic/02-From Prompt to Agent\",\"title\":\"From Prompt to Agent: 为什么 LLM 本身不是 Agent\",\"description\":\"LLM 是一个无状态的文本函数，Agent 是一个有状态的推理系统。本文从 LLM 的五大局限出发，精确定义 Agent 的组件模型与控制循环，并沿 Chatbot → Agent 的光谱逐级拆解，帮助你建立从 Prompt 到 Agent 的完整认知框架。\",\"pubDate\":\"2025-12-05\",\"tags\":[\"Agentic\",\"AI Engineering\",\"LLM\"],\"heroImage\":\"$undefined\",\"content\":\"$17\"},\"next\":{\"slug\":\"engineering/agentic/04-The Agent Control Loop\",\"title\":\"The Agent Control Loop: Agent 运行时的核心抽象\",\"description\":\"Agent 的本质不是一次函数调用，而是一个可中断的控制循环。本文从状态机模型出发，深入剖析 Agent Control Loop 的每个阶段——OBSERVE、THINK、ACT、REFLECT，对比 ReAct 与 Plan-then-Execute 两种主流模式，讨论状态管理、错误处理与性能优化策略，并给出一个不依赖任何框架的完整 Python 实现。\",\"pubDate\":\"2025-12-14\",\"tags\":[\"Agentic\",\"AI Engineering\",\"Runtime\"],\"heroImage\":\"$undefined\",\"content\":\"$18\"}},\"tagNav\":{\"Agentic\":{\"prev\":\"$5:props:children:props:children:props:children:2:props:children:props:globalNav:prev\",\"next\":\"$5:props:children:props:children:props:children:2:props:children:props:globalNav:next\"},\"AI Engineering\":{\"prev\":\"$5:props:children:props:children:props:children:2:props:children:props:globalNav:prev\",\"next\":\"$5:props:children:props:children:props:children:2:props:children:props:globalNav:next\"},\"Architecture\":{\"prev\":null,\"next\":null}}}]}],[\"$\",\"$L19\",null,{}]]}]}]}]\n"])</script><script>self.__next_f.push([1,"8:null\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n7:null\n"])</script><script>self.__next_f.push([1,"a:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Agent vs Workflow vs Automation: 选对抽象才是关键 - Skyfalling Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"不是所有问题都需要 Agent。本文系统比较 Rule-based Automation、Workflow/DAG、Agent 三种执行范式，从确定性、成本、可观测性等维度给出选型框架，帮助工程师在真实场景中选对抽象层次。\"}],[\"$\",\"meta\",\"2\",{\"property\":\"og:title\",\"content\":\"Agent vs Workflow vs Automation: 选对抽象才是关键\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:description\",\"content\":\"不是所有问题都需要 Agent。本文系统比较 Rule-based Automation、Workflow/DAG、Agent 三种执行范式，从确定性、成本、可观测性等维度给出选型框架，帮助工程师在真实场景中选对抽象层次。\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"5\",{\"property\":\"article:published_time\",\"content\":\"2025-12-09\"}],[\"$\",\"meta\",\"6\",{\"property\":\"article:author\",\"content\":\"Skyfalling\"}],[\"$\",\"meta\",\"7\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:title\",\"content\":\"Agent vs Workflow vs Automation: 选对抽象才是关键\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:description\",\"content\":\"不是所有问题都需要 Agent。本文系统比较 Rule-based Automation、Workflow/DAG、Agent 三种执行范式，从确定性、成本、可观测性等维度给出选型框架，帮助工程师在真实场景中选对抽象层次。\"}],[\"$\",\"link\",\"10\",{\"rel\":\"shortcut icon\",\"href\":\"/favicon.png\"}],[\"$\",\"link\",\"11\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"link\",\"12\",{\"rel\":\"icon\",\"href\":\"/favicon.png\"}],[\"$\",\"link\",\"13\",{\"rel\":\"apple-touch-icon\",\"href\":\"/favicon.png\"}]],\"error\":null,\"digest\":\"$undefined\"}\n12:{\"metadata\":\"$a:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>