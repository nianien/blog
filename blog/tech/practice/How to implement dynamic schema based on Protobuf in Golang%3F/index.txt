1:"$Sreact.fragment"
2:I[10616,["874","static/chunks/874-f2e46e41114bd221.js","177","static/chunks/app/layout-c432974c723daafe.js"],"default"]
3:I[87555,[],""]
4:I[31295,[],""]
5:I[6874,["874","static/chunks/874-f2e46e41114bd221.js","968","static/chunks/968-bf93abe4de13a5fc.js","909","static/chunks/app/blog/%5B...slug%5D/page-26cc6d1a0064a78b.js"],""]
7:I[59665,[],"OutletBoundary"]
a:I[74911,[],"AsyncMetadataOutlet"]
c:I[59665,[],"ViewportBoundary"]
e:I[59665,[],"MetadataBoundary"]
10:I[26614,[],""]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/0a7d53676a1eb136.css","style"]
0:{"P":null,"b":"fEmHZchMN56N6zbkBdACr","p":"","c":["","blog","tech","practice","How%20to%20implement%20dynamic%20schema%20based%20on%20Protobuf%20in%20Golang%3F",""],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","tech/practice/How%20to%20implement%20dynamic%20schema%20based%20on%20Protobuf%20in%20Golang%3F","c"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/0a7d53676a1eb136.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"zh-CN","children":["$","body",null,{"className":"__className_f367f3","children":["$","div",null,{"className":"min-h-screen flex flex-col","children":[["$","$L2",null,{}],["$","main",null,{"className":"flex-1","children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"bg-[var(--background)]","children":["$","div",null,{"className":"mx-auto max-w-7xl px-6 py-12 md:flex md:items-center md:justify-between lg:px-8","children":[["$","div",null,{"className":"flex justify-center space-x-6 md:order-2","children":[["$","$L5",null,{"href":"/about","className":"text-gray-600 hover:text-gray-800","children":"关于"}],["$","$L5",null,{"href":"/blog","className":"text-gray-600 hover:text-gray-800","children":"博客"}],["$","$L5",null,{"href":"/contact","className":"text-gray-600 hover:text-gray-800","children":"联系"}]]}],["$","div",null,{"className":"mt-8 md:order-1 md:mt-0","children":["$","p",null,{"className":"text-center text-xs leading-5 text-gray-600","children":"© 2024 Skyfalling Blog. All rights reserved."}]}]]}]}]]}]}]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","tech/practice/How%20to%20implement%20dynamic%20schema%20based%20on%20Protobuf%20in%20Golang%3F","c"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L6",null,["$","$L7",null,{"children":["$L8","$L9",["$","$La",null,{"promise":"$@b"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","2mEOnGucMAOqiLt3Wkf4hv",{"children":[["$","$Lc",null,{"children":"$Ld"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$Le",null,{"children":"$Lf"}]]}],false]],"m":"$undefined","G":["$10","$undefined"],"s":false,"S":true}
11:"$Sreact.suspense"
12:I[74911,[],"AsyncMetadata"]
14:I[32923,["874","static/chunks/874-f2e46e41114bd221.js","968","static/chunks/968-bf93abe4de13a5fc.js","909","static/chunks/app/blog/%5B...slug%5D/page-26cc6d1a0064a78b.js"],"default"]
16:I[40780,["874","static/chunks/874-f2e46e41114bd221.js","968","static/chunks/968-bf93abe4de13a5fc.js","909","static/chunks/app/blog/%5B...slug%5D/page-26cc6d1a0064a78b.js"],"default"]
19:I[85300,["874","static/chunks/874-f2e46e41114bd221.js","968","static/chunks/968-bf93abe4de13a5fc.js","909","static/chunks/app/blog/%5B...slug%5D/page-26cc6d1a0064a78b.js"],"default"]
f:["$","div",null,{"hidden":true,"children":["$","$11",null,{"fallback":null,"children":["$","$L12",null,{"promise":"$@13"}]}]}]
15:T4029,<h2>1. Background</h2>
<p>Protocol Buffers (Protobuf) is a language-neutral, platform-neutral, extensible mechanism for serializing structured data. It was developed by Google to efficiently serialize data for use in a variety of applications, including network communication, data storage, and inter-process communication (IPC).  Protobuf messages are smaller and more efficient than text-based formats like JSON and XML ,and provides fast serialization and deserialization, which is crucial for high-performance systems. </p>
<h2>2. Protobuf compiler</h2>
<p>Protobuf allows you to define the structure of your data (messages) in a .proto file, and then use the Protobuf compiler (protoc) to generate source code in selected programming language that can serialize and deserialize data in the Protobuf format.  Protobuf adds overhead to the development process compared to formats like JSON, where you can directly parse or serialize data without needing code generation. Additionally, any changes to the schema may require re-compiling the code to handle new or modified message types.</p>
<h2>3. Dynamic compilation(Dynamic Schema)</h2>
<p>Typically, Protobuf schemas are compiled using the protoc compiler ahead of time, which generates source code in various programming languages (such as C++, Java, Python, etc.) for serialization and deserialization. However, in some scenarios, you might need to work with Protobuf messages dynamically without relying on pre-generated codes. </p>
<p>This is especially useful when:</p>
<ul>
<li>You want to work with Protobuf messages dynamically at runtime, where the schema is not known in advance.</li>
<li>You need to handle multiple or evolving Protobuf schemas at runtime without recompiling your code.</li>
<li>You want to dynamically serialize or deserialize Protobuf messages in a generic way, perhaps for applications like plugins, dynamic API handling, or protocol-based communication where the schema might change often.</li>
</ul>
<h3>Key Concepts for Dynamic Compilation in Protobuf</h3>
<ul>
<li>Dynamic Message (Dynamic Parsing)</li>
<li>Reflection API in Protobuf</li>
<li>Dynamic Code Generation</li>
</ul>
<h3>3.1 Dynamic Message (Dynamic Parsing)</h3>
<p>In Protobuf, a Dynamic Message is an object where you can manipulate the message’s fields dynamically, without needing a pre-generated class for the specific message type. This is enabled through Protobuf&#39;s Reflection API.<br>You can use dynamic messages when:</p>
<ul>
<li>The Protobuf schema is available at runtime, but you don&#39;t know the message types ahead of time.</li>
<li>You want to work with messages whose types are determined dynamically (e.g., reading from a file or network stream that specifies the message type).<br>To use dynamic messages, you typically:</li>
<li>Load the schema definition (e.g., .proto file) at runtime.</li>
<li>Use the Protobuf Reflection API to create message types and set/get fields.</li>
</ul>
<h3>3.2 Reflection API</h3>
<p>The Protobuf Reflection API allows you to inspect the structure of Protobuf messages at runtime, and dynamically access their fields. This is the core tool for implementing dynamic compilation because it lets you:</p>
<ul>
<li>Discover available fields.</li>
<li>Inspect field types (e.g., int32, string, nested messages).</li>
<li>Set and get field values dynamically.<br>The key components of the Reflection API in Protobuf are:</li>
<li>Descriptors: These are metadata objects that describe the fields, types, and structure of messages.</li>
<li>Dynamic Messages: These are messages created dynamically using descriptors, where you can set/get fields without needing a pre-compiled class.</li>
</ul>
<h3>3.3 Dynamic Code Generation with Protobuf Compiler</h3>
<p>In some use cases, you may need to generate Protobuf code dynamically based on new or unknown schemas. This would involve invoking the Protobuf compiler (protoc) programmatically to generate the code at runtime, either directly from .proto files or from a descriptor or a set of schema definitions.<br>This process typically involves the following steps:</p>
<ol>
<li>Obtain the .proto files or schema descriptors.</li>
<li>Run protoc programmatically to generate source code.</li>
<li>Use the generated code within the application to work with dynamic messages.</li>
</ol>
<h2>4. Code Implementation</h2>
<h3>4.1 Code Analysis</h3>
<p>The above method is valid for languages that support dynamic loading such as java, but not for golang. Since golang doesn&#39;t support dynamic loading, we can&#39;t use the generated source code. However, through technical analysis, we know that the conversion path from a text file to a binary message is: file.proto --&gt; FileDescriptor --&gt; proto.message, and there are two key points: </p>
<ol>
<li>how to get FileDescriptor from a proto file at runtime?</li>
<li>how to create a proto.message using FileDescriptor?</li>
</ol>
<p>For the second point, the solution is not complicated. We can use dynamicpb.message mentioned before. The following code demonstrates how to create a proto.message by dynamicpb.message:</p>
<pre><code class="language-golang">func NeweMessages(fd protoreflect.FileDescriptor, msgName string)proto.Message{
  fm := fd.Messages()
  md = fm.ByName(protoreflect.Name(msgName))
  return dynamicpb.NewMessage(md)
}
</code></pre>
<p>Now let&#39;s look at the second question, how to get FileFescriptor from a proto file at runtime? To get the FileFescriptor dynamically in golang, we first need to know how the FileFescriptor is generated. Typically, we can not get FileFescriptor directly from proto file. But we analyzed the source code in google.golang.org/protobuf , and found that FileDescriptor is created from FileDescriptorProto as follows:</p>
<pre><code class="language-golang">fdp := new(descriptorpb.FileDescriptorProto)
//Unmarshal([]byte,fdp)
fd, err := protodesc.NewFile(fdp, nil)
</code></pre>
<p>Thus, the conversion path from a text file to a binary message is changed to:  file.proto --&gt; FileDescriptorProto --&gt; FileDescriptor --&gt; proto.message,So the final question is, how do we get the FileDescriptorProto object?<br>FileDescriptorProto is a proto.message object, so it can be serialized to binary and deserialized from binary. In fact, we can use protoc to generate the binary data of FileDescriptorProto at the same time as the source code with option: --descriptor_set_out=your_file_descriptord_proto.pb.<br>Further analyzing the source code of protoc , there are plugins that receive the compiled binary streams from proto file for data processing, including code generation. </p>
<p>The following is the source code analysis, and the highlighted part is the key code:</p>
<pre><code class="language-golang">func main() {
  //...
  protogen.Options{
     ParamFunc: flags.Set,
  }.Run(func(gen *protogen.Plugin) error {
     //...
     for _, f := range gen.Files {
        if f.Generate {
           gengo.GenerateFile(gen, f)
        }
     }
     //...
     return nil
  })
}
//It reads a [pluginpb.CodeGeneratorRequest] message from [os.Stdin], invokes the plugin function, and writes a [pluginpb.CodeGeneratorResponse] message to [os.Stdout].
func (opts Options) Run(f func(*Plugin) error) {
    if err := run(opts, f); err != nil {
       fmt.Fprintf(os.Stderr, &quot;%s: %v\n&quot;, filepath.Base(os.Args[0]), err)
       os.Exit(1)
    }
}

func run(opts Options, f func(*Plugin) error) error {
    if len(os.Args) &gt; 1 {
       return fmt.Errorf(&quot;unknown argument %q (this program should be run by protoc, not directly)&quot;, os.Args[1])
    }
    //Here is the compiled binary stream from protoc
    in, err := io.ReadAll(os.Stdin)
    if err != nil {
       return err
    }
    
    req := &amp;pluginpb.CodeGeneratorRequest{}
    if err := proto.Unmarshal(in, req); err != nil {
       return err
    }
    gen, err := opts.New(req)
    if err != nil {
       return err
    }
    //This is the plugin&#39;s custom processing logic
    if err := f(gen); err != nil {
       gen.Error(err)
    }
    resp := gen.Response()
    out, err := proto.Marshal(resp)
    if err != nil {
       return err
    }
    //Write the source code to a file
    if _, err := os.Stdout.Write(out); err != nil {
       return err
    }
    return nil
}

//CodeGeneratorRequest defines a FileDescriptorProto field
type CodeGeneratorRequest struct {
    state         protoimpl.MessageState
    sizeCache     protoimpl.SizeCache
    unknownFields protoimpl.UnknownFields

    FileToGenerate []string 
    Parameter *string 
    ProtoFile []*desriptorpb.FileDescriptorProto
    SourceFileDescriptors []*descriptorpb.FileDescriptorProto 
    CompilerVersion *Version
}
</code></pre>
<p>By analyzing the source code of protoc, we can also implement a custom plugin to output the text-format serialized data of FileDescriptorProto.<br>Actually, there are two text-format serialization schemes for FileDescriptorProto: json/proto-text.</p>
<ul>
<li>JSON</li>
</ul>
<p>For json format, we can serialize/deserialize FileDescriptorProto via protojson.Marshal/Unmarshal.<br>Example of json-format:</p>
<pre><code class="language-json">{
  &quot;name&quot;:  &quot;protobuf/tns_demo.proto&quot;,
  &quot;package&quot;:  &quot;tns.search.proto&quot;,
  &quot;messageType&quot;:  [
    {
      &quot;name&quot;:  &quot;TnsDemo&quot;,
      &quot;field&quot;:  [
        {
          &quot;name&quot;:  &quot;id&quot;,
          &quot;number&quot;:  1,
          &quot;label&quot;:  &quot;LABEL_OPTIONAL&quot;,
          &quot;type&quot;:  &quot;TYPE_INT64&quot;,
          &quot;jsonName&quot;:  &quot;id&quot;
        },
        {
          &quot;name&quot;:  &quot;status&quot;,
          &quot;number&quot;:  2,
          &quot;label&quot;:  &quot;LABEL_OPTIONAL&quot;,
          &quot;type&quot;:  &quot;TYPE_INT32&quot;,
          &quot;jsonName&quot;:  &quot;status&quot;
        },
        {
          &quot;name&quot;:  &quot;result&quot;,
          &quot;number&quot;:  3,
          &quot;label&quot;:  &quot;LABEL_REPEATED&quot;,
          &quot;type&quot;:  &quot;TYPE_MESSAGE&quot;,
          &quot;typeName&quot;:  &quot;.tns.search.proto.TnsDemo.ResultEntry&quot;,
          &quot;jsonName&quot;:  &quot;result&quot;
        },
        {
          &quot;name&quot;:  &quot;reasons&quot;,
          &quot;number&quot;:  4,
          &quot;label&quot;:  &quot;LABEL_REPEATED&quot;,
          &quot;type&quot;:  &quot;TYPE_INT32&quot;,
          &quot;jsonName&quot;:  &quot;reasons&quot;
        }
      ],
      &quot;nestedType&quot;:  [
        {
          &quot;name&quot;:  &quot;ResultEntry&quot;,
          &quot;field&quot;:  [
            {
              &quot;name&quot;:  &quot;key&quot;,
              &quot;number&quot;:  1,
              &quot;label&quot;:  &quot;LABEL_OPTIONAL&quot;,
              &quot;type&quot;:  &quot;TYPE_STRING&quot;,
              &quot;jsonName&quot;:  &quot;key&quot;
            },
            {
              &quot;name&quot;:  &quot;value&quot;,
              &quot;number&quot;:  2,
              &quot;label&quot;:  &quot;LABEL_OPTIONAL&quot;,
              &quot;type&quot;:  &quot;TYPE_STRING&quot;,
              &quot;jsonName&quot;:  &quot;value&quot;
            }
          ],
          &quot;options&quot;:  {
            &quot;mapEntry&quot;:  true
          }
        }
      ]
    }
  ],
  &quot;options&quot;:  {
    &quot;goPackage&quot;:  &quot;./gen;protobuf&quot;
  },
  &quot;syntax&quot;:  &quot;proto3&quot;
}
</code></pre>
<ul>
<li>Proto-Text</li>
</ul>
<p>The Text Format  is a human-readable format used for serializing and displaying protobuf messages in text form. It is often used for debugging or configuration purposes when you want to quickly inspect the contents of a protobuf message.<br>In the text format, protobuf messages are represented in a straightforward key-value style, where each field in the message is written in a human-readable way, with the field name followed by the value. This format is defined in the Protocol Buffers specification.<br>For text-format, we can serialize/deserialize FileDescriptorProto via prototext.Marshal/Unmarshal<br>Example of text-format:</p>
<pre><code class="language-protobuf">name:  &quot;protobuf/tns_demo.proto&quot;
package:  &quot;tns.search.proto&quot;
message_type:  {
  name:  &quot;TnsDemo&quot;
  field:  {
    name:  &quot;id&quot;
    number:  1
    label:  LABEL_OPTIONAL
    type:  TYPE_INT64
    json_name:  &quot;id&quot;
  }
  field:  {
    name:  &quot;status&quot;
    number:  2
    label:  LABEL_OPTIONAL
    type:  TYPE_INT32
    json_name:  &quot;status&quot;
  }
  field:  {
    name:  &quot;result&quot;
    number:  3
    label:  LABEL_REPEATED
    type:  TYPE_MESSAGE
    type_name:  &quot;.tns.search.proto.TnsDemo.ResultEntry&quot;
    json_name:  &quot;result&quot;
  }
  field:  {
    name:  &quot;reasons&quot;
    number:  4
    label:  LABEL_REPEATED
    type:  TYPE_INT32
    json_name:  &quot;reasons&quot;
  }
  nested_type:  {
    name:  &quot;ResultEntry&quot;
    field:  {
      name:  &quot;key&quot;
      number:  1
      label:  LABEL_OPTIONAL
      type:  TYPE_STRING
      json_name:  &quot;key&quot;
    }
    field:  {
      name:  &quot;value&quot;
      number:  2
      label:  LABEL_OPTIONAL
      type:  TYPE_STRING
      json_name:  &quot;value&quot;
    }
    options:  {
      map_entry:  true
    }
  }
}
options:  {
  go_package:  &quot;./gen;protobuf&quot;
}
syntax:  &quot;proto3&quot;
</code></pre>
<h3>4.2 protoc-plugin</h3>
<p>In order to keep the generality, we choose the json-format as the serialization solution. </p>
<pre><code class="language-golang">func main() {
    protogen.Options{}.Run(func(gen *protogen.Plugin) error {
       gen.SupportedFeatures = SupportedFeatures
       for _, file := range gen.Files {
          // Skip files that are not part of the plugin&#39;s current output.
          if !file.Generate {
             continue
          }
          genJsonFile(file, gen)
          genExtFile(file, gen)
       }
       return nil
    })
}
func genJsonFile(file *protogen.File, gen *protogen.Plugin) {
    fd := file.Proto
    sci := fd.SourceCodeInfo
    fd.SourceCodeInfo = nil
    jsonFile := gen.NewGeneratedFile(file.GeneratedFilenamePrefix+&quot;.json&quot;, &quot;.&quot;)
    jsonFile.P(protojson.Format(fd))
    fd.SourceCodeInfo = sci
}
</code></pre>
<p>Once we get serialized content in json format, the rest is easy. We can save the json to tcc and load it later at runtime to marshal/unmarshal proto.Message. If the proto file changes, we can manually compile it offline and then update the new json data in tcc to realize the hot update. </p>
<h3>4.3 How to use dynamic schema</h3>
<ul>
<li>generate json schema with plugin</li>
</ul>
<pre><code class="language-shell">SRC_DIR=$(pwd)
go build -o $SRC_DIR/protoc-gen-ext
protoc --proto_path=$SRC_DIR \
--plugin=protoc-gen-go=$(which protoc-gen-go) \
--go_out=$SRC_DIR/protobuf \
--fastpb_out=$SRC_DIR/protobuf \
--plugin=protoc-gen-ext=$SRC_DIR/protoc-gen-ext \
--ext_out=$SRC_DIR/protobuf \
$SRC_DIR/protobuf/*.proto
</code></pre>
<ul>
<li>manipulate dynamic schema</li>
</ul>
<pre><code class="language-golang">    //jsonData:=getJsonFromTcc(...)
    fdp := new(descriptorpb.FileDescriptorProto)
    //load schema from json
    if err := protojson.Unmarshal([]byte(jsonData), fdp); err != nil {
       panic(err)
    }
    //create FileDescriptorfrom FileDescriptorProto
    fd, err := protodesc.NewFile(fdp, nil)
    if err != nil {
       panic(err)
    }
   // find the MessageDescriptor by name
    md = fd.Messages().ByName(protoreflect.Name(msg))
   // create dynamicpb message
    dynamicpb.NewMessage(md)
</code></pre>
<h2>5. Conlusion</h2>
<p>Dynamic compilation of Protobuf is a powerful technique for situations where the schema cannot be known ahead of time or needs to be handled at runtime. By using Protobuf’s Reflection API, Dynamic Messages, and Any fields, you can create, manipulate, and serialize Protobuf messages dynamically. This allows for greater flexibility in scenarios such as plugin-based architectures, evolving APIs, or systems that need to work with arbitrary message types at runtime. However, dynamic compilation is more complex than static compilation and may introduce performance overhead due to reflection and dynamic handling of schema data.</p>
17:T3dd1,<blockquote>
<p>在分布式一致性一文中主要介绍了分布式系统中存在的一致性问题。本文将简单介绍如何有效的解决分布式的一致性问题,其中包括什么是分布式事务，二阶段提交和三阶段提交。</p>
</blockquote>
<h3>分布式一致性回顾</h3>
<p>在分布式系统中，为了保证数据的高可用，通常，我们会将数据保留多个副本(replica)，这些副本会放置在不同的物理的机器上。为了对用户提供正确的增\删\改\差等语义，我们需要保证这些放置在不同物理机器上的副本是一致的。为了解决这种分布式一致性问题，前人在性能和数据一致性的反反复复权衡过程中总结了许多典型的协议和算法。其中比较著名的有二阶提交协议（Two Phase Commitment Protocol）、三阶提交协议（Three Phase Commitment Protocol）和Paxos算法。</p>
<h3>分布式事务</h3>
<blockquote>
<p>分布式事务是指会涉及到操作多个数据库的事务。其实就是将对同一库事务的概念扩大到了对多个库的事务。目的是为了保证分布式系统中的数据一致性。分布式事务处理的关键是必须有一种方法可以知道事务在任何地方所做的所有动作，提交或回滚事务的决定必须产生统一的结果（全部提交或全部回滚）</p>
</blockquote>
<p>在分布式系统中，各个节点之间在物理上相互独立，通过网络进行沟通和协调。由于存在事务机制，可以保证每个独立节点上的数据操作可以满足ACID。但是，相互独立的节点之间无法准确的知道其他节点中的事务执行情况。所以从理论上讲，两台机器理论上无法达到一致的状态。如果想让分布式部署的多台机器中的数据保持一致性，那么就要保证在所有节点的数据写操作，要不全部都执行，要么全部的都不执行。但是，一台机器在执行本地事务的时候无法知道其他机器中的本地事务的执行结果。所以他也就不知道本次事务到底应该commit还是 roolback。所以，常规的解决办法就是引入一个“协调者”的组件来统一调度所有分布式节点的执行。</p>
<h3>XA规范</h3>
<p>X/Open 组织（即现在的 Open Group ）定义了分布式事务处理模型。 X/Open DTP 模型（ 1994 ）包括应用程序（ AP ）、事务管理器（ TM ）、资源管理器（ RM ）、通信资源管理器（ CRM ）四部分。一般，常见的事务管理器（ TM ）是交易中间件，常见的资源管理器（ RM ）是数据库，常见的通信资源管理器（ CRM ）是消息中间件。    通常把一个数据库内部的事务处理，如对多个表的操作，作为本地事务看待。数据库的事务处理对象是本地事务，而分布式事务处理的对象是全局事务。   所谓全局事务，是指分布式事务处理环境中，多个数据库可能需要共同完成一个工作，这个工作即是一个全局事务，例如，一个事务中可能更新几个不同的数据库。对数据库的操作发生在系统的各处但必须全部被提交或回滚。此时一个数据库对自己内部所做操作的提交不仅依赖本身操作是否成功，还要依赖与全局事务相关的其它数据库的操作是否成功，如果任一数据库的任一操作失败，则参与此事务的所有数据库所做的所有操作都必须回滚。     一般情况下，某一数据库无法知道其它数据库在做什么，因此，在一个 DTP 环境中，交易中间件是必需的，由它通知和协调相关数据库的提交或回滚。而一个数据库只将其自己所做的操作（可恢复）影射到全局事务中。   </p>
<blockquote>
<p>XA 就是 X/Open DTP 定义的交易中间件与数据库之间的接口规范（即接口函数），交易中间件用它来通知数据库事务的开始、结束以及提交、回滚等。 XA 接口函数由数据库厂商提供。</p>
</blockquote>
<p>二阶提交协议和三阶提交协议就是根据这一思想衍生出来的。可以说二阶段提交其实就是实现XA分布式事务的关键(确切地说：两阶段提交主要保证了分布式事务的原子性：即所有结点要么全做要么全不做)</p>
<h3>2PC</h3>
<blockquote>
<p>二阶段提交(Two-phaseCommit)是指，在计算机网络以及数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法(Algorithm)。通常，二阶段提交也被称为是一种协议(Protocol))。在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为协调者的组件来统一掌控所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。因此，二阶段提交的算法思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。</p>
</blockquote>
<p>所谓的两个阶段是指：第一阶段：准备阶段(投票阶段)和第二阶段：提交阶段（执行阶段）。</p>
<h4>准备阶段</h4>
<p>事务协调者(事务管理器)给每个参与者(资源管理器)发送Prepare消息，每个参与者要么直接返回失败(如权限验证失败)，要么在本地执行事务，写本地的redo和undo日志，但不提交，到达一种“万事俱备，只欠东风”的状态。可以进一步将准备阶段分为以下三个步骤：</p>
<blockquote>
<p>1）协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。2）参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作）3）各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。</p>
</blockquote>
<h4>提交阶段</h4>
<p>如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚(Rollback)消息；否则，发送提交(Commit)消息；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。(注意:必须在最后阶段释放锁资源)接下来分两种情况分别讨论提交阶段的过程。当协调者节点从所有参与者节点获得的相应消息都为”同意”时:</p>
<p><img src="/images/blog/tech/middleware/img_20250723_01.png" alt=""></p>
<blockquote>
<p>1）协调者节点向所有参与者节点发出”正式提交(commit)”的请求。2）参与者节点正式完成操作，并释放在整个事务期间内占用的资源。3）参与者节点向协调者节点发送”完成”消息。4）协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。</p>
</blockquote>
<p>如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时：</p>
<p><img src="/images/blog/tech/middleware/img_20250723_02.png" alt=""></p>
<blockquote>
<p>1）协调者节点向所有参与者节点发出”回滚操作(rollback)”的请求。2）参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。3）参与者节点向协调者节点发送”回滚完成”消息。4）协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务。</p>
</blockquote>
<p>　　不管最后结果如何，第二阶段都会结束当前事务。二阶段提交看起来确实能够提供原子性的操作，但是不幸的事，二阶段提交还是有几个缺点的：</p>
<blockquote>
<p>1、同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。2、单点故障。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）3、数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。4、二阶段无法解决的问题：协调者在发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。</p>
</blockquote>
<p>由于二阶段提交存在着诸如同步阻塞、单点问题、脑裂等缺陷，所以，研究者们在二阶段提交的基础上做了改进，提出了三阶段提交。</p>
<h3>3PC</h3>
<blockquote>
<p>三阶段提交（Three-phase commit），也叫三阶段提交协议（Three-phase commit protocol），是二阶段提交（2PC）的改进版本。</p>
</blockquote>
<p><img src="/images/blog/tech/middleware/img_20250723_03.png" alt="图片"></p>
<p>与两阶段提交不同的是，三阶段提交有两个改动点。1、引入超时机制。同时在协调者和参与者中都引入超时机制。2、在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。</p>
<h4>CanCommit阶段</h4>
<p>3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。</p>
<blockquote>
<p>1.事务询问 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。2.响应反馈 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No</p>
</blockquote>
<h4>PreCommit阶段</h4>
<p>协调者根据参与者的反应情况来决定是否可以进行事务的PreCommit操作。根据响应情况，有以下两种可能。假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。</p>
<blockquote>
<p>1.发送预提交请求 协调者向参与者发送PreCommit请求，并进入Prepared阶段。2.事务预提交 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。3.响应反馈 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。</p>
</blockquote>
<p>假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。</p>
<blockquote>
<p>1.发送中断请求 协调者向所有参与者发送abort请求。2.中断事务 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。</p>
</blockquote>
<h4>doCommit阶段</h4>
<p>该阶段进行真正的事务提交，也可以分为以下两种情况。执行提交</p>
<blockquote>
<p>1.发送提交请求 协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。2.事务提交 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。3.响应反馈 事务提交完之后，向协调者发送Ack响应。4.完成事务 协调者接收到所有参与者的ack响应之后，完成事务。</p>
</blockquote>
<p>中断事务 协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。</p>
<blockquote>
<p>1.发送中断请求 协调者向所有参与者发送abort请求2.事务回滚 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。3.反馈结果 参与者完成事务回滚之后，向协调者发送ACK消息4.中断事务 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。</p>
</blockquote>
<p>在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。）</p>
<h3>2PC与3PC的区别</h3>
<p>相对于2PC，3PC主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。</p>
<hr>
<p>了解了2PC和3PC之后，我们可以发现，无论是二阶段提交还是三阶段提交都无法彻底解决分布式的一致性问题。Google Chubby的作者Mike Burrows说过， there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos. 意即<strong>世上只有一种一致性算法，那就是Paxos</strong>，所有其他一致性算法都是Paxos算法的不完整版。后面的文章会介绍这个公认为难于理解但是行之有效的Paxos算法。</p>
<h3>参考资料：</h3>
<p><a href="http://www.mamicode.com/info-detail-890945.html">分布式协议之两阶段提交协议（2PC）和改进三阶段提交协议（3PC）</a> <a href="http://blog.csdn.net/bluishglc/article/details/7612811">关于分布式事务、两阶段提交、一阶段提交、Best Efforts 1PC模式和事务补偿机制的研究</a> <a href="http://www.tuicool.com/articles/mARV3u">两阶段提交协议与三阶段提交协议</a></p>
18:T4fc0,<p>你有没有遇到过因为没有打印SQL导致问题排查困难？如果你使用了成熟ORM框架，那么很容易支撑SQL的拦截和监控，例如Mybatis的Interceptor或JOOQ的Listener都支持SQL执行过程的跟踪监控，但是，如果你的ORM框架不支持SQL监控，那么很不幸，你就只能在代码中手动打印日志了。然而，为了防SQL注入，应用中的SQL语句都是参数化的，直接打印的话，SQL语句未绑定参数，ORM框架一般都提供了SQL参数绑定的功能，原生的JDBC这样就失去了一定的监控价值。</p>
<p>另外，在TOB的业务中，有些场景SQL参数超长，如大IN查询，SQL语句会长达到几万甚至十几万，此时，我们又需要对SQL语句进行缩略打印。注意，这里的SQL缩略打印不是简单的对SQL语句进行截断，而是对SQL语句中的参数列表进行截断，例如下面的SQL</p>
<pre><code class="language-sql">select * from user 
where id in (1001,1001, 1002, 1003, 1004, 1005, 1006, 1007) 
and name in(sql
select name from whitelist 
where name in(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;,&#39;g&#39;,&#39;h&#39;,&#39;i&#39;,&#39;j&#39;,&#39;k&#39;,&#39;l&#39;,&#39;m&#39;)
)
</code></pre>
<p>缩略下印如下：</p>
<pre><code class="language-sql">select * from user 
where id in (1001,1001, 1002, 1003, 1004,...) 
and name in(
select name from whitelist 
where name in(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,...)
)
</code></pre>
<p>既然SQL 监控很重要，那么对于应用层的SQL监控都有哪些手段呢？一个SQL请求的执行链路，一般从DAO层开始：DAO -&gt; ORM -&gt; DataSource  -&gt; Connection -&gt; Driver -&gt; DB，那么在这个链路上有哪些环节可以切入监控呢？ DAO层是数据访问层的入口，而我们的目标是应用层监控，因此，能够实现SQL监控的环节只有：ORM -&gt; DataSource  -&gt; Connection -&gt; Driver，而要实现通用的非侵入式监控，则应该独立于ORM，因此我们可以从<strong>DataSource  -&gt; Connection -&gt; Driver</strong>三个环节进行入手：</p>
<h3><strong>一、SQL Profile监控</strong></h3>
<h4><strong>1、驱动层监控</strong></h4>
<p>如果Driver层支持日志监控，则最方便，例如MySQL，可以在jdbc url中添加logger：</p>
<pre><code class="language-properties">jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=UTC&amp;logger=Slf4JLogger&amp;profileSQL=true
</code></pre>
<p>基于Driver监控的问题在于：一方面强依赖于DB，和ORM层面临一样的问题，不具有通用性上述的问题，且需要厂商的支持，例如Oracle Driver就不支持日志监控；另一方面SQL格式固定，无法进行定制化输出。</p>
<h4><strong>2、连接层监控</strong></h4>
<p>如果厂商驱动不支持SQL日志，可以Driver进行代理实现SQL监控功能，常用的开源组件如<a href="https://p6spy.readthedocs.io/en/latest/">P6Spy</a>、<a href="https://github.com/arthurblake/log4jdbc">log4jdbc</a> 等，其原理都是代理了厂商的驱动，因此只需要修改jdbc url：</p>
<ul>
<li>pyspy</li>
</ul>
<pre><code class="language-properties">jdbc:p6spy:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=UTC
</code></pre>
<ul>
<li>log4jdbc</li>
</ul>
<pre><code class="language-properties">jdbc:log4jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=UTC
</code></pre>
<h4><strong>3、数据源层监控</strong></h4>
<p>可以通过对DataSource进行代理实现SQL监控</p>
<ul>
<li>P6Spy：</li>
</ul>
<pre><code class="language-java">@Bean
@Primary
public DataSource spyDataSource(@Autowired DataSource dataSource) {
  // wrap a datasource using P6SpyDataSource
  return new P6DataSource(dataSource);
}
</code></pre>
<ul>
<li>log4jdbc</li>
</ul>
<pre><code class="language-java">public DataSource spyDataSource(DataSource dataSource) {
    // wrap the provided dataSource
  return new DataSource() {
    @Override
    public Connection getConnection() throws SQLException {
      // wrap the connection with log4jdbc
      return new ConnectionSpy(dataSource.getConnection());
    }
      
    @Override
    public Connection getConnection(String username, String password) throws SQLException {
       // wrap the connection with log4jdbc
      return new ConnectionSpy(dataSource.getConnection(username, password));
     }
      //...
  };
}
</code></pre>
<p>上述三种方案都可以实现SQL监控，那么在实际应用场景中选择哪种方式更好呢？这和实际的生产方式有关。在我手，数据库是基于KDB的，Java应用是基于KsBoot，其中，数据库连接是在KDB平台配置的，底层的数据源是使用ShardingSphere+HikariDataSource进行魔改的。</p>
<p>第一种方案，由于数据库连接是由DBA维护的，升级需求修改数据库连接，因此不建议。</p>
<p>第二种方案，同理需要修改数据库连接，且比第一种更容易配错，因此也不建议。</p>
<p>排除上述两种方式，剩下的只有第三种方案了，但是第三种方案有很大的挑战，原因在于需要兼容快手kuaishou-framework奇葩的JdbcTemplate使用方式。确切地说，在于使用了DataSourceConfig。</p>
<pre><code class="language-java">public interface DataSourceConfig extends HasBizDef {

    /**
     * 数据源名称，必须与KDB申请时填写的一致
     */String bizName();

    /**
     * 获取当前可用区单库只读的JdbcTemplate
     */
    default NamedParameterJdbcTemplate read() {
        return InternalDatasourceConfig.readForceAz(this, currentAz(), currentPaz(), &quot;read&quot;);
    }   

    /**
     * 获取当前可用区单库读写的JdbcTemplate
     */
    default NamedParameterJdbcTemplate write() {
        return InternalDatasourceConfig.writeForceAz(this, currentAz(), currentPaz(), &quot;write&quot;);
    }	
  //....
}
</code></pre>
<p>DefaultDataSourceConfig是一个接口类，默认封装了NamedParameterJdbcTemplate的创建，业务方通过继承该接口来定义数据源:</p>
<pre><code class="language-kotlin">enum class AdDataSources(
    private val bizDef: BizDef,
    private val forTest: AdDataSources? = null,
    private val usingNewZk: Boolean = false
) : DataSourceConfig{
    adFansTopProfileDashboardTest,
    adFansTopProfileDashboard,
    adChargeTest,
    adCharge,
    adChargeReadOnly,
    adDspReadOnlyTest,
    adDspReadOnly;
    public open fun bizName(): String {
        return bizDef.bizName
    }
}
</code></pre>
<p>如果在业务中直接使用了DataSourceConfig创建的NamedParameterJdbcTemplate，那么我们就需要修改过程中创建的DataSource对象。那么，这里的DataSource究竟是怎么创建的呢？</p>
<p>具体扒代码的过程就不赘述了，直接说结果吧，kuaishou-framework的数据源最终是通过DataSourceFactory进行创建的，具体代码如下：</p>
<pre><code class="language-java">public static ListenableDataSource&lt;Failover&lt;Instance&gt;&gt; create(Instance i) {
   //...
   try {
       return supplyWithRetry(
        DATA_SOURCE_BUILD_RETRY,
        DATA_SOURCE_BUILD_RETRY_DELAY,
        () -&gt; new ListenableDataSource&lt;&gt;(
              bizName, 
              new HikariDataSource(config), ds -&gt; i.toString(), i),
              DataSourceFactory::needRetry);
                               
  } catch (Throwable e) {/**/}
}
</code></pre>
<p>由代码可以看到，这里的数据源实际上是通过new HikariDataSource(config)手动创建的，而DataSourceConfig又没有对外暴露创建的数据源，所以，我们该如何对DataSource代理呢?</p>
<h3><strong>二、动态修改加载类</strong></h3>
<p>成本最低的方式就是直接修改这段代码，将其中&#x7684;<em>&#x6E;ew HikariDataSource(config)</em>&#x4FEE;改&#x6210;<em>&#x6E;ew P6DataSource(new HikariDataSource(config))，</em>&#x90A3;么问题来了，这段代码属于基础组件包中的代码，基础架构组没有动力去修改，而我们又没有修改的权限，要想动这块代码，只能使用黑科技了。黑科技的手段有很多，那么问题又来了，哪种手段更合适呢？</p>
<p>首先我们来分析一下，有哪些手段可以修改Java字节码？</p>
<ul>
<li>方案一、编译时修改，需要开发maven插件</li>
</ul>
<p>（不使用maven插件的同学咋办？）</p>
<ul>
<li>方案二、加载时修改，重写类加载器</li>
</ul>
<p>需要在代码中指定特定的类加载器，用有一定的侵入式</p>
<ul>
<li>方案三、运行时修改，使用JavaAgent</li>
</ul>
<p>需要修改应用启动参数，运维成本有点高</p>
<p>首先要说明的是，这里不是对类方法进行增强，所以想使用cglib动态代理的想法是不可行的。前面三种方案都有一定的局限性：方案一比较麻烦，方案二侵入性强，方案三则需要使用JavaAgent技术，那有没有方案不使用Agent就可以动态修改已经加载的字节码呢？答案是没有，至少理论上没有。不过，好在天无绝人之路，JDK9之后，可以动态启动JavaAgent，这样就不用修改启动参数了。这里，我们选择使用byte-buddy进行字节码重写。</p>
<p><em>下面是对动态启动Java Agent技术的解释</em></p>
<blockquote>
<p>Note that starting with Java 9, there is the Launcher-Agent-Class manifest attribute for jar files that can specify the class of a Java Agent to start before the class specified with the Main-Class is launched. That way, you can easily have your Agent collaborating with your application code in your JVM, without the need for any additional command line options. The Agent can be as simple as having an agentmain method in your main class storing the Instrumentation reference in a static variable.</p>
</blockquote>
<blockquote>
<p>See <a href="https://docs.oracle.com/en/java/javase/15/docs/api/java.instrument/java/lang/instrument/package-summary.html#package.description">the java.lang.instrument package documentation</a>…</p>
</blockquote>
<blockquote>
<p>Getting hands on an Instrumentation instance when the JVM has not been started with Agents is trickier. It must support launching Agents after startup in general, e.g. via the Attach API. <a href="https://stackoverflow.com/a/19912148/2711488">This answer</a> demonstrates at its end such a self-attach to get hands on the Instrumentation. When you have the necessary manifest attribute in your application jar file, you could even use that as agent jar and omit the creation of a temporary stub file.</p>
</blockquote>
<blockquote>
<p>However, recent JVMs forbid self-attaching unless -Djdk.attach.allowAttachSelf=true has been specified at startup, but I suppose, taking additional steps at startup time, is precisely what you don’t want to do. One way to circumvent this, is to use another process. All this process has to to, is to attach to your original process and tell the JVM to start the Agent. Then, it may already terminate and everything else works the same way as before the introduction of this restriction.</p>
</blockquote>
<blockquote>
<p>As mentioned in <a href="https://stackoverflow.com/questions/56787777/?noredirect=1&lq=1#comment100160373_56787777">this comment</a>, Byte-Buddy has already implemented those necessary steps and the stripped-down Byte-Buddy-Agent contains that logic only, so you can use it to build your own logic atop it.</p>
</blockquote>
<ul>
<li>字节码工具对比</li>
</ul>
<p><img src="https://static.yximgs.com/udata/pkg/EE-KSTACK/4223630ea14c6367968188fd52cafa26.png" alt="图片"></p>
<ul>
<li>使用bytebuddy修改字节码</li>
</ul>
<p>在实现代码之前，我们回过头来再看一下快手的数据源生成：</p>
<pre><code class="language-java">new ListenableDataSource&lt;&gt;(bizName, new HikariDataSource(config), ds -&gt; i.toString());
</code></pre>
<p>这里实际生成的数据源类型是ListenableDataSource，而ListenableDataSource刚好继承了DelegatingDataSource类，而DelegatingDataSource的构造方法如下：</p>
<pre><code class="language-java">public class DelegatingDataSource implements DataSource {
   //...
  public DelegatingDataSource(DataSource targetDataSource) {
    this.setTargetDataSource(targetDataSource);
   }

  public void setTargetDataSource(@Nullable DataSource targetDataSource) {
      this.targetDataSource = targetDataSource;
  }
  //...
}
</code></pre>
<p>因此，我们可以通过改写DelegatingDataSource#setTargetDataSource方法，实现同样的效果，修改后的方法应该如下：</p>
<pre><code class="language-java">public void setTargetDataSource(@Nullable DataSource targetDataSource) {
        this.targetDataSource = new P6DataSource(targetDataSource;
}
</code></pre>
<p>那么具体如何修改字节码呢？这里是<a href="https://bytebuddy.net/#/tutorial">官方文档</a>，原理我们不做赘述，直接介绍实现了。实现方式有三种：</p>
<h4><strong>1、类文件替换</strong></h4>
<p>假设你已经通过Java代码编译了新的类，现在要替换JVM中类的定义，代码如下：</p>
<pre><code class="language-java">new ByteBuddy()
  .redefine(NewDelegatingDataSource.class)
  .name(DelegatingDataSource.class.getName())
  .make()
  .load(Thread.currentThread().getContextClassLoader(), 
        ClassReloadingStrategy.fromInstalledAgent());
</code></pre>
<h4><strong>2、操作字节码：</strong></h4>
<pre><code class="language-java">new ByteBuddy()
    .redefine(DelegatingDataSource.class)
    //重写DelegatingDataSource#setTargetDataSource方法
    .method(named(&quot;setTargetDataSource&quot;))
    .intercept(MyImplementation.INSTANCE)
    .make()
    .load(Thread.currentThread().getContextClassLoader(),
          ClassReloadingStrategy.fromInstalledAgent());

enum MyImplementation implements Implementation {

INSTANCE; // singleton

  @Override
  public InstrumentedType prepare(InstrumentedType instrumentedType) {
  return instrumentedType;
  }
  
  @Override
  public ByteCodeAppender appender(Target implementationTarget) {
  return MyAppender.INSTANCE;
  }
  
}
//字节码定义
enum MyAppender implements ByteCodeAppender {

INSTANCE; // singleton

@Override
public Size apply(MethodVisitor methodVisitor,
        Implementation.Context implementationContext,
        MethodDescription instrumentedMethod) {
  Label label0 = new Label();
  methodVisitor.visitLabel(label0);
  methodVisitor.visitLineNumber(70, label0);
  methodVisitor.visitVarInsn(ALOAD, 0);
  methodVisitor.visitTypeInsn(NEW, &quot;com/p6spy/engine/spy/P6DataSource&quot;);
  methodVisitor.visitInsn(DUP);
  methodVisitor.visitVarInsn(ALOAD, 1);
  methodVisitor.visitMethodInsn(INVOKESPECIAL, &quot;com/p6spy/engine/spy/P6DataSource&quot;, &quot;&lt;init&gt;&quot;, &quot;(Ljavax/sql/DataSource;)V&quot;, false);
  methodVisitor.visitFieldInsn(PUTFIELD, &quot;org/springframework/jdbc/datasource/DelegatingDataSource&quot;, &quot;targetDataSource&quot;, &quot;Ljavax/sql/DataSource;&quot;);
  Label label1 = new Label();
  methodVisitor.visitLabel(label1);
  methodVisitor.visitLineNumber(71, label1);
  methodVisitor.visitInsn(RETURN);
  Label label2 = new Label();
  methodVisitor.visitLabel(label2);
  methodVisitor.visitLocalVariable(&quot;this&quot;, &quot;Lorg/springframework/jdbc/datasource/DelegatingDataSource;&quot;, null, label0, label2, 0);
  methodVisitor.visitLocalVariable(&quot;targetDataSource&quot;, &quot;Ljavax/sql/DataSource;&quot;, null, label0, label2, 1);
  methodVisitor.visitMaxs(4, 2);
  return new Size(4, 2);
  }
}
</code></pre>
<p>上述代码的核心思想是字节操作字节码，操作字节码是非常复杂和繁重的事情，且无法debug，那么有没有比较方便的方式呢？</p>
<p>我们可以手动改写Java代码，然后利用插件生成对应的字节码，然后在其基础上进行修改，研发成本会低很多。这里推荐IDEA的一个插件：Byte-Code-Analyzer，使用该插件可以查看类对应的ASM字节码:</p>
<p><img src="https://static.yximgs.com/udata/pkg/EE-KSTACK/e31962a90f6598880e78d8254d6c74d9" alt="图片"></p>
<h4><strong>3、利用byte-buddy的Advice</strong></h4>
<pre><code class="language-java"> public static void redefine() {
   new ByteBuddy()
     .redefine(DelegatingDataSource.class)
     .visit(Advice.to(Decorator.class)
            .on(ElementMatchers.named(&quot;setTargetDataSource&quot;)))
     .make()
     .load(Thread.currentThread().getContextClassLoader(),
           ClassReloadingStrategy.fromInstalledAgent()).getLoaded();
 }

static class Decorator {

  //在方法开始插入代码
  @Advice.OnMethodEnter
    public static void enter(@Advice.Argument(value = 0, readOnly = false) DataSource dataSource) {
    dataSource = new P6DataSource(dataSource);
  }
}
</code></pre>
<p>byte-buddy的Advisor和动态代理的原理不一样，他是直接修改方法体的字节码，上面的方法就是表示在方法开始插入一行，其效果如下：</p>
<pre><code class="language-java">public void setTargetDataSource(@Nullable DataSource targetDataSource) {
  //插入的代码
  targetDataSource = new P6DataSource(targetDataSource);
  this.targetDataSource = targetDataSource;
}
</code></pre>
<p>注：</p>
<ol>
<li>动态修改已加载的类，是有限制条件的，不能添加方法或者字段，因此通过byte-buddy的Methoddelegation方法修改字节码是不可行的。</li>
<li>使用byte-buddy的Advice，可以对非Spring托管的类进行动态增强，因为是直接修改字节码，性能更好。</li>
</ol>
<h3><strong>三、自动生效</strong></h3>
<p>前面我们讲了如何修改字节码，以提供SQL监控功能，那么如何让SQL监控自动生效呢？我们的目标是非侵入式解决方案：既不能修改业务代码，也不能更改系统配置。鉴于Java世界的事实标准，我们利用了SpringBoot-Starter功能，只需增加一个maven依赖，就自动提供了SQL监控能力。</p>
<pre><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;com.kuaishou.ad&lt;/groupId&gt;
  &lt;artifactId&gt;sqllog-spring-boot-starter&lt;/artifactId&gt;
  &lt;version&gt;制品库查询最新版&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>至于SpringBoot-Starter的实现原理，网上资料很多，核心思想就是提供默认配置，开箱即用。需要注意的是，Spring6.0自动配置的方案有了调整，原来基于spring.factories的配置改成了org.springframework.boot.autoconfigure.AutoConfiguration.imports，原有的方式还支持，这对应普通应用没有影响，但是在实现Spring多容器隔离的方案上有一定的影响，后面有时间会展开讲一下。</p>
<pre><code class="language-java">private static String[] getConfigurations(File file) {
  @EnableAutoConfiguration
  class NoScan {
    //用于扫描META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports,该类定义在方法中,是为了避免扫描当前类时被加载
  }
  FileClassLoader classLoader = new FileClassLoader(file);
  AutoConfigurationImportSelector selector = new AutoConfigurationImportSelector();
  selector.setBeanClassLoader(classLoader);
  selector.setResourceLoader(new ClassLoaderResourcePatternResolver(classLoader));
  selector.setEnvironment(new StandardEnvironment());
  String[] configurations = selector.selectImports(new StandardAnnotationMetadata(NoScan.class));
  return configurations;
}
</code></pre>
<h3><strong>四、SQL打印效果</strong></h3>
<p>sqllog-spring-boot-starter默认基于p6spy，并对SQL输出提供了扩展，打印SQL日志如下：</p>
<p><img src="https://static.yximgs.com/udata/pkg/EE-KSTACK/28cd44d1451c960cfb982773aab6ec44" alt=""></p>
<p>SQL的打印内容分为三部分：</p>
<p>第一行，显示执行时间、耗时、SQL操作、数据库连接等信息</p>
<p>第二行，显示参数化SQL</p>
<p>第三行，显示绑定参数后的实际执行的SQL</p>
<p>通过日志看到，当SQL语句超长时，系统会对参数化SQL进行个性化缩略，而对实际执行的SQL，则保持原样输出，这样可以检索关键信息。</p>
6:["$","article",null,{"className":"min-h-screen","children":["$","div",null,{"className":"mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12","children":[["$","header",null,{"className":"mb-8","children":[["$","div",null,{"className":"flex items-center mb-6","children":["$","div",null,{"className":"inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal","children":[["$","svg",null,{"className":"w-4 h-4 mr-2 text-gray-400","fill":"none","stroke":"currentColor","viewBox":"0 0 24 24","children":["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","strokeWidth":2,"d":"M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z"}]}],["$","time",null,{"dateTime":"2025-07-29","children":"2025年07月29日"}]]}]}],["$","h1",null,{"className":"text-4xl font-bold text-gray-900 mb-6 text-center","children":"How to implement dynamic schema based on Protobuf in Golang?"}],["$","div",null,{"className":"flex flex-wrap gap-2 mb-6 justify-center","children":[["$","$L5","技术实战",{"href":"/blog/tag/%E6%8A%80%E6%9C%AF%E5%AE%9E%E6%88%98/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"技术实战"}],["$","$L5","Protobuf",{"href":"/blog/tag/Protobuf/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"Protobuf"}],["$","$L5","Dynamic",{"href":"/blog/tag/Dynamic/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"Dynamic"}]]}]]}],["$","div",null,{"className":"max-w-5xl mx-auto","children":["$","$L14",null,{"content":"$15"}]}],["$","$11",null,{"fallback":["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":"加载导航中..."}],"children":["$","$L16",null,{"globalNav":{"prev":{"slug":"tech/middleware/分布式事务之两阶段提交和三阶提交","title":"分布式事务之两阶段提交和三阶提交","description":"在分布式一致性一文中主要介绍了分布式系统中存在的一致性问题。本文将简单介绍如何有效的解决分布式的一致性问题,其中包括什么是分布式事务，二阶段提交和三阶段提交。","pubDate":"2025-07-23","tags":["分布式事务","技术专题"],"heroImage":"$undefined","content":"$17"},"next":null},"tagNav":{"技术实战":{"prev":{"slug":"tech/practice/非侵入式SQL监控","title":"非侵入式SQL监控","description":"你有没有因为应用程序没有打印SQL而导致问题排查困难？有没有因为SQL没有显示参数而导致日志毫无意义？有没有因为SQL超长而导致查看痛苦？有没有因为缺少SQL性能监控而导致无法报警？...","pubDate":"2024-04-07","tags":["技术实战"],"heroImage":"$undefined","content":"$18"},"next":null},"Protobuf":{"prev":null,"next":null},"Dynamic":{"prev":null,"next":null}}}]}],["$","$L19",null,{}]]}]}]}]
9:null
d:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
8:null
b:{"metadata":[["$","title","0",{"children":"How to implement dynamic schema based on Protobuf in Golang? - Skyfalling Blog"}],["$","meta","1",{"name":"description","content":"Protocol Buffers (Protobuf) is a language-neutral, platform-neutral, extensible mechanism for serializing structured data. It was developed by Google to efficiently serialize data for use in a variety of applications, including network communication, data storage, and inter-process communication (IPC).  Protobuf messages are smaller and more efficient than text-based formats like JSON and XML, and provides fast serialization and deserialization, which is crucial for high-performance systems."}],["$","meta","2",{"property":"og:title","content":"How to implement dynamic schema based on Protobuf in Golang?"}],["$","meta","3",{"property":"og:description","content":"Protocol Buffers (Protobuf) is a language-neutral, platform-neutral, extensible mechanism for serializing structured data. It was developed by Google to efficiently serialize data for use in a variety of applications, including network communication, data storage, and inter-process communication (IPC).  Protobuf messages are smaller and more efficient than text-based formats like JSON and XML, and provides fast serialization and deserialization, which is crucial for high-performance systems."}],["$","meta","4",{"property":"og:type","content":"article"}],["$","meta","5",{"property":"article:published_time","content":"2025-07-29"}],["$","meta","6",{"property":"article:author","content":"Skyfalling"}],["$","meta","7",{"name":"twitter:card","content":"summary"}],["$","meta","8",{"name":"twitter:title","content":"How to implement dynamic schema based on Protobuf in Golang?"}],["$","meta","9",{"name":"twitter:description","content":"Protocol Buffers (Protobuf) is a language-neutral, platform-neutral, extensible mechanism for serializing structured data. It was developed by Google to efficiently serialize data for use in a variety of applications, including network communication, data storage, and inter-process communication (IPC).  Protobuf messages are smaller and more efficient than text-based formats like JSON and XML, and provides fast serialization and deserialization, which is crucial for high-performance systems."}],["$","link","10",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","link","11",{"rel":"icon","href":"/favicon.svg"}]],"error":null,"digest":"$undefined"}
13:{"metadata":"$b:metadata","error":null,"digest":"$undefined"}
