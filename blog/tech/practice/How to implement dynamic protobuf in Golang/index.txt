1:"$Sreact.fragment"
2:I[10616,["874","static/chunks/874-f2e46e41114bd221.js","177","static/chunks/app/layout-c432974c723daafe.js"],"default"]
3:I[87555,[],""]
4:I[31295,[],""]
5:I[6874,["874","static/chunks/874-f2e46e41114bd221.js","968","static/chunks/968-bf93abe4de13a5fc.js","909","static/chunks/app/blog/%5B...slug%5D/page-26cc6d1a0064a78b.js"],""]
7:I[59665,[],"OutletBoundary"]
a:I[74911,[],"AsyncMetadataOutlet"]
c:I[59665,[],"ViewportBoundary"]
e:I[59665,[],"MetadataBoundary"]
10:I[26614,[],""]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/0a7d53676a1eb136.css","style"]
0:{"P":null,"b":"IFjusvZ1EBHzNsdVqsZlF","p":"","c":["","blog","tech","practice","How%20to%20implement%20dynamic%20protobuf%20in%20Golang",""],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","tech/practice/How%20to%20implement%20dynamic%20protobuf%20in%20Golang","c"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/0a7d53676a1eb136.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"zh-CN","children":["$","body",null,{"className":"__className_f367f3","children":["$","div",null,{"className":"min-h-screen flex flex-col","children":[["$","$L2",null,{}],["$","main",null,{"className":"flex-1","children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"bg-[var(--background)]","children":["$","div",null,{"className":"mx-auto max-w-7xl px-6 py-12 md:flex md:items-center md:justify-between lg:px-8","children":[["$","div",null,{"className":"flex justify-center space-x-6 md:order-2","children":[["$","$L5",null,{"href":"/about","className":"text-gray-600 hover:text-gray-800","children":"关于"}],["$","$L5",null,{"href":"/blog","className":"text-gray-600 hover:text-gray-800","children":"博客"}],["$","$L5",null,{"href":"/contact","className":"text-gray-600 hover:text-gray-800","children":"联系"}]]}],["$","div",null,{"className":"mt-8 md:order-1 md:mt-0","children":["$","p",null,{"className":"text-center text-xs leading-5 text-gray-600","children":"© 2024 Skyfalling Blog. All rights reserved."}]}]]}]}]]}]}]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","tech/practice/How%20to%20implement%20dynamic%20protobuf%20in%20Golang","c"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L6",null,["$","$L7",null,{"children":["$L8","$L9",["$","$La",null,{"promise":"$@b"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","TxUA0HeUGatagI6CJq0ikv",{"children":[["$","$Lc",null,{"children":"$Ld"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$Le",null,{"children":"$Lf"}]]}],false]],"m":"$undefined","G":["$10","$undefined"],"s":false,"S":true}
11:"$Sreact.suspense"
12:I[74911,[],"AsyncMetadata"]
14:I[32923,["874","static/chunks/874-f2e46e41114bd221.js","968","static/chunks/968-bf93abe4de13a5fc.js","909","static/chunks/app/blog/%5B...slug%5D/page-26cc6d1a0064a78b.js"],"default"]
16:I[40780,["874","static/chunks/874-f2e46e41114bd221.js","968","static/chunks/968-bf93abe4de13a5fc.js","909","static/chunks/app/blog/%5B...slug%5D/page-26cc6d1a0064a78b.js"],"default"]
1a:I[85300,["874","static/chunks/874-f2e46e41114bd221.js","968","static/chunks/968-bf93abe4de13a5fc.js","909","static/chunks/app/blog/%5B...slug%5D/page-26cc6d1a0064a78b.js"],"default"]
f:["$","div",null,{"hidden":true,"children":["$","$11",null,{"fallback":null,"children":["$","$L12",null,{"promise":"$@13"}]}]}]
15:T4029,<h2>1. Background</h2>
<p>Protocol Buffers (Protobuf) is a language-neutral, platform-neutral, extensible mechanism for serializing structured data. It was developed by Google to efficiently serialize data for use in a variety of applications, including network communication, data storage, and inter-process communication (IPC).  Protobuf messages are smaller and more efficient than text-based formats like JSON and XML ,and provides fast serialization and deserialization, which is crucial for high-performance systems. </p>
<h2>2. Protobuf compiler</h2>
<p>Protobuf allows you to define the structure of your data (messages) in a .proto file, and then use the Protobuf compiler (protoc) to generate source code in selected programming language that can serialize and deserialize data in the Protobuf format.  Protobuf adds overhead to the development process compared to formats like JSON, where you can directly parse or serialize data without needing code generation. Additionally, any changes to the schema may require re-compiling the code to handle new or modified message types.</p>
<h2>3. Dynamic compilation(Dynamic Schema)</h2>
<p>Typically, Protobuf schemas are compiled using the protoc compiler ahead of time, which generates source code in various programming languages (such as C++, Java, Python, etc.) for serialization and deserialization. However, in some scenarios, you might need to work with Protobuf messages dynamically without relying on pre-generated codes. </p>
<p>This is especially useful when:</p>
<ul>
<li>You want to work with Protobuf messages dynamically at runtime, where the schema is not known in advance.</li>
<li>You need to handle multiple or evolving Protobuf schemas at runtime without recompiling your code.</li>
<li>You want to dynamically serialize or deserialize Protobuf messages in a generic way, perhaps for applications like plugins, dynamic API handling, or protocol-based communication where the schema might change often.</li>
</ul>
<h3>Key Concepts for Dynamic Compilation in Protobuf</h3>
<ul>
<li>Dynamic Message (Dynamic Parsing)</li>
<li>Reflection API in Protobuf</li>
<li>Dynamic Code Generation</li>
</ul>
<h3>3.1 Dynamic Message (Dynamic Parsing)</h3>
<p>In Protobuf, a Dynamic Message is an object where you can manipulate the message’s fields dynamically, without needing a pre-generated class for the specific message type. This is enabled through Protobuf&#39;s Reflection API.<br>You can use dynamic messages when:</p>
<ul>
<li>The Protobuf schema is available at runtime, but you don&#39;t know the message types ahead of time.</li>
<li>You want to work with messages whose types are determined dynamically (e.g., reading from a file or network stream that specifies the message type).<br>To use dynamic messages, you typically:</li>
<li>Load the schema definition (e.g., .proto file) at runtime.</li>
<li>Use the Protobuf Reflection API to create message types and set/get fields.</li>
</ul>
<h3>3.2 Reflection API</h3>
<p>The Protobuf Reflection API allows you to inspect the structure of Protobuf messages at runtime, and dynamically access their fields. This is the core tool for implementing dynamic compilation because it lets you:</p>
<ul>
<li>Discover available fields.</li>
<li>Inspect field types (e.g., int32, string, nested messages).</li>
<li>Set and get field values dynamically.<br>The key components of the Reflection API in Protobuf are:</li>
<li>Descriptors: These are metadata objects that describe the fields, types, and structure of messages.</li>
<li>Dynamic Messages: These are messages created dynamically using descriptors, where you can set/get fields without needing a pre-compiled class.</li>
</ul>
<h3>3.3 Dynamic Code Generation with Protobuf Compiler</h3>
<p>In some use cases, you may need to generate Protobuf code dynamically based on new or unknown schemas. This would involve invoking the Protobuf compiler (protoc) programmatically to generate the code at runtime, either directly from .proto files or from a descriptor or a set of schema definitions.<br>This process typically involves the following steps:</p>
<ol>
<li>Obtain the .proto files or schema descriptors.</li>
<li>Run protoc programmatically to generate source code.</li>
<li>Use the generated code within the application to work with dynamic messages.</li>
</ol>
<h2>4. Code Implementation</h2>
<h3>4.1 Code Analysis</h3>
<p>The above method is valid for languages that support dynamic loading such as java, but not for golang. Since golang doesn&#39;t support dynamic loading, we can&#39;t use the generated source code. However, through technical analysis, we know that the conversion path from a text file to a binary message is: file.proto --&gt; FileDescriptor --&gt; proto.message, and there are two key points: </p>
<ol>
<li>how to get FileDescriptor from a proto file at runtime?</li>
<li>how to create a proto.message using FileDescriptor?</li>
</ol>
<p>For the second point, the solution is not complicated. We can use dynamicpb.message mentioned before. The following code demonstrates how to create a proto.message by dynamicpb.message:</p>
<pre><code class="language-golang">func NeweMessages(fd protoreflect.FileDescriptor, msgName string)proto.Message{
  fm := fd.Messages()
  md = fm.ByName(protoreflect.Name(msgName))
  return dynamicpb.NewMessage(md)
}
</code></pre>
<p>Now let&#39;s look at the second question, how to get FileFescriptor from a proto file at runtime? To get the FileFescriptor dynamically in golang, we first need to know how the FileFescriptor is generated. Typically, we can not get FileFescriptor directly from proto file. But we analyzed the source code in google.golang.org/protobuf , and found that FileDescriptor is created from FileDescriptorProto as follows:</p>
<pre><code class="language-golang">fdp := new(descriptorpb.FileDescriptorProto)
//Unmarshal([]byte,fdp)
fd, err := protodesc.NewFile(fdp, nil)
</code></pre>
<p>Thus, the conversion path from a text file to a binary message is changed to:  file.proto --&gt; FileDescriptorProto --&gt; FileDescriptor --&gt; proto.message,So the final question is, how do we get the FileDescriptorProto object?<br>FileDescriptorProto is a proto.message object, so it can be serialized to binary and deserialized from binary. In fact, we can use protoc to generate the binary data of FileDescriptorProto at the same time as the source code with option: --descriptor_set_out=your_file_descriptord_proto.pb.<br>Further analyzing the source code of protoc , there are plugins that receive the compiled binary streams from proto file for data processing, including code generation. </p>
<p>The following is the source code analysis, and the highlighted part is the key code:</p>
<pre><code class="language-golang">func main() {
  //...
  protogen.Options{
     ParamFunc: flags.Set,
  }.Run(func(gen *protogen.Plugin) error {
     //...
     for _, f := range gen.Files {
        if f.Generate {
           gengo.GenerateFile(gen, f)
        }
     }
     //...
     return nil
  })
}
//It reads a [pluginpb.CodeGeneratorRequest] message from [os.Stdin], invokes the plugin function, and writes a [pluginpb.CodeGeneratorResponse] message to [os.Stdout].
func (opts Options) Run(f func(*Plugin) error) {
    if err := run(opts, f); err != nil {
       fmt.Fprintf(os.Stderr, &quot;%s: %v\n&quot;, filepath.Base(os.Args[0]), err)
       os.Exit(1)
    }
}

func run(opts Options, f func(*Plugin) error) error {
    if len(os.Args) &gt; 1 {
       return fmt.Errorf(&quot;unknown argument %q (this program should be run by protoc, not directly)&quot;, os.Args[1])
    }
    //Here is the compiled binary stream from protoc
    in, err := io.ReadAll(os.Stdin)
    if err != nil {
       return err
    }
    
    req := &amp;pluginpb.CodeGeneratorRequest{}
    if err := proto.Unmarshal(in, req); err != nil {
       return err
    }
    gen, err := opts.New(req)
    if err != nil {
       return err
    }
    //This is the plugin&#39;s custom processing logic
    if err := f(gen); err != nil {
       gen.Error(err)
    }
    resp := gen.Response()
    out, err := proto.Marshal(resp)
    if err != nil {
       return err
    }
    //Write the source code to a file
    if _, err := os.Stdout.Write(out); err != nil {
       return err
    }
    return nil
}

//CodeGeneratorRequest defines a FileDescriptorProto field
type CodeGeneratorRequest struct {
    state         protoimpl.MessageState
    sizeCache     protoimpl.SizeCache
    unknownFields protoimpl.UnknownFields

    FileToGenerate []string 
    Parameter *string 
    ProtoFile []*desriptorpb.FileDescriptorProto
    SourceFileDescriptors []*descriptorpb.FileDescriptorProto 
    CompilerVersion *Version
}
</code></pre>
<p>By analyzing the source code of protoc, we can also implement a custom plugin to output the text-format serialized data of FileDescriptorProto.<br>Actually, there are two text-format serialization schemes for FileDescriptorProto: json/proto-text.</p>
<ul>
<li>JSON</li>
</ul>
<p>For json format, we can serialize/deserialize FileDescriptorProto via protojson.Marshal/Unmarshal.<br>Example of json-format:</p>
<pre><code class="language-json">{
  &quot;name&quot;:  &quot;protobuf/tns_demo.proto&quot;,
  &quot;package&quot;:  &quot;tns.search.proto&quot;,
  &quot;messageType&quot;:  [
    {
      &quot;name&quot;:  &quot;TnsDemo&quot;,
      &quot;field&quot;:  [
        {
          &quot;name&quot;:  &quot;id&quot;,
          &quot;number&quot;:  1,
          &quot;label&quot;:  &quot;LABEL_OPTIONAL&quot;,
          &quot;type&quot;:  &quot;TYPE_INT64&quot;,
          &quot;jsonName&quot;:  &quot;id&quot;
        },
        {
          &quot;name&quot;:  &quot;status&quot;,
          &quot;number&quot;:  2,
          &quot;label&quot;:  &quot;LABEL_OPTIONAL&quot;,
          &quot;type&quot;:  &quot;TYPE_INT32&quot;,
          &quot;jsonName&quot;:  &quot;status&quot;
        },
        {
          &quot;name&quot;:  &quot;result&quot;,
          &quot;number&quot;:  3,
          &quot;label&quot;:  &quot;LABEL_REPEATED&quot;,
          &quot;type&quot;:  &quot;TYPE_MESSAGE&quot;,
          &quot;typeName&quot;:  &quot;.tns.search.proto.TnsDemo.ResultEntry&quot;,
          &quot;jsonName&quot;:  &quot;result&quot;
        },
        {
          &quot;name&quot;:  &quot;reasons&quot;,
          &quot;number&quot;:  4,
          &quot;label&quot;:  &quot;LABEL_REPEATED&quot;,
          &quot;type&quot;:  &quot;TYPE_INT32&quot;,
          &quot;jsonName&quot;:  &quot;reasons&quot;
        }
      ],
      &quot;nestedType&quot;:  [
        {
          &quot;name&quot;:  &quot;ResultEntry&quot;,
          &quot;field&quot;:  [
            {
              &quot;name&quot;:  &quot;key&quot;,
              &quot;number&quot;:  1,
              &quot;label&quot;:  &quot;LABEL_OPTIONAL&quot;,
              &quot;type&quot;:  &quot;TYPE_STRING&quot;,
              &quot;jsonName&quot;:  &quot;key&quot;
            },
            {
              &quot;name&quot;:  &quot;value&quot;,
              &quot;number&quot;:  2,
              &quot;label&quot;:  &quot;LABEL_OPTIONAL&quot;,
              &quot;type&quot;:  &quot;TYPE_STRING&quot;,
              &quot;jsonName&quot;:  &quot;value&quot;
            }
          ],
          &quot;options&quot;:  {
            &quot;mapEntry&quot;:  true
          }
        }
      ]
    }
  ],
  &quot;options&quot;:  {
    &quot;goPackage&quot;:  &quot;./gen;protobuf&quot;
  },
  &quot;syntax&quot;:  &quot;proto3&quot;
}
</code></pre>
<ul>
<li>Proto-Text</li>
</ul>
<p>The Text Format  is a human-readable format used for serializing and displaying protobuf messages in text form. It is often used for debugging or configuration purposes when you want to quickly inspect the contents of a protobuf message.<br>In the text format, protobuf messages are represented in a straightforward key-value style, where each field in the message is written in a human-readable way, with the field name followed by the value. This format is defined in the Protocol Buffers specification.<br>For text-format, we can serialize/deserialize FileDescriptorProto via prototext.Marshal/Unmarshal<br>Example of text-format:</p>
<pre><code class="language-protobuf">name:  &quot;protobuf/tns_demo.proto&quot;
package:  &quot;tns.search.proto&quot;
message_type:  {
  name:  &quot;TnsDemo&quot;
  field:  {
    name:  &quot;id&quot;
    number:  1
    label:  LABEL_OPTIONAL
    type:  TYPE_INT64
    json_name:  &quot;id&quot;
  }
  field:  {
    name:  &quot;status&quot;
    number:  2
    label:  LABEL_OPTIONAL
    type:  TYPE_INT32
    json_name:  &quot;status&quot;
  }
  field:  {
    name:  &quot;result&quot;
    number:  3
    label:  LABEL_REPEATED
    type:  TYPE_MESSAGE
    type_name:  &quot;.tns.search.proto.TnsDemo.ResultEntry&quot;
    json_name:  &quot;result&quot;
  }
  field:  {
    name:  &quot;reasons&quot;
    number:  4
    label:  LABEL_REPEATED
    type:  TYPE_INT32
    json_name:  &quot;reasons&quot;
  }
  nested_type:  {
    name:  &quot;ResultEntry&quot;
    field:  {
      name:  &quot;key&quot;
      number:  1
      label:  LABEL_OPTIONAL
      type:  TYPE_STRING
      json_name:  &quot;key&quot;
    }
    field:  {
      name:  &quot;value&quot;
      number:  2
      label:  LABEL_OPTIONAL
      type:  TYPE_STRING
      json_name:  &quot;value&quot;
    }
    options:  {
      map_entry:  true
    }
  }
}
options:  {
  go_package:  &quot;./gen;protobuf&quot;
}
syntax:  &quot;proto3&quot;
</code></pre>
<h3>4.2 protoc-plugin</h3>
<p>In order to keep the generality, we choose the json-format as the serialization solution. </p>
<pre><code class="language-golang">func main() {
    protogen.Options{}.Run(func(gen *protogen.Plugin) error {
       gen.SupportedFeatures = SupportedFeatures
       for _, file := range gen.Files {
          // Skip files that are not part of the plugin&#39;s current output.
          if !file.Generate {
             continue
          }
          genJsonFile(file, gen)
          genExtFile(file, gen)
       }
       return nil
    })
}
func genJsonFile(file *protogen.File, gen *protogen.Plugin) {
    fd := file.Proto
    sci := fd.SourceCodeInfo
    fd.SourceCodeInfo = nil
    jsonFile := gen.NewGeneratedFile(file.GeneratedFilenamePrefix+&quot;.json&quot;, &quot;.&quot;)
    jsonFile.P(protojson.Format(fd))
    fd.SourceCodeInfo = sci
}
</code></pre>
<p>Once we get serialized content in json format, the rest is easy. We can save the json to tcc and load it later at runtime to marshal/unmarshal proto.Message. If the proto file changes, we can manually compile it offline and then update the new json data in tcc to realize the hot update. </p>
<h3>4.3 How to use dynamic schema</h3>
<ul>
<li>generate json schema with plugin</li>
</ul>
<pre><code class="language-shell">SRC_DIR=$(pwd)
go build -o $SRC_DIR/protoc-gen-ext
protoc --proto_path=$SRC_DIR \
--plugin=protoc-gen-go=$(which protoc-gen-go) \
--go_out=$SRC_DIR/protobuf \
--fastpb_out=$SRC_DIR/protobuf \
--plugin=protoc-gen-ext=$SRC_DIR/protoc-gen-ext \
--ext_out=$SRC_DIR/protobuf \
$SRC_DIR/protobuf/*.proto
</code></pre>
<ul>
<li>manipulate dynamic schema</li>
</ul>
<pre><code class="language-golang">    //jsonData:=getJsonFromTcc(...)
    fdp := new(descriptorpb.FileDescriptorProto)
    //load schema from json
    if err := protojson.Unmarshal([]byte(jsonData), fdp); err != nil {
       panic(err)
    }
    //create FileDescriptorfrom FileDescriptorProto
    fd, err := protodesc.NewFile(fdp, nil)
    if err != nil {
       panic(err)
    }
   // find the MessageDescriptor by name
    md = fd.Messages().ByName(protoreflect.Name(msg))
   // create dynamicpb message
    dynamicpb.NewMessage(md)
</code></pre>
<h2>5. Conlusion</h2>
<p>Dynamic compilation of Protobuf is a powerful technique for situations where the schema cannot be known ahead of time or needs to be handled at runtime. By using Protobuf’s Reflection API, Dynamic Messages, and Any fields, you can create, manipulate, and serialize Protobuf messages dynamically. This allows for greater flexibility in scenarios such as plugin-based architectures, evolving APIs, or systems that need to work with arbitrary message types at runtime. However, dynamic compilation is more complex than static compilation and may introduce performance overhead due to reflection and dynamic handling of schema data.</p>
17:T3dd1,<blockquote>
<p>在分布式一致性一文中主要介绍了分布式系统中存在的一致性问题。本文将简单介绍如何有效的解决分布式的一致性问题,其中包括什么是分布式事务，二阶段提交和三阶段提交。</p>
</blockquote>
<h3>分布式一致性回顾</h3>
<p>在分布式系统中，为了保证数据的高可用，通常，我们会将数据保留多个副本(replica)，这些副本会放置在不同的物理的机器上。为了对用户提供正确的增\删\改\差等语义，我们需要保证这些放置在不同物理机器上的副本是一致的。为了解决这种分布式一致性问题，前人在性能和数据一致性的反反复复权衡过程中总结了许多典型的协议和算法。其中比较著名的有二阶提交协议（Two Phase Commitment Protocol）、三阶提交协议（Three Phase Commitment Protocol）和Paxos算法。</p>
<h3>分布式事务</h3>
<blockquote>
<p>分布式事务是指会涉及到操作多个数据库的事务。其实就是将对同一库事务的概念扩大到了对多个库的事务。目的是为了保证分布式系统中的数据一致性。分布式事务处理的关键是必须有一种方法可以知道事务在任何地方所做的所有动作，提交或回滚事务的决定必须产生统一的结果（全部提交或全部回滚）</p>
</blockquote>
<p>在分布式系统中，各个节点之间在物理上相互独立，通过网络进行沟通和协调。由于存在事务机制，可以保证每个独立节点上的数据操作可以满足ACID。但是，相互独立的节点之间无法准确的知道其他节点中的事务执行情况。所以从理论上讲，两台机器理论上无法达到一致的状态。如果想让分布式部署的多台机器中的数据保持一致性，那么就要保证在所有节点的数据写操作，要不全部都执行，要么全部的都不执行。但是，一台机器在执行本地事务的时候无法知道其他机器中的本地事务的执行结果。所以他也就不知道本次事务到底应该commit还是 roolback。所以，常规的解决办法就是引入一个“协调者”的组件来统一调度所有分布式节点的执行。</p>
<h3>XA规范</h3>
<p>X/Open 组织（即现在的 Open Group ）定义了分布式事务处理模型。 X/Open DTP 模型（ 1994 ）包括应用程序（ AP ）、事务管理器（ TM ）、资源管理器（ RM ）、通信资源管理器（ CRM ）四部分。一般，常见的事务管理器（ TM ）是交易中间件，常见的资源管理器（ RM ）是数据库，常见的通信资源管理器（ CRM ）是消息中间件。    通常把一个数据库内部的事务处理，如对多个表的操作，作为本地事务看待。数据库的事务处理对象是本地事务，而分布式事务处理的对象是全局事务。   所谓全局事务，是指分布式事务处理环境中，多个数据库可能需要共同完成一个工作，这个工作即是一个全局事务，例如，一个事务中可能更新几个不同的数据库。对数据库的操作发生在系统的各处但必须全部被提交或回滚。此时一个数据库对自己内部所做操作的提交不仅依赖本身操作是否成功，还要依赖与全局事务相关的其它数据库的操作是否成功，如果任一数据库的任一操作失败，则参与此事务的所有数据库所做的所有操作都必须回滚。     一般情况下，某一数据库无法知道其它数据库在做什么，因此，在一个 DTP 环境中，交易中间件是必需的，由它通知和协调相关数据库的提交或回滚。而一个数据库只将其自己所做的操作（可恢复）影射到全局事务中。   </p>
<blockquote>
<p>XA 就是 X/Open DTP 定义的交易中间件与数据库之间的接口规范（即接口函数），交易中间件用它来通知数据库事务的开始、结束以及提交、回滚等。 XA 接口函数由数据库厂商提供。</p>
</blockquote>
<p>二阶提交协议和三阶提交协议就是根据这一思想衍生出来的。可以说二阶段提交其实就是实现XA分布式事务的关键(确切地说：两阶段提交主要保证了分布式事务的原子性：即所有结点要么全做要么全不做)</p>
<h3>2PC</h3>
<blockquote>
<p>二阶段提交(Two-phaseCommit)是指，在计算机网络以及数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法(Algorithm)。通常，二阶段提交也被称为是一种协议(Protocol))。在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为协调者的组件来统一掌控所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。因此，二阶段提交的算法思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。</p>
</blockquote>
<p>所谓的两个阶段是指：第一阶段：准备阶段(投票阶段)和第二阶段：提交阶段（执行阶段）。</p>
<h4>准备阶段</h4>
<p>事务协调者(事务管理器)给每个参与者(资源管理器)发送Prepare消息，每个参与者要么直接返回失败(如权限验证失败)，要么在本地执行事务，写本地的redo和undo日志，但不提交，到达一种“万事俱备，只欠东风”的状态。可以进一步将准备阶段分为以下三个步骤：</p>
<blockquote>
<p>1）协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。2）参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作）3）各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。</p>
</blockquote>
<h4>提交阶段</h4>
<p>如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚(Rollback)消息；否则，发送提交(Commit)消息；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。(注意:必须在最后阶段释放锁资源)接下来分两种情况分别讨论提交阶段的过程。当协调者节点从所有参与者节点获得的相应消息都为”同意”时:</p>
<p><img src="/images/blog/tech/middleware/img_20250723_01.png" alt=""></p>
<blockquote>
<p>1）协调者节点向所有参与者节点发出”正式提交(commit)”的请求。2）参与者节点正式完成操作，并释放在整个事务期间内占用的资源。3）参与者节点向协调者节点发送”完成”消息。4）协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。</p>
</blockquote>
<p>如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时：</p>
<p><img src="/images/blog/tech/middleware/img_20250723_02.png" alt=""></p>
<blockquote>
<p>1）协调者节点向所有参与者节点发出”回滚操作(rollback)”的请求。2）参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。3）参与者节点向协调者节点发送”回滚完成”消息。4）协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务。</p>
</blockquote>
<p>　　不管最后结果如何，第二阶段都会结束当前事务。二阶段提交看起来确实能够提供原子性的操作，但是不幸的事，二阶段提交还是有几个缺点的：</p>
<blockquote>
<p>1、同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。2、单点故障。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）3、数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。4、二阶段无法解决的问题：协调者在发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。</p>
</blockquote>
<p>由于二阶段提交存在着诸如同步阻塞、单点问题、脑裂等缺陷，所以，研究者们在二阶段提交的基础上做了改进，提出了三阶段提交。</p>
<h3>3PC</h3>
<blockquote>
<p>三阶段提交（Three-phase commit），也叫三阶段提交协议（Three-phase commit protocol），是二阶段提交（2PC）的改进版本。</p>
</blockquote>
<p><img src="/images/blog/tech/middleware/img_20250723_03.png" alt="图片"></p>
<p>与两阶段提交不同的是，三阶段提交有两个改动点。1、引入超时机制。同时在协调者和参与者中都引入超时机制。2、在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。</p>
<h4>CanCommit阶段</h4>
<p>3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。</p>
<blockquote>
<p>1.事务询问 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。2.响应反馈 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No</p>
</blockquote>
<h4>PreCommit阶段</h4>
<p>协调者根据参与者的反应情况来决定是否可以进行事务的PreCommit操作。根据响应情况，有以下两种可能。假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。</p>
<blockquote>
<p>1.发送预提交请求 协调者向参与者发送PreCommit请求，并进入Prepared阶段。2.事务预提交 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。3.响应反馈 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。</p>
</blockquote>
<p>假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。</p>
<blockquote>
<p>1.发送中断请求 协调者向所有参与者发送abort请求。2.中断事务 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。</p>
</blockquote>
<h4>doCommit阶段</h4>
<p>该阶段进行真正的事务提交，也可以分为以下两种情况。执行提交</p>
<blockquote>
<p>1.发送提交请求 协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。2.事务提交 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。3.响应反馈 事务提交完之后，向协调者发送Ack响应。4.完成事务 协调者接收到所有参与者的ack响应之后，完成事务。</p>
</blockquote>
<p>中断事务 协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。</p>
<blockquote>
<p>1.发送中断请求 协调者向所有参与者发送abort请求2.事务回滚 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。3.反馈结果 参与者完成事务回滚之后，向协调者发送ACK消息4.中断事务 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。</p>
</blockquote>
<p>在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。）</p>
<h3>2PC与3PC的区别</h3>
<p>相对于2PC，3PC主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。</p>
<hr>
<p>了解了2PC和3PC之后，我们可以发现，无论是二阶段提交还是三阶段提交都无法彻底解决分布式的一致性问题。Google Chubby的作者Mike Burrows说过， there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos. 意即<strong>世上只有一种一致性算法，那就是Paxos</strong>，所有其他一致性算法都是Paxos算法的不完整版。后面的文章会介绍这个公认为难于理解但是行之有效的Paxos算法。</p>
<h3>参考资料：</h3>
<p><a href="http://www.mamicode.com/info-detail-890945.html">分布式协议之两阶段提交协议（2PC）和改进三阶段提交协议（3PC）</a> <a href="http://blog.csdn.net/bluishglc/article/details/7612811">关于分布式事务、两阶段提交、一阶段提交、Best Efforts 1PC模式和事务补偿机制的研究</a> <a href="http://www.tuicool.com/articles/mARV3u">两阶段提交协议与三阶段提交协议</a></p>
18:T777e,<h2>一、比特币的起点与困境</h2>
<h3>1.1 起源：去中心化与“电子现金”的理想</h3>
<p>2008 年，中本聪（Satoshi Nakamoto）在密码学邮件列表上发表了论文《Bitcoin: A Peer-to-Peer Electronic Cash System》。文中提出了一种革命性的设想：建立一个点对点的电子现金系统，不依赖银行或清算机构，交易双方即可直接完成价值转移。</p>
<p>2009 年，比特币网络正式上线。创世区块中被刻意写入《泰晤士报》当天的新闻标题：“Chancellor on brink of second bailout for banks”（财政大臣正处于对银行进行第二轮救助的边缘），象征着比特币对传统金融体系的抗议：在金融危机动荡中，构建一个去中心化、抗审查的货币替代品。</p>
<h3>1.2 技术创新：区块链与 PoW 共识</h3>
<p>比特币引入了几项划时代的技术：</p>
<ul>
<li><p><strong>区块链（Blockchain）</strong><br>一个分布式、不可篡改的账本，所有节点都能验证交易，保障透明与安全。</p>
</li>
<li><p><strong>PoW（Proof of Work，工作量证明）</strong><br>矿工通过算力竞争打包新区块，谁先解出符合条件的哈希值就获得记账权与奖励。PoW 保证了篡改账本的成本极高（需要超过全网 50% 算力），从而实现安全性。</p>
</li>
<li><p><strong>总量限制</strong><br>比特币发行量上限为 2100 万枚，每约四年区块奖励减半，模拟了黄金的稀缺性。</p>
</li>
</ul>
<p>这些机制让比特币成为第一个真正意义上的去中心化货币试验，也为后续的加密产业奠定了基础。</p>
<h3>1.3 现实意义上的缺陷</h3>
<p>比特币在技术上无疑是伟大的创新，但从现实经济和货币职能角度，它存在一系列结构性问题：</p>
<ol>
<li><p><strong>总量刚性，与经济扩张脱节</strong><br>现实经济不断增长，需要货币供给与之匹配。比特币的总量锁死在 2100 万枚，必然导致通缩倾向，资金更容易向早期持有者集中，不利于经济流通。</p>
</li>
<li><p><strong>PoW 高耗能，却不创造现实生产力</strong><br>挖矿每年消耗的电力相当于一个中等国家的能耗。其唯一产出是“账本安全”，没有对现实经济产生额外价值，是一种“纯消耗”。</p>
</li>
<li><p><strong>分配不公：早期红利与后期风险</strong><br>早期几乎零成本挖矿者获取了大量比特币，享受了极高红利。后进入者只能以高价买入，承担高风险。这种“先来者得利，后来者接盘”的结构使其天然存在财富不平等。</p>
</li>
<li><p><strong>纯量博弈</strong><br>随着风险提升，越来越多后来者会理性拒绝入场，市场逐渐演化为存量参与者之间的零和博弈。价格波动更多取决于筹码交换，而不是现实价值创造。</p>
</li>
<li><p><strong>沉睡/丢失币不可递补</strong><br>比特币的持有完全依赖私钥，一旦遗忘或遗失，资产即永久失效。研究估计已有超过 10% 的比特币处于“沉睡”状态。这使得有效供给递减、流动性下降，市场更脆弱。</p>
</li>
<li><p><strong>“最后一枚比特币”与安全预算困境</strong><br>按照设计，比特币将在 2140 年左右全部挖出。届时矿工只能依赖交易手续费维持网络安全。若交易量不足，将陷入两难：</p>
<ul>
<li>要么手续费高昂 → 日常支付不可用；</li>
<li>要么安全预算不足 → 网络抗攻击能力下降。</li>
</ul>
</li>
</ol>
<p>这种“要么贵，要么脆”的困境让比特币难以成为全球普适的货币。</p>
<h3>1.4 阶段性定位</h3>
<p>综上，比特币作为一项技术实验具有划时代意义，但作为现实货币，其功能极度受限：</p>
<ul>
<li>它不能灵活适应经济规模变化；</li>
<li>它的生产过程浪费资源；</li>
<li>它的分配机制固化不平等；</li>
<li>它的网络安全逻辑存在未来隐忧。</li>
</ul>
<p>因此，比特币更适合作为一种数字黄金，而非日常使用的货币。</p>
<h3>1.5 比特币作为“数字黄金”的合理性探讨</h3>
<p>比特币常被称为“数字黄金”，但这一类比合理吗？</p>
<p><strong>（1）黄金的货币地位基础</strong><br>黄金数千年来作为货币，依赖于其独特属性：</p>
<ul>
<li>稀缺性：开采难度高，储量有限；</li>
<li>耐久性：不会腐蚀，便于长期保存；</li>
<li>可分割性与便携性：可铸成不同大小的金币；</li>
<li>内在使用价值：珠宝、工业、储备需求；</li>
<li>跨文明共识：几乎所有国家与文化都承认黄金的价值。</li>
</ul>
<p>黄金的货币属性是“物理特性 + 历史共识”的结合。</p>
<p><strong>（2）比特币的相似点</strong></p>
<ul>
<li>稀缺性：总量上限 2100 万，模拟黄金有限供给；</li>
<li>获取成本：挖矿需要电力与算力，类似“开采难度”；</li>
<li>不可篡改：区块链保证账本透明防伪；</li>
<li>全球流动性：可随时跨境转移。</li>
</ul>
<p>因此，它具备部分“类黄金”的特征。</p>
<p><strong>（3）比特币的差异与不足</strong></p>
<ul>
<li>缺乏非货币价值：黄金即使不作为货币，依然有工业和装饰用途；比特币完全依赖共识，没有现实应用价值兜底。</li>
<li>波动性过高：黄金年波动率约 10%–15%，比特币常超过 60%–80%，不适合作为稳定储值。</li>
<li>市场深度有限：黄金市值数十万亿美元，央行普遍持有；比特币市值远小，流动性脆弱。</li>
<li>共识脆弱：黄金有千年历史验证，而比特币仅十余年，尚未跨越制度与代际考验。</li>
</ul>
<p><strong>（4）合理性评估</strong><br>比特币的“数字黄金”定位更像是一种比喻性的共识实验：</p>
<ul>
<li>它在稀缺性和去中心化方面模拟黄金；</li>
<li>但缺乏黄金那样的物理属性与历史积淀。</li>
</ul>
<p>因此，它可能长期作为“高风险的储值资产”存在，但其“数字黄金地位”并不稳固，完全依赖未来市场共识能否持续。</p>
<h3>1.6 小结</h3>
<p>比特币的历史使命是打开了数字货币与区块链的大门。<br>它用技术证明了：去中心化账本可以运行，点对点的价值传输是可能的。<br>但它同时也揭示了局限：缺乏与现实经济的深度耦合，难以承担现代货币的全部功能。</p>
<p>因此，比特币的合理定位是“数字黄金”：一种稀缺的投机性储值工具，而不是未来全球金融的基础货币。</p>
<h2>二、稳定币的兴起与现实意义</h2>
<h3>2.1 稳定币的提出</h3>
<p>比特币的价格波动极其剧烈，使其很难作为日常支付工具。于是，市场逐渐孕育出一种新型的数字货币形态——<strong>稳定币（Stablecoin）</strong>。<br>稳定币的核心目标，是锚定现实中的低波动资产（通常是美元、欧元等法币），并通过储备、抵押或算法机制来保持价格稳定。</p>
<p>它的出现，弥补了比特币作为支付手段的缺陷：在比特币的去中心化理想之外，用户需要一种<strong>价值稳定、便于结算</strong>的货币工具。可以说，如果比特币是“数字黄金”，那么稳定币就是“数字现金”。</p>
<h3>2.2 稳定币的主要类型</h3>
<p>稳定币的设计模式大致可以分为三类：</p>
<ol>
<li><p><strong>法币储备型</strong></p>
<ul>
<li>代表：USDT（Tether）、USDC（Circle/ Coinbase）。</li>
<li>机制：每发行 1 枚稳定币，就在银行账户中存放 1 美元或等价资产。</li>
<li>优点：价格锚定直接，使用体验接近法币。</li>
<li>风险：储备透明度不足，过度依赖托管银行，存在合规和冻结风险。</li>
</ul>
</li>
<li><p><strong>加密抵押型</strong></p>
<ul>
<li>代表：DAI（MakerDAO）。</li>
<li>机制：用户抵押 ETH 等数字资产，并以超额担保的方式生成稳定币。</li>
<li>优点：完全链上运行，透明度高，不依赖银行体系。</li>
<li>风险：抵押物价格剧烈波动时，可能触发大规模清算，导致稳定币脱锚。</li>
</ul>
</li>
<li><p><strong>算法型</strong></p>
<ul>
<li>代表：UST（Terra，已崩溃）。</li>
<li>机制：通过算法自动调节稳定币的供需，维持与美元的挂钩。</li>
<li>风险：一旦市场信心崩溃，算法无法对抗恐慌性抛售，极易陷入“死亡螺旋”。</li>
</ul>
</li>
</ol>
<p>通过对比可以看出，只有前两类模式在现实中具有可持续性，而算法型稳定币更多停留在“失败的实验”。</p>
<h3>2.3 稳定币的现实意义</h3>
<p>稳定币不仅仅是一种加密资产，它的意义远远超出了“币价稳定”本身：</p>
<ul>
<li><p><strong>提供统一的计价单位</strong><br>在加密世界中，价格波动剧烈的比特币难以充当“记账单位”。稳定币则扮演了“美元替代品”的角色，让所有链上资产和交易都能以稳定的单位计价。</p>
</li>
<li><p><strong>跨境支付与结算的高效工具</strong><br>稳定币转账可以 7×24 小时进行，几分钟到账，手续费极低。相比传统跨境汇款动辄数日、数十美元的成本，稳定币支付优势明显。</p>
</li>
<li><p><strong>桥接现实金融与区块链金融</strong><br>法币储备型稳定币需要持有现实中的现金或国债作为担保。这使得稳定币成为现实金融与加密金融之间的桥梁：一端连着美元储备，另一端连着区块链交易。</p>
</li>
<li><p><strong>可编程货币</strong><br>稳定币不仅能“存放在钱包里”，还能嵌入智能合约，用于自动化清算、借贷协议、收益分配。它的货币功能因可编程性而大大扩展，这是传统电子现金无法比拟的。</p>
</li>
</ul>
<p>因此，稳定币可以被视为加密世界的“润滑剂”，推动区块链应用从投机走向实用。</p>
<h3>2.4 风险与挑战</h3>
<p>稳定币虽然有巨大潜力，但其设计模式和运行逻辑也暴露出一系列风险：</p>
<ol>
<li><p><strong>脱锚风险</strong><br>一旦储备不足或抵押物暴跌，稳定币可能迅速失去与美元的锚定关系。UST 的崩盘就是前车之鉴。</p>
</li>
<li><p><strong>储备透明度</strong><br>以 USDT 为例，长期因储备是否充足、是否存在未公开的商业票据而饱受质疑。缺乏透明度会削弱用户信任。</p>
</li>
<li><p><strong>监管挑战</strong><br>在美国，稳定币被视为可能具有系统性风险的支付工具。欧洲的 MiCA 法案也已将稳定币纳入监管，需要遵守资本金与流动性规定。</p>
</li>
<li><p><strong>集中化风险</strong><br>尤其是法币储备型，依赖托管银行和发行公司。一旦账户被冻结或遭遇监管打击，稳定币用户可能遭受损失。</p>
</li>
</ol>
<p>这些风险表明，稳定币虽已成为加密经济的“关键基础设施”，但它的未来高度依赖于透明度建设与监管框架的完善。</p>
<h3>2.5 稳定币与比特币的互补</h3>
<p>比特币和稳定币的关系并非替代，而是互补。</p>
<ul>
<li><strong>比特币</strong>：作为稀缺资产，承担“价值储藏”和“投机品”角色。</li>
<li><strong>稳定币</strong>：作为低波动货币，承担“支付媒介”和“记账单位”。</li>
</ul>
<p>两者在区块链世界形成了“双层货币体系”：比特币相当于“数字黄金”，而稳定币则是“数字现金”。它们共同支撑了去中心化金融的基本运作。</p>
<h3>2.6 小结</h3>
<p>稳定币的兴起，是比特币之后加密货币演进中的必然阶段。它通过锚定现实资产，提供了一个低波动的货币单位，使区块链世界能够进行更广泛的支付、结算与金融创新。</p>
<p>但稳定币本身也存在不可忽视的风险：脱锚、储备不透明、合规与集中化问题。它不是数字货币的终点，而是连接虚拟与现实的重要桥梁。稳定币未来将继续在加密经济中扮演关键角色，但其设计与监管必须不断完善，才能实现真正的可持续发展。</p>
<h2>三、RWA（现实世界资产代币化）</h2>
<h3>3.1 定义与意义</h3>
<p>RWA（Real-World Assets，现实世界资产代币化）是指将现实中的资产权益通过区块链技术确权、分割和数字化。<br>传统金融资产如债券、房地产、应收账款，乃至碳排放额度、知识产权，都可以被代币化，从而以数字凭证的形式在链上流转。</p>
<p>RWA 的出现，使区块链从“虚拟货币的自循环”真正走向了与实体经济的结合。它不仅能提升资产流动性，也能降低融资门槛，让更多投资者能够以小额资金参与原本门槛极高的市场。</p>
<p>一句话：<strong>稳定币解决“用什么钱在链上结算”，RWA 解决“把什么现实资产搬到链上交易”。</strong></p>
<h3>3.2 投资闭环与流程拆解</h3>
<p>RWA 与稳定币结合，可以形成一个完整的投资闭环：现实货币 → 稳定币 → RWA → 稳定币 → 现实货币。</p>
<p>具体流程如下：</p>
<ol>
<li><p><strong>入口 – 稳定币</strong><br>投资者用现实货币（USD、RMB 等）兑换稳定币（如 USDC、USDT），进入链上钱包，成为可编程的“数字现金”。</p>
</li>
<li><p><strong>投资 – 购买 RWA</strong><br>投资者用稳定币认购代币化的现实资产，如国债、房地产收益权、应收账款等。<br>交易采用 <strong>DvP（货银对付，Delivery vs Payment）</strong> 原子结算：稳定币支付的同时，RWA 代币立即到账。</p>
</li>
<li><p><strong>增值 – RWA 产生现金流</strong><br>持有期间，底层资产产生票息、租金、分红等收益。合约或托管方自动按比例发放，通常以稳定币结算。</p>
</li>
<li><p><strong>退出 – RWA 转换回稳定币</strong><br>投资者到期赎回或在二级市场卖出 RWA 代币，换回稳定币。</p>
</li>
<li><p><strong>回归 – 稳定币兑换现实货币</strong><br>稳定币通过合规渠道兑换为现实货币（如提现至银行账户），完成资金循环。</p>
</li>
</ol>
<p>这构成了一个完整的金融闭环：<strong>稳定币是入口与出口，RWA 是增值来源。</strong></p>
<h3>3.3 稳定币与 RWA 的职责边界</h3>
<ul>
<li><strong>稳定币（Stablecoin）</strong>：将现实中的货币或其等价物（美元存款、短期国债等）数字化，在链上作为低波动的计价与结算媒介使用。核心承诺是 <strong>1:1 赎回与储备披露（PoR, Proof of Reserve）</strong>。</li>
<li><strong>RWA（Real-World Assets）</strong>：将现实世界的可计量资产或现金流（国债、票据、应收账款、地产收益权、碳配额等）代币化，使其在链上可转移、可分割、可编程。</li>
</ul>
<p>稳定币提供流动性与支付手段，RWA 提供价值与收益，两者结合形成互补关系。</p>
<h3>3.4 两者的强关联机制</h3>
<p>稳定币和 RWA 的结合并不是简单的“支付+资产”，而是通过五条主链路形成深度绑定：</p>
<ol>
<li><p><strong>发行与一级认购：DvP 落地</strong><br>资产方设立 SPV/托管，披露底层资产信息与合规条件。投资者用稳定币认购，合约在收到稳定币时同步发放 RWA 代币，实现 DvP。</p>
</li>
<li><p><strong>二级流动性：稳定币是天然的报价货币</strong><br>无论 AMM（自动做市）还是订单簿，RWA 都以稳定币计价结算，统一了报价和流动性管理。</p>
</li>
<li><p><strong>收益与现金流分配：自动化支付</strong><br>底层资产的票息、租金、分红由合约自动结算并发放稳定币，收益分配透明且高效。</p>
</li>
<li><p><strong>抵押与信用扩展</strong><br>投资者可用 RWA 抵押借出稳定币，或用稳定币抵押获取 RWA 信贷，形成信用与流动性循环。</p>
</li>
<li><p><strong>储备与锚定</strong><br>法币储备型稳定币本身常配置国债、货币基金等 RWA 作为储备，以产生利息覆盖成本。稳定币依赖 RWA 获得收益稳固锚定，RWA 则依赖稳定币提供流动性和交易场景。</p>
</li>
</ol>
<h3>3.5 应用场景</h3>
<p>RWA 的应用正在多个领域落地，典型场景包括：</p>
<ol>
<li><p><strong>国债与票据代币化</strong></p>
<ul>
<li>SPV（特殊目的载体）托管真实国债，发行对应代币。</li>
<li>投资者持有代币，即享受票息，收益自动通过智能合约发放。</li>
<li>特点：低风险、高透明度，已在美国、欧洲试点。</li>
</ul>
</li>
<li><p><strong>房地产与租金收益</strong></p>
<ul>
<li>房地产收益权（如租金）代币化，每月现金流以稳定币分发。</li>
<li>投资者可小额参与房地产市场，提高流动性。</li>
</ul>
</li>
<li><p><strong>应收账款与供应链金融</strong></p>
<ul>
<li>企业将应收账款打包代币化，发行给投资者换取稳定币融资。</li>
<li>到期付款后，合约自动兑付稳定币给投资者。</li>
<li>特点：降低中小企业融资门槛，提高融资透明度。</li>
</ul>
</li>
<li><p><strong>碳配额与绿色金融</strong></p>
<ul>
<li>碳减排凭证代币化，可在链上交易。</li>
<li>与 ESG 投资结合，满足监管要求，同时拓展绿色金融市场。</li>
</ul>
</li>
</ol>
<p>这些场景展示了 RWA 的多样性：既涵盖传统低风险资产（如国债），也覆盖新兴市场（如碳配额）。</p>
<h3>3.6 风险与风控要点</h3>
<p>RWA 的发展需要严谨的制度和风险管理：</p>
<ul>
<li><strong>法律结构</strong>：通过 SPV/信托安排实现破产隔离，保障投资者权益。</li>
<li><strong>身份与权限</strong>：执行 KYC/AML、地址白名单和地域限制，确保合规。</li>
<li><strong>储备与托管</strong>：第三方托管与 PoR 证明，避免链上链下错配。</li>
<li><strong>预言机与会计</strong>：采用多源价格喂价与冗余机制，避免操纵风险。</li>
<li><strong>清算与交割</strong>：通过 DvP/PvP 原子结算避免对手方风险。</li>
<li><strong>流动性安排</strong>：设立回购机制和二级市场支持，降低挤兑风险。</li>
<li><strong>跨法域合规</strong>：不同司法辖区标准差异大，需要明确合规路由。</li>
</ul>
<p>常见风险包括：</p>
<ul>
<li><strong>双重计提与风险错配</strong>：稳定币和 RWA 储备交叉使用导致风险放大。</li>
<li><strong>稳定币脱锚传导</strong>：稳定币的短期波动可能直接冲击 RWA 定价。</li>
<li><strong>跨链与预言机风险</strong>：技术攻击可能导致链上价格或结算失效。</li>
</ul>
<h3>3.7 小结</h3>
<p>RWA 是区块链走向现实世界的重要桥梁。它通过代币化把现实价值带上链条，提升资产流动性，扩大投资者参与范围。稳定币与 RWA 相辅相成：稳定币提供支付与结算的流动性，RWA 提供可验证的资产与现金流。两者结合，构建了一个完整的投资与价值闭环，使区块链真正嵌入现实金融。 </p>
<p>随着技术与监管的成熟，RWA未来有望成为主流资产配置的一部分，推动全球金融市场的数字化转型。</p>
<h2>四、CBDC 的出现与国家化路径</h2>
<h3>4.1 概念与特征</h3>
<p>CBDC（Central Bank Digital Currency，央行数字货币）是法定货币的数字形态。<br>它由央行直接发行并背书，具有国家信用和法律效力。</p>
<p>典型案例包括：中国的数字人民币（e-CNY）、欧洲的数字欧元，以及美国正在探索的数字美元。</p>
<p>如果说稳定币是“市场版数字现金”，那么 CBDC 就是“国家版数字现金”。</p>
<h3>4.2 与稳定币的关联与区别</h3>
<p>CBDC 与稳定币常被同时提及，但二者有本质差别：</p>
<ul>
<li><p><strong>发行主体</strong></p>
<ul>
<li>稳定币：由私人机构发行（如 Circle 发行 USDC，Tether 发行 USDT）。</li>
<li>CBDC：由国家央行直接发行。</li>
</ul>
</li>
<li><p><strong>价值锚定</strong></p>
<ul>
<li>稳定币：以储备资产（美元存款、短期国债等）作为锚定，需依赖 Proof of Reserve（储备证明）。</li>
<li>CBDC：本身就是法币，不需要额外锚定。</li>
</ul>
</li>
<li><p><strong>信用背书</strong></p>
<ul>
<li>稳定币：信用依赖发行方和托管机构，可能存在违约或透明度不足。</li>
<li>CBDC：由国家主权担保，具备最高级别的信用。</li>
</ul>
</li>
<li><p><strong>监管地位</strong></p>
<ul>
<li>稳定币：受到严格监管，甚至可能被限制或取代。</li>
<li>CBDC：属于法定货币体系的一部分，具备天然合法性。</li>
</ul>
</li>
</ul>
<p>两者的关系可以理解为：<strong>稳定币是过渡产品，填补了数字支付需求与法币数字化之间的空白，而 CBDC 则是最终的国家化解决方案。</strong></p>
<h3>4.3 投资闭环的升级</h3>
<p>在稳定币体系下，投资闭环是：</p>
<p><strong>法币 → 稳定币 → RWA → 稳定币 → 法币</strong></p>
<p>而 CBDC 出现后，流程被大幅简化：</p>
<p><strong>CBDC → RWA → CBDC</strong></p>
<p>因为 CBDC 本身就是法币，省去了“稳定币 ↔ 法币”的兑换环节，使得链上资产投资和清算更加直接。</p>
<h3>4.4 政策价值</h3>
<p>CBDC 的推出不仅是支付工具的升级，更是货币政策和金融治理的重要抓手：</p>
<ol>
<li><p><strong>宏观调控</strong></p>
<ul>
<li>CBDC 可编程，财政补贴或消费券可以精准投放。</li>
<li>货币可以设定有效期，用于刺激即时消费。</li>
</ul>
</li>
<li><p><strong>监管与反洗钱</strong></p>
<ul>
<li>CBDC 交易全程可追溯，洗钱与地下资金流动更难隐藏。</li>
</ul>
</li>
<li><p><strong>支付体系统一化</strong></p>
<ul>
<li>打破第三方支付平台的垄断，使央行直接掌握支付数据和流动性。</li>
</ul>
</li>
<li><p><strong>跨境结算</strong></p>
<ul>
<li>如果多个国家 CBDC 实现互认，有可能成为绕开 SWIFT 的新型国际支付工具。</li>
</ul>
</li>
</ol>
<h3>4.5 面临的挑战</h3>
<p>CBDC的实施也带来诸多难题：</p>
<ul>
<li><strong>隐私问题</strong>：用户担心交易数据被过度监控。</li>
<li><strong>商业银行角色</strong>：资金可能流向央行钱包，削弱商业银行中介功能。</li>
<li><strong>国际化难题</strong>：跨境互认需要法律、监管和技术标准协调，难度极高。</li>
<li><strong>系统安全</strong>：CBDC 必须应对极高强度的黑客攻击与系统宕机风险。</li>
</ul>
<h3>4.6 小结</h3>
<p>CBDC 是稳定币的国家化形态。它通过国家信用取代了私人信用，把“数字货币”与“法定货币”真正合二为一。</p>
<p>从长远看，CBDC的普及将重塑全球金融格局，让数字货币从“私人实验”进入“国家秩序”阶段。  </p>
<h2>五、中美路径的比较</h2>
<h3>5.1 美国路径：市场驱动与创新优先</h3>
<p>美国的数字货币发展呈现出典型的“市场先行、监管滞后”特征。</p>
<ol>
<li><p><strong>稳定币兴起</strong></p>
<ul>
<li>USDT、USDC 等稳定币几乎占据了全球稳定币市场的绝大多数份额。</li>
<li>稳定币被广泛用于加密交易、跨境支付和 DeFi（去中心化金融）生态。</li>
</ul>
</li>
<li><p><strong>RWA 实践</strong></p>
<ul>
<li>美国金融市场成熟，代币化国债、票据和基金最先落地。</li>
<li>例如部分项目已实现用 USDC 直接认购代币化短期美债，并定期分配票息。</li>
</ul>
</li>
<li><p><strong>CBDC 探索</strong></p>
<ul>
<li>美联储对数字美元保持谨慎，担心对商业银行体系造成冲击。</li>
<li>政策层更强调“保持美元霸权”和“防御他国 CBDC 竞争”，而非短期落地。</li>
</ul>
</li>
</ol>
<p>美国的路径特点是：<strong>市场化创新先行，RWA 与稳定币结合形成全球流动性优势，但 CBDC 推进缓慢。</strong></p>
<h3>5.2 中国路径：政策主导与金融安全</h3>
<p>中国的数字货币路线则体现出“国家主导、顶层设计”的风格。</p>
<ol>
<li><p><strong>稳定币严格受限</strong></p>
<ul>
<li>中国监管部门对民间稳定币保持高压态度，禁止大规模发行与流通。</li>
<li>原因在于稳定币可能威胁人民币主权和资本管控。</li>
</ul>
</li>
<li><p><strong>RWA 探索有限</strong></p>
<ul>
<li>中国的 RWA 试点更多局限在供应链金融、票据数字化等场景。</li>
<li>与 DeFi 场景不同，更强调合规与可控性。</li>
</ul>
</li>
<li><p><strong>CBDC 先行</strong></p>
<ul>
<li>数字人民币（e-CNY）已进入大规模试点，在零售支付、政务补贴和跨境支付场景中逐步落地。</li>
<li>政府目标明确：既是支付工具升级，也是维护金融安全与货币主权的战略手段。</li>
</ul>
</li>
</ol>
<p>中国的路径特点是：<strong>绕过稳定币阶段，直接以 CBDC 为核心，RWA 更多依附于官方体系。</strong></p>
<h3>5.3 路径差异背后的逻辑</h3>
<ul>
<li><p><strong>金融体系角色</strong></p>
<ul>
<li>美国依赖成熟的资本市场，允许稳定币和 RWA 在市场中试错。</li>
<li>中国强调货币主权安全，避免私人稳定币蚕食官方信用。</li>
</ul>
</li>
<li><p><strong>创新与监管平衡</strong></p>
<ul>
<li>美国更倾向“宽松—爆发—再监管”的循环模式。</li>
<li>中国则倾向“先设制度边界，再有限度创新”。</li>
</ul>
</li>
<li><p><strong>国际化考量</strong></p>
<ul>
<li>美国希望稳定币与美元体系绑定，继续输出美元霸权。</li>
<li>中国希望数字人民币突破 SWIFT 体系，在跨境支付中增强独立性。</li>
</ul>
</li>
</ul>
<h3>5.4 长远影响</h3>
<ul>
<li>在美国，稳定币和 RWA 的发展可能继续强化美元在全球金融中的结算地位，即使 CBDC 推进较慢，也不会削弱其国际影响力。</li>
<li>在中国，CBDC 可能成为金融数字化的底层工具，推动人民币在区域内的跨境使用，逐步扩大人民币的国际化程度。</li>
<li>中美的差异最终可能形成“双轨格局”：<ul>
<li>美国主导 <strong>稳定币+RWA 市场化金融生态</strong>；</li>
<li>中国主导 <strong>CBDC 国家化数字货币体系</strong>。</li>
</ul>
</li>
</ul>
<h3>5.5 小结</h3>
<p>中美在数字货币路径上的差异，既反映了两国金融体系的不同，也折射出地缘政治格局的考量。美国依靠市场创新，利用稳定币与 RWA 扩展美元影响力；中国依靠国家主导，通过 CBDC 强化货币主权。未来，全球数字货币体系很可能在这两种模式之间找到平衡点。  </p>
<h2>六、未来趋势与终局假设</h2>
<h3>6.1 数字货币发展的驱动力</h3>
<p>数字货币的发展不是孤立的，它受到三大核心力量推动：</p>
<ol>
<li><strong>技术进步</strong>：区块链、智能合约、跨链协议和隐私计算不断成熟，为数字货币提供更高的安全性和扩展性。</li>
<li><strong>金融效率需求</strong>：全球支付和结算体系需要更低成本、更高效率的工具，传统清算体系的延迟和高成本正逐渐无法满足需求。</li>
<li><strong>地缘政治博弈</strong>：美元霸权、人民币国际化、欧元的金融独立诉求，都会加速数字货币的探索与竞争。</li>
</ol>
<h3>6.2 三种可能的演化路径</h3>
<ol>
<li><p><strong>双轨并行模式</strong></p>
<ul>
<li>美国继续依托市场化路径，强化稳定币 + RWA 生态。</li>
<li>中国和部分国家推动 CBDC 成为核心支付工具。</li>
<li>全球同时存在 <strong>私人主导的美元稳定币体系</strong> 和 <strong>国家主导的 CBDC 体系</strong>，二者在不同区域、不同场景并行。</li>
</ul>
</li>
<li><p><strong>全球协同标准</strong></p>
<ul>
<li>各国央行逐步达成共识，推动 CBDC 的互认和互操作。</li>
<li>出现类似“国际清算所（BIS）”的全球 CBDC 清算平台。</li>
<li>稳定币逐步被纳入监管框架，成为 CBDC 的补充工具，而非替代品。</li>
</ul>
</li>
<li><p><strong>世界级数字货币</strong></p>
<ul>
<li>在长期假设下，可能出现一种由国际组织（如 IMF）牵头的全球数字货币，锚定一篮子主要经济体的 GDP 或储备资产。</li>
<li>这种货币类似于“数字版 SDR（特别提款权）”，成为跨国结算和储备货币的统一基准。</li>
<li>各国 CBDC 在国内流通，而跨境交易由该全球货币清算。</li>
</ul>
</li>
</ol>
<h3>6.3 未来的底层逻辑</h3>
<ul>
<li><p><strong>现实价值锚定不可或缺</strong><br>无论是稳定币还是 CBDC，最终都必须与现实经济活动挂钩，否则就会陷入类似比特币那样的“纯量博弈”。</p>
</li>
<li><p><strong>合规与透明度是核心竞争力</strong><br>稳定币需要储备审计，RWA 需要链下资产对接，CBDC 需要法律与制度框架支撑。谁能提供更高的透明度和信任，谁就能获得更大市场份额。</p>
</li>
<li><p><strong>技术标准决定国际话语权</strong><br>数字货币不仅是金融竞争，也是技术标准竞争。谁能制定跨境支付、身份认证、合规追踪的国际标准，谁就能在未来的数字货币格局中掌握主动权。</p>
</li>
</ul>
<h3>6.4 小结</h3>
<p>未来的数字货币格局可能不会只有一种模式，而是多种形态并存：</p>
<ul>
<li><strong>比特币</strong>继续作为高风险的投机性“数字黄金”存在；</li>
<li><strong>稳定币 + RWA</strong>构建出市场化的全球数字金融生态；</li>
<li><strong>CBDC</strong>逐渐取代纸币，成为各国法币的数字化版本；</li>
<li><strong>国际协调工具</strong>或将出现，用来解决跨境支付和清算的碎片化问题。</li>
</ul>
<p>最终，数字货币的演化方向取决于技术突破、监管合作以及国际博弈。它不仅是金融的升级，更是全球秩序重构的一部分。  </p>
19:T4fc0,<p>你有没有遇到过因为没有打印SQL导致问题排查困难？如果你使用了成熟ORM框架，那么很容易支撑SQL的拦截和监控，例如Mybatis的Interceptor或JOOQ的Listener都支持SQL执行过程的跟踪监控，但是，如果你的ORM框架不支持SQL监控，那么很不幸，你就只能在代码中手动打印日志了。然而，为了防SQL注入，应用中的SQL语句都是参数化的，直接打印的话，SQL语句未绑定参数，ORM框架一般都提供了SQL参数绑定的功能，原生的JDBC这样就失去了一定的监控价值。</p>
<p>另外，在TOB的业务中，有些场景SQL参数超长，如大IN查询，SQL语句会长达到几万甚至十几万，此时，我们又需要对SQL语句进行缩略打印。注意，这里的SQL缩略打印不是简单的对SQL语句进行截断，而是对SQL语句中的参数列表进行截断，例如下面的SQL</p>
<pre><code class="language-sql">select * from user 
where id in (1001,1001, 1002, 1003, 1004, 1005, 1006, 1007) 
and name in(sql
select name from whitelist 
where name in(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;,&#39;g&#39;,&#39;h&#39;,&#39;i&#39;,&#39;j&#39;,&#39;k&#39;,&#39;l&#39;,&#39;m&#39;)
)
</code></pre>
<p>缩略下印如下：</p>
<pre><code class="language-sql">select * from user 
where id in (1001,1001, 1002, 1003, 1004,...) 
and name in(
select name from whitelist 
where name in(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,...)
)
</code></pre>
<p>既然SQL 监控很重要，那么对于应用层的SQL监控都有哪些手段呢？一个SQL请求的执行链路，一般从DAO层开始：DAO -&gt; ORM -&gt; DataSource  -&gt; Connection -&gt; Driver -&gt; DB，那么在这个链路上有哪些环节可以切入监控呢？ DAO层是数据访问层的入口，而我们的目标是应用层监控，因此，能够实现SQL监控的环节只有：ORM -&gt; DataSource  -&gt; Connection -&gt; Driver，而要实现通用的非侵入式监控，则应该独立于ORM，因此我们可以从<strong>DataSource  -&gt; Connection -&gt; Driver</strong>三个环节进行入手：</p>
<h3><strong>一、SQL Profile监控</strong></h3>
<h4><strong>1、驱动层监控</strong></h4>
<p>如果Driver层支持日志监控，则最方便，例如MySQL，可以在jdbc url中添加logger：</p>
<pre><code class="language-properties">jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=UTC&amp;logger=Slf4JLogger&amp;profileSQL=true
</code></pre>
<p>基于Driver监控的问题在于：一方面强依赖于DB，和ORM层面临一样的问题，不具有通用性上述的问题，且需要厂商的支持，例如Oracle Driver就不支持日志监控；另一方面SQL格式固定，无法进行定制化输出。</p>
<h4><strong>2、连接层监控</strong></h4>
<p>如果厂商驱动不支持SQL日志，可以Driver进行代理实现SQL监控功能，常用的开源组件如<a href="https://p6spy.readthedocs.io/en/latest/">P6Spy</a>、<a href="https://github.com/arthurblake/log4jdbc">log4jdbc</a> 等，其原理都是代理了厂商的驱动，因此只需要修改jdbc url：</p>
<ul>
<li>pyspy</li>
</ul>
<pre><code class="language-properties">jdbc:p6spy:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=UTC
</code></pre>
<ul>
<li>log4jdbc</li>
</ul>
<pre><code class="language-properties">jdbc:log4jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=UTC
</code></pre>
<h4><strong>3、数据源层监控</strong></h4>
<p>可以通过对DataSource进行代理实现SQL监控</p>
<ul>
<li>P6Spy：</li>
</ul>
<pre><code class="language-java">@Bean
@Primary
public DataSource spyDataSource(@Autowired DataSource dataSource) {
  // wrap a datasource using P6SpyDataSource
  return new P6DataSource(dataSource);
}
</code></pre>
<ul>
<li>log4jdbc</li>
</ul>
<pre><code class="language-java">public DataSource spyDataSource(DataSource dataSource) {
    // wrap the provided dataSource
  return new DataSource() {
    @Override
    public Connection getConnection() throws SQLException {
      // wrap the connection with log4jdbc
      return new ConnectionSpy(dataSource.getConnection());
    }
      
    @Override
    public Connection getConnection(String username, String password) throws SQLException {
       // wrap the connection with log4jdbc
      return new ConnectionSpy(dataSource.getConnection(username, password));
     }
      //...
  };
}
</code></pre>
<p>上述三种方案都可以实现SQL监控，那么在实际应用场景中选择哪种方式更好呢？这和实际的生产方式有关。在我手，数据库是基于KDB的，Java应用是基于KsBoot，其中，数据库连接是在KDB平台配置的，底层的数据源是使用ShardingSphere+HikariDataSource进行魔改的。</p>
<p>第一种方案，由于数据库连接是由DBA维护的，升级需求修改数据库连接，因此不建议。</p>
<p>第二种方案，同理需要修改数据库连接，且比第一种更容易配错，因此也不建议。</p>
<p>排除上述两种方式，剩下的只有第三种方案了，但是第三种方案有很大的挑战，原因在于需要兼容快手kuaishou-framework奇葩的JdbcTemplate使用方式。确切地说，在于使用了DataSourceConfig。</p>
<pre><code class="language-java">public interface DataSourceConfig extends HasBizDef {

    /**
     * 数据源名称，必须与KDB申请时填写的一致
     */String bizName();

    /**
     * 获取当前可用区单库只读的JdbcTemplate
     */
    default NamedParameterJdbcTemplate read() {
        return InternalDatasourceConfig.readForceAz(this, currentAz(), currentPaz(), &quot;read&quot;);
    }   

    /**
     * 获取当前可用区单库读写的JdbcTemplate
     */
    default NamedParameterJdbcTemplate write() {
        return InternalDatasourceConfig.writeForceAz(this, currentAz(), currentPaz(), &quot;write&quot;);
    }	
  //....
}
</code></pre>
<p>DefaultDataSourceConfig是一个接口类，默认封装了NamedParameterJdbcTemplate的创建，业务方通过继承该接口来定义数据源:</p>
<pre><code class="language-kotlin">enum class AdDataSources(
    private val bizDef: BizDef,
    private val forTest: AdDataSources? = null,
    private val usingNewZk: Boolean = false
) : DataSourceConfig{
    adFansTopProfileDashboardTest,
    adFansTopProfileDashboard,
    adChargeTest,
    adCharge,
    adChargeReadOnly,
    adDspReadOnlyTest,
    adDspReadOnly;
    public open fun bizName(): String {
        return bizDef.bizName
    }
}
</code></pre>
<p>如果在业务中直接使用了DataSourceConfig创建的NamedParameterJdbcTemplate，那么我们就需要修改过程中创建的DataSource对象。那么，这里的DataSource究竟是怎么创建的呢？</p>
<p>具体扒代码的过程就不赘述了，直接说结果吧，kuaishou-framework的数据源最终是通过DataSourceFactory进行创建的，具体代码如下：</p>
<pre><code class="language-java">public static ListenableDataSource&lt;Failover&lt;Instance&gt;&gt; create(Instance i) {
   //...
   try {
       return supplyWithRetry(
        DATA_SOURCE_BUILD_RETRY,
        DATA_SOURCE_BUILD_RETRY_DELAY,
        () -&gt; new ListenableDataSource&lt;&gt;(
              bizName, 
              new HikariDataSource(config), ds -&gt; i.toString(), i),
              DataSourceFactory::needRetry);
                               
  } catch (Throwable e) {/**/}
}
</code></pre>
<p>由代码可以看到，这里的数据源实际上是通过new HikariDataSource(config)手动创建的，而DataSourceConfig又没有对外暴露创建的数据源，所以，我们该如何对DataSource代理呢?</p>
<h3><strong>二、动态修改加载类</strong></h3>
<p>成本最低的方式就是直接修改这段代码，将其中&#x7684;<em>&#x6E;ew HikariDataSource(config)</em>&#x4FEE;改&#x6210;<em>&#x6E;ew P6DataSource(new HikariDataSource(config))，</em>&#x90A3;么问题来了，这段代码属于基础组件包中的代码，基础架构组没有动力去修改，而我们又没有修改的权限，要想动这块代码，只能使用黑科技了。黑科技的手段有很多，那么问题又来了，哪种手段更合适呢？</p>
<p>首先我们来分析一下，有哪些手段可以修改Java字节码？</p>
<ul>
<li>方案一、编译时修改，需要开发maven插件</li>
</ul>
<p>（不使用maven插件的同学咋办？）</p>
<ul>
<li>方案二、加载时修改，重写类加载器</li>
</ul>
<p>需要在代码中指定特定的类加载器，用有一定的侵入式</p>
<ul>
<li>方案三、运行时修改，使用JavaAgent</li>
</ul>
<p>需要修改应用启动参数，运维成本有点高</p>
<p>首先要说明的是，这里不是对类方法进行增强，所以想使用cglib动态代理的想法是不可行的。前面三种方案都有一定的局限性：方案一比较麻烦，方案二侵入性强，方案三则需要使用JavaAgent技术，那有没有方案不使用Agent就可以动态修改已经加载的字节码呢？答案是没有，至少理论上没有。不过，好在天无绝人之路，JDK9之后，可以动态启动JavaAgent，这样就不用修改启动参数了。这里，我们选择使用byte-buddy进行字节码重写。</p>
<p><em>下面是对动态启动Java Agent技术的解释</em></p>
<blockquote>
<p>Note that starting with Java 9, there is the Launcher-Agent-Class manifest attribute for jar files that can specify the class of a Java Agent to start before the class specified with the Main-Class is launched. That way, you can easily have your Agent collaborating with your application code in your JVM, without the need for any additional command line options. The Agent can be as simple as having an agentmain method in your main class storing the Instrumentation reference in a static variable.</p>
</blockquote>
<blockquote>
<p>See <a href="https://docs.oracle.com/en/java/javase/15/docs/api/java.instrument/java/lang/instrument/package-summary.html#package.description">the java.lang.instrument package documentation</a>…</p>
</blockquote>
<blockquote>
<p>Getting hands on an Instrumentation instance when the JVM has not been started with Agents is trickier. It must support launching Agents after startup in general, e.g. via the Attach API. <a href="https://stackoverflow.com/a/19912148/2711488">This answer</a> demonstrates at its end such a self-attach to get hands on the Instrumentation. When you have the necessary manifest attribute in your application jar file, you could even use that as agent jar and omit the creation of a temporary stub file.</p>
</blockquote>
<blockquote>
<p>However, recent JVMs forbid self-attaching unless -Djdk.attach.allowAttachSelf=true has been specified at startup, but I suppose, taking additional steps at startup time, is precisely what you don’t want to do. One way to circumvent this, is to use another process. All this process has to to, is to attach to your original process and tell the JVM to start the Agent. Then, it may already terminate and everything else works the same way as before the introduction of this restriction.</p>
</blockquote>
<blockquote>
<p>As mentioned in <a href="https://stackoverflow.com/questions/56787777/?noredirect=1&lq=1#comment100160373_56787777">this comment</a>, Byte-Buddy has already implemented those necessary steps and the stripped-down Byte-Buddy-Agent contains that logic only, so you can use it to build your own logic atop it.</p>
</blockquote>
<ul>
<li>字节码工具对比</li>
</ul>
<p><img src="https://static.yximgs.com/udata/pkg/EE-KSTACK/4223630ea14c6367968188fd52cafa26.png" alt="图片"></p>
<ul>
<li>使用bytebuddy修改字节码</li>
</ul>
<p>在实现代码之前，我们回过头来再看一下快手的数据源生成：</p>
<pre><code class="language-java">new ListenableDataSource&lt;&gt;(bizName, new HikariDataSource(config), ds -&gt; i.toString());
</code></pre>
<p>这里实际生成的数据源类型是ListenableDataSource，而ListenableDataSource刚好继承了DelegatingDataSource类，而DelegatingDataSource的构造方法如下：</p>
<pre><code class="language-java">public class DelegatingDataSource implements DataSource {
   //...
  public DelegatingDataSource(DataSource targetDataSource) {
    this.setTargetDataSource(targetDataSource);
   }

  public void setTargetDataSource(@Nullable DataSource targetDataSource) {
      this.targetDataSource = targetDataSource;
  }
  //...
}
</code></pre>
<p>因此，我们可以通过改写DelegatingDataSource#setTargetDataSource方法，实现同样的效果，修改后的方法应该如下：</p>
<pre><code class="language-java">public void setTargetDataSource(@Nullable DataSource targetDataSource) {
        this.targetDataSource = new P6DataSource(targetDataSource;
}
</code></pre>
<p>那么具体如何修改字节码呢？这里是<a href="https://bytebuddy.net/#/tutorial">官方文档</a>，原理我们不做赘述，直接介绍实现了。实现方式有三种：</p>
<h4><strong>1、类文件替换</strong></h4>
<p>假设你已经通过Java代码编译了新的类，现在要替换JVM中类的定义，代码如下：</p>
<pre><code class="language-java">new ByteBuddy()
  .redefine(NewDelegatingDataSource.class)
  .name(DelegatingDataSource.class.getName())
  .make()
  .load(Thread.currentThread().getContextClassLoader(), 
        ClassReloadingStrategy.fromInstalledAgent());
</code></pre>
<h4><strong>2、操作字节码：</strong></h4>
<pre><code class="language-java">new ByteBuddy()
    .redefine(DelegatingDataSource.class)
    //重写DelegatingDataSource#setTargetDataSource方法
    .method(named(&quot;setTargetDataSource&quot;))
    .intercept(MyImplementation.INSTANCE)
    .make()
    .load(Thread.currentThread().getContextClassLoader(),
          ClassReloadingStrategy.fromInstalledAgent());

enum MyImplementation implements Implementation {

INSTANCE; // singleton

  @Override
  public InstrumentedType prepare(InstrumentedType instrumentedType) {
  return instrumentedType;
  }
  
  @Override
  public ByteCodeAppender appender(Target implementationTarget) {
  return MyAppender.INSTANCE;
  }
  
}
//字节码定义
enum MyAppender implements ByteCodeAppender {

INSTANCE; // singleton

@Override
public Size apply(MethodVisitor methodVisitor,
        Implementation.Context implementationContext,
        MethodDescription instrumentedMethod) {
  Label label0 = new Label();
  methodVisitor.visitLabel(label0);
  methodVisitor.visitLineNumber(70, label0);
  methodVisitor.visitVarInsn(ALOAD, 0);
  methodVisitor.visitTypeInsn(NEW, &quot;com/p6spy/engine/spy/P6DataSource&quot;);
  methodVisitor.visitInsn(DUP);
  methodVisitor.visitVarInsn(ALOAD, 1);
  methodVisitor.visitMethodInsn(INVOKESPECIAL, &quot;com/p6spy/engine/spy/P6DataSource&quot;, &quot;&lt;init&gt;&quot;, &quot;(Ljavax/sql/DataSource;)V&quot;, false);
  methodVisitor.visitFieldInsn(PUTFIELD, &quot;org/springframework/jdbc/datasource/DelegatingDataSource&quot;, &quot;targetDataSource&quot;, &quot;Ljavax/sql/DataSource;&quot;);
  Label label1 = new Label();
  methodVisitor.visitLabel(label1);
  methodVisitor.visitLineNumber(71, label1);
  methodVisitor.visitInsn(RETURN);
  Label label2 = new Label();
  methodVisitor.visitLabel(label2);
  methodVisitor.visitLocalVariable(&quot;this&quot;, &quot;Lorg/springframework/jdbc/datasource/DelegatingDataSource;&quot;, null, label0, label2, 0);
  methodVisitor.visitLocalVariable(&quot;targetDataSource&quot;, &quot;Ljavax/sql/DataSource;&quot;, null, label0, label2, 1);
  methodVisitor.visitMaxs(4, 2);
  return new Size(4, 2);
  }
}
</code></pre>
<p>上述代码的核心思想是字节操作字节码，操作字节码是非常复杂和繁重的事情，且无法debug，那么有没有比较方便的方式呢？</p>
<p>我们可以手动改写Java代码，然后利用插件生成对应的字节码，然后在其基础上进行修改，研发成本会低很多。这里推荐IDEA的一个插件：Byte-Code-Analyzer，使用该插件可以查看类对应的ASM字节码:</p>
<p><img src="https://static.yximgs.com/udata/pkg/EE-KSTACK/e31962a90f6598880e78d8254d6c74d9" alt="图片"></p>
<h4><strong>3、利用byte-buddy的Advice</strong></h4>
<pre><code class="language-java"> public static void redefine() {
   new ByteBuddy()
     .redefine(DelegatingDataSource.class)
     .visit(Advice.to(Decorator.class)
            .on(ElementMatchers.named(&quot;setTargetDataSource&quot;)))
     .make()
     .load(Thread.currentThread().getContextClassLoader(),
           ClassReloadingStrategy.fromInstalledAgent()).getLoaded();
 }

static class Decorator {

  //在方法开始插入代码
  @Advice.OnMethodEnter
    public static void enter(@Advice.Argument(value = 0, readOnly = false) DataSource dataSource) {
    dataSource = new P6DataSource(dataSource);
  }
}
</code></pre>
<p>byte-buddy的Advisor和动态代理的原理不一样，他是直接修改方法体的字节码，上面的方法就是表示在方法开始插入一行，其效果如下：</p>
<pre><code class="language-java">public void setTargetDataSource(@Nullable DataSource targetDataSource) {
  //插入的代码
  targetDataSource = new P6DataSource(targetDataSource);
  this.targetDataSource = targetDataSource;
}
</code></pre>
<p>注：</p>
<ol>
<li>动态修改已加载的类，是有限制条件的，不能添加方法或者字段，因此通过byte-buddy的Methoddelegation方法修改字节码是不可行的。</li>
<li>使用byte-buddy的Advice，可以对非Spring托管的类进行动态增强，因为是直接修改字节码，性能更好。</li>
</ol>
<h3><strong>三、自动生效</strong></h3>
<p>前面我们讲了如何修改字节码，以提供SQL监控功能，那么如何让SQL监控自动生效呢？我们的目标是非侵入式解决方案：既不能修改业务代码，也不能更改系统配置。鉴于Java世界的事实标准，我们利用了SpringBoot-Starter功能，只需增加一个maven依赖，就自动提供了SQL监控能力。</p>
<pre><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;com.kuaishou.ad&lt;/groupId&gt;
  &lt;artifactId&gt;sqllog-spring-boot-starter&lt;/artifactId&gt;
  &lt;version&gt;制品库查询最新版&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>至于SpringBoot-Starter的实现原理，网上资料很多，核心思想就是提供默认配置，开箱即用。需要注意的是，Spring6.0自动配置的方案有了调整，原来基于spring.factories的配置改成了org.springframework.boot.autoconfigure.AutoConfiguration.imports，原有的方式还支持，这对应普通应用没有影响，但是在实现Spring多容器隔离的方案上有一定的影响，后面有时间会展开讲一下。</p>
<pre><code class="language-java">private static String[] getConfigurations(File file) {
  @EnableAutoConfiguration
  class NoScan {
    //用于扫描META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports,该类定义在方法中,是为了避免扫描当前类时被加载
  }
  FileClassLoader classLoader = new FileClassLoader(file);
  AutoConfigurationImportSelector selector = new AutoConfigurationImportSelector();
  selector.setBeanClassLoader(classLoader);
  selector.setResourceLoader(new ClassLoaderResourcePatternResolver(classLoader));
  selector.setEnvironment(new StandardEnvironment());
  String[] configurations = selector.selectImports(new StandardAnnotationMetadata(NoScan.class));
  return configurations;
}
</code></pre>
<h3><strong>四、SQL打印效果</strong></h3>
<p>sqllog-spring-boot-starter默认基于p6spy，并对SQL输出提供了扩展，打印SQL日志如下：</p>
<p><img src="https://static.yximgs.com/udata/pkg/EE-KSTACK/28cd44d1451c960cfb982773aab6ec44" alt=""></p>
<p>SQL的打印内容分为三部分：</p>
<p>第一行，显示执行时间、耗时、SQL操作、数据库连接等信息</p>
<p>第二行，显示参数化SQL</p>
<p>第三行，显示绑定参数后的实际执行的SQL</p>
<p>通过日志看到，当SQL语句超长时，系统会对参数化SQL进行个性化缩略，而对实际执行的SQL，则保持原样输出，这样可以检索关键信息。</p>
6:["$","article",null,{"className":"min-h-screen","children":["$","div",null,{"className":"mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"rounded-2xl shadow-2xl border border-gray-200 hover:shadow-3xl transition-all duration-300 p-8 sm:p-12","children":[["$","header",null,{"className":"mb-8","children":[["$","div",null,{"className":"flex items-center mb-6","children":["$","div",null,{"className":"inline-flex items-center px-3 py-1.5 bg-gray-50 text-gray-600 rounded-md text-sm font-normal","children":[["$","svg",null,{"className":"w-4 h-4 mr-2 text-gray-400","fill":"none","stroke":"currentColor","viewBox":"0 0 24 24","children":["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","strokeWidth":2,"d":"M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z"}]}],["$","time",null,{"dateTime":"2025-07-29","children":"2025年07月29日"}]]}]}],["$","h1",null,{"className":"text-4xl font-bold text-gray-900 mb-6 text-center","children":"How to implement dynamic protobuf in Golang?"}],["$","div",null,{"className":"flex flex-wrap gap-2 mb-6 justify-center","children":[["$","$L5","技术实战",{"href":"/blog/tag/%E6%8A%80%E6%9C%AF%E5%AE%9E%E6%88%98/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"技术实战"}],["$","$L5","Protobuf",{"href":"/blog/tag/Protobuf/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"Protobuf"}],["$","$L5","Dynamic",{"href":"/blog/tag/Dynamic/page/1/","className":"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-gray-200 text-gray-800 hover:bg-gray-300 hover:text-gray-900 transition-colors","children":"Dynamic"}]]}]]}],["$","div",null,{"className":"max-w-5xl mx-auto","children":["$","$L14",null,{"content":"$15"}]}],["$","$11",null,{"fallback":["$","div",null,{"className":"mt-12 pt-8 border-t border-gray-200","children":"加载导航中..."}],"children":["$","$L16",null,{"globalNav":{"prev":{"slug":"tech/middleware/分布式事务之两阶段提交和三阶提交","title":"分布式事务之两阶段提交和三阶提交","description":"在分布式一致性一文中主要介绍了分布式系统中存在的一致性问题。本文将简单介绍如何有效的解决分布式的一致性问题,其中包括什么是分布式事务，二阶段提交和三阶段提交。","pubDate":"2025-07-23","tags":["分布式事务","技术专题"],"heroImage":"$undefined","content":"$17"},"next":{"slug":"thoughts/数字货币的演进逻辑","title":"数字货币的演进逻辑：从比特币到稳定币、RWA、CBDC与未来格局","description":"文章系统梳理了数字货币的发展逻辑：比特币以区块链和 PoW 开创去中心化金融实验，却因总量刚性和缺乏现实锚定更像“数字黄金”；稳定币通过锚定法币成为数字世界的现金，解决了计价与结算问题；RWA 将现实资产代币化，把真实经济价值带上链，形成“法币—稳定币—RWA—法币”的投资闭环；CBDC 则代表国家化终局，省去兑换环节并增强宏观调控能力；在此基础上，美国依靠稳定币和 RWA 延续美元霸权，中国通过数字人民币探索换道超车，未来全球格局可能从双轨竞争走向多极化，甚至演化为由世界央行统一发行的数字货币体系。","pubDate":"2025-09-13","tags":["比特币","稳定币","RWA"],"heroImage":"$undefined","content":"$18"}},"tagNav":{"技术实战":{"prev":{"slug":"tech/practice/非侵入式SQL监控","title":"非侵入式SQL监控","description":"你有没有因为应用程序没有打印SQL而导致问题排查困难？有没有因为SQL没有显示参数而导致日志毫无意义？有没有因为SQL超长而导致查看痛苦？有没有因为缺少SQL性能监控而导致无法报警？...","pubDate":"2024-04-07","tags":["技术实战"],"heroImage":"$undefined","content":"$19"},"next":null},"Protobuf":{"prev":null,"next":null},"Dynamic":{"prev":null,"next":null}}}]}],["$","$L1a",null,{}]]}]}]}]
9:null
d:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
8:null
b:{"metadata":[["$","title","0",{"children":"How to implement dynamic protobuf in Golang? - Skyfalling Blog"}],["$","meta","1",{"name":"description","content":"Protocol Buffers (Protobuf) is a language-neutral, platform-neutral, extensible mechanism for serializing structured data. It was developed by Google to efficiently serialize data for use in a variety of applications, including network communication, data storage, and inter-process communication (IPC).  Protobuf messages are smaller and more efficient than text-based formats like JSON and XML, and provides fast serialization and deserialization, which is crucial for high-performance systems."}],["$","meta","2",{"property":"og:title","content":"How to implement dynamic protobuf in Golang?"}],["$","meta","3",{"property":"og:description","content":"Protocol Buffers (Protobuf) is a language-neutral, platform-neutral, extensible mechanism for serializing structured data. It was developed by Google to efficiently serialize data for use in a variety of applications, including network communication, data storage, and inter-process communication (IPC).  Protobuf messages are smaller and more efficient than text-based formats like JSON and XML, and provides fast serialization and deserialization, which is crucial for high-performance systems."}],["$","meta","4",{"property":"og:type","content":"article"}],["$","meta","5",{"property":"article:published_time","content":"2025-07-29"}],["$","meta","6",{"property":"article:author","content":"Skyfalling"}],["$","meta","7",{"name":"twitter:card","content":"summary"}],["$","meta","8",{"name":"twitter:title","content":"How to implement dynamic protobuf in Golang?"}],["$","meta","9",{"name":"twitter:description","content":"Protocol Buffers (Protobuf) is a language-neutral, platform-neutral, extensible mechanism for serializing structured data. It was developed by Google to efficiently serialize data for use in a variety of applications, including network communication, data storage, and inter-process communication (IPC).  Protobuf messages are smaller and more efficient than text-based formats like JSON and XML, and provides fast serialization and deserialization, which is crucial for high-performance systems."}],["$","link","10",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","link","11",{"rel":"icon","href":"/favicon.svg"}]],"error":null,"digest":"$undefined"}
13:{"metadata":"$b:metadata","error":null,"digest":"$undefined"}
